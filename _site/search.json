[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Customize font family of Visual Mode in Rstudio and in Zotero\n\n\n\n\n\n\ntutorial\n\n\ntheming\n\n\n\n\n\n\nFeb 10, 2024\n\n\nJihong Zhang\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up High Performance Computing of University of Arkansas\n\n\n\n\n\n\ntutorial\n\n\nhpc\n\n\n\n\n\n\nJan 14, 2024\n\n\nJihong Zhang\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nESRM 6553: Advanced Multivariate Analysis\n\n\nSpring 2024, Mondays, 5:00-7:45PM, Classroom GRAD 0229\n\n\n\nclass\n\n\n\n\n\n\nJan 13, 2024\n\n\nJihong Zhang\n\n\n45 min\n\n\n\n\n\n\n\n\n\n\n\n\nData visualization for survey data\n\n\n\n\n\n\nblog\n\n\nggplot2\n\n\n\n\n\n\nJul 4, 2023\n\n\nJihong Zhang\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial for forester R package\n\n\n\n\n\n\ntutorial\n\n\npackage\n\n\n\n\n\n\nJun 28, 2023\n\n\nJihong Zhang\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nOne Template for Quarto Report\n\n\n\n\n\n\ntutorial\n\n\nquarto\n\n\n\n\n\n\nJun 27, 2023\n\n\nJihong Zhang\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nMoving My Website to Quarto\n\n\n\n\n\n\nquarto\n\n\n\n\n\n\nJun 25, 2023\n\n\nJihong Zhang\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Conversation between R and Python on Data Analysis and Machine Learning\n\n\n\n\n\n\nblog\n\n\nR\n\n\nPyTorch\n\n\nDeep Learning\n\n\n\n\n\n\nJun 20, 2023\n\n\nJihong Zhang\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Class Model: Batch Mplus using R on Mac\n\n\n\n\n\n\nblog\n\n\nLCM\n\n\nR\n\n\n\n\n\n\nJun 10, 2023\n\n\nJihong Zhang\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nDissertation Defence\n\n\nTITLE: A novel method for model selection in Bayesian Diagnostic Classification Modeling\n\n\n\n\n\nOct 23, 2022\n\n\nJihong Zhang\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nNSF Grant on Bayesian Cognitive Diagnosis\n\n\nData visualization using ggplot2\n\n\n\n\n\n\n\n\nMay 20, 2022\n\n\nJihong Zhang\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping Academia Institute’s Grant Fundings using R\n\n\n\n\n\n\nblog\n\n\nWeb Scrapping\n\n\n\n\n\n\nMay 12, 2022\n\n\nJihong Zhang\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nPower Analysis for Structural Equation Modeling\n\n\n\n\n\n\nR\n\n\nrshiny\n\n\nwebpower\n\n\nlavaan\n\n\nblog\n\n\ntutorial\n\n\n\n\n\n\nApr 29, 2022\n\n\nJihong Zhang\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n[Workshop] Creating Academic Blog via R\n\n\nThis post reviews the procedure of creating Github Pages Website using distill package and Github Pages in a step-to-step way.\n\n\n\nR\n\n\nGithub\n\n\nRstudio\n\n\nWorkshop\n\n\n\n\n\n\nApr 24, 2022\n\n\nJihong Zhang\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Care Data Analysis with Apple Watch\n\n\n\n\n\n\nblog\n\n\nR\n\n\nhealth\n\n\n\n\n\n\nJul 6, 2021\n\n\nJihong Zhang\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis\n\n\n\n\n\n\nR\n\n\n\n\n\n\nJan 31, 2021\n\n\nJihong\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nStudy Notes: gt package and format table\n\n\n\n\n\n\nR\n\n\n\n\n\n\nMay 25, 2020\n\n\nJihong\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Latent Attribute Network Analysis\n\n\n\n\n\n\nblog\n\n\nNetwork\n\n\nDCM\n\n\n\n\n\n\nOct 20, 2019\n\n\nJihong Zhang\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Country own the most Liberty Nobel Prizes? France? Ireland?\n\n\n\n\n\n\nvisualization\n\n\nblog\n\n\nggplot2\n\n\n\n\n\n\nMay 20, 2019\n\n\nJihong Zhang\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nMake a Game in R\n\n\n\n\n\n\nblog\n\n\n\n\n\n\nApr 19, 2019\n\n\njihong\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduce gganimate for Psychometric\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\n\n\n\n\nFeb 22, 2019\n\n\nJihong\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLasso Regression Example using glmnet package in R\n\n\n\n\n\n\nR\n\n\nmanual\n\n\n\n\n\n\nFeb 19, 2019\n\n\nJihong Zhang\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nVisuliaztion of Item Information Curves In R\n\n\n\n\n\n\nblog\n\n\n\n\n\n\nJan 10, 2019\n\n\nJihong\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n[Manual]Using Jags and R2jags in R\n\n\n\n\n\n\nManual\n\n\n\n\n\n\nSep 12, 2018\n\n\nJihong\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduce Descrepancy Measures\n\n\n\n\n\n\nR\n\n\n\n\n\n\nSep 11, 2018\n\n\nJihong\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do Data Cleaning in R\n\n\n\n\n\n\nblog\n\n\nR\n\n\ntidyverse\n\n\n\n\n\n\nSep 4, 2018\n\n\nJihong Zhang\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nMy New Shiny App Cut Score Consistency\n\n\n\n\n\n\nR\n\n\nShiny\n\n\n\n\n\n\nMar 10, 2018\n\n\nJihong Zhang\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Profile Analysis using MCLUST (in R)\n\n\n\n\n\n\nR\n\n\nclustering\n\n\nlatent class\n\n\n\n\n\n\nNov 23, 2017\n\n\nJihong Zhang\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nOne Example of Measurement Invariance\n\n\n\n\n\n\nR\n\n\nSEM\n\n\nlavaan\n\n\n\n\n\n\nNov 12, 2017\n\n\nJihong Zhang\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation Study of Linking Using Mirt\n\n\n\n\n\n\nR\n\n\nmirt\n\n\nlinking\n\n\n\n\n\n\nNov 10, 2017\n\n\nJihong Zhang\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use lavaan package to perform Confirmatory Factor Analysis\n\n\n\n\n\n\nManual\n\n\nR\n\n\nLavaan\n\n\n\n\n\n\nOct 19, 2017\n\n\nJihong\n\n\n14 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Initialize R Project\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nJihong Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nShinyApp: Danmu Cloud for Bilibili\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nJihong Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nShinyapp: Scoring and Reliability Analysis of Psychometric Network\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects/2024-Mixing-Network-DCM/index.html",
    "href": "projects/2024-Mixing-Network-DCM/index.html",
    "title": "Shinyapp: Scoring and Reliability Analysis of Psychometric Network",
    "section": "",
    "text": "Loading required package: pacman\n\n\n\n\n\n\n⌘+Cexp1 &lt;- matrix(c(\n  0, .8, .5,\n  .8, 0, 0.1,\n  .5, .1, 0\n), nrow = 3, ncol = 3, byrow = T)\nrownames(exp1) = colnames(exp1) = c(\"A\", \"B\", \"C\")\nqgraph::qgraph(input = exp1, edge.labels=T, edge.label.cex = 1.7, edge.label.color = \"black\")\n\n\n\n\n\n\n\nThe centrality measures for three nodes:\n\n⌘+Cmykbl(centrality(exp1))\n\n\n\nnode\nBetweenness\nCloseness\nStrength\nExpectedInfluence\n\n\n\nA\n1.155\n1.114\n1.044\n1.044\n\n\nB\n-0.577\n-0.295\n-0.095\n-0.095\n\n\nC\n-0.577\n-0.819\n-0.949\n-0.949\n\n\n\n\n\nAssume that there are two individuals with different scores on three items:\n\n⌘+Cresponse &lt;- data.frame(\n  A = c(1, 5),\n  B = c(1, 1),\n  C = c(5, 1)\n)\nrownames(response) &lt;- c(\"Person1\", \"Person2\")\nmykbl(response)\n\n\n\n\nA\nB\nC\n\n\n\nPerson1\n1\n1\n5\n\n\nPerson2\n5\n1\n1\n\n\n\n\n\nWe can calculate the weighted network scores for them based on Strength centrality measures and their item scores\n\n⌘+Cas.matrix(response) %*% centrality(exp1)[['Strength']] |&gt; mykbl()\n\n\n\nPerson1\n-3.797\n\n\nPerson2\n4.176\n\n\n\n\nThe strength-based network score reflect one’s overall level of clusters of nodes in a network."
  },
  {
    "objectID": "projects/2024-Mixing-Network-DCM/index.html#minimal-example",
    "href": "projects/2024-Mixing-Network-DCM/index.html#minimal-example",
    "title": "Shinyapp: Scoring and Reliability Analysis of Psychometric Network",
    "section": "",
    "text": "Loading required package: pacman\n\n\n\n\n\n\n⌘+Cexp1 &lt;- matrix(c(\n  0, .8, .5,\n  .8, 0, 0.1,\n  .5, .1, 0\n), nrow = 3, ncol = 3, byrow = T)\nrownames(exp1) = colnames(exp1) = c(\"A\", \"B\", \"C\")\nqgraph::qgraph(input = exp1, edge.labels=T, edge.label.cex = 1.7, edge.label.color = \"black\")\n\n\n\n\n\n\n\nThe centrality measures for three nodes:\n\n⌘+Cmykbl(centrality(exp1))\n\n\n\nnode\nBetweenness\nCloseness\nStrength\nExpectedInfluence\n\n\n\nA\n1.155\n1.114\n1.044\n1.044\n\n\nB\n-0.577\n-0.295\n-0.095\n-0.095\n\n\nC\n-0.577\n-0.819\n-0.949\n-0.949\n\n\n\n\n\nAssume that there are two individuals with different scores on three items:\n\n⌘+Cresponse &lt;- data.frame(\n  A = c(1, 5),\n  B = c(1, 1),\n  C = c(5, 1)\n)\nrownames(response) &lt;- c(\"Person1\", \"Person2\")\nmykbl(response)\n\n\n\n\nA\nB\nC\n\n\n\nPerson1\n1\n1\n5\n\n\nPerson2\n5\n1\n1\n\n\n\n\n\nWe can calculate the weighted network scores for them based on Strength centrality measures and their item scores\n\n⌘+Cas.matrix(response) %*% centrality(exp1)[['Strength']] |&gt; mykbl()\n\n\n\nPerson1\n-3.797\n\n\nPerson2\n4.176\n\n\n\n\nThe strength-based network score reflect one’s overall level of clusters of nodes in a network."
  },
  {
    "objectID": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html",
    "href": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html",
    "title": "Health Care Data Analysis with Apple Watch",
    "section": "",
    "text": "The data contained within Health app on iPhone can be saved and exported for other uses. Perhaps you want to export Health app data to use in another health or fitness app, importing it to elsewhere, or maybe you want to use the raw Health data for your own purposes.\nExporting Health data from iPhone results in a zip archive that contains the raw data as gathered by Health app in XML format. This exported Health data will include any data stored or gathered by the Health app and any associated devices, including any Medical ID data, the native iPhone step counter and distance tracker, any data from an Apple Watch, and any data gathered from any third party devices that are syncing to Health app, like a smart scale or blood pressure monitor.\nYou can find more details about exporting data from How to Export Health Data form iPhone and How to Export, Parse and Explore Your Apple Health Data with Python."
  },
  {
    "objectID": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html#load-required-packages",
    "href": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html#load-required-packages",
    "title": "Health Care Data Analysis with Apple Watch",
    "section": "\n1 Load Required Packages",
    "text": "1 Load Required Packages\n\n\n\n\nrm(list = ls(all = TRUE))\n\nif(!require(XML)) install.packages(\"XML\")\nif(!require(tidyverse)) install.packages(\"tidyverse\")\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\nif(!require(lubridate)) install.packages(\"lubridate\")\nif(!require(scales)) install.packages(\"scales\")\n\nWarning: package 'scales' was built under R version 4.2.3\n\nif(!require(ggthemes)) remotes::install_github(c(\"hadley/ggplot2\", \"jrnold/ggthemes\"))\nif(!require(ggridges)) remotes::install_github(\"wilkelab/ggridges\")\nif(!require(rpart)) install.packages(\"rpart\")\nif(!require(kableExtra)) install.packages(\"kableExtra\")"
  },
  {
    "objectID": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html#read-in-xml-data",
    "href": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html#read-in-xml-data",
    "title": "Health Care Data Analysis with Apple Watch",
    "section": "\n2 Read in XML data",
    "text": "2 Read in XML data\nThe .xml file could be downloaded and exported directly from iphone’s health app. To read in the XML file and transform it into data frame, XML R package will help.\n\nloc&lt;-\"导出.xml\" # enter file path location of export.xml file here #\n\nxml &lt;- xmlParse(loc)\n\nrc &lt;-  data.frame(XML:::xmlAttrsToDataFrame(xml[\"//Record\"]), stringsAsFactors = F)\nsaveRDS(rc, \"export_health_care.rds\")\n\n\nrc &lt;- readRDS(\"export_health_care.rds\")\n\n## Filter useful data\napple_watch &lt;- rc %&gt;% filter(grepl(\"JZ\",sourceName), unit == 'count/min', \n                             type == \"HKQuantityTypeIdentifierRestingHeartRate\")\n\n# Adjusting to Local Timezone\napple_watch_dt &lt;- apple_watch %&gt;%\n  mutate(cdt=as_datetime(creationDate, tz=\"US/Central\"),\n         stm=as_datetime(startDate, tz=\"US/Central\"),\n         etm=as_datetime(endDate, tz=\"US/Central\"),\n         dst=as.numeric(as.character(value))) \n\n# %&gt;%\n#   group_by(creationDate) %&gt;%\n#   mutate(TotalTime=cumsum(value)) %&gt;% # cumulative distance covered #\n#   mutate(hr=hour(stm), min=minute(stm)) %&gt;%\n#   mutate(elt=as.numeric(etm-mntm)) %&gt;% # total elapsed time #\n#   mutate(dtm=as.numeric(etm-lag(etm))) %&gt;%\n#   ungroup()\n\nhead(apple_watch_dt[,-1]) %&gt;% \n  select(sourceName, unit, cdt, stm, etm, dst) %&gt;% \n  kbl() %&gt;% \n  kable_material_dark(full_width = F, html_font = \"Maven Pro\") |&gt; \n  kable_styling(font_size = 10)\n\n\n\nsourceName\nunit\ncdt\nstm\netm\ndst\n\n\n\nJZ’sApple Watch\ncount/min\n2020-07-27 22:42:32\n2020-07-27 08:45:05\n2020-07-27 22:37:26\n60\n\n\nJZ’sApple Watch\ncount/min\n2020-07-28 19:43:28\n2020-07-28 00:03:04\n2020-07-28 19:40:28\n59\n\n\nJZ’sApple Watch\ncount/min\n2020-07-29 14:22:15\n2020-07-29 08:40:16\n2020-07-29 14:18:28\n54\n\n\nJZ’sApple Watch\ncount/min\n2020-07-30 17:44:05\n2020-07-30 09:06:24\n2020-07-30 17:41:27\n54\n\n\nJZ’sApple Watch\ncount/min\n2020-07-31 17:36:00\n2020-07-31 08:55:46\n2020-07-31 17:27:30\n52\n\n\nJZ’sApple Watch\ncount/min\n2020-08-01 12:17:02\n2020-08-01 00:03:31\n2020-08-01 12:14:41\n60"
  },
  {
    "objectID": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html#other-materials",
    "href": "posts/2021-07-06-health-care-data-analysis-with-apple-watch/index.html#other-materials",
    "title": "Health Care Data Analysis with Apple Watch",
    "section": "\n3 Other materials",
    "text": "3 Other materials\nThere a decent blog illustrating how to handle with apple watch export file."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#todays-lecture-objectives",
    "title": "Lecture 01",
    "section": "Today’s Lecture Objectives",
    "text": "Today’s Lecture Objectives\n\nIntroduce myself\nSyllabus\nExtra Course Information\nIntroduce Bayesian Analysis\n\nbut, before we begin…",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#introduce-myself",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#introduce-myself",
    "title": "Lecture 01",
    "section": "Introduce myself",
    "text": "Introduce myself\nLet me introduce myself first…\n\n\n\n\n\n\nUniversity if Iowa\n\n\n\n\n\n\n\nHong Kong: Victoria Harbour\n\n\n\n\n\n\n\nChinese University of Hong Kong",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#syllabus",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#syllabus",
    "title": "Lecture 01",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus of ESRM 6553",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#course-time",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#course-time",
    "title": "Lecture 01",
    "section": "Course Time",
    "text": "Course Time\n\nMonday 5PM to 7:45PM:\n\n5PM to 6:15PM: First half class\n6:15PM to 6:30PM: 15-min break\n6:30PM to 7:45PM: Second half class",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#office-hours",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#office-hours",
    "title": "Lecture 01",
    "section": "Office Hours",
    "text": "Office Hours\n\nTuesday 1:30PM to 4:30PM\nYou should be able to find me in GRAD Room 109",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#materials",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#materials",
    "title": "Lecture 01",
    "section": "Materials",
    "text": "Materials\n\nI will provide R codes and slides at the weekends before next class. You may download them on Blackboard or My website (jihongzhang.org)",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#quiz-what-is-bayesian",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#quiz-what-is-bayesian",
    "title": "Lecture 01",
    "section": "Quiz: What is Bayesian?",
    "text": "Quiz: What is Bayesian?\n\nHow to Pronounce Bayesian (Real Life Examples!)\n\n“B-Asian” or “Bayes-ian”?\nWhat “Bayesian” mean? Assign probabilities to everything!\n\nFrequentist vs Bayesian\n\nExample: In U.S., more Male Asian Faculty or Female Asian Faculty?\n\nFrequentist: If “Male:Female = 1:1 out of Asian Faculty” is fixed, how is the probability that the data happens?\nBayesian: If I believe Male:Female = 1:2 but the data says 1:1, to what degree I need to update my mind?",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-model-components",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-model-components",
    "title": "Lecture 01",
    "section": "Bayesian Model Components",
    "text": "Bayesian Model Components\n\nWhat we see: Observed Data\nWhat we cannot see: Future Data, Data yet to be collected, Parameters",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-why-it-is-used",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-why-it-is-used",
    "title": "Lecture 01",
    "section": "Bayesian Analysis: Why It Is Used?",
    "text": "Bayesian Analysis: Why It Is Used?\nThere are at least four main reasons why people use Bayesian Analysis:\n\nMissing data\n\nMultiple imputation\nMore complicated model for certain types of missing data\n\nLack of software capable of handing large sized analyses\n\nHave a zero-inflated Poisson model with 1000 observations and 1000 parameters? No problem in Bayesian!",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-why-it-is-used-cont.",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-why-it-is-used-cont.",
    "title": "Lecture 01",
    "section": "Bayesian Analysis: Why It Is Used? (Cont.)",
    "text": "Bayesian Analysis: Why It Is Used? (Cont.)\n\nNew complex models not available in frequentist framework\n\nHave a new model? (A model that estimates the probability students choose the right answers then choose the wrong answers in a multiple choice test?)\n\nEnjoy the Bayesian thinking process\n\nIt is a way of thinking that everything is random and everything can be expressed as probability. It is a way of thinking that we can update our belief as we collect more data. It is a way of thinking that we can use our prior knowledge to help us understand the data.",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-why-it-is-used-cont.-1",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-why-it-is-used-cont.-1",
    "title": "Lecture 01",
    "section": "Bayesian Analysis: Why It Is Used? (Cont.)",
    "text": "Bayesian Analysis: Why It Is Used? (Cont.)",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-issues",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-issues",
    "title": "Lecture 01",
    "section": "Bayesian Analysis: Issues",
    "text": "Bayesian Analysis: Issues\n\nSubjective vs. Objective\n\nPrior distribution is subjective. It is based on your prior knowledge.\nHowever, 1) Scientific judgement is always subjective 2) you can use objective prior distribution to avoid this issue.\n\nComputationally Intensive\n\nIt is not a problem anymore. We have computers.\nBut we still need weeks or months to get results for some complated model and big data\n\nDifficult to understand",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-is-popular",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#bayesian-analysis-is-popular",
    "title": "Lecture 01",
    "section": "Bayesian Analysis is popular",
    "text": "Bayesian Analysis is popular\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.qmd",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#what-topics-bayesian-analysis-can-cover",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#what-topics-bayesian-analysis-can-cover",
    "title": "Lecture 01",
    "section": "What topics Bayesian Analysis can cover?",
    "text": "What topics Bayesian Analysis can cover?",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#what-topics-bayesian-analysis-can-cover-1",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#what-topics-bayesian-analysis-can-cover-1",
    "title": "Lecture 01",
    "section": "What topics Bayesian Analysis can cover?",
    "text": "What topics Bayesian Analysis can cover?\n\nFunding available only for NIH, CDC, FDA, AHRQ, and ACF 2020 Spring. Source: https://report.nih.gov/",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#wrapping-up",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#wrapping-up",
    "title": "Lecture 01",
    "section": "Wrapping Up",
    "text": "Wrapping Up\n\nWe know what is “Bayesian” and its components.\nWhy Bayesian Estimation is different from other estimation, say maximum likelihood\nWe also know that Bayesian analysis is popular in many fields, especially complex data &gt;&gt;&gt;&gt;&gt;&gt;&gt; 6e75efe (update 2024 CV):posts/2024-01-12-syllabus-adv-multivariate-esrm-6554/Lecture01/Lecture01.qmd",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#next-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#next-class",
    "title": "Lecture 01",
    "section": "Next Class",
    "text": "Next Class\nWe will talk about how Bayesian methods works in a little bit more technical way.",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#suggestions",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture01/Lecture01.html#suggestions",
    "title": "Lecture 01",
    "section": "Suggestions",
    "text": "Suggestions\nYour opinions are very important to me. Feel free to let me know if you have any suggestions on the course.\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553",
    "crumbs": [
      "Home<span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 01"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#todays-lecture-objectives",
    "title": "Lecture 05",
    "section": "Today’s Lecture Objectives",
    "text": "Today’s Lecture Objectives\n\nBayesian methods for determining how well a model fits the data (absolute fit)\nBayesian methods for determining which model fits better (relative model fit)\n\nIn previous class…\n\n\nWe estimated the empty model and the full model (you can try other constrained model between the empty and full model)\n\nEmpty model: a model without any covariate (predictors)\nFull model: a model with all possible covariates and up-to highest interaction effects\n\n\nWe also make the Stan code more efficient by vectorizing the parameters and the data\n\nThe next question is how to determine the model is “good” enough\n\n“good” has multiple meanings\na typical criteria is to what degree the model can generate a “simulated” data that is similar to the original data\nor, the likelihood of original data given the parameters of model",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#types-of-model-fit",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#types-of-model-fit",
    "title": "Lecture 05",
    "section": "Types of Model fit",
    "text": "Types of Model fit\n\n\nAbsolute model fit\n\nPPMC\nIn SEM, RMSEA, chi-square, SRMR, and GFI\n\n\n\nRelative model fit\n\ninformation criterion\n\n\n\nIncremental model fit (not frequently used other than SEM)\n\nA special type of absolute model fit - how a model fits to a saturated model\nIn SEM, comparative fit index (CFI), Tucker-Lewis index (TLI)",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#absolute-model-fit-ppmc",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#absolute-model-fit-ppmc",
    "title": "Lecture 05",
    "section": "Absolute Model Fit: PPMC",
    "text": "Absolute Model Fit: PPMC\nPosterior predictive model checking (Gelman, Meng, and Stern 1996) is a Bayesian method evaluation technique for determining if a model fits the data.\n\n\nAbsolute model fit: “Does my model fit my data well?”\n\nRecall that “model is a simplified version of the true data-generation process”\nThus, the model should be able to reproduce the “data” that is similar to observed data\nIn machine learning, this is also called “validation”, typically used as a separate “validation data sets”\n\n\nOverall idea: if a model fits the data well, then simulated data based on the model will resemble the observed data",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#ppmc-example-linear-models",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#ppmc-example-linear-models",
    "title": "Lecture 05",
    "section": "PPMC Example: Linear Models",
    "text": "PPMC Example: Linear Models\nThe linear model from our example was:\n\\[\n\\text{WeightLB}_p = \\beta_0 + \\beta_1 \\text{HeightIN}_p + \\beta_2 \\text{Group2}_p + \\beta_3\\text{Group3}_p \\\\\n+\\beta_4 \\text{HeightIN}_p\\text{Group2}_p \\\\\n+\\beta_5 \\text{HeightIN}_p\\text{Group3}_p \\\\\n+ e_p\n\\]\nwith:\n\n\n\n\\(\\text{Group2}_p\\) the binary indicator of person \\(p\\) being in group 2\n\n\\(\\text{Group}3_p\\) the binary indicator of person \\(p\\) being in group 3\n\\(e_p \\sim N(0, \\sigma_e)\\)\n\n\n\nfit_full_new$summary(variables = c('beta', 'sigma'))\n\n# A tibble: 7 × 10\n  variable    mean  median    sd   mad     q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 beta[1]  148.    148.    3.27  3.23  142.   153.     1.00    1402.    1838.\n2 beta[2]   -0.374  -0.372 0.483 0.479  -1.16   0.414  1.00    1522.    2019.\n3 beta[3]  -24.1   -24.2   4.59  4.45  -31.6  -16.5    1.00    1746.    2190.\n4 beta[4]   81.3    81.3   4.44  4.40   74.1   88.5    1.00    1538.    2091.\n5 beta[5]    2.47    2.48  0.683 0.676   1.32   3.56   1.00    1805.    2118.\n6 beta[6]    3.57    3.57  0.646 0.643   2.52   4.62   1.00    1678.    2218.\n7 sigma      8.25    8.10  1.26  1.20    6.50  10.5    1.00    2265.    2212.",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#ppmc-characteristics",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#ppmc-characteristics",
    "title": "Lecture 05",
    "section": "PPMC Characteristics",
    "text": "PPMC Characteristics\nPPMC methods are very useful\n\n\nThey provide a visual way to determine if the model fits the observed data\nThey are the main method of assessing absolute fit in Bayesian models\nAbsolute fit assesses if a model fits the data instead of comparing to another model\n\n\nBut, there are some drawbacks to PPMC methods\n\n\nAlmost any statistic can be used\n\nSome are better than others (mean and SD of outcomes are nice choices for linear regression)\n\n\nNo standard determining how much misfit is too much\nMay be overwhelming to compute depending on your model\n\n\nQuestion: Can PPMC be used for models with maximum likelihood estimation or ordinary least squares?",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#posterior-predictive-p-values",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#posterior-predictive-p-values",
    "title": "Lecture 05",
    "section": "Posterior Predictive P-values",
    "text": "Posterior Predictive P-values\n\n\nWe can summarize the PPMC using a type of “p-value”\n\nPersonally, I don’t like the name “p-value”, sounds like we are trying to justify our results using significance testing\n\nDifferent from the frequentist “p-value” (if the null hypothesis is true, the probability of the observed data existing)\n\nThe PPP-value: the proportion of times the statistic from the simulated data exceeds that of the observed data\nUseful to determine how far off a statistic is from its posterior predictive distribution\n\n\nIf these p-values were:\n\nnear 0 or 1, indicating your model is far off your data\nnear .5, indicating your model fits your data in terms of the statistics you examined\n\nThe PPP-value for mean:\n\nmean(simMean &gt; mean(dat$WeightLB))\n\n[1] 0.4985\n\n\nThe PPP-value for SD:\n\nmean(simSD &gt; sd(dat$WeightLB))\n\n[1] 0.52075",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#compute-ppp-values-within-stan",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#compute-ppp-values-within-stan",
    "title": "Lecture 05",
    "section": "Compute PPP-values within Stan",
    "text": "Compute PPP-values within Stan\nWe can use the generated quantities block of Stan to compute PPP-values for us:\n\ngenerated quantities{\n  // simulated data\n  array[N] real weightLB_rep = normal_rng(X*beta, sigma);\n  // posterior predictive distribution for mean and SD\n  real mean_weightLB = mean(weightLB);\n  real sd_weightLB = sd(weightLB);\n  real mean_weightLB_rep = mean(to_vector(weightLB_rep));\n  real&lt;lower=0&gt; sd_weightLB_rep = sd(to_vector(weightLB_rep));\n  // ppp-values for mean and sd\n  int&lt;lower=0, upper=1&gt; ppp_mean = (mean_weightLB_rep &gt; mean_weightLB);\n  int&lt;lower=0, upper=1&gt; ppp_sd = (sd_weightLB_rep &gt; sd_weightLB);\n}\n\nIt will give us:\n\nfit_full_ppp$summary(variables = c('mean_weightLB_rep', 'sd_weightLB_rep', 'ppp_mean', 'ppp_sd'))\n\n# A tibble: 4 × 10\n  variable           mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mean_weightLB_… 171.     171.  2.16   2.13 168.  175.  1.00     4680.    3511.\n2 sd_weightLB_rep  49.7     49.7 2.18   2.10  46.2  53.4 1.00     4000.    3808.\n3 ppp_mean          0.503    1   0.500  0      0     1   1.00     4294.      NA \n4 ppp_sd            0.537    1   0.499  0      0     1   0.999    3893.      NA",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#advantages-or-disadvantages-of-computing-ppp-within-stan",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#advantages-or-disadvantages-of-computing-ppp-within-stan",
    "title": "Lecture 05",
    "section": "Advantages or disadvantages of Computing PPP within Stan",
    "text": "Advantages or disadvantages of Computing PPP within Stan\n\n\nPros:\n\nBuilt-in functions of Stan to generate simulated data for example normal_rng(), making PPP-values estimated much faster\nNice visual inspection tools existed - bayesplot\n\n\nCons:\n\nNot allowed to debug each step in PPMC if something wrong\nCannot adjust the statistics and need to re-run the whole MCMC sampling, which is time-consuming\n\n\n\nbayesplot::mcmc_dens_chains(fit_full_ppp$draws('mean_weightLB_rep'))\n\n\n\nPosterior predictive distribution for mean of weight by chains",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#relative-model-fit",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#relative-model-fit",
    "title": "Lecture 05",
    "section": "Relative Model Fit",
    "text": "Relative Model Fit\nRelative model fit: used to compare 2 or more competing models in terms of their mode fit. Sometime, it is also called model selection.\n\n\nIn non-Bayesian models, Information Criteria are often used to make comparisons\n\nAIC, BIC, DIC etc.\nTypically IC is a function of log-likelihood and penalty\nThe model with the lowest IC is the model that fits best\n\n\n\nBayesian model fit is similar\n\nUses an index value\nThe model with the lowest index is the model that fits best\n\n\n\nRecent advances in Bayesian model fit use indices that are tied to make cross-validation predictions (inspired by machine learning):\n\nFit model leaving one observation out (LOO)\nCalculate statistics related to prediction (for instance, log-likelihood of that observation conditional on model parameters)\nDo for all observations\n\n\nNew Bayesian indices try to mirror these leave-one-out predictions (but approximate these due to time constraints)",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#deviance-information-indices",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#deviance-information-indices",
    "title": "Lecture 05",
    "section": "Deviance Information Indices",
    "text": "Deviance Information Indices\nWhen late 1990s and early 2000s, the Deviance Information Criterion was popular for relative Bayesian model fit comparisons. It is proved not as good as LOO or WAIC. But let’s have a look at:\n\\[\n\\text{DIC} = p_D + \\overline{D(\\theta)}\n\\]\nwhere \\(p_D\\) is the estimated number of parameters as follows:\n\\[p_D = \\overline{D(\\theta)} - D(\\bar\\theta)\\]and where\n\\[\nD(\\theta) = -2 \\log(p(y \\mid \\theta)) + C\n\\]\nC is a constant that cancels out when model comparisons are made\nHere.\n\n\\(\\overline{D(\\theta)}\\) is the average log likelihood of the data (y) given the parameters (\\(\\theta\\)) computed across all samples\n\\(D(\\bar\\theta)\\) is the log likelihood of the data (y) computed at the average of the parameters (\\(\\theta\\)) computed across all samples",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#watanabe-akaike-information-criterion-waic",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#watanabe-akaike-information-criterion-waic",
    "title": "Lecture 05",
    "section": "Watanabe-Akaike information criterion (WAIC)",
    "text": "Watanabe-Akaike information criterion (WAIC)\n\n\nA more frequently used model comparison indices for Bayesian analysis\nUsing the loo R package, we can calculate WAIC with the waic() function:\n\n\n\n\nWAIC for the full model\n\nloo::waic(fit_full_ppp$draws('log_lik'))\n\n\nComputed from 4000 by 30 log-likelihood matrix\n\n          Estimate   SE\nelpd_waic   -109.4  6.9\np_waic         6.7  2.6\nwaic         218.8 13.8\n\n5 (16.7%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\n\nWAIC for the empty model\n\nloo::waic(fit_empty_ppp$draws('log_lik'))\n\n\nComputed from 4000 by 30 log-likelihood matrix\n\n          Estimate  SE\nelpd_waic   -161.0 2.7\np_waic         1.4 0.3\nwaic         322.0 5.3\n\n\n\n\n\n\nHere:\n\nelpd_waic is the expected log pointwise predictive density for WAIC\np_waic is the WAIC-version estimated number of parameter, similar to \\(p(D)\\) in DIC, which is a penalty to the likelihood for more parameters\nwaic is the WAIC index used for model comparisons (lowest value is best fitting; -2*elpd_waic)\n\n\nNote that WAIC needs a log_lik variable in the model analysis to be calculated correctly. cmdstan will automate calculate this variable.",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#loo-approximation-to-leave-one-out",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#loo-approximation-to-leave-one-out",
    "title": "Lecture 05",
    "section": "LOO: Approximation to Leave-one-out",
    "text": "LOO: Approximation to Leave-one-out\nBig picture:\n\n\nBesides WAIC, other comparative fit indices include LOO via Pareto Smoothed Important Sampling (PSIS) via Stan’s LOO package\nWAIC/LOO can be used for model comparison with lowest value suggesting better model fit\n\nDifferent from DIC, LOO via PSIS attempts to “approximate” the process of leave-one-out cross-validation (LOO-CV) using a sampling based-approach\n\n\nGives a finite-sample approximation\nImplemented in Stan\nCan quickly compare models\nGives warnings when it may be less reliable to use\n\n\n\nThe details of computation of LOO are very technical, but are nicely compiled in Vehtari, Gelman, and Gabry (2017).",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#compile-the-relative-model-fit-indices",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#compile-the-relative-model-fit-indices",
    "title": "Lecture 05",
    "section": "Compile the relative model fit indices",
    "text": "Compile the relative model fit indices\nBelow shows the all three model comparison fit indices for the empty model and the full model.\n\ndata.frame(\n  Model = c(\"Full Model\", \"Empty Model\"),\n  DIC = c(DIC_full, DIC_empty),\n  WAIC = c(WAIC_full, WAIC_empty),\n  LOOIC = c(LOO_full, LOO_empty)\n)\n\n        Model      DIC     WAIC    LOOIC\n1  Full Model 211.6029 218.7536 219.4379\n2 Empty Model 322.1444 321.9548 321.9715\n\n\n\n\nAll fit indices suggest the full model is better than the empty model\nFor simple models, the results of DIC/WAIC/LOOIC should be consistent\nIn some situations that they are inconsistent, please use WAIC/LOO as standards.",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#general-points-about-bayesian-model-comparison",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#general-points-about-bayesian-model-comparison",
    "title": "Lecture 05",
    "section": "General points about Bayesian Model Comparison",
    "text": "General points about Bayesian Model Comparison\n\n\nNote, WAIC and LOO will converge as sample size increases (WAIC is asymptotic value of LOO)\nLatent variable models present challenges (could be your dissertation project)\n\nNeed log likelihood with latent variable being integrated out\n\n\nMissing data present challenges\n\nNeed log likelihood with missing data integrated out\n\n\nGenerally, using LOO is recommended (but providing both is appropriate)",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#cutting-edge-research-field-approximation-algorithm",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#cutting-edge-research-field-approximation-algorithm",
    "title": "Lecture 05",
    "section": "Cutting-Edge Research Field: Approximation Algorithm",
    "text": "Cutting-Edge Research Field: Approximation Algorithm\n\n\nMCMC sampling could be very computational time-consuming.\nFor number of parameters up to thousands or even millions, it could be a challenge to get the estimation.\nThus, approximation algorithms has been developed, such as Laplace Approximation, Variational Inference (Dhaka et al. 2020; Yao et al. 2018), Pathfinder methods (Zhang et al. 2021).\n\n\nQuoted from Gelman’s blog:\n\n\n\nAdvantages of Laplace:\n\n\nRelatively cheap to compute, given that we already have a mode-finder in Stan.\nEasy to understand, use, and communicate.\nWorks reasonably well in many examples (wherever the posterior can be well approximated by a normal distribution).\nEasy to take draws from the normal approximation, also easy to compute importance ratios and use Pareto-smoothed importance sampling.\n\n\n\nLimitations of Laplace:\n\n\nSometimes the normal approx is pretty bad (funnels, multimodal distributions, long tails).\nSometimes the joint mode is useless or does not even exist (funnels, etc.), in which case the model itself would need to be altered in some way to get a stable mode.\n\n\n\n\n\n\nCode# Stan's LBFGS algorithm\nfit_full_optim &lt;- mod_full_ppp$optimize(data = data_full_new, seed = 1234, jacobian = TRUE)\nfit_full_laplace &lt;- mod_full_ppp$laplace(data = data_full_new, mode = fit_full_optim, draws = 4000)\n\n# Run 'variational' method to use ADVI to approximate posterior\nfit_full_vb &lt;- mod_full_ppp$variational(data = data_full_new, seed = 1234, draws = 4000)\n\n# Run 'pathfinder' method, a new alternative to the variational method\nfit_pf &lt;- mod_full_ppp$pathfinder(data = data_full_new, seed = 1234, draws = 4000)",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#wrapping-up",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#wrapping-up",
    "title": "Lecture 05",
    "section": "Wrapping up",
    "text": "Wrapping up\nThe three lectures using linear models was built to show nearly all parts needed in a Bayesian analysis\n\nMCMC specifications\nPrior specifications\nAssessing MCMC convergence\nReporting MCMC results\nDetermining if a model fits the data (absolute fit)\nDetermining which model fits the data better (relative fit)\n\nAll of these topics will be with us when we start model complicated models in our future lecture.",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#next-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#next-class",
    "title": "Lecture 05",
    "section": "Next Class",
    "text": "Next Class\n\nGeneralized measurement models",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#reference",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture05/Lecture05.html#reference",
    "title": "Lecture 05",
    "section": "Reference",
    "text": "Reference\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553\n\n\n\n\nDhaka, Akash Kumar, Alejandro Catalina, Michael Riis Andersen, Måns Magnusson, Jonathan H. Huggins, and Aki Vehtari. 2020. “Robust, Accurate Stochastic Optimization for Variational Inference.” arXiv. https://doi.org/10.48550/ARXIV.2009.00666.\n\n\nGelman, Andrew, Xiao-Li Meng, and Hal Stern. 1996. “Posterior Predictive Assessment of Model Fitness Via Realized Discrepancies.” Statistica Sinica.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2016. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” Statistics and Computing 27 (5): 1413–32. https://doi.org/10.1007/s11222-016-9696-4.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Yes, but Did It Work?: Evaluating Variational Inference.” arXiv. https://doi.org/10.48550/ARXIV.1802.02538.\n\n\nZhang, Lu, Bob Carpenter, Andrew Gelman, and Aki Vehtari. 2021. “Pathfinder: Parallel Quasi-Newton Variational Inference.” https://doi.org/10.48550/ARXIV.2108.03782.",
    "crumbs": [
      "Home",
      "ESRM6553-Adv.Multivariate",
      "Lecture 05"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#todays-lecture-objectives",
    "title": "Lecture 03",
    "section": "Today’s Lecture Objectives",
    "text": "Today’s Lecture Objectives\n\nAn Introduction to MCMC\nAn Introduction to Stan\nExample: Linear Regression\n\nbut, before we begin…\n\nGo through qmd file Lecture03.qmd;\nDownload R file DietDataExample.R and data file DietData.csv",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#quiz",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#quiz",
    "title": "Lecture 03",
    "section": "Quiz:",
    "text": "Quiz:\n\nWhat is a conjugate prior?\nWhy we want to use conjugate priors?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#in-previous-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#in-previous-class",
    "title": "Lecture 03",
    "section": "In previous class…",
    "text": "In previous class…\n\nWe work with a simplest Bayesian model: roll “1” from a 6-size dice\nWe talked about the selection of prior distributions from uninformative to informative priors\nWe talked about binomial distribution as the likelihood function\nThe posterior distribution is directly derived by update \\(\\alpha\\) and \\(\\beta\\)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#markov-chain-monte-carlo-estimation",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#markov-chain-monte-carlo-estimation",
    "title": "Lecture 03",
    "section": "Markov Chain Monte Carlo Estimation",
    "text": "Markov Chain Monte Carlo Estimation\nToday, we dive deeper into the estimation process.\n\nBayesian analysis is all about estimating the posterior distribution.\nUp until now, we’ve worked with the posterior distribution that are well-known\n\nBeta-Binomial conjugate pairs had a Beta posterior distribution\nIn general, likelihood distributions from the exponential family have conjugate priors\n\nConjugate prior: the family of the prior is equivalent to the family of posterior.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#example-data-post-diet-weights",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#example-data-post-diet-weights",
    "title": "Lecture 03",
    "section": "Example Data: Post-Diet Weights",
    "text": "Example Data: Post-Diet Weights\nDownload example data UCLA website\nInformation about data:\n\nIn this example data file, it contains 30 subjects who used one of three diets: diet 1 (diet=1), diet 2 (diet=2), and a control group (diet=3).\nThe file DietData.csv contains the data we needed.\nVariables in the data set are:\n\nRespondent: Respondent ID 1-30\nDietGroup: 1, 2, 3 representing the group to which a respondent was assigned\nHeightIN: The respondents’ height in inches\nWeightLB (Dependent Variable): The respondents’ weight in pounds\n\nResearch Question: Are there differences in final weights between the three diet groups, and, if so, what the nature of the differences?\nBefore we conduct the analysis, let’s look at the data",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#introduction-to-stan",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#introduction-to-stan",
    "title": "Lecture 03",
    "section": "Introduction to Stan",
    "text": "Introduction to Stan\n\nStan is an MCMC estimation program\n\nMost recent; has many convenient features\nActually does several methods of estimation (ML, Variational Bayes)\n\nYou create a model using Stan’s syntax\n\nStan translates your model to a custom-built C++ syntax\nStan then compiles your model into its own executable program\n\nYou then run the program to estimate your model\n\nIf you use R, the interface can be seamless",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#and-95-credible-interval",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#and-95-credible-interval",
    "title": "Lecture 03",
    "section": "90% and 95% Credible Interval",
    "text": "90% and 95% Credible Interval\n\n\n\nquantile(model00.samples$draws('beta0'), c(.05, .95))\n\n      5%      95% \n155.2948 186.4850 \n\nquantile(model00.samples$draws('beta0'), c(.025, .975))\n\n    2.5%    97.5% \n152.0660 189.8721 \n\n\n\nbayesplot::mcmc_dens(model00.samples$draws('beta0')) +\n  geom_vline(xintercept = quantile(model00.samples$draws('beta0'), c(.025, .975)), col = \"red\", size = 1.2) +\n  geom_vline(xintercept = quantile(model00.samples$draws('beta0'), c(.05, .95)), col = \"green\", size = 1.2)\n\n\n\n\n\n\n\n\n\n\nquantile(model00.samples$draws('sigma'), c(.05, .95))\n\n      5%      95% \n41.53700 64.82484 \n\nquantile(model00.samples$draws('sigma'), c(.025, .975))\n\n    2.5%    97.5% \n40.08569 68.21151 \n\n\n\nbayesplot::mcmc_dens(model00.samples$draws('sigma')) +\n  geom_vline(xintercept = quantile(model00.samples$draws('sigma'), c(.025, .975)), col = \"red\", size = 1.2) +\n  geom_vline(xintercept = quantile(model00.samples$draws('sigma'), c(.05, .95)), col = \"green\", size = 1.2)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#wrapping-up",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture03/Lecture03.html#wrapping-up",
    "title": "Lecture 03",
    "section": "Wrapping up",
    "text": "Wrapping up\n\nMCMC algorithm includes the Bayes’ theorm and sampling approaches\nStan syntax: need .stan storing the model information and .r storing R codes to run and summarize the model\ncmdstanr will compile the .stan and run the MCMC\nTo run MCMC, you need to specify number of chains, interations, burnins. You also translate data information into a data list\nAfter you finish estimation, check the convergence using \\(\\hat{R}\\) and visual inspection of traceplot\n\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture03",
      "Lecture 03"
    ]
  },
  {
    "objectID": "posts/2017-11-12-MESUREMENT-INVARIANCE/index.html",
    "href": "posts/2017-11-12-MESUREMENT-INVARIANCE/index.html",
    "title": "One Example of Measurement Invariance",
    "section": "",
    "text": "Recently, I was asked by my friend why should we use Measurement Invariance in real research. Why not just ignore this complex and tedious process? As far I’m concerned, measurement invariance should be widely used if you have large data scale and figure out what’s going on between groups difference. In this post, I want to elaborate some problems in Measurement Invariance: 1) What is measurement invariance 2) why should we care about measurement invariance 3) how to do measurement invariance using R Lavaan Package."
  },
  {
    "objectID": "posts/2017-11-12-MESUREMENT-INVARIANCE/index.html#an-example-of-multiple-group-cfa-invariance",
    "href": "posts/2017-11-12-MESUREMENT-INVARIANCE/index.html#an-example-of-multiple-group-cfa-invariance",
    "title": "One Example of Measurement Invariance",
    "section": "\n1.1 An example of Multiple Group CFA Invariance:",
    "text": "1.1 An example of Multiple Group CFA Invariance:\nThis example data is from Brown Chapter 7. Major Depression Criteria across Men and Women (n =345)\n9 items rated by clinicians on a scale of 0 to 8 (0=none, 8 =very severely disturbing/disabling)\n\nDepressed mood\nLoss of interest in usual activities\nWeight/appetite change\nSleep disturbance\nPsychomotor agitation/retardation\nFatigue/loss of energy\nFeelings of worthless/guilt\nConcentration difficulties\nThoughts of death/suicidality\n\nJonathan in his Measurement Invariance Example elaborated the manual version so that learner could learn what you are doing first. I will show you how to use shortcuts.\n\n1.1.1 Data Import\n\n\n  V1 V2 V3 V4 V5 V6 V7 V8 V9 V10\n1  0  5  4  1  6  5  6  5  4   2\n2  0  5  5  5  5  4  5  4  5   4\n3  0  4  5  4  2  6  6  0  0   0\n4  0  5  5  3  3  5  5  6  4   0\n5  0  5  5  0  5  0  4  6  0   0\n6  0  6  6  4  6  4  6  5  6   2\n\n\nThe sample size of female reference groups is as same as the male. The model for 2 groups should be same and check how many changes are allowed to differ.\n\n1.1.2 Model Specification\nmodel1.config &lt;- \"\n# Constrain the factor loadings and intercepts of marker variable in ALL groups\n# depress =~ c(L1F, L1M)*item1 + c(L2F, L2M)*item2 + c(L3F, L3M)*item3 +\n#            c(L4F, L4M)*item4 + c(L5F, L5M)*item5 + c(L6F, L6M)*item6 + \n#            c(L7F, L7M)*item7 + c(L8F, L8M)*item8 + c(L9F, L9M)*item9\ndepress =~ item1 + item2 + item3 +\n           item4 + item5 + item6 + \n           item7 + item8 + item9\n\n#Item intercepts all freely estimated in both groups with label for each group\nitem1 ~ 1; item2 ~ 1; item3 ~ 1; \nitem4 ~ 1; item5 ~ 1; item6 ~ 1; \nitem7 ~ 1; item8 ~ 1; item9 ~ 1;\n\n#Redidual variances all freely estimated with label for each group\nitem1 ~~ item1; item2 ~~ item2; item3 ~~ item3; \nitem4 ~~ item4; item5 ~~ item5; item6 ~~ item6; \nitem7 ~~ item7; item8 ~~ item8; item9 ~~ item9;\n\n#Residual covariance freely estimated in both groups with label for each group\nitem1 ~~ item2\n\n#==================================================\n#Factor variance fixed to 1 for identification in each group\ndepress ~~ c(1,NA)*depress\n\n#Factor mean fixed to zero for identification in each group\ndepress ~ c(0,NA)*0\n\"\n\n1.1.3 Model Options\nConfigural Invariance Model is the first-step model which allows all estimation different for two groups except that mean and variance of factor are fixed to 0 and 1, because the model uses z-score scalling.\nCompared to configural invariance, metic invariance model constrains the factor loadings for two groups equal with each other. To test metric invariance, we could use absolute model fit indices (CFI, TLI, RMSEA, SRMR) and comparable model fit indices (Log-likelihood test). It deserves noting that in metric invariance model, factor means are still constrained to be equal for two groups but the variances of factor are different. The variance of factor for reference group is fixed to 1 but that for other group is free to estimate. Since if we constrain both factor loadings and factor variances to equal, then the residual variances of items will also be equal. This is next step. Freeing one group’s factor variance will let model not too strict to Residual Variance.\nNext model is Scalar Invariance Model, which constrain the intercepts of items to be equal.\n\nfit.config &lt;- sem(model1.config, data = mddAll, \n                  meanstructure = T , std.lv = T,\n                  estimator = \"MLR\", mimic = \"mplus\",\n                  group = \"sex\",\n                  group.equal = c(\"lv.variances\", \"means\")) # latent variance both equal to 1\n                  \nfit.metric &lt;- sem(model1.config, data = mddAll, \n                  meanstructure = T , std.lv = T,\n                  estimator = \"MLR\", mimic = \"mplus\",\n                  group = \"sex\",\n                  group.equal = c(\"loadings\", \"means\")) # factor mean should be equal to 0\nfit.scalar &lt;- sem(model1.config, data = mddAll, \n                  meanstructure = T , std.lv = T,\n                  estimator = \"MLR\", mimic = \"mplus\",\n                  group = \"sex\",\n                  group.equal = c(\"loadings\",\"intercepts\"))\n# same: factor loadings, item intercepts\n# different: reference factor mean is 1, another factor mean is 0\n\nfit.scalar2 &lt;- sem(model1.config, data = mddAll, \n                  meanstructure = T , std.lv = T,\n                  estimator = \"MLR\", mimic = \"mplus\",\n                  group = \"sex\",\n                  group.equal = c(\"loadings\",\"intercepts\"),\n                  group.partial = c(\"item7~1\"))\n\nfit.strict &lt;- sem(model1.config, data = mddAll, \n                  meanstructure = T , std.lv = T,\n                  estimator = \"MLR\", mimic = \"mplus\",\n                  group = \"sex\",\n                  group.equal = c(\"loadings\",\"intercepts\", \"residuals\"),\n                  group.partial = c(\"item7~1\", \"item7~~item7\"))\nfit.strict.cov &lt;- sem(model1.config, data = mddAll, \n                  meanstructure = T , std.lv = T,\n                  estimator = \"MLR\", mimic = \"mplus\",\n                  group = \"sex\",\n                  group.equal = c(\"loadings\",\"intercepts\", \"residuals\", \n                                  \"residual.covariances\"),\n                  group.partial = c(\"item7~1\", \"item7~~item7\"))\n\n\n1.1.4 Runing Model\n\nsummary(fit.config, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 47 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        56\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                98.911      94.175\n  Degrees of freedom                                52          52\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.050\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      52.954      50.418\n    Male                                        45.957      43.756\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.963       0.963\n  Tucker-Lewis Index (TLI)                       0.949       0.949\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.965\n  Robust Tucker-Lewis Index (TLI)                            0.952\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13706.898  -13706.898\n  Scaling correction factor                                  0.981\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27525.796   27525.796\n  Bayesian (BIC)                             27784.520   27784.520\n  Sample-size adjusted Bayesian (SABIC)      27606.698   27606.698\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.049       0.047\n  90 Percent confidence interval - lower         0.034       0.031\n  90 Percent confidence interval - upper         0.064       0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.522       0.636\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.048\n  90 Percent confidence interval - lower                     0.032\n  90 Percent confidence interval - upper                     0.063\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.581\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.039       0.039\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1             1.251    0.095   13.155    0.000    1.251    0.730\n    item2             1.385    0.103   13.426    0.000    1.385    0.688\n    item3             0.911    0.104    8.775    0.000    0.911    0.435\n    item4             1.140    0.115    9.874    0.000    1.140    0.516\n    item5             1.015    0.106    9.615    0.000    1.015    0.477\n    item6             1.155    0.103   11.238    0.000    1.155    0.577\n    item7             0.764    0.115    6.618    0.000    0.764    0.371\n    item8             1.224    0.113   10.817    0.000    1.224    0.569\n    item9             0.606    0.094    6.412    0.000    0.606    0.339\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.393    0.166    2.364    0.018    0.393    0.230\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             4.184    0.089   47.258    0.000    4.184    2.440\n   .item2             3.725    0.104   35.848    0.000    3.725    1.851\n   .item3             1.952    0.108   18.058    0.000    1.952    0.933\n   .item4             3.589    0.114   31.458    0.000    3.589    1.624\n   .item5             2.256    0.110   20.522    0.000    2.256    1.060\n   .item6             3.955    0.103   38.237    0.000    3.955    1.975\n   .item7             3.869    0.106   36.382    0.000    3.869    1.879\n   .item8             3.595    0.111   32.331    0.000    3.595    1.670\n   .item9             1.205    0.092   13.053    0.000    1.205    0.674\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.375    0.194    7.090    0.000    1.375    0.468\n   .item2             2.132    0.236    9.049    0.000    2.132    0.527\n   .item3             3.551    0.201   17.678    0.000    3.551    0.810\n   .item4             3.583    0.272   13.166    0.000    3.583    0.734\n   .item5             3.501    0.223   15.733    0.000    3.501    0.773\n   .item6             2.677    0.269    9.967    0.000    2.677    0.667\n   .item7             3.658    0.276   13.270    0.000    3.658    0.862\n   .item8             3.137    0.291   10.785    0.000    3.137    0.677\n   .item9             2.831    0.195   14.538    0.000    2.831    0.885\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.532\n    item2             0.473\n    item3             0.190\n    item4             0.266\n    item5             0.227\n    item6             0.333\n    item7             0.138\n    item8             0.323\n    item9             0.115\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1             1.024    0.099   10.384    0.000    1.024    0.642\n    item2             1.266    0.112   11.283    0.000    1.266    0.628\n    item3             0.805    0.115    7.011    0.000    0.805    0.385\n    item4             1.193    0.123    9.729    0.000    1.193    0.535\n    item5             0.982    0.113    8.678    0.000    0.982    0.466\n    item6             1.159    0.116   10.010    0.000    1.159    0.549\n    item7             0.784    0.131    5.994    0.000    0.784    0.343\n    item8             1.043    0.121    8.610    0.000    1.043    0.480\n    item9             0.647    0.102    6.359    0.000    0.647    0.362\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.920    0.205    4.499    0.000    0.920    0.479\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             4.171    0.082   50.608    0.000    4.171    2.613\n   .item2             3.685    0.104   35.414    0.000    3.685    1.829\n   .item3             1.739    0.108   16.098    0.000    1.739    0.831\n   .item4             3.357    0.115   29.160    0.000    3.357    1.506\n   .item5             2.235    0.109   20.560    0.000    2.235    1.062\n   .item6             3.661    0.109   33.598    0.000    3.661    1.735\n   .item7             3.421    0.118   29.014    0.000    3.421    1.498\n   .item8             3.517    0.112   31.372    0.000    3.517    1.620\n   .item9             1.259    0.092   13.649    0.000    1.259    0.705\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.499    0.216    6.932    0.000    1.499    0.588\n   .item2             2.459    0.274    8.989    0.000    2.459    0.606\n   .item3             3.727    0.205   18.167    0.000    3.727    0.852\n   .item4             3.547    0.291   12.189    0.000    3.547    0.713\n   .item5             3.467    0.236   14.716    0.000    3.467    0.783\n   .item6             3.111    0.296   10.520    0.000    3.111    0.698\n   .item7             4.599    0.279   16.457    0.000    4.599    0.882\n   .item8             3.626    0.296   12.267    0.000    3.626    0.769\n   .item9             2.770    0.208   13.291    0.000    2.770    0.869\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.412\n    item2             0.394\n    item3             0.148\n    item4             0.287\n    item5             0.217\n    item6             0.302\n    item7             0.118\n    item8             0.231\n    item9             0.131\n\nsummary(fit.metric, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 48 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        57\n  Number of equality constraints                     9\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               102.839      99.532\n  Degrees of freedom                                60          60\n  P-value (Chi-square)                           0.000       0.001\n  Scaling correction factor                                  1.033\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      54.745      52.985\n    Male                                        48.094      46.547\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966       0.966\n  Tucker-Lewis Index (TLI)                       0.960       0.959\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.968\n  Robust Tucker-Lewis Index (TLI)                            0.961\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13708.862  -13708.862\n  Scaling correction factor                                  0.834\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27513.724   27513.724\n  Bayesian (BIC)                             27735.488   27735.488\n  Sample-size adjusted Bayesian (SABIC)      27583.069   27583.069\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044       0.042\n  90 Percent confidence interval - lower         0.029       0.027\n  90 Percent confidence interval - upper         0.058       0.056\n  P-value H_0: RMSEA &lt;= 0.050                    0.758       0.818\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.043\n  90 Percent confidence interval - lower                     0.027\n  90 Percent confidence interval - upper                     0.057\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.785\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.042       0.042\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.180    0.082   14.455    0.000    1.180    0.701\n    item2   (.p2.)    1.386    0.088   15.667    0.000    1.386    0.687\n    item3   (.p3.)    0.888    0.084   10.542    0.000    0.888    0.426\n    item4   (.p4.)    1.202    0.091   13.153    0.000    1.202    0.538\n    item5   (.p5.)    1.035    0.084   12.301    0.000    1.035    0.485\n    item6   (.p6.)    1.191    0.084   14.198    0.000    1.191    0.591\n    item7   (.p7.)    0.792    0.092    8.642    0.000    0.792    0.383\n    item8   (.p8.)    1.186    0.094   12.595    0.000    1.186    0.555\n    item9   (.p9.)    0.647    0.073    8.813    0.000    0.647    0.359\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.439    0.158    2.777    0.005    0.439    0.249\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             4.184    0.089   47.258    0.000    4.184    2.484\n   .item2             3.725    0.104   35.848    0.000    3.725    1.846\n   .item3             1.952    0.108   18.058    0.000    1.952    0.936\n   .item4             3.589    0.114   31.458    0.000    3.589    1.608\n   .item5             2.256    0.110   20.522    0.000    2.256    1.058\n   .item6             3.955    0.103   38.237    0.000    3.955    1.961\n   .item7             3.869    0.106   36.382    0.000    3.869    1.869\n   .item8             3.595    0.111   32.331    0.000    3.595    1.684\n   .item9             1.205    0.092   13.053    0.000    1.205    0.669\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.444    0.189    7.646    0.000    1.444    0.509\n   .item2             2.151    0.220    9.794    0.000    2.151    0.528\n   .item3             3.556    0.190   18.738    0.000    3.556    0.818\n   .item4             3.540    0.261   13.543    0.000    3.540    0.710\n   .item5             3.479    0.206   16.850    0.000    3.479    0.765\n   .item6             2.648    0.261   10.140    0.000    2.648    0.651\n   .item7             3.656    0.271   13.482    0.000    3.656    0.853\n   .item8             3.153    0.275   11.465    0.000    3.153    0.692\n   .item9             2.827    0.195   14.492    0.000    2.827    0.871\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.491\n    item2             0.472\n    item3             0.182\n    item4             0.290\n    item5             0.235\n    item6             0.349\n    item7             0.147\n    item8             0.308\n    item9             0.129\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.180    0.082   14.455    0.000    1.097    0.675\n    item2   (.p2.)    1.386    0.088   15.667    0.000    1.288    0.638\n    item3   (.p3.)    0.888    0.084   10.542    0.000    0.825    0.393\n    item4   (.p4.)    1.202    0.091   13.153    0.000    1.117    0.506\n    item5   (.p5.)    1.035    0.084   12.301    0.000    0.961    0.458\n    item6   (.p6.)    1.191    0.084   14.198    0.000    1.107    0.529\n    item7   (.p7.)    0.792    0.092    8.642    0.000    0.736    0.324\n    item8   (.p8.)    1.186    0.094   12.595    0.000    1.102    0.503\n    item9   (.p9.)    0.647    0.073    8.813    0.000    0.601    0.339\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.862    0.187    4.610    0.000    0.862    0.463\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             4.171    0.082   50.608    0.000    4.171    2.568\n   .item2             3.685    0.104   35.414    0.000    3.685    1.827\n   .item3             1.739    0.108   16.098    0.000    1.739    0.828\n   .item4             3.357    0.115   29.160    0.000    3.357    1.522\n   .item5             2.235    0.109   20.560    0.000    2.235    1.064\n   .item6             3.661    0.109   33.598    0.000    3.661    1.748\n   .item7             3.421    0.118   29.014    0.000    3.421    1.506\n   .item8             3.517    0.112   31.372    0.000    3.517    1.605\n   .item9             1.259    0.092   13.649    0.000    1.259    0.710\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.436    0.203    7.060    0.000    1.436    0.544\n   .item2             2.412    0.245    9.854    0.000    2.412    0.593\n   .item3             3.731    0.196   19.064    0.000    3.731    0.846\n   .item4             3.617    0.258   14.027    0.000    3.617    0.744\n   .item5             3.488    0.216   16.176    0.000    3.488    0.790\n   .item6             3.161    0.270   11.688    0.000    3.161    0.721\n   .item7             4.619    0.260   17.798    0.000    4.619    0.895\n   .item8             3.587    0.276   12.998    0.000    3.587    0.747\n   .item9             2.781    0.208   13.395    0.000    2.781    0.885\n    depress           0.863    0.112    7.728    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.456\n    item2             0.407\n    item3             0.154\n    item4             0.256\n    item5             0.210\n    item6             0.279\n    item7             0.105\n    item8             0.253\n    item9             0.115\n\nsummary(fit.scalar, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 52 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        58\n  Number of equality constraints                    18\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               115.309     111.951\n  Degrees of freedom                                68          68\n  P-value (Chi-square)                           0.000       0.001\n  Scaling correction factor                                  1.030\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      60.715      58.946\n    Male                                        54.594      53.004\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.963       0.962\n  Tucker-Lewis Index (TLI)                       0.961       0.959\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.964\n  Robust Tucker-Lewis Index (TLI)                            0.962\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13715.097  -13715.097\n  Scaling correction factor                                  0.681\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27510.194   27510.194\n  Bayesian (BIC)                             27694.997   27694.997\n  Sample-size adjusted Bayesian (SABIC)      27567.981   27567.981\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043       0.042\n  90 Percent confidence interval - lower         0.029       0.027\n  90 Percent confidence interval - upper         0.056       0.055\n  P-value H_0: RMSEA &lt;= 0.050                    0.794       0.846\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.042\n  90 Percent confidence interval - lower                     0.028\n  90 Percent confidence interval - upper                     0.056\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.817\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.046       0.046\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.171    0.081   14.385    0.000    1.171    0.696\n    item2   (.p2.)    1.377    0.089   15.534    0.000    1.377    0.683\n    item3   (.p3.)    0.894    0.084   10.621    0.000    0.894    0.429\n    item4   (.p4.)    1.209    0.091   13.343    0.000    1.209    0.541\n    item5   (.p5.)    1.033    0.084   12.275    0.000    1.033    0.485\n    item6   (.p6.)    1.199    0.083   14.424    0.000    1.199    0.593\n    item7   (.p7.)    0.803    0.091    8.853    0.000    0.803    0.386\n    item8   (.p8.)    1.184    0.094   12.534    0.000    1.184    0.555\n    item9   (.p9.)    0.640    0.074    8.604    0.000    0.640    0.356\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.454    0.159    2.852    0.004    0.454    0.255\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.240    0.077   54.984    0.000    4.240    2.520\n   .item2   (.11.)    3.773    0.092   41.111    0.000    3.773    1.872\n   .item3   (.12.)    1.897    0.087   21.735    0.000    1.897    0.909\n   .item4   (.13.)    3.541    0.096   37.066    0.000    3.541    1.584\n   .item5   (.14.)    2.303    0.090   25.622    0.000    2.303    1.080\n   .item6   (.15.)    3.882    0.091   42.556    0.000    3.882    1.921\n   .item7   (.16.)    3.711    0.087   42.428    0.000    3.711    1.784\n   .item8   (.17.)    3.620    0.094   38.567    0.000    3.620    1.696\n   .item9   (.18.)    1.268    0.072   17.592    0.000    1.268    0.704\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.460    0.193    7.576    0.000    1.460    0.516\n   .item2             2.166    0.223    9.726    0.000    2.166    0.533\n   .item3             3.555    0.191   18.619    0.000    3.555    0.816\n   .item4             3.535    0.261   13.520    0.000    3.535    0.708\n   .item5             3.478    0.206   16.880    0.000    3.478    0.765\n   .item6             2.648    0.260   10.183    0.000    2.648    0.648\n   .item7             3.682    0.267   13.767    0.000    3.682    0.851\n   .item8             3.155    0.277   11.377    0.000    3.155    0.692\n   .item9             2.834    0.192   14.790    0.000    2.834    0.874\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.484\n    item2             0.467\n    item3             0.184\n    item4             0.292\n    item5             0.235\n    item6             0.352\n    item7             0.149\n    item8             0.308\n    item9             0.126\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.171    0.081   14.385    0.000    1.089    0.671\n    item2   (.p2.)    1.377    0.089   15.534    0.000    1.280    0.635\n    item3   (.p3.)    0.894    0.084   10.621    0.000    0.831    0.395\n    item4   (.p4.)    1.209    0.091   13.343    0.000    1.123    0.509\n    item5   (.p5.)    1.033    0.084   12.275    0.000    0.960    0.457\n    item6   (.p6.)    1.199    0.083   14.424    0.000    1.114    0.531\n    item7   (.p7.)    0.803    0.091    8.853    0.000    0.746    0.327\n    item8   (.p8.)    1.184    0.094   12.534    0.000    1.100    0.502\n    item9   (.p9.)    0.640    0.074    8.604    0.000    0.595    0.336\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.879    0.185    4.754    0.000    0.879    0.468\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.240    0.077   54.984    0.000    4.240    2.611\n   .item2   (.11.)    3.773    0.092   41.111    0.000    3.773    1.870\n   .item3   (.12.)    1.897    0.087   21.735    0.000    1.897    0.902\n   .item4   (.13.)    3.541    0.096   37.066    0.000    3.541    1.604\n   .item5   (.14.)    2.303    0.090   25.622    0.000    2.303    1.097\n   .item6   (.15.)    3.882    0.091   42.556    0.000    3.882    1.850\n   .item7   (.16.)    3.711    0.087   42.428    0.000    3.711    1.625\n   .item8   (.17.)    3.620    0.094   38.567    0.000    3.620    1.653\n   .item9   (.18.)    1.268    0.072   17.592    0.000    1.268    0.715\n    depress          -0.112    0.083   -1.345    0.179   -0.120   -0.120\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.451    0.200    7.258    0.000    1.451    0.550\n   .item2             2.431    0.240   10.124    0.000    2.431    0.597\n   .item3             3.730    0.196   19.059    0.000    3.730    0.844\n   .item4             3.611    0.258   13.975    0.000    3.611    0.741\n   .item5             3.489    0.216   16.166    0.000    3.489    0.791\n   .item6             3.161    0.276   11.468    0.000    3.161    0.718\n   .item7             4.658    0.277   16.831    0.000    4.658    0.893\n   .item8             3.588    0.274   13.119    0.000    3.588    0.748\n   .item9             2.788    0.213   13.105    0.000    2.788    0.887\n    depress           0.864    0.112    7.720    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.450\n    item2             0.403\n    item3             0.156\n    item4             0.259\n    item5             0.209\n    item6             0.282\n    item7             0.107\n    item8             0.252\n    item9             0.113\n\nsummary(fit.scalar2, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 53 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        58\n  Number of equality constraints                    17\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               109.216     106.031\n  Degrees of freedom                                67          67\n  P-value (Chi-square)                           0.001       0.002\n  Scaling correction factor                                  1.030\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      57.897      56.209\n    Male                                        51.318      49.822\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.967       0.966\n  Tucker-Lewis Index (TLI)                       0.964       0.963\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.968\n  Robust Tucker-Lewis Index (TLI)                            0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13712.050  -13712.050\n  Scaling correction factor                                  0.699\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27506.100   27506.100\n  Bayesian (BIC)                             27695.523   27695.523\n  Sample-size adjusted Bayesian (SABIC)      27565.332   27565.332\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041       0.039\n  90 Percent confidence interval - lower         0.026       0.025\n  90 Percent confidence interval - upper         0.055       0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.855       0.896\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.025\n  90 Percent confidence interval - upper                     0.054\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.873\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044       0.044\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.174    0.082   14.377    0.000    1.174    0.698\n    item2   (.p2.)    1.381    0.089   15.564    0.000    1.381    0.685\n    item3   (.p3.)    0.894    0.084   10.598    0.000    0.894    0.428\n    item4   (.p4.)    1.208    0.091   13.309    0.000    1.208    0.540\n    item5   (.p5.)    1.034    0.084   12.287    0.000    1.034    0.485\n    item6   (.p6.)    1.198    0.083   14.364    0.000    1.198    0.592\n    item7   (.p7.)    0.791    0.092    8.603    0.000    0.791    0.382\n    item8   (.p8.)    1.185    0.094   12.561    0.000    1.185    0.555\n    item9   (.p9.)    0.642    0.074    8.630    0.000    0.642    0.356\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.449    0.159    2.825    0.005    0.449    0.253\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.228    0.078   54.510    0.000    4.228    2.512\n   .item2   (.11.)    3.761    0.092   40.840    0.000    3.761    1.865\n   .item3   (.12.)    1.887    0.087   21.651    0.000    1.887    0.904\n   .item4   (.13.)    3.529    0.096   36.780    0.000    3.529    1.578\n   .item5   (.14.)    2.292    0.090   25.462    0.000    2.292    1.075\n   .item6   (.15.)    3.870    0.092   42.207    0.000    3.870    1.915\n   .item7             3.869    0.106   36.382    0.000    3.869    1.869\n   .item8   (.17.)    3.609    0.094   38.382    0.000    3.609    1.690\n   .item9   (.18.)    1.261    0.072   17.570    0.000    1.261    0.700\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.455    0.191    7.595    0.000    1.455    0.513\n   .item2             2.160    0.222    9.738    0.000    2.160    0.531\n   .item3             3.557    0.191   18.613    0.000    3.557    0.817\n   .item4             3.539    0.261   13.545    0.000    3.539    0.708\n   .item5             3.478    0.206   16.874    0.000    3.478    0.765\n   .item6             2.651    0.260   10.205    0.000    2.651    0.649\n   .item7             3.658    0.271   13.485    0.000    3.658    0.854\n   .item8             3.154    0.277   11.404    0.000    3.154    0.692\n   .item9             2.832    0.192   14.743    0.000    2.832    0.873\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.487\n    item2             0.469\n    item3             0.183\n    item4             0.292\n    item5             0.235\n    item6             0.351\n    item7             0.146\n    item8             0.308\n    item9             0.127\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.174    0.082   14.377    0.000    1.091    0.672\n    item2   (.p2.)    1.381    0.089   15.564    0.000    1.283    0.636\n    item3   (.p3.)    0.894    0.084   10.598    0.000    0.830    0.395\n    item4   (.p4.)    1.208    0.091   13.309    0.000    1.122    0.508\n    item5   (.p5.)    1.034    0.084   12.287    0.000    0.961    0.457\n    item6   (.p6.)    1.198    0.083   14.364    0.000    1.113    0.530\n    item7   (.p7.)    0.791    0.092    8.603    0.000    0.735    0.324\n    item8   (.p8.)    1.185    0.094   12.561    0.000    1.101    0.503\n    item9   (.p9.)    0.642    0.074    8.630    0.000    0.597    0.337\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.872    0.186    4.696    0.000    0.872    0.466\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.228    0.078   54.510    0.000    4.228    2.604\n   .item2   (.11.)    3.761    0.092   40.840    0.000    3.761    1.864\n   .item3   (.12.)    1.887    0.087   21.651    0.000    1.887    0.897\n   .item4   (.13.)    3.529    0.096   36.780    0.000    3.529    1.598\n   .item5   (.14.)    2.292    0.090   25.462    0.000    2.292    1.091\n   .item6   (.15.)    3.870    0.092   42.207    0.000    3.870    1.844\n   .item7             3.493    0.123   28.376    0.000    3.493    1.538\n   .item8   (.17.)    3.609    0.094   38.382    0.000    3.609    1.647\n   .item9   (.18.)    1.261    0.072   17.570    0.000    1.261    0.712\n    depress          -0.090    0.083   -1.087    0.277   -0.097   -0.097\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1             1.445    0.201    7.186    0.000    1.445    0.548\n   .item2             2.423    0.242   10.026    0.000    2.423    0.595\n   .item3             3.733    0.196   19.086    0.000    3.733    0.844\n   .item4             3.615    0.260   13.913    0.000    3.615    0.742\n   .item5             3.488    0.216   16.160    0.000    3.488    0.791\n   .item6             3.166    0.277   11.417    0.000    3.166    0.719\n   .item7             4.620    0.259   17.804    0.000    4.620    0.895\n   .item8             3.587    0.274   13.071    0.000    3.587    0.747\n   .item9             2.787    0.212   13.148    0.000    2.787    0.887\n    depress           0.864    0.112    7.725    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.452\n    item2             0.405\n    item3             0.156\n    item4             0.258\n    item5             0.209\n    item6             0.281\n    item7             0.105\n    item8             0.253\n    item9             0.113\n\nsummary(fit.strict, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 54 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        58\n  Number of equality constraints                    25\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               114.059     112.019\n  Degrees of freedom                                75          75\n  P-value (Chi-square)                           0.002       0.004\n  Scaling correction factor                                  1.018\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      60.752      59.666\n    Male                                        53.306      52.353\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.969       0.968\n  Tucker-Lewis Index (TLI)                       0.971       0.969\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.970\n  Robust Tucker-Lewis Index (TLI)                            0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13714.472  -13714.472\n  Scaling correction factor                                  0.572\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27494.944   27494.944\n  Bayesian (BIC)                             27647.406   27647.406\n  Sample-size adjusted Bayesian (SABIC)      27542.618   27542.618\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.037       0.036\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.051       0.050\n  P-value H_0: RMSEA &lt;= 0.050                    0.942       0.956\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.037\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.050\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.948\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048       0.048\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.167    0.082   14.180    0.000    1.167    0.696\n    item2   (.p2.)    1.372    0.089   15.358    0.000    1.372    0.671\n    item3   (.p3.)    0.888    0.083   10.655    0.000    0.888    0.422\n    item4   (.p4.)    1.203    0.090   13.341    0.000    1.203    0.537\n    item5   (.p5.)    1.031    0.084   12.316    0.000    1.031    0.484\n    item6   (.p6.)    1.197    0.083   14.492    0.000    1.197    0.575\n    item7   (.p7.)    0.787    0.092    8.593    0.000    0.787    0.381\n    item8   (.p8.)    1.178    0.093   12.608    0.000    1.178    0.540\n    item9   (.p9.)    0.639    0.074    8.602    0.000    0.639    0.356\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.484    0.160    3.030    0.002    0.484    0.266\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.229    0.078   53.943    0.000    4.229    2.522\n   .item2   (.11.)    3.763    0.093   40.533    0.000    3.763    1.840\n   .item3   (.12.)    1.886    0.087   21.609    0.000    1.886    0.895\n   .item4   (.13.)    3.528    0.096   36.880    0.000    3.528    1.574\n   .item5   (.14.)    2.292    0.090   25.455    0.000    2.292    1.076\n   .item6   (.15.)    3.862    0.091   42.539    0.000    3.862    1.855\n   .item7             3.869    0.106   36.382    0.000    3.869    1.872\n   .item8   (.17.)    3.609    0.094   38.326    0.000    3.609    1.655\n   .item9   (.18.)    1.261    0.071   17.668    0.000    1.261    0.703\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.447    0.145    9.954    0.000    1.447    0.515\n   .item2   (.20.)    2.300    0.177   12.965    0.000    2.300    0.550\n   .item3   (.21.)    3.646    0.143   25.449    0.000    3.646    0.822\n   .item4   (.22.)    3.574    0.197   18.123    0.000    3.574    0.712\n   .item5   (.23.)    3.479    0.161   21.647    0.000    3.479    0.766\n   .item6   (.24.)    2.903    0.199   14.558    0.000    2.903    0.670\n   .item7             3.653    0.271   13.462    0.000    3.653    0.855\n   .item8   (.26.)    3.367    0.207   16.293    0.000    3.367    0.708\n   .item9   (.27.)    2.809    0.143   19.650    0.000    2.809    0.873\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.485\n    item2             0.450\n    item3             0.178\n    item4             0.288\n    item5             0.234\n    item6             0.330\n    item7             0.145\n    item8             0.292\n    item9             0.127\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.167    0.082   14.180    0.000    1.097    0.674\n    item2   (.p2.)    1.372    0.089   15.358    0.000    1.289    0.648\n    item3   (.p3.)    0.888    0.083   10.655    0.000    0.834    0.400\n    item4   (.p4.)    1.203    0.090   13.341    0.000    1.130    0.513\n    item5   (.p5.)    1.031    0.084   12.316    0.000    0.968    0.461\n    item6   (.p6.)    1.197    0.083   14.492    0.000    1.124    0.551\n    item7   (.p7.)    0.787    0.092    8.593    0.000    0.739    0.325\n    item8   (.p8.)    1.178    0.093   12.608    0.000    1.107    0.516\n    item9   (.p9.)    0.639    0.074    8.602    0.000    0.600    0.337\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.832    0.151    5.497    0.000    0.832    0.456\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.229    0.078   53.943    0.000    4.229    2.598\n   .item2   (.11.)    3.763    0.093   40.533    0.000    3.763    1.890\n   .item3   (.12.)    1.886    0.087   21.609    0.000    1.886    0.905\n   .item4   (.13.)    3.528    0.096   36.880    0.000    3.528    1.602\n   .item5   (.14.)    2.292    0.090   25.455    0.000    2.292    1.091\n   .item6   (.15.)    3.862    0.091   42.539    0.000    3.862    1.892\n   .item7             3.493    0.123   28.381    0.000    3.493    1.535\n   .item8   (.17.)    3.609    0.094   38.326    0.000    3.609    1.684\n   .item9   (.18.)    1.261    0.071   17.668    0.000    1.261    0.708\n    depress          -0.091    0.084   -1.084    0.279   -0.097   -0.097\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.447    0.145    9.954    0.000    1.447    0.546\n   .item2   (.20.)    2.300    0.177   12.965    0.000    2.300    0.581\n   .item3   (.21.)    3.646    0.143   25.449    0.000    3.646    0.840\n   .item4   (.22.)    3.574    0.197   18.123    0.000    3.574    0.737\n   .item5   (.23.)    3.479    0.161   21.647    0.000    3.479    0.788\n   .item6   (.24.)    2.903    0.199   14.558    0.000    2.903    0.697\n   .item7             4.629    0.260   17.815    0.000    4.629    0.894\n   .item8   (.26.)    3.367    0.207   16.293    0.000    3.367    0.733\n   .item9   (.27.)    2.809    0.143   19.650    0.000    2.809    0.886\n    depress           0.883    0.111    7.936    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.454\n    item2             0.419\n    item3             0.160\n    item4             0.263\n    item5             0.212\n    item6             0.303\n    item7             0.106\n    item8             0.267\n    item9             0.114\n\nsummary(fit.strict.cov, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 55 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        58\n  Number of equality constraints                    26\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               123.351     119.281\n  Degrees of freedom                                76          76\n  P-value (Chi-square)                           0.000       0.001\n  Scaling correction factor                                  1.034\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      65.102      62.954\n    Male                                        58.248      56.327\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.963       0.962\n  Tucker-Lewis Index (TLI)                       0.965       0.964\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.965\n  Robust Tucker-Lewis Index (TLI)                            0.967\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13719.118  -13719.118\n  Scaling correction factor                                  0.534\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27502.235   27502.235\n  Bayesian (BIC)                             27650.078   27650.078\n  Sample-size adjusted Bayesian (SABIC)      27548.465   27548.465\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041       0.039\n  90 Percent confidence interval - lower         0.027       0.025\n  90 Percent confidence interval - upper         0.054       0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.877       0.920\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.025\n  90 Percent confidence interval - upper                     0.053\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.897\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048       0.048\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.164    0.082   14.182    0.000    1.164    0.695\n    item2   (.p2.)    1.355    0.088   15.395    0.000    1.355    0.666\n    item3   (.p3.)    0.883    0.084   10.551    0.000    0.883    0.420\n    item4   (.p4.)    1.200    0.091   13.260    0.000    1.200    0.536\n    item5   (.p5.)    1.031    0.084   12.293    0.000    1.031    0.484\n    item6   (.p6.)    1.191    0.083   14.394    0.000    1.191    0.573\n    item7   (.p7.)    0.782    0.091    8.554    0.000    0.782    0.378\n    item8   (.p8.)    1.173    0.094   12.528    0.000    1.173    0.539\n    item9   (.p9.)    0.637    0.074    8.581    0.000    0.637    0.355\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2   (.28.)    0.671    0.132    5.072    0.000    0.671    0.366\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.231    0.079   53.848    0.000    4.231    2.524\n   .item2   (.11.)    3.767    0.092   40.777    0.000    3.767    1.850\n   .item3   (.12.)    1.886    0.087   21.608    0.000    1.886    0.896\n   .item4   (.13.)    3.528    0.096   36.854    0.000    3.528    1.576\n   .item5   (.14.)    2.292    0.090   25.422    0.000    2.292    1.077\n   .item6   (.15.)    3.862    0.091   42.534    0.000    3.862    1.857\n   .item7             3.869    0.106   36.382    0.000    3.869    1.873\n   .item8   (.17.)    3.610    0.094   38.318    0.000    3.610    1.657\n   .item9   (.18.)    1.261    0.071   17.661    0.000    1.261    0.703\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.455    0.147    9.904    0.000    1.455    0.518\n   .item2   (.20.)    2.309    0.179   12.908    0.000    2.309    0.557\n   .item3   (.21.)    3.648    0.143   25.433    0.000    3.648    0.824\n   .item4   (.22.)    3.570    0.197   18.084    0.000    3.570    0.712\n   .item5   (.23.)    3.470    0.161   21.569    0.000    3.470    0.765\n   .item6   (.24.)    2.906    0.199   14.565    0.000    2.906    0.672\n   .item7             3.658    0.272   13.465    0.000    3.658    0.857\n   .item8   (.26.)    3.368    0.207   16.303    0.000    3.368    0.710\n   .item9   (.27.)    2.808    0.143   19.650    0.000    2.808    0.874\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.482\n    item2             0.443\n    item3             0.176\n    item4             0.288\n    item5             0.235\n    item6             0.328\n    item7             0.143\n    item8             0.290\n    item9             0.126\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.164    0.082   14.182    0.000    1.103    0.675\n    item2   (.p2.)    1.355    0.088   15.395    0.000    1.284    0.645\n    item3   (.p3.)    0.883    0.084   10.551    0.000    0.836    0.401\n    item4   (.p4.)    1.200    0.091   13.260    0.000    1.137    0.516\n    item5   (.p5.)    1.031    0.084   12.293    0.000    0.977    0.464\n    item6   (.p6.)    1.191    0.083   14.394    0.000    1.128    0.552\n    item7   (.p7.)    0.782    0.091    8.554    0.000    0.741    0.325\n    item8   (.p8.)    1.173    0.094   12.528    0.000    1.111    0.518\n    item9   (.p9.)    0.637    0.074    8.581    0.000    0.603    0.339\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2   (.28.)    0.671    0.132    5.072    0.000    0.671    0.366\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.231    0.079   53.848    0.000    4.231    2.589\n   .item2   (.11.)    3.767    0.092   40.777    0.000    3.767    1.894\n   .item3   (.12.)    1.886    0.087   21.608    0.000    1.886    0.904\n   .item4   (.13.)    3.528    0.096   36.854    0.000    3.528    1.600\n   .item5   (.14.)    2.292    0.090   25.422    0.000    2.292    1.090\n   .item6   (.15.)    3.862    0.091   42.534    0.000    3.862    1.890\n   .item7             3.493    0.123   28.383    0.000    3.493    1.535\n   .item8   (.17.)    3.610    0.094   38.318    0.000    3.610    1.683\n   .item9   (.18.)    1.261    0.071   17.661    0.000    1.261    0.708\n    depress          -0.091    0.084   -1.086    0.278   -0.097   -0.097\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.455    0.147    9.904    0.000    1.455    0.545\n   .item2   (.20.)    2.309    0.179   12.908    0.000    2.309    0.584\n   .item3   (.21.)    3.648    0.143   25.433    0.000    3.648    0.839\n   .item4   (.22.)    3.570    0.197   18.084    0.000    3.570    0.734\n   .item5   (.23.)    3.470    0.161   21.569    0.000    3.470    0.784\n   .item6   (.24.)    2.906    0.199   14.565    0.000    2.906    0.696\n   .item7             4.630    0.260   17.825    0.000    4.630    0.894\n   .item8   (.26.)    3.368    0.207   16.303    0.000    3.368    0.732\n   .item9   (.27.)    2.808    0.143   19.650    0.000    2.808    0.885\n    depress           0.897    0.113    7.932    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.455\n    item2             0.416\n    item3             0.161\n    item4             0.266\n    item5             0.216\n    item6             0.304\n    item7             0.106\n    item8             0.268\n    item9             0.115\n\n\n\n1.1.5 Model Comparison\n\nmodel_fit &lt;-  function(lavobject) {\n  vars &lt;- c(\"cfi\", \"tli\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"rmsea.pvalue\", \"srmr\")\n  return(fitmeasures(lavobject)[vars] %&gt;% data.frame() %&gt;% round(2) %&gt;% t())\n}\n\ntable_fit &lt;- \n  list(model_fit(fit.config), model_fit(fit.metric), \n       model_fit(fit.scalar), model_fit(fit.scalar2),\n       model_fit(fit.strict), model_fit(fit.strict.cov)) %&gt;% \n  reduce(rbind)\n\nrownames(table_fit) &lt;- c(\"Configural\", \"Metric\", \"Scalar\", \"Scalar2\",\"Strict\",\"Strict+Cov\")\n\ntable_lik.test &lt;- \n  list(anova(fit.config, fit.metric),\n       anova(fit.metric, fit.scalar),\n       anova(fit.scalar, fit.scalar2),\n       anova(fit.scalar2, fit.strict),\n       anova(fit.strict, fit.strict.cov)\n       ) %&gt;%  \n  reduce(rbind) %&gt;% \n  .[-c(3,5,7,9),]\nrownames(table_lik.test) &lt;- c(\"Configural\", \"Metric\", \"Scalar\", \"Scalar2\",\"Strict\",\"Strict+Cov\")\n\nkable(table_fit, caption = \"Model Fit Indices Table\")\n\n\nModel Fit Indices Table\n\n\n\n\n\n\n\n\n\n\n\n\ncfi\ntli\nrmsea\nrmsea.ci.lower\nrmsea.ci.upper\nrmsea.pvalue\nsrmr\n\n\n\nConfigural\n0.96\n0.95\n0.05\n0.03\n0.06\n0.52\n0.04\n\n\nMetric\n0.97\n0.96\n0.04\n0.03\n0.06\n0.76\n0.04\n\n\nScalar\n0.96\n0.96\n0.04\n0.03\n0.06\n0.79\n0.05\n\n\nScalar2\n0.97\n0.96\n0.04\n0.03\n0.05\n0.85\n0.04\n\n\nStrict\n0.97\n0.97\n0.04\n0.02\n0.05\n0.94\n0.05\n\n\nStrict+Cov\n0.96\n0.96\n0.04\n0.03\n0.05\n0.88\n0.05\n\n\n\n\nkable(table_lik.test, caption = \"Model Comparision Table\")\n\n\nModel Comparision Table\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\nConfigural\n52\n27525.80\n27784.52\n98.91085\nNA\nNA\nNA\n\n\nMetric\n60\n27513.72\n27735.49\n102.83941\n4.259305\n8\n0.8330029\n\n\nScalar\n68\n27510.19\n27695.00\n115.30933\n12.398256\n8\n0.1342996\n\n\nScalar2\n68\n27510.19\n27695.00\n115.30933\n5.929566\n1\n0.0148889\n\n\nStrict\n75\n27494.94\n27647.41\n114.05887\n5.269043\n8\n0.7284714\n\n\nStrict+Cov\n76\n27502.24\n27650.08\n123.35057\n4.172431\n1\n0.0410868"
  },
  {
    "objectID": "posts/2017-11-12-MESUREMENT-INVARIANCE/index.html#structual-invariance-tests",
    "href": "posts/2017-11-12-MESUREMENT-INVARIANCE/index.html#structual-invariance-tests",
    "title": "One Example of Measurement Invariance",
    "section": "\n1.2 STRUCTUAL INVARIANCE TESTS",
    "text": "1.2 STRUCTUAL INVARIANCE TESTS\n\n1.2.1 Factor Variance Invariance Model\n\n\nlavaan 0.6.17 ended normally after 54 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        57\n  Number of equality constraints                    25\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               114.904     113.113\n  Degrees of freedom                                76          76\n  P-value (Chi-square)                           0.003       0.004\n  Scaling correction factor                                  1.016\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      61.213      60.259\n    Male                                        53.691      52.854\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.969       0.968\n  Tucker-Lewis Index (TLI)                       0.971       0.969\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.970\n  Robust Tucker-Lewis Index (TLI)                            0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13714.894  -13714.894\n  Scaling correction factor                                  0.567\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27493.789   27493.789\n  Bayesian (BIC)                             27641.631   27641.631\n  Sample-size adjusted Bayesian (SABIC)      27540.019   27540.019\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.037       0.036\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.050       0.049\n  P-value H_0: RMSEA &lt;= 0.050                    0.947       0.958\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.036\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.050\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.952\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.050       0.050\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.132    0.069   16.495    0.000    1.132    0.685\n    item2   (.p2.)    1.332    0.076   17.634    0.000    1.332    0.660\n    item3   (.p3.)    0.861    0.076   11.269    0.000    0.861    0.411\n    item4   (.p4.)    1.169    0.083   14.123    0.000    1.169    0.526\n    item5   (.p5.)    1.000    0.076   13.226    0.000    1.000    0.473\n    item6   (.p6.)    1.162    0.077   15.167    0.000    1.162    0.564\n    item7   (.p7.)    0.765    0.086    8.889    0.000    0.765    0.371\n    item8   (.p8.)    1.142    0.082   13.922    0.000    1.142    0.528\n    item9   (.p9.)    0.620    0.069    8.931    0.000    0.620    0.347\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.490    0.159    3.077    0.002    0.490    0.268\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.229    0.078   53.980    0.000    4.229    2.558\n   .item2   (.11.)    3.763    0.093   40.555    0.000    3.763    1.864\n   .item3   (.12.)    1.886    0.087   21.616    0.000    1.886    0.900\n   .item4   (.13.)    3.528    0.096   36.887    0.000    3.528    1.588\n   .item5   (.14.)    2.292    0.090   25.463    0.000    2.292    1.083\n   .item6   (.15.)    3.862    0.091   42.543    0.000    3.862    1.873\n   .item7             3.869    0.106   36.382    0.000    3.869    1.879\n   .item8   (.17.)    3.610    0.094   38.348    0.000    3.610    1.670\n   .item9   (.18.)    1.261    0.071   17.671    0.000    1.261    0.706\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.452    0.145    9.988    0.000    1.452    0.531\n   .item2   (.20.)    2.301    0.178   12.925    0.000    2.301    0.565\n   .item3   (.21.)    3.646    0.143   25.467    0.000    3.646    0.831\n   .item4   (.22.)    3.571    0.197   18.119    0.000    3.571    0.723\n   .item5   (.23.)    3.478    0.161   21.626    0.000    3.478    0.777\n   .item6   (.24.)    2.900    0.199   14.536    0.000    2.900    0.682\n   .item7             3.655    0.271   13.480    0.000    3.655    0.862\n   .item8   (.26.)    3.368    0.207   16.280    0.000    3.368    0.721\n   .item9   (.27.)    2.809    0.143   19.649    0.000    2.809    0.880\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.469\n    item2             0.435\n    item3             0.169\n    item4             0.277\n    item5             0.223\n    item6             0.318\n    item7             0.138\n    item8             0.279\n    item9             0.120\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.132    0.069   16.495    0.000    1.132    0.685\n    item2   (.p2.)    1.332    0.076   17.634    0.000    1.332    0.660\n    item3   (.p3.)    0.861    0.076   11.269    0.000    0.861    0.411\n    item4   (.p4.)    1.169    0.083   14.123    0.000    1.169    0.526\n    item5   (.p5.)    1.000    0.076   13.226    0.000    1.000    0.473\n    item6   (.p6.)    1.162    0.077   15.167    0.000    1.162    0.564\n    item7   (.p7.)    0.765    0.086    8.889    0.000    0.765    0.335\n    item8   (.p8.)    1.142    0.082   13.922    0.000    1.142    0.528\n    item9   (.p9.)    0.620    0.069    8.931    0.000    0.620    0.347\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.834    0.152    5.483    0.000    0.834    0.456\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.229    0.078   53.980    0.000    4.229    2.558\n   .item2   (.11.)    3.763    0.093   40.555    0.000    3.763    1.864\n   .item3   (.12.)    1.886    0.087   21.616    0.000    1.886    0.900\n   .item4   (.13.)    3.528    0.096   36.887    0.000    3.528    1.588\n   .item5   (.14.)    2.292    0.090   25.463    0.000    2.292    1.083\n   .item6   (.15.)    3.862    0.091   42.543    0.000    3.862    1.873\n   .item7             3.493    0.123   28.386    0.000    3.493    1.530\n   .item8   (.17.)    3.610    0.094   38.348    0.000    3.610    1.670\n   .item9   (.18.)    1.261    0.071   17.671    0.000    1.261    0.706\n    depress          -0.094    0.085   -1.098    0.272   -0.094   -0.094\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.452    0.145    9.988    0.000    1.452    0.531\n   .item2   (.20.)    2.301    0.178   12.925    0.000    2.301    0.565\n   .item3   (.21.)    3.646    0.143   25.467    0.000    3.646    0.831\n   .item4   (.22.)    3.571    0.197   18.119    0.000    3.571    0.723\n   .item5   (.23.)    3.478    0.161   21.626    0.000    3.478    0.777\n   .item6   (.24.)    2.900    0.199   14.536    0.000    2.900    0.682\n   .item7             4.626    0.260   17.771    0.000    4.626    0.888\n   .item8   (.26.)    3.368    0.207   16.280    0.000    3.368    0.721\n   .item9   (.27.)    2.809    0.143   19.649    0.000    2.809    0.880\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.469\n    item2             0.435\n    item3             0.169\n    item4             0.277\n    item5             0.223\n    item6             0.318\n    item7             0.112\n    item8             0.279\n    item9             0.120\n\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan NOTE:\n    The \"Chisq\" column contains standard test statistics, not the\n    robust test that should be reported per model. A robust difference\n    test is a function of two standard (not robust) statistics.\n \n                       Df   AIC   BIC  Chisq Chisq diff Df diff Pr(&gt;Chisq)\nfit.strict             75 27495 27647 114.06                              \nfit.structuralVariance 76 27494 27642 114.90     1.0095       1      0.315\n\n\n\n1.2.2 Factor Mean Invariance Model\n\n\nlavaan 0.6.17 ended normally after 54 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        56\n  Number of equality constraints                    25\n\n  Number of observations per group:                   \n    Female                                         375\n    Male                                           375\n  Number of missing patterns per group:               \n    Female                                           1\n    Male                                             1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               116.143     114.340\n  Degrees of freedom                                77          77\n  P-value (Chi-square)                           0.003       0.004\n  Scaling correction factor                                  1.016\n    Yuan-Bentler correction (Mplus variant)                       \n  Test statistic for each group:\n    Female                                      61.790      60.831\n    Male                                        54.353      53.509\n\nModel Test Baseline Model:\n\n  Test statistic                              1343.575    1218.364\n  Degrees of freedom                                72          72\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.103\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.969       0.967\n  Tucker-Lewis Index (TLI)                       0.971       0.970\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.970\n  Robust Tucker-Lewis Index (TLI)                            0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -13715.514  -13715.514\n  Scaling correction factor                                  0.559\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -13657.442  -13657.442\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               27493.027   27493.027\n  Bayesian (BIC)                             27636.250   27636.250\n  Sample-size adjusted Bayesian (SABIC)      27537.813   27537.813\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.037       0.036\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.050       0.049\n  P-value H_0: RMSEA &lt;= 0.050                    0.950       0.961\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.036\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.050\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.954\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.050       0.050\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [Female]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.135    0.068   16.637    0.000    1.135    0.686\n    item2   (.p2.)    1.336    0.075   17.802    0.000    1.336    0.661\n    item3   (.p3.)    0.860    0.077   11.228    0.000    0.860    0.411\n    item4   (.p4.)    1.168    0.083   14.063    0.000    1.168    0.526\n    item5   (.p5.)    1.001    0.076   13.194    0.000    1.001    0.473\n    item6   (.p6.)    1.161    0.077   15.096    0.000    1.161    0.563\n    item7   (.p7.)    0.766    0.086    8.914    0.000    0.766    0.372\n    item8   (.p8.)    1.144    0.082   13.946    0.000    1.144    0.529\n    item9   (.p9.)    0.622    0.069    9.001    0.000    0.622    0.348\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.485    0.159    3.047    0.002    0.485    0.266\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.176    0.060   69.282    0.000    4.176    2.525\n   .item2   (.11.)    3.702    0.074   50.291    0.000    3.702    1.833\n   .item3   (.12.)    1.845    0.077   24.121    0.000    1.845    0.881\n   .item4   (.13.)    3.473    0.081   42.797    0.000    3.473    1.563\n   .item5   (.14.)    2.245    0.077   29.048    0.000    2.245    1.061\n   .item6   (.15.)    3.808    0.075   50.564    0.000    3.808    1.846\n   .item7             3.842    0.104   37.048    0.000    3.842    1.866\n   .item8   (.17.)    3.556    0.079   45.035    0.000    3.556    1.644\n   .item9   (.18.)    1.232    0.065   18.878    0.000    1.232    0.689\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.447    0.145    9.949    0.000    1.447    0.529\n   .item2   (.20.)    2.295    0.178   12.893    0.000    2.295    0.563\n   .item3   (.21.)    3.649    0.143   25.557    0.000    3.649    0.831\n   .item4   (.22.)    3.576    0.197   18.172    0.000    3.576    0.724\n   .item5   (.23.)    3.478    0.161   21.617    0.000    3.478    0.776\n   .item6   (.24.)    2.906    0.199   14.596    0.000    2.906    0.683\n   .item7             3.654    0.271   13.478    0.000    3.654    0.862\n   .item8   (.26.)    3.368    0.207   16.275    0.000    3.368    0.720\n   .item9   (.27.)    2.807    0.143   19.666    0.000    2.807    0.879\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.471\n    item2             0.437\n    item3             0.169\n    item4             0.276\n    item5             0.224\n    item6             0.317\n    item7             0.138\n    item8             0.280\n    item9             0.121\n\n\nGroup 2 [Male]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  depress =~                                                            \n    item1   (.p1.)    1.135    0.068   16.637    0.000    1.135    0.686\n    item2   (.p2.)    1.336    0.075   17.802    0.000    1.336    0.661\n    item3   (.p3.)    0.860    0.077   11.228    0.000    0.860    0.411\n    item4   (.p4.)    1.168    0.083   14.063    0.000    1.168    0.526\n    item5   (.p5.)    1.001    0.076   13.194    0.000    1.001    0.473\n    item6   (.p6.)    1.161    0.077   15.096    0.000    1.161    0.563\n    item7   (.p7.)    0.766    0.086    8.914    0.000    0.766    0.336\n    item8   (.p8.)    1.144    0.082   13.946    0.000    1.144    0.529\n    item9   (.p9.)    0.622    0.069    9.001    0.000    0.622    0.348\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .item1 ~~                                                              \n   .item2             0.829    0.152    5.448    0.000    0.829    0.455\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.10.)    4.176    0.060   69.282    0.000    4.176    2.525\n   .item2   (.11.)    3.702    0.074   50.291    0.000    3.702    1.833\n   .item3   (.12.)    1.845    0.077   24.121    0.000    1.845    0.881\n   .item4   (.13.)    3.473    0.081   42.797    0.000    3.473    1.563\n   .item5   (.14.)    2.245    0.077   29.048    0.000    2.245    1.061\n   .item6   (.15.)    3.808    0.075   50.564    0.000    3.808    1.846\n   .item7             3.448    0.116   29.819    0.000    3.448    1.510\n   .item8   (.17.)    3.556    0.079   45.035    0.000    3.556    1.644\n   .item9   (.18.)    1.232    0.065   18.878    0.000    1.232    0.689\n    depress           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .item1   (.19.)    1.447    0.145    9.949    0.000    1.447    0.529\n   .item2   (.20.)    2.295    0.178   12.893    0.000    2.295    0.563\n   .item3   (.21.)    3.649    0.143   25.557    0.000    3.649    0.831\n   .item4   (.22.)    3.576    0.197   18.172    0.000    3.576    0.724\n   .item5   (.23.)    3.478    0.161   21.617    0.000    3.478    0.776\n   .item6   (.24.)    2.906    0.199   14.596    0.000    2.906    0.683\n   .item7             4.625    0.260   17.769    0.000    4.625    0.887\n   .item8   (.26.)    3.368    0.207   16.275    0.000    3.368    0.720\n   .item9   (.27.)    2.807    0.143   19.666    0.000    2.807    0.879\n    depress           1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    item1             0.471\n    item2             0.437\n    item3             0.169\n    item4             0.276\n    item5             0.224\n    item6             0.317\n    item7             0.113\n    item8             0.280\n    item9             0.121\n\n\n\n1.2.3 Model Comparison\n\n\n\nModel Fit Indices Table\n\n\n\n\n\n\n\n\n\n\n\n\ncfi\ntli\nrmsea\nrmsea.ci.lower\nrmsea.ci.upper\nrmsea.pvalue\nsrmr\n\n\n\nConfigural\n0.96\n0.95\n0.05\n0.03\n0.06\n0.52\n0.04\n\n\nstructuralVariance\n0.97\n0.97\n0.04\n0.02\n0.05\n0.95\n0.05\n\n\nstructuralMean\n0.97\n0.97\n0.04\n0.02\n0.05\n0.95\n0.05\n\n\n\n\n\n\nModel Comparision Table\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\nConfigural\n52\n27525.80\n27784.52\n98.91085\nNA\nNA\nNA\n\n\nstructuralVariance\n76\n27493.79\n27641.63\n114.90425\n16.993054\n24\n0.8489582\n\n\nstructuralMean\n77\n27493.03\n27636.25\n116.14270\n1.225188\n1\n0.2683450"
  },
  {
    "objectID": "posts/2021-08-30-gradient-descent-via-julia/index.html",
    "href": "posts/2021-08-30-gradient-descent-via-julia/index.html",
    "title": "Gradient Descent Algorithm via julia",
    "section": "",
    "text": "using RDatasets\nusing DataFrames\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n\n32 rows × 12 columns (omitted printing of 4 columns)\n\n\n\n\n\n\n\nModel\n\n\nMPG\n\n\nCyl\n\n\nDisp\n\n\nHP\n\n\nDRat\n\n\nWT\n\n\nQSec\n\n\n\n\n\n\nString\n\n\nFloat64\n\n\nInt64\n\n\nFloat64\n\n\nInt64\n\n\nFloat64\n\n\nFloat64\n\n\nFloat64\n\n\n\n\n\n\n1\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160.0\n\n\n110\n\n\n3.9\n\n\n2.62\n\n\n16.46\n\n\n\n\n2\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160.0\n\n\n110\n\n\n3.9\n\n\n2.875\n\n\n17.02\n\n\n\n\n3\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108.0\n\n\n93\n\n\n3.85\n\n\n2.32\n\n\n18.61\n\n\n\n\n4\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258.0\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n\n\n5\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360.0\n\n\n175\n\n\n3.15\n\n\n3.44\n\n\n17.02\n\n\n\n\n6\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225.0\n\n\n105\n\n\n2.76\n\n\n3.46\n\n\n20.22\n\n\n\n\n7\n\n\nDuster 360\n\n\n14.3\n\n\n8\n\n\n360.0\n\n\n245\n\n\n3.21\n\n\n3.57\n\n\n15.84\n\n\n\n\n8\n\n\nMerc 240D\n\n\n24.4\n\n\n4\n\n\n146.7\n\n\n62\n\n\n3.69\n\n\n3.19\n\n\n20.0\n\n\n\n\n9\n\n\nMerc 230\n\n\n22.8\n\n\n4\n\n\n140.8\n\n\n95\n\n\n3.92\n\n\n3.15\n\n\n22.9\n\n\n\n\n10\n\n\nMerc 280\n\n\n19.2\n\n\n6\n\n\n167.6\n\n\n123\n\n\n3.92\n\n\n3.44\n\n\n18.3\n\n\n\n\n11\n\n\nMerc 280C\n\n\n17.8\n\n\n6\n\n\n167.6\n\n\n123\n\n\n3.92\n\n\n3.44\n\n\n18.9\n\n\n\n\n12\n\n\nMerc 450SE\n\n\n16.4\n\n\n8\n\n\n275.8\n\n\n180\n\n\n3.07\n\n\n4.07\n\n\n17.4\n\n\n\n\n13\n\n\nMerc 450SL\n\n\n17.3\n\n\n8\n\n\n275.8\n\n\n180\n\n\n3.07\n\n\n3.73\n\n\n17.6\n\n\n\n\n14\n\n\nMerc 450SLC\n\n\n15.2\n\n\n8\n\n\n275.8\n\n\n180\n\n\n3.07\n\n\n3.78\n\n\n18.0\n\n\n\n\n15\n\n\nCadillac Fleetwood\n\n\n10.4\n\n\n8\n\n\n472.0\n\n\n205\n\n\n2.93\n\n\n5.25\n\n\n17.98\n\n\n\n\n16\n\n\nLincoln Continental\n\n\n10.4\n\n\n8\n\n\n460.0\n\n\n215\n\n\n3.0\n\n\n5.424\n\n\n17.82\n\n\n\n\n17\n\n\nChrysler Imperial\n\n\n14.7\n\n\n8\n\n\n440.0\n\n\n230\n\n\n3.23\n\n\n5.345\n\n\n17.42\n\n\n\n\n18\n\n\nFiat 128\n\n\n32.4\n\n\n4\n\n\n78.7\n\n\n66\n\n\n4.08\n\n\n2.2\n\n\n19.47\n\n\n\n\n19\n\n\nHonda Civic\n\n\n30.4\n\n\n4\n\n\n75.7\n\n\n52\n\n\n4.93\n\n\n1.615\n\n\n18.52\n\n\n\n\n20\n\n\nToyota Corolla\n\n\n33.9\n\n\n4\n\n\n71.1\n\n\n65\n\n\n4.22\n\n\n1.835\n\n\n19.9\n\n\n\n\n21\n\n\nToyota Corona\n\n\n21.5\n\n\n4\n\n\n120.1\n\n\n97\n\n\n3.7\n\n\n2.465\n\n\n20.01\n\n\n\n\n22\n\n\nDodge Challenger\n\n\n15.5\n\n\n8\n\n\n318.0\n\n\n150\n\n\n2.76\n\n\n3.52\n\n\n16.87\n\n\n\n\n23\n\n\nAMC Javelin\n\n\n15.2\n\n\n8\n\n\n304.0\n\n\n150\n\n\n3.15\n\n\n3.435\n\n\n17.3\n\n\n\n\n24\n\n\nCamaro Z28\n\n\n13.3\n\n\n8\n\n\n350.0\n\n\n245\n\n\n3.73\n\n\n3.84\n\n\n15.41\n\n\n\n\n25\n\n\nPontiac Firebird\n\n\n19.2\n\n\n8\n\n\n400.0\n\n\n175\n\n\n3.08\n\n\n3.845\n\n\n17.05\n\n\n\n\n26\n\n\nFiat X1-9\n\n\n27.3\n\n\n4\n\n\n79.0\n\n\n66\n\n\n4.08\n\n\n1.935\n\n\n18.9\n\n\n\n\n27\n\n\nPorsche 914-2\n\n\n26.0\n\n\n4\n\n\n120.3\n\n\n91\n\n\n4.43\n\n\n2.14\n\n\n16.7\n\n\n\n\n28\n\n\nLotus Europa\n\n\n30.4\n\n\n4\n\n\n95.1\n\n\n113\n\n\n3.77\n\n\n1.513\n\n\n16.9\n\n\n\n\n29\n\n\nFord Pantera L\n\n\n15.8\n\n\n8\n\n\n351.0\n\n\n264\n\n\n4.22\n\n\n3.17\n\n\n14.5\n\n\n\n\n30\n\n\nFerrari Dino\n\n\n19.7\n\n\n6\n\n\n145.0\n\n\n175\n\n\n3.62\n\n\n2.77\n\n\n15.5\n\n\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮"
  },
  {
    "objectID": "posts/2021-08-30-gradient-descent-via-julia/index.html#load-julia-modules",
    "href": "posts/2021-08-30-gradient-descent-via-julia/index.html#load-julia-modules",
    "title": "Gradient Descent Algorithm via julia",
    "section": "",
    "text": "using RDatasets\nusing DataFrames\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n\n32 rows × 12 columns (omitted printing of 4 columns)\n\n\n\n\n\n\n\nModel\n\n\nMPG\n\n\nCyl\n\n\nDisp\n\n\nHP\n\n\nDRat\n\n\nWT\n\n\nQSec\n\n\n\n\n\n\nString\n\n\nFloat64\n\n\nInt64\n\n\nFloat64\n\n\nInt64\n\n\nFloat64\n\n\nFloat64\n\n\nFloat64\n\n\n\n\n\n\n1\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160.0\n\n\n110\n\n\n3.9\n\n\n2.62\n\n\n16.46\n\n\n\n\n2\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160.0\n\n\n110\n\n\n3.9\n\n\n2.875\n\n\n17.02\n\n\n\n\n3\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108.0\n\n\n93\n\n\n3.85\n\n\n2.32\n\n\n18.61\n\n\n\n\n4\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258.0\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n\n\n5\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360.0\n\n\n175\n\n\n3.15\n\n\n3.44\n\n\n17.02\n\n\n\n\n6\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225.0\n\n\n105\n\n\n2.76\n\n\n3.46\n\n\n20.22\n\n\n\n\n7\n\n\nDuster 360\n\n\n14.3\n\n\n8\n\n\n360.0\n\n\n245\n\n\n3.21\n\n\n3.57\n\n\n15.84\n\n\n\n\n8\n\n\nMerc 240D\n\n\n24.4\n\n\n4\n\n\n146.7\n\n\n62\n\n\n3.69\n\n\n3.19\n\n\n20.0\n\n\n\n\n9\n\n\nMerc 230\n\n\n22.8\n\n\n4\n\n\n140.8\n\n\n95\n\n\n3.92\n\n\n3.15\n\n\n22.9\n\n\n\n\n10\n\n\nMerc 280\n\n\n19.2\n\n\n6\n\n\n167.6\n\n\n123\n\n\n3.92\n\n\n3.44\n\n\n18.3\n\n\n\n\n11\n\n\nMerc 280C\n\n\n17.8\n\n\n6\n\n\n167.6\n\n\n123\n\n\n3.92\n\n\n3.44\n\n\n18.9\n\n\n\n\n12\n\n\nMerc 450SE\n\n\n16.4\n\n\n8\n\n\n275.8\n\n\n180\n\n\n3.07\n\n\n4.07\n\n\n17.4\n\n\n\n\n13\n\n\nMerc 450SL\n\n\n17.3\n\n\n8\n\n\n275.8\n\n\n180\n\n\n3.07\n\n\n3.73\n\n\n17.6\n\n\n\n\n14\n\n\nMerc 450SLC\n\n\n15.2\n\n\n8\n\n\n275.8\n\n\n180\n\n\n3.07\n\n\n3.78\n\n\n18.0\n\n\n\n\n15\n\n\nCadillac Fleetwood\n\n\n10.4\n\n\n8\n\n\n472.0\n\n\n205\n\n\n2.93\n\n\n5.25\n\n\n17.98\n\n\n\n\n16\n\n\nLincoln Continental\n\n\n10.4\n\n\n8\n\n\n460.0\n\n\n215\n\n\n3.0\n\n\n5.424\n\n\n17.82\n\n\n\n\n17\n\n\nChrysler Imperial\n\n\n14.7\n\n\n8\n\n\n440.0\n\n\n230\n\n\n3.23\n\n\n5.345\n\n\n17.42\n\n\n\n\n18\n\n\nFiat 128\n\n\n32.4\n\n\n4\n\n\n78.7\n\n\n66\n\n\n4.08\n\n\n2.2\n\n\n19.47\n\n\n\n\n19\n\n\nHonda Civic\n\n\n30.4\n\n\n4\n\n\n75.7\n\n\n52\n\n\n4.93\n\n\n1.615\n\n\n18.52\n\n\n\n\n20\n\n\nToyota Corolla\n\n\n33.9\n\n\n4\n\n\n71.1\n\n\n65\n\n\n4.22\n\n\n1.835\n\n\n19.9\n\n\n\n\n21\n\n\nToyota Corona\n\n\n21.5\n\n\n4\n\n\n120.1\n\n\n97\n\n\n3.7\n\n\n2.465\n\n\n20.01\n\n\n\n\n22\n\n\nDodge Challenger\n\n\n15.5\n\n\n8\n\n\n318.0\n\n\n150\n\n\n2.76\n\n\n3.52\n\n\n16.87\n\n\n\n\n23\n\n\nAMC Javelin\n\n\n15.2\n\n\n8\n\n\n304.0\n\n\n150\n\n\n3.15\n\n\n3.435\n\n\n17.3\n\n\n\n\n24\n\n\nCamaro Z28\n\n\n13.3\n\n\n8\n\n\n350.0\n\n\n245\n\n\n3.73\n\n\n3.84\n\n\n15.41\n\n\n\n\n25\n\n\nPontiac Firebird\n\n\n19.2\n\n\n8\n\n\n400.0\n\n\n175\n\n\n3.08\n\n\n3.845\n\n\n17.05\n\n\n\n\n26\n\n\nFiat X1-9\n\n\n27.3\n\n\n4\n\n\n79.0\n\n\n66\n\n\n4.08\n\n\n1.935\n\n\n18.9\n\n\n\n\n27\n\n\nPorsche 914-2\n\n\n26.0\n\n\n4\n\n\n120.3\n\n\n91\n\n\n4.43\n\n\n2.14\n\n\n16.7\n\n\n\n\n28\n\n\nLotus Europa\n\n\n30.4\n\n\n4\n\n\n95.1\n\n\n113\n\n\n3.77\n\n\n1.513\n\n\n16.9\n\n\n\n\n29\n\n\nFord Pantera L\n\n\n15.8\n\n\n8\n\n\n351.0\n\n\n264\n\n\n4.22\n\n\n3.17\n\n\n14.5\n\n\n\n\n30\n\n\nFerrari Dino\n\n\n19.7\n\n\n6\n\n\n145.0\n\n\n175\n\n\n3.62\n\n\n2.77\n\n\n15.5\n\n\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮"
  },
  {
    "objectID": "posts/2021-08-30-gradient-descent-via-julia/index.html#julia-function-for-gradient-descent",
    "href": "posts/2021-08-30-gradient-descent-via-julia/index.html#julia-function-for-gradient-descent",
    "title": "Gradient Descent Algorithm via julia",
    "section": "2 Julia Function for Gradient Descent",
    "text": "2 Julia Function for Gradient Descent\n\nlearn_rate: the magnitude of the steps the algorithm takes along the slope of the MSE function\nconv_threshold: threshold for convergence of gradient descent n: number of iternations\nmax_iter: maximum of iteration before the algorithm stopss\n\nfunction gradientDesc(x, y, learn_rate, conv_threshold, n, max_iter)\n    β = rand(Float64, 1)[1]\n    α = rand(Float64, 1)[1]\n    ŷ = α .+ β .* x\n    MSE = sum((y .- ŷ).^2)/n\n    converged = false\n    iterations = 0\n\n    while converged == false\n        # Implement the gradient descent algorithm\n        β_new = β - learn_rate*((1/n)*(sum((ŷ .- y) .* x)))\n        α_new = α - learn_rate*((1/n)*(sum(ŷ .- y)))\n        α = α_new\n        β = β_new\n        ŷ = β.*x .+ α\n        MSE_new = sum((y.-ŷ).^2)/n\n        # decide on whether it is converged or not\n        if (MSE - MSE_new) &lt;= conv_threshold\n            converged = true\n            println(\"Optimal intercept: $α; Optimal slope: $β\")\n        end\n        iterations += 1\n        if iterations &gt; max_iter\n            converged = true\n            println(\"Optimal intercept: $α; Optimal slope: $β\")\n        end\n    end\nend\ngradientDesc (generic function with 1 method)\ngradientDesc(mtcars[:,:Disp], mtcars[:,:MPG], 0.0000293, 0.001, 32, 2500000)\nOptimal intercept: 29.599851506041713; Optimal slope: -0.0412151089535404"
  },
  {
    "objectID": "posts/2021-08-30-gradient-descent-via-julia/index.html#compared-to-linear-regression",
    "href": "posts/2021-08-30-gradient-descent-via-julia/index.html#compared-to-linear-regression",
    "title": "Gradient Descent Algorithm via julia",
    "section": "3 Compared to linear regression",
    "text": "3 Compared to linear regression\nusing GLM\nlinearRegressor = lm(@formula(MPG ~ Disp), mtcars)\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n\nMPG ~ 1 + Disp\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(&gt;|t|)  Lower 95%   Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  29.5999     1.22972     24.07    &lt;1e-20  27.0884    32.1113\nDisp         -0.0412151  0.00471183  -8.75    &lt;1e-09  -0.050838  -0.0315923\n───────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2018-04-29-updateing-R/index.html",
    "href": "posts/2018-04-29-updateing-R/index.html",
    "title": "Updating R Version Without missing packages",
    "section": "",
    "text": "After updating to new R version (4.5) from old version, you have to re-install all packages by default. However, there’re some solution for that."
  },
  {
    "objectID": "posts/2018-04-29-updateing-R/index.html#unix-macos-linux",
    "href": "posts/2018-04-29-updateing-R/index.html#unix-macos-linux",
    "title": "Updating R Version Without missing packages",
    "section": "1 Unix (MacOs, Linux)",
    "text": "1 Unix (MacOs, Linux)\n1.Create a new folder in home directory to store the packages. Sometimes, you need to change the permission level for this folder, or R may not have access to write this folder. Rlibs is a special folder where you can store all you packages.\nsudo mkdir ~/Rlibs\n2.Edit the .Reviron file in your home directory (“~”) (create a new file if you don’t have it). Add the code below to let R know where is the installed Packages. R will read the configuration in the background from the path “~/.Reviron”.\nR_LIBS=~/Rlibs\n3.Re-install you packages. After that you shall see your packages are stored in Rlibs folder."
  },
  {
    "objectID": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html",
    "href": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html",
    "title": "How to do Data Cleaning in R",
    "section": "",
    "text": "This blog is trying to elaborate steps for cleaning the data. Since datasets varied, this blog could not cover all. Depedent on the data you’re using, different methods should be used."
  },
  {
    "objectID": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#libraries",
    "href": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#libraries",
    "title": "How to do Data Cleaning in R",
    "section": "\n1 Libraries",
    "text": "1 Libraries\nI use tidyverse as my main tool to clean the data. Tidyverse is a very useful package created by Hadley. It includes several sub-packages, such as dplyr (data manipulation), tidyr (data transforming), readr (data import), ggplot2 (data visulization) etc. If you haven’t installed this package yet, please run install.packages(\"tidyverse\") in your R console.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3"
  },
  {
    "objectID": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-1-import-data",
    "href": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-1-import-data",
    "title": "How to do Data Cleaning in R",
    "section": "\n2 Step 1: Import Data",
    "text": "2 Step 1: Import Data\nIf your data is csv format, you could use read.csv() to import the data into R. Be careful to add stringsAsFactors = FALSE argument in the function, or all string variables will automate convert to factor by default. This will lead to some issues when you do further checking. # Configuration of Academic # Documentation: https://sourcethemes.com/academic/ # # This file is formatted using TOML syntax - learn more at https://learnxinyminutes.com/docs/toml/ # Each configuration section is defined by a name in square brackets (e.g. [outputs]).\n\ndat1 &lt;- read.csv(\"nfl_2010-2017.csv\", stringsAsFactors = FALSE)"
  },
  {
    "objectID": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-2-initial-check",
    "href": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-2-initial-check",
    "title": "How to do Data Cleaning in R",
    "section": "\n3 Step 2: Initial Check",
    "text": "3 Step 2: Initial Check\nPerform some initial check before doing further manipulation. This step is to let you get familiar with you data and have a big picture on what you need to do next.\n\n3.1 Step 2.1: check variables\nstr() is a very useful fuction in R base package which provides you sample sizes, number of variabels, variables names, variabile types and several sample responses. In the sample data, there are 81,525 observations and 23 variables.\n\nstr(dat1)\n\n'data.frame':   81525 obs. of  23 variables:\n $ X           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ name        : chr  \"Duce Staley\" \"Lamar Smith\" \"Tiki Barber\" \"Stephen Davis\" ...\n $ team        : chr  \"PHI\" \"MIA\" \"NYG\" \"WAS\" ...\n $ game_year   : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ game_week   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ rush_att    : int  26 27 13 23 28 27 30 14 15 10 ...\n $ rush_yds    : int  201 145 144 133 124 119 110 109 88 87 ...\n $ rush_avg    : num  7.7 5.4 11.1 5.8 4.4 4.4 3.7 7.8 5.9 8.7 ...\n $ rush_tds    : int  1 1 2 1 1 0 1 0 0 1 ...\n $ rush_fumbles: int  0 1 0 0 1 0 0 0 0 0 ...\n $ rec         : int  4 1 3 4 6 4 6 2 2 NA ...\n $ rec_yds     : int  61 12 25 37 40 32 34 3 20 NA ...\n $ rec_avg     : num  15.3 12 8.3 9.3 6.7 8 5.7 1.5 10 NA ...\n $ rec_tds     : int  0 0 0 0 1 0 1 0 0 NA ...\n $ rec_fumbles : int  0 0 0 0 0 0 0 0 0 NA ...\n $ pass_att    : int  NA NA NA NA NA NA NA NA NA 41 ...\n $ pass_yds    : int  NA NA NA NA NA NA NA NA NA 290 ...\n $ pass_tds    : int  NA NA NA NA NA NA NA NA NA 2 ...\n $ int         : int  NA NA NA NA NA NA NA NA NA 0 ...\n $ sck         : int  NA NA NA NA NA NA NA NA NA 2 ...\n $ pass_fumbles: int  NA NA NA NA NA NA NA NA NA 0 ...\n $ rate        : num  NA NA NA NA NA ...\n $ position    : chr  \"RB\" \"RB\" \"RB\" \"RB\" ...\n\n\n\n3.2 step 2.2: check missing values and ranges\nMissing values could be checked one by one variables or in case-level. Knowing which one or more variablea have high missing values will help you think about the reasons.\n\nsummary(dat1)\n\n       X             name               team             game_year   \n Min.   :    1   Length:81525       Length:81525       Min.   :2000  \n 1st Qu.:20382   Class :character   Class :character   1st Qu.:2004  \n Median :40763   Mode  :character   Mode  :character   Median :2009  \n Mean   :40763                                         Mean   :2009  \n 3rd Qu.:61144                                         3rd Qu.:2013  \n Max.   :81525                                         Max.   :2017  \n                                                                     \n   game_week         rush_att        rush_yds         rush_avg     \n Min.   : 1.000   Min.   : 0.00   Min.   :-34.00   Min.   :-34.00  \n 1st Qu.: 4.000   1st Qu.: 1.00   1st Qu.:  4.00   1st Qu.:  2.00  \n Median : 9.000   Median : 4.00   Median : 15.00   Median :  3.50  \n Mean   : 8.592   Mean   : 6.96   Mean   : 28.91   Mean   :  4.05  \n 3rd Qu.:13.000   3rd Qu.:10.00   3rd Qu.: 42.00   3rd Qu.:  5.30  \n Max.   :16.000   Max.   :43.00   Max.   :296.00   Max.   : 77.00  \n                  NA's   :47710   NA's   :47710    NA's   :47710   \n    rush_tds      rush_fumbles        rec            rec_yds      \n Min.   :0.00    Min.   :0.00    Min.   : 0.000   Min.   :-22.00  \n 1st Qu.:0.00    1st Qu.:0.00    1st Qu.: 1.000   1st Qu.:  8.00  \n Median :0.00    Median :0.00    Median : 2.000   Median : 21.00  \n Mean   :0.21    Mean   :0.08    Mean   : 2.716   Mean   : 31.23  \n 3rd Qu.:0.00    3rd Qu.:0.00    3rd Qu.: 4.000   3rd Qu.: 45.00  \n Max.   :5.00    Max.   :3.00    Max.   :21.000   Max.   :329.00  \n NA's   :47710   NA's   :47710   NA's   :16722    NA's   :16722   \n    rec_avg          rec_tds       rec_fumbles       pass_att    \n Min.   :-22.00   Min.   :0.000   Min.   :0.000   Min.   : 0.00  \n 1st Qu.:  5.10   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:21.00  \n Median :  9.00   Median :0.000   Median :0.000   Median :30.00  \n Mean   : 10.15   Mean   :0.188   Mean   :0.032   Mean   :27.64  \n 3rd Qu.: 13.50   3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:37.00  \n Max.   : 99.00   Max.   :4.000   Max.   :2.000   Max.   :68.00  \n NA's   :16722    NA's   :16722   NA's   :16722   NA's   :71044  \n    pass_yds        pass_tds          int             sck       \n Min.   :-11.0   Min.   :0.00    Min.   :0.0     Min.   : 0.00  \n 1st Qu.:126.0   1st Qu.:0.00    1st Qu.:0.0     1st Qu.: 1.00  \n Median :204.0   Median :1.00    Median :1.0     Median : 2.00  \n Mean   :193.1   Mean   :1.16    Mean   :0.8     Mean   : 1.88  \n 3rd Qu.:267.0   3rd Qu.:2.00    3rd Qu.:1.0     3rd Qu.: 3.00  \n Max.   :527.0   Max.   :7.00    Max.   :7.0     Max.   :12.00  \n NA's   :71044   NA's   :71044   NA's   :71044   NA's   :71044  \n  pass_fumbles        rate          position        \n Min.   :0.00    Min.   :  0.00   Length:81525      \n 1st Qu.:0.00    1st Qu.: 60.20   Class :character  \n Median :0.00    Median : 82.10   Mode  :character  \n Mean   :0.15    Mean   : 81.57                     \n 3rd Qu.:0.00    3rd Qu.:103.20                     \n Max.   :3.00    Max.   :158.30                     \n NA's   :71044   NA's   :71044                      \n\n\nIn this sample, variables rush_att, rush_yds, rush_avg, rush_tds, rush_fumbles have 47,710 missing values; variables rec, rec_yds, rec_avg, rec_tds, rec_fumbles have 16,722 missing values, variables pass_att, pass_yds, pass_tds, int, sck, pass_fumbles, rate have 71,044 missing values.\nAlso, look at the range (Min., Max.) of variables in summary output, sometimes the responses may exceed expected range. You may have to delete those cases in this situation.\n\n3.3 step 2.3: check first and last cases\nSometimes you may find the second row of data including some information you don’t want. For example, Qualtrics survey data will put some background log information in row 2. Thus, it’s better to check the head and tail of dataset. You can use functions below:\n\nhead(dat1)\n\n  X           name team game_year game_week rush_att rush_yds rush_avg rush_tds\n1 1    Duce Staley  PHI      2000         1       26      201      7.7        1\n2 2    Lamar Smith  MIA      2000         1       27      145      5.4        1\n3 3    Tiki Barber  NYG      2000         1       13      144     11.1        2\n4 4  Stephen Davis  WAS      2000         1       23      133      5.8        1\n5 5 Edgerrin James  IND      2000         1       28      124      4.4        1\n6 6  Priest Holmes  BAL      2000         1       27      119      4.4        0\n  rush_fumbles rec rec_yds rec_avg rec_tds rec_fumbles pass_att pass_yds\n1            0   4      61    15.3       0           0       NA       NA\n2            1   1      12    12.0       0           0       NA       NA\n3            0   3      25     8.3       0           0       NA       NA\n4            0   4      37     9.3       0           0       NA       NA\n5            1   6      40     6.7       1           0       NA       NA\n6            0   4      32     8.0       0           0       NA       NA\n  pass_tds int sck pass_fumbles rate position\n1       NA  NA  NA           NA   NA       RB\n2       NA  NA  NA           NA   NA       RB\n3       NA  NA  NA           NA   NA       RB\n4       NA  NA  NA           NA   NA       RB\n5       NA  NA  NA           NA   NA       RB\n6       NA  NA  NA           NA   NA       RB\n\ntail(dat1)\n\n          X           name team game_year game_week rush_att rush_yds rush_avg\n81520 81520     Jared Goff   LA      2017        15       NA       NA       NA\n81521 81521    Andy Dalton  CIN      2017        15       NA       NA       NA\n81522 81522 Trevor Siemian  DEN      2017        15       NA       NA       NA\n81523 81523  A.J. McCarron  CIN      2017        15       NA       NA       NA\n81524 81524 Derek Anderson  CAR      2017        16       NA       NA       NA\n81525 81525  Johnny Hekker   LA      2017        16       NA       NA       NA\n      rush_tds rush_fumbles rec rec_yds rec_avg rec_tds rec_fumbles pass_att\n81520       NA           NA  NA      NA      NA      NA          NA       21\n81521       NA           NA  NA      NA      NA      NA          NA       22\n81522       NA           NA  NA      NA      NA      NA          NA        9\n81523       NA           NA  NA      NA      NA      NA          NA        6\n81524       NA           NA  NA      NA      NA      NA          NA        1\n81525       NA           NA  NA      NA      NA      NA          NA        1\n      pass_yds pass_tds int sck pass_fumbles rate position\n81520      120        2   1   2            0 93.4       QB\n81521      113        0   2   3            0 27.3       QB\n81522       67        0   1   2            0 39.8       QB\n81523       19        0   0   0            0 56.9       QB\n81524        0        0   0   0            0 39.6    WR/TE\n81525        0        0   0   0            0 39.6    WR/TE\n\n\nThe R output provide first 6 cases and last 6 case. You can use head(dat1, 10) to output first 10 cases."
  },
  {
    "objectID": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-3-select-and-rename-variables",
    "href": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-3-select-and-rename-variables",
    "title": "How to do Data Cleaning in R",
    "section": "\n4 Step 3: Select and rename Variables",
    "text": "4 Step 3: Select and rename Variables\nAfter initial checks, some basic data cleaning could be done. You may want to select some variables you want and remove others. You can use select function to do that:\n\ndat2 &lt;- dat1 %&gt;% select(name, team, pass_att)\n\nI selected 3 variables (name, team, pass_att) from the dat1 and assign 3-variables data to “dat2”.\n\nhead(dat2)\n\n            name team pass_att\n1    Duce Staley  PHI       NA\n2    Lamar Smith  MIA       NA\n3    Tiki Barber  NYG       NA\n4  Stephen Davis  WAS       NA\n5 Edgerrin James  IND       NA\n6  Priest Holmes  BAL       NA\n\n\nTo rename the variables’ names, you could use set_names() from purrr packages.\n\ndat2 %&gt;% purrr::set_names(nm = \"Players\", \"Team\", \"Pass_Attribute\") %&gt;% head()\n\n         Players Team Pass_Attribute\n1    Duce Staley  PHI             NA\n2    Lamar Smith  MIA             NA\n3    Tiki Barber  NYG             NA\n4  Stephen Davis  WAS             NA\n5 Edgerrin James  IND             NA\n6  Priest Holmes  BAL             NA\n\ndat2 %&gt;% purrr::set_names(nm = \"V1\", \"V2\", \"V3\") %&gt;% head()\n\n              V1  V2 V3\n1    Duce Staley PHI NA\n2    Lamar Smith MIA NA\n3    Tiki Barber NYG NA\n4  Stephen Davis WAS NA\n5 Edgerrin James IND NA\n6  Priest Holmes BAL NA"
  },
  {
    "objectID": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-4-remove-missing-values",
    "href": "posts/2018-09-04-how-to-do-data-cleaning-in-r/index.html#step-4-remove-missing-values",
    "title": "How to do Data Cleaning in R",
    "section": "\n5 Step 4: Remove missing values",
    "text": "5 Step 4: Remove missing values\nIf you prefer cases with no missing cases at all. You can use the function below.\n\ndat_complete_cases &lt;- dat2 %&gt;% \n  filter_all(all_vars(!is.na(.)))\n\nsummary(dat_complete_cases)\n\n     name               team              pass_att    \n Length:10481       Length:10481       Min.   : 0.00  \n Class :character   Class :character   1st Qu.:21.00  \n Mode  :character   Mode  :character   Median :30.00  \n                                       Mean   :27.64  \n                                       3rd Qu.:37.00  \n                                       Max.   :68.00  \n\n\nOr if you want remove the cases whose Pass_Attribute is missing, you can use:\n\ndat_partialcomplete_cases &lt;- dat2 %&gt;% \n  filter(!is.na(pass_att))\n\nsummary(dat_complete_cases)\n\n     name               team              pass_att    \n Length:10481       Length:10481       Min.   : 0.00  \n Class :character   Class :character   1st Qu.:21.00  \n Mode  :character   Mode  :character   Median :30.00  \n                                       Mean   :27.64  \n                                       3rd Qu.:37.00  \n                                       Max.   :68.00  \n\n\nTo be continued…"
  },
  {
    "objectID": "posts/2019-11-14-use-r-as-a-bash/index.html",
    "href": "posts/2019-11-14-use-r-as-a-bash/index.html",
    "title": "Use R as a bash language",
    "section": "",
    "text": "R language could be easily used as a bash script using Rscript *.R. system() is a R base function which could run command line within R.\nBelow is a simple example which allows to automate create a new blog post: (1) Ask users to type in filename, title and language (2) Create a new markdown file in specific directory (i.e. your local posts saved path) (3) Add some metadata in .md file (4) Open the file using your favorite markdown editor.\ncat('Your filename &gt; ')\nfilename &lt;- scan(file = \"stdin\", what = \"character\", n=1, sep = \"-\")\n\ncat(\"Your post's title &gt; \")\ntitle &lt;- scan(\"stdin\", what = \"character\", n=1, sep = \"-\")\n\ncat(\"language is zh or en &gt; \")\nlang &lt;- scan(\"stdin\", character(), n=1)\n\ndraft = TRUE\n\nmd.metadata =\n  paste0(\"---\ntitle: \", title,\"\ndate: \",Sys.Date(),\"\ndraft: \", tolower(draft) ,\"\ncategories:\n  - blog\ntags:\n  - Blog\n---\")\n\nif (lang == \"zh\"){\n  ## create a file name\n  postname = paste0(Sys.Date(), \"-\",gsub(\" \", \"-\", filename, fixed = TRUE), \".\" ,lang,\".md\")\n  ## Chinese Path\n   filepath = paste0(\n    \"/Users/jihong/Documents/hugo-academic-jihong/content/post/zh/\",\n    postname)\n\n   cmd = paste0(\"cd /Users/jihong/Documents/hugo-academic-jihong/content/post/zh/ && open \", postname)\n} else {\n  postname = paste0(Sys.Date(), \"-\",gsub(\" \", \"-\", filename, fixed = TRUE), \".md\")\n  ## English Path\n  filepath = paste0(\n    \"/Users/jihong/Documents/hugo-academic-jihong/content/post/\",\n    postname)\n\n  cmd = paste0(\"cd /Users/jihong/Documents/hugo-academic-jihong/content/post/ && open \", postname)\n}\n\n\nwrite.table(md.metadata, file = filepath, quote = FALSE, row.names = FALSE,col.names = FALSE)\n\nsystem(cmd)\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2022-05-12-web-scrapping-grant-fundings-in-r/index.html",
    "href": "posts/2022-05-12-web-scrapping-grant-fundings-in-r/index.html",
    "title": "Web Scraping Academia Institute’s Grant Fundings using R",
    "section": "",
    "text": "This is an example of how to web scrape grants of active research in college’s official website. Please follow the academia institute’s website robots rules.\nlibrary(XML)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/2022-05-12-web-scrapping-grant-fundings-in-r/index.html#web-scrapping",
    "href": "posts/2022-05-12-web-scrapping-grant-fundings-in-r/index.html#web-scrapping",
    "title": "Web Scraping Academia Institute’s Grant Fundings using R",
    "section": "\n1 Web Scrapping",
    "text": "1 Web Scrapping\n\ntopics = rep(NA, 84)\nfundings = rep(NA, 84)\niter = 0\nfor (page in 1:9) {\n  ## parent webpage\n  scrape_url &lt;- paste0('http://XXXXXXXXXXXXXXXXXXXXXX', page)\n  \n  html_form_page &lt;- read_html(scrape_url)\n  \n  ## find the child web page containing projects' name, total award, topic etc.\n  child_url = html_form_page |&gt; html_elements(\"h3 a[href]\") |&gt; html_attr(\"href\") \n  \n  for (item in 1:length(child_url)) {\n    iter = iter + 1\n    child_html_text &lt;- child_url[item] |&gt; read_html() |&gt; html_elements(\"div[class='study_wrapper']\") |&gt; html_text() \n    topic = child_html_text |&gt; str_extract(pattern = \"Topic\\\\(s\\\\)\\\\: [a-zA-Z]+\\\\b\") |&gt; str_replace(pattern = \"Topic\\\\(s\\\\)\\\\: \", \"\")\n    funding = child_html_text |&gt; str_extract(pattern = \"\\\\$\\\\d+\\\\,\\\\d+\") |&gt; str_replace_all(pattern = \"\\\\$|\\\\,\", \"\") |&gt; as.numeric()\n    topics[iter] = topic\n    fundings[iter] = funding\n  }\n}\n\ndat &lt;- data.frame(topic = topics, funding_amount = fundings) |&gt; \n  add_row(topic = \"Marijuana\", funding_amount = 3743) |&gt; \n  mutate(topic = ifelse(topic == \"TobaccoMarijuana\", \"Tobacco\", topic)) \n\n\n\n\n\n\ntopic\nfunding_amount\n\n\n\nOther\n97666\n\n\nOther\n15000\n\n\nTobacco\n1271\n\n\nCancer\nNA\n\n\nOther\n199836\n\n\nOther\n64614\n\n\nCancer\n200000\n\n\nOther\n75000\n\n\nOther\n72972\n\n\nCancer\n329234"
  },
  {
    "objectID": "posts/2022-05-12-web-scrapping-grant-fundings-in-r/index.html#visualization-in-ggplot2",
    "href": "posts/2022-05-12-web-scrapping-grant-fundings-in-r/index.html#visualization-in-ggplot2",
    "title": "Web Scraping Academia Institute’s Grant Fundings using R",
    "section": "\n2 Visualization in ggplot2",
    "text": "2 Visualization in ggplot2\n\n## funding per project\ndat1 &lt;- dat |&gt; \n  group_by(topic) |&gt; \n  summarise(funding_amount_mean = mean(funding_amount, na.rm = T)) |&gt; \n  mutate(topic = fct_reorder(topic, desc(funding_amount_mean)))\n\nggplot(dat1) +\n  aes(x = topic, y = funding_amount_mean) +\n  geom_col(fill = \"darkblue\") +\n  scale_y_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6)) +\n  labs(y = \"funding per project\", title = \"Funding for each project during 2019 to 2022\") +\n  theme(legend.position = \"none\", text = element_text(size = 12)) # remove lengend\n\n\n\n\n\n\n\n\n## Total funding amount\ndat2 &lt;- dat |&gt; \n  group_by(topic) |&gt; \n  summarise(\n    funding_amount_sum = sum(funding_amount, na.rm = T), \n    n = n()) |&gt; \n  mutate(\n    topic = fct_reorder(topic, desc(funding_amount_sum)),\n    highlight = ifelse(funding_amount_sum == max(funding_amount_sum), 1, 0) |&gt; as.factor())\n\nggplot(dat2) +\n  aes(x = topic, y = funding_amount_sum, fill = highlight) +\n  geom_col() +\n  geom_text(aes(label = round(funding_amount_sum/ 10^6, 3)), vjust = 0.001, size = 5) +\n  geom_label(aes(label = n), vjust = 0.999, size = 5, color = \"white\") +\n  scale_fill_manual(values = c(\"darkblue\", \"red2\")) +\n  labs(x = \"\", y = \"total amount of funding\", \n       title = \"Total Amount of Funding and Number of Grant Projects during 2019-2022\", \n       subtitle = \"Active research at Health Promotion Center from 2019 to 2022\",\n       caption = \"source: https://healthpromotionresearch.org/Active-Studies/\") +\n  scale_y_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6)) +\n  theme(legend.position = \"none\", text = element_text(size = 10)) # remove lengend"
  },
  {
    "objectID": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html",
    "href": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html",
    "title": "Latent Class Model: Batch Mplus using R on Mac",
    "section": "",
    "text": "Mplus is a very popular program for varied latent varaible modeling: (1) Structural equation modeling (2) multilevel analysis (3) mixture modeling (latent class model). R, in the other hand, is a general statistical programming language which not limits to certain statistical models. However, there are scarcely online resources illustrate how to run Mplus using R on Mac."
  },
  {
    "objectID": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html#requirement",
    "href": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html#requirement",
    "title": "Latent Class Model: Batch Mplus using R on Mac",
    "section": "\n1 Requirement",
    "text": "1 Requirement\nTo make sure Mplus can been successfully called in R, the very first thing is checking whether your MacOS can use command-line version of Mplus. Simply type Mplus command in your terminal app on Mac, if you see “Mplus VERSION (Mac)”, then Mplus on MacOS should work and can be called within R. Otherwise, you should check whether you install Mplus software successfully beforehand.\n\nSecond, you should install R + Rstudio (Optional). Rstudio is optional but strongly recommended.\nThird, MplusAutomation package should also been installed in R:\n\nif(!require(MplusAutomation)) install.packages(\"MplusAutomation\")\nlibrary(tidyverse)\nlibrary(purrr) # for map functions\nlibrary(furrr) # for parallel map\n\nIf everything looks good, we are ready to go."
  },
  {
    "objectID": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html#latent-class-analysis",
    "href": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html#latent-class-analysis",
    "title": "Latent Class Model: Batch Mplus using R on Mac",
    "section": "\n2 Latent Class Analysis",
    "text": "2 Latent Class Analysis\nA vanilla Mplus input file for latent class model look like followings:\n\nTITLE:\nBatch Analysis of Latent Class Models;\nDATA:\nFILE = \"Code/Mplus/datFile.dat\";\n \nVARIABLE:\nNAMES = Male Age Chinese EduF_Med EduF_High EduM_Med EduM_High EduS_Med EduS_High YM\n     HY SD PSD YASB YRCS YO YCR SEN Subgroup CE1 SC1 SI1 YCDC1 CLDH1; \n MISSING=.;\n \n      CATEGORICAL = Male-SEN;\n      CLASSES = c(10);\n      AUXILIARY = CE1-CLDH1 (R3STEP);\n      \nANALYSIS:\n\n      TYPE = MIXTURE;\n      \nOUTPUT:\nTECH1 TECH8 TECH11;\nSAVEDATA:\n\n      FILE IS model1_savedata.txt;\n      SAVE IS cprob;\n      FORMAT IS free;\n\nIt looks very cumbersome to write one by one line by hand. For example, VARIABLE &gt; NAMES in Mplus input file contains all indicators and auxiliary variables used for latent class modeling. Any typos may give rise to Mplus throwing out error messages in the estimation.\nThankfully, there’s no need to write the Mplus syntax manually in text editor. Instead, you can write R code to automatically compile Mplus input files for you. In the following sections, I will illustrate how to do that using a real example.\n\n2.1 An example\nThe example makes use of a data set including nine variables of demographic variables of youth, such as gender, age, ethnicity, education levels for father, mother and self etc. The goal is to cluster youth into multiple latent classes based on their demographic characteristics.\n\ndat &lt;- read.csv(\"exampleDat.csv\", row.names = \"X\")\nglimpse(dat)\n\nRows: 2,175\nColumns: 9\n$ Male      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, …\n$ Age       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Chinese   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ EduF_Med  &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, …\n$ EduF_High &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, …\n$ EduM_Med  &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ EduM_High &lt;int&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, …\n$ EduS_Med  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ EduS_High &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nUsing mplusObject function, we can create a R object (or a Mplus model object stored in R), which holds all necessary sections for Mplus input syntax of latent class modeling. As shown in the code chunk below, users specify each Mplus section by assigning text strings to arguments. The only difference is that we don’t have to put variables’ names into a text file, instead, we can assign a R dataframe - dat to this model object directly. Then, it can be read by mplusModeler function later to run a latent class model with 2 latent classes.\nIn other words, we assigned text strings to three arguments -VARIABLE, ANALYSIS, SAVEDATA in mplusObject, which correspond to the sections in Mplus input file. Moreover, the most convenient part of MplusAutomation is allowing users to input variables’ names directly (usevariables = colnames(dat)). That is, rdata and usevariables arguments can be used to specify the variables’ names used for LCA in Mplus, which is equal to NAMES = in vanilla mplus input file.\nAfter we run and extract Mplus results using mplusModeler function, the next step is to parse and extract results from another Mplus model object with results - res_m1. Here, I’m interested in model fit indices of the model, then I can use get_summaries function in R to extract those information of this 2-class latent class model. Pretty neat, right?\n\n#------------#\n# 2-class model\n#------------#\nm1 &lt;- mplusObject(\n  TITLE = \"Latent Class Models using R;\",\n  VARIABLE = \"\n  CATEGORICAL = Male-EduS_High;\n  CLASSES = c(2);\",\n  ANALYSIS = \"\n    TYPE = MIXTURE;\n    ALGORITHM = INTEGRATION;\n    STARTS = 500 20;\n  \",\n  SAVEDATA = \"\n    FILE IS LCA_M1_save.txt;\n    SAVE IS cprob;\n    FORMAT IS free;\n  \",\n  OUTPUT = \"TECH1 TECH8;\",\n  rdata = dat,\n  usevariables = colnames(dat),\n  autov = TRUE\n)\nsuppressWarnings(res_m1 &lt;- mplusModeler(m1, modelout = \"Mplus/model.inp\", run = 1L))\nsummary_m1 &lt;- get_summaries(res_m1, simplify = T)\nkbl(summary_m1, digits = 3) |&gt; \n  kable_material_dark(full_width = F, html_font = \"Maven Pro\") |&gt; \n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\"))\n\n\n\n\n\nModel_1\n\n\n\nMplus.version\n8.7\n\n\nTitle\nLatent Class Models using R;\n\n\nAnalysisType\nMIXTURE\n\n\nDataType\nINDIVIDUAL\n\n\nEstimator\nMLR\n\n\nObservations\n2175\n\n\nNGroups\n1\n\n\nNDependentVars\n9\n\n\nNIndependentVars\n0\n\n\nNContinuousLatentVars\n0\n\n\nNCategoricalLatentVars\n1\n\n\nParameters\n19\n\n\nChiSqCategoricalPearson_Value\n2255.894\n\n\nChiSqCategoricalPearson_DF\n491\n\n\nChiSqCategoricalPearson_PValue\n0\n\n\nChiSqCategoricalLRT_Value\n1669.323\n\n\nChiSqCategoricalLRT_DF\n491\n\n\nChiSqCategoricalLRT_PValue\n0\n\n\nChiSqMCARUnrestrictedPearson_Value\n54.839\n\n\nChiSqMCARUnrestrictedPearson_DF\n637\n\n\nChiSqMCARUnrestrictedPearson_PValue\n1\n\n\nChiSqMCARUnrestrictedLRT_Value\n26.06\n\n\nChiSqMCARUnrestrictedLRT_DF\n637\n\n\nChiSqMCARUnrestrictedLRT_PValue\n1\n\n\nLL\n-8527.781\n\n\nLLCorrectionFactor\n1.0086\n\n\nAIC\n17093.562\n\n\nBIC\n17201.573\n\n\naBIC\n17141.207\n\n\nEntropy\n0.936\n\n\nAICC\n17093.9146682135\n\n\nFilename\nmodel.out\n\n\nNLatentClasses\n2\n\n\n\n\n\n\nThis is a simplistic procedure for one model. What about a bunch of models?\n\n2.2 Batch LCA in R: select number of latent classes\nOne most important step of latent class analysis is to select the number of latent classes, which best summarize the data. This research question can be translated to “when we have multiple alternative latent class models with different number of latent class (C), which one has optimal model fit”. The general rule of model selection is using information criterion such as AIC, BIC. Alternatively, model comparison method such as likelihood ratio test can be used to compare nested models. Here, I created a function to batch run multiple LCA across different number of classes to select the optimal models.\n\nfitMplus &lt;- function(data = dat, class, \n                     uvars = \"Male-EduS_High\", auxvars= NULL, \n                     modelout = NULL, savedata = NULL ) {\n  ## specify the used variables\n  fitIndices &lt;- c(\"LL\", \"AIC\", \"BIC\", \"aBIC\", \"Entropy\", \"T11_LMR_PValue\")\n  \n  if (is.null(savedata)) {\n    savedata = \"Mplus/model_Temp_save.txt\"\n  }\n  \n  if (!is.null(auxvars)) {\n    start_index = which(colnames(data) == str_split(uvars, \"-\")[[1]][1])\n    end_index = which(colnames(data) == str_split(auxvars, \"-\")[[1]][2])\n    modelTEMP &lt;- mplusObject(\n      TITLE = \"Batch Analysis of Latent Class Models;\",\n      VARIABLE = paste0(\"\n      CATEGORICAL = \", uvars, \";\n      CLASSES = c(\", class, \");\n      AUXILIARY = \", auxvars,\" (R3STEP);\n      \"),\n      ANALYSIS = \"\n      TYPE = MIXTURE;\n      \",\n      SAVEDATA = \n      paste0(\"\n      FILE IS \", savedata,\";\n      SAVE IS cprob;\n      FORMAT IS free;\n      \"),\n      OUTPUT = \"TECH1 TECH8 TECH11;\",\n      rdata = data,\n      usevariables = colnames(data[start_index:end_index]),\n      autov = TRUE\n    )\n  }else{\n    start_index = which(colnames(data) == str_split(uvars, \"-\")[[1]][1])\n    end_index = which(colnames(data) == str_split(uvars, \"-\")[[1]][2])\n    modelTEMP &lt;- mplusObject(\n      TITLE = \"Batch Analysis of Latent Class Models;\",\n      VARIABLE = paste0(\"\n      CATEGORICAL = \", uvars, \";\n      CLASSES = c(\", class, \");\n      \"),\n      ANALYSIS = \"\n      TYPE = MIXTURE;\n      \",\n      SAVEDATA = \n      paste0(\"\n      FILE IS \", savedata,\";\n      SAVE IS cprob;\n      FORMAT IS free;\n      \"),\n      OUTPUT = \"TECH1 TECH8 TECH11;\",\n      rdata = data,\n      usevariables = colnames(data[start_index:end_index]),\n      autov = TRUE\n    )\n  }\n  \n  if (!is.null(modelout)) {\n    suppressMessages(resTEMP &lt;- mplusModeler(modelTEMP, modelout = modelout, run = 1L))\n  }else{\n    suppressMessages(resTEMP &lt;- mplusModeler(modelTEMP, modelout = \"Mplus/modelTEMP.inp\", run = 1L))\n  }\n  \n  summaryTEMP &lt;- get_summaries(resTEMP, simplify = T)\n  summaryTEMP &lt;- data.frame(X = unlist(summaryTEMP[rownames(summaryTEMP) %in% fitIndices, ]))\n  colnames(summaryTEMP) = paste0(class, \"-class Model\")\n  \n  # return\n  list(\n    model = resTEMP,\n    summary = t(summaryTEMP)  \n  )\n}\n\nThen, I ran a series of latent class models with different number of classes (from 2 to 7 latent classes):\n\nlcmFit = as.data.frame(Reduce(rbind, future_map(\n  2:7, \\(x) fitMplus(data = dat, class = x, \n                     uvars = \"Male-EduS_High\",\n                     modelout = \"Mplus/model1fit.inp\",\n                     savedata = \"Mplus/model_Temp_save.txt\")$summary\n)))\nknitr::kable(lcmFit, digits = 3) |&gt; \n  kable_material_dark(full_width = F, html_font = \"Maven Pro\") |&gt; \n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\"))\n\n\n\n\n\nLL\nAIC\nBIC\naBIC\nEntropy\nT11_LMR_PValue\n\n\n\n2-class Model\n-8527.781\n17093.56\n17201.57\n17141.21\n0.936\n0\n\n\n3-class Model\n-8240.625\n16539.25\n16704.11\n16611.97\n0.952\n0\n\n\n4-class Model\n-8049.198\n16176.40\n16398.10\n16274.19\n0.969\n0\n\n\n5-class Model\n-7978.962\n16055.92\n16334.48\n16178.80\n0.951\n0\n\n\n6-class Model\n-7917.136\n15952.27\n16287.67\n16100.22\n0.990\n0\n\n\n7-class Model\n-7849.506\n15837.01\n16229.26\n16010.04\n0.934\n0\n\n\n\n\n\n\nAs the table shown, 7-class model appears to have the best model fit among 6 alternative models (lowest AIC/BIC/aBIC)."
  },
  {
    "objectID": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html#conclusion",
    "href": "posts/2023-06-10-latent-class-model-batch-mplus-using-r-on-mac/index.html#conclusion",
    "title": "Latent Class Model: Batch Mplus using R on Mac",
    "section": "\n3 Conclusion",
    "text": "3 Conclusion\nMplusAutomation + R could be good tools for LCA. However, extracting information from Mplus output file is not so easy. For example, when there are auxiliary variables exists, the regression coefficients regressed on latent classes needed to be parsed and extract manually. For example, following functions can be used to extract the regression coefficients of latent class model using the 3-step approach.\n\n#------------#\n# Extract output regression table from R3Step LCA\n#------------#\nextract_r3step &lt;- function(model_path){\n  # browser()\n  tibble(x = read_lines(model_path)) %&gt;%\n    mutate(row1 = (1:n())[str_detect(x, 'NUMBER OF OBSERVATIONS USED')],\n           row2 = (1:n())[str_detect(x, \n                                     '^ODDS RATIOS FOR TESTS OF CATEGORICAL LATENT VARIABLE')]) %&gt;%\n    slice((.$row1[1] + 2):(.$row2[1] - 1)) %&gt;%\n    mutate(type_comparison = str_detect(x, 'Parameterization using Reference'),\n           type_colname = str_detect(x, 'Estimate'),\n           type_classnum = str_detect(x, 'C#[0-9]+'),\n           type_predictor = str_detect(x, '[\\\\.\\\\-0-9]+[\\\\s]+[\\\\.\\\\-0-9]+')) %&gt;%\n    mutate(class = ifelse(type_classnum, parse_number(x), NA),\n           predictors = ifelse(type_predictor, map(x, parse_predictors), list()),\n           comparison_group = cumsum(type_comparison)) %&gt;%\n    group_by(comparison_group) %&gt;%\n    mutate(class_group = cumsum(type_classnum)) %&gt;%\n    ungroup() %&gt;%\n    mutate(comparison_group = ifelse(comparison_group == 0, \n                                     max(class, na.rm = TRUE), comparison_group)) %&gt;%\n    group_by(comparison_group, class_group) %&gt;%\n    filter(class_group != 0) %&gt;%\n    mutate(class_group2 = class[!is.na(class)]) %&gt;%\n    ungroup() %&gt;%\n    filter(type_predictor) %&gt;%\n    select(comparison_group, class_group2, predictors) %&gt;%\n    unnest() %&gt;%\n    rename(comparison_class = comparison_group,\n           class = class_group2) %&gt;%\n    mutate(estimate = as.numeric(estimate),\n           se = as.numeric(se),\n           tval = as.numeric(tval),\n           pval = as.numeric(pval)) \n}\n  \n#------------#\n# Read in save data from Mplus\n#------------#\nread_saveddata &lt;- function(path_output, path_savedata, ...) {\n  ## extract data and latent classes\n  mplusOuput &lt;- readLines(path_output)\n  top &lt;- which(str_detect(mplusOuput, \"Order of variables\")) # start linenumber\n  bottom &lt;- which(str_detect(mplusOuput, \"Save file format\")) # end linenumber\n  rawVars &lt;- mplusOuput[(top+1):(bottom-1)]\n  savevarnames &lt;- trimws(rawVars[rawVars != \"\"])\n  \n  ## read saved data from Mplus output\n  savedDat &lt;- read.table(path_savedata, na.strings = \"*\", header = FALSE, \n                         col.names = savevarnames, ...)\n  savedDat\n}\n\nHope this is helpful."
  },
  {
    "objectID": "posts/2017-11-20-EFA-and-CFA/index.html",
    "href": "posts/2017-11-20-EFA-and-CFA/index.html",
    "title": "EFA v.s. CFA",
    "section": "",
    "text": "I always found exploratory tools and confirmatory tools have distinct fans. The fans of exploratory tools believe the conclusion should be data-driven, nothing else beyond data is needed in order to keep object. On the other hand, some confirmatory fans believe that data could provide nothing without context.\n\nDaniel (1988) stated that factor analysis is “designed to examine the covariance structure of a set of variables and to provide an explanation of the relationships among those variables in terms of a smaller number of unobserved latent variables called factors.”\nRecently, when I was working on my project about Confirmatory Factor Analysis, I found the model indicated very bad model fit (e.g. CFI, TLI, RMSEA, SRMR). Then, should I use modification indices or go back to exploratory factor analysis to find some potential structural issues? Both paths have their own supporters and opposers. EFA path supports claim that EFA could provide some data structure that CFA couldn’t. Allowing cross-loading items, communalities could be calculated. But its disadvantage is that data-driven method are strongly influenced by data. If the data is bad, the EFA will give you wrong answers.\n\n\nCFA is a confirmatory technique - it is theory driven. Therefore, the planning of the analysis is driven by the theoretical relationships among the observed and unobserved variables. When a CFA is conducted, the researcher uses a hypothesized model to estimate a population covariance matrix that is compared with the observed covariance matrix. Technically, the researcher wants to minimize the difference between the estimated and observed matrices.\nCFA model believe that context of the research should be guild line to data. Thus, the model specification should be theory-driven. If the model does not fit the data very well, modification as well as residual variance-covariance matrix could be use to inspect the issues. The problem is that model modification does not lead the final model to true model according to many simulation studies. In that way, CFA models are as similar as EFA."
  },
  {
    "objectID": "posts/2017-11-20-EFA-and-CFA/index.html#big-question",
    "href": "posts/2017-11-20-EFA-and-CFA/index.html#big-question",
    "title": "EFA v.s. CFA",
    "section": "",
    "text": "I always found exploratory tools and confirmatory tools have distinct fans. The fans of exploratory tools believe the conclusion should be data-driven, nothing else beyond data is needed in order to keep object. On the other hand, some confirmatory fans believe that data could provide nothing without context.\n\nDaniel (1988) stated that factor analysis is “designed to examine the covariance structure of a set of variables and to provide an explanation of the relationships among those variables in terms of a smaller number of unobserved latent variables called factors.”\nRecently, when I was working on my project about Confirmatory Factor Analysis, I found the model indicated very bad model fit (e.g. CFI, TLI, RMSEA, SRMR). Then, should I use modification indices or go back to exploratory factor analysis to find some potential structural issues? Both paths have their own supporters and opposers. EFA path supports claim that EFA could provide some data structure that CFA couldn’t. Allowing cross-loading items, communalities could be calculated. But its disadvantage is that data-driven method are strongly influenced by data. If the data is bad, the EFA will give you wrong answers.\n\n\nCFA is a confirmatory technique - it is theory driven. Therefore, the planning of the analysis is driven by the theoretical relationships among the observed and unobserved variables. When a CFA is conducted, the researcher uses a hypothesized model to estimate a population covariance matrix that is compared with the observed covariance matrix. Technically, the researcher wants to minimize the difference between the estimated and observed matrices.\nCFA model believe that context of the research should be guild line to data. Thus, the model specification should be theory-driven. If the model does not fit the data very well, modification as well as residual variance-covariance matrix could be use to inspect the issues. The problem is that model modification does not lead the final model to true model according to many simulation studies. In that way, CFA models are as similar as EFA."
  },
  {
    "objectID": "posts/2017-11-20-EFA-and-CFA/index.html#which-one-should-be-trust",
    "href": "posts/2017-11-20-EFA-and-CFA/index.html#which-one-should-be-trust",
    "title": "EFA v.s. CFA",
    "section": "2 Which one should be trust?",
    "text": "2 Which one should be trust?\nI think it depends on your research project. If your research project has strong theoretical support then CFA should be first step. If your research is new, there’re few related researches about that. Then try EFA first."
  },
  {
    "objectID": "posts/2023-06-27-one-template-for-quarto-report/index.html",
    "href": "posts/2023-06-27-one-template-for-quarto-report/index.html",
    "title": "One Template for Quarto Report",
    "section": "",
    "text": "R Codelibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nR Codelibrary(hexSticker)\n\np &lt;- ggplot(aes(x = Sepal.Length, y = Sepal.Width), data = iris) + \n  geom_point(shape = 6, color = \"white\")\np &lt;- p + theme_void() + theme_transparent()\n\nsticker(p, package=\"quarto\", p_size=20, s_x=1, s_y=.75, \n        s_width=0.7, s_height = 0.7,\n        h_fill = \"#B4464B\", filename=\"ggplot2.png\")\n\nknitr::include_graphics(\"ggplot2.png\")\n\n\n\n\n\n\n\n#| echo: fenced\ntitle: \"NEET Analysis\"\nauthor: \"Jihong Zhang\"\nformat: \n  html:\n    header-includes: |\n      &lt;link href=\"https://fonts.googleapis.com/css2?family=Ysabeau+Infant&display=swap\" rel=\"stylesheet\"&gt;\n    mainfont: 'Ysabeau Infant'\n    self-contained: true\n    toc: true\n    code-fold: true\n    code-summary: ' R Code'\n    code-line-numbers: true\n    number-sections: true\n    number-offset: 1\n\nR Code```{r}\n#| label: setup\n#| eval: false\n#| message: false\n#| warning: false\nroot_dir = \"/Users/clap/OneDrive - The Chinese University of Hong Kong/01_CLAP@JC/LatentClassModel\"\nknitr::opts_chunk$set(echo = TRUE, message=FALSE, warnings=FALSE, include = FALSE)\nknitr::opts_knit$set(root.dir = root_dir)\nlibrary(here)\nlibrary(glue)\nlibrary(bruceR)\nlibrary(xtable)\nlibrary(formattable) # format styles of table \nlibrary(reshape2)\nlibrary(tidyverse)\nlibrary(ggtext) \nlibrary(kableExtra)\nlibrary(MplusAutomation)\nlibrary(fastDummies)\noptions(knitr.kable.NA = '')\nmycolors = c(\"#4682B4\", \"#B4464B\", \"#B4AF46\", \n             \"#1B9E77\", \"#D95F02\", \"#7570B3\",\n             \"#E7298A\", \"#66A61E\", \"#B4F60A\")\nsoftcolors = c(\"#B4464B\", \"#F3DCD4\", \"#ECC9C7\", \n               \"#D9E3DA\", \"#D1CFC0\", \"#C2C2B4\")\nmykbl &lt;- function(x, ...){\n  kbl(x, digits = 2, ...) |&gt; kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) \n}\n```\n\n\nOutput example\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html",
    "href": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html",
    "title": "A Conversation between R and Python on Data Analysis and Machine Learning",
    "section": "",
    "text": "When I play with machine learning using python and R, I keep wondering whether I can combine R and Python code in a better way, such as using R for data cleaning/visualization and Python for machine learning. Online resources for PyTorch are basically written using python code. However, I always struggle with pandas and numpy but prefer tidyverse for data cleaning, and data manipulation. That’s why I write this blog. I will start with the Pytorch tensor manipulation and import Pytorch tensor into R, then talk about mixing R/python data analysis with R/python visualization."
  },
  {
    "objectID": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#pytorch-tensor",
    "href": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#pytorch-tensor",
    "title": "A Conversation between R and Python on Data Analysis and Machine Learning",
    "section": "\n1 PyTorch Tensor",
    "text": "1 PyTorch Tensor\nLet’s first get familiar with data manipulation using PyTorch. First, torch.zeors(R, C) function can create a zero tensor with the size of R by C.\n\n⌘+C```{python, echo = \"fenced\"}\nimport torch\n\nz = torch.zeros(5, 3)\nprint(z)\n```\n\n&gt;&gt; tensor([[0., 0., 0.],\n&gt;&gt;         [0., 0., 0.],\n&gt;&gt;         [0., 0., 0.],\n&gt;&gt;         [0., 0., 0.],\n&gt;&gt;         [0., 0., 0.]])\n\n\nSimilarly, torch.ones(R, C) creates a tensor filled with ones with the size of R by C with dtype=torch.int16 argument can coverting elements into integer.\n\n⌘+C```{python}\ni = torch.ones((5,3), dtype=torch.int16)\nprint(i)\n```\n\n&gt;&gt; tensor([[1, 1, 1],\n&gt;&gt;         [1, 1, 1],\n&gt;&gt;         [1, 1, 1],\n&gt;&gt;         [1, 1, 1],\n&gt;&gt;         [1, 1, 1]], dtype=torch.int16)\n\n\nWe can generate random numbers to tensor using torch.rand(R, C). It’s recommended to set up a random seed using torch.manual_seed(seed_num) to make sure we can replicate our random tensor.\n\n⌘+C```{python}\ntorch.manual_seed(1729)\nr1 = torch.rand(2, 2)\nprint(f'A random tensor:\\n {r1}')\n```\n\n&gt;&gt; &lt;torch._C.Generator object at 0x10b69d210&gt;\n&gt;&gt; A random tensor:\n&gt;&gt;  tensor([[0.3126, 0.3791],\n&gt;&gt;         [0.3087, 0.0736]])\n\n\n\n⌘+C```{python}\nb = torch.arange(4 * 5 * 6).view(4, 5, 6)\nprint(f\"A sequence of number arraged to 4 matrix of the size 5 X 6: \\n{b}\\n\" )\n```\n\n&gt;&gt; A sequence of number arraged to 4 matrix of the size 5 X 6: \n&gt;&gt; tensor([[[  0,   1,   2,   3,   4,   5],\n&gt;&gt;          [  6,   7,   8,   9,  10,  11],\n&gt;&gt;          [ 12,  13,  14,  15,  16,  17],\n&gt;&gt;          [ 18,  19,  20,  21,  22,  23],\n&gt;&gt;          [ 24,  25,  26,  27,  28,  29]],\n&gt;&gt; \n&gt;&gt;         [[ 30,  31,  32,  33,  34,  35],\n&gt;&gt;          [ 36,  37,  38,  39,  40,  41],\n&gt;&gt;          [ 42,  43,  44,  45,  46,  47],\n&gt;&gt;          [ 48,  49,  50,  51,  52,  53],\n&gt;&gt;          [ 54,  55,  56,  57,  58,  59]],\n&gt;&gt; \n&gt;&gt;         [[ 60,  61,  62,  63,  64,  65],\n&gt;&gt;          [ 66,  67,  68,  69,  70,  71],\n&gt;&gt;          [ 72,  73,  74,  75,  76,  77],\n&gt;&gt;          [ 78,  79,  80,  81,  82,  83],\n&gt;&gt;          [ 84,  85,  86,  87,  88,  89]],\n&gt;&gt; \n&gt;&gt;         [[ 90,  91,  92,  93,  94,  95],\n&gt;&gt;          [ 96,  97,  98,  99, 100, 101],\n&gt;&gt;          [102, 103, 104, 105, 106, 107],\n&gt;&gt;          [108, 109, 110, 111, 112, 113],\n&gt;&gt;          [114, 115, 116, 117, 118, 119]]])\n\n\n\n1.1 Arithmetic operations\n\n⌘+C```{python}\nones = torch.ones(2, 3)\nprint(f\"First tensor: \\n{ones}\\n\")\n\ntwos = torch.ones(2, 3) * 2 # multiple each element by 2\nprint(f\"Second tensor: \\n{twos}\\n\")\n\nthrees = ones + twos # addition allowed when shapes of two tensors are similar\nprint(f\"Third tensor: \\n{threes}\\n\")\n```\n\n&gt;&gt; First tensor: \n&gt;&gt; tensor([[1., 1., 1.],\n&gt;&gt;         [1., 1., 1.]])\n&gt;&gt; \n&gt;&gt; Second tensor: \n&gt;&gt; tensor([[2., 2., 2.],\n&gt;&gt;         [2., 2., 2.]])\n&gt;&gt; \n&gt;&gt; Third tensor: \n&gt;&gt; tensor([[3., 3., 3.],\n&gt;&gt;         [3., 3., 3.]])\n\n\n\n1.2 More examples of the matrix operations:\n\n⌘+C```{python}\ntorch.manual_seed(1729)\na = (torch.rand(2, 2) - 0.5) * 2\nprint(f'A random matrix: \\n{a}\\n')\n\na_abs = torch.abs(a) # Absolute values for elements\nprint(f'A absolute matrix: \\n{a_abs}\\n')\n\na_asin = torch.asin(a) # trigonometric functions\nprint(f'Inverse sine of r: \\n{a_asin}\\n')\n\na_det = torch.det(a) # Determinant and singular value decomposition\nprint(f'Determinant of r: \\n{a_det}\\n')\na_svd = torch.svd(a) \nprint(f'Singlular value decomposition of r: \\n{a_svd}\\n')\n\na_std_mean = torch.std_mean(a) # Statistical and aggregate operations:\nprint(f'Average and standard deviation of r: \\n{a_std_mean}\\n')\na_max = torch.max(a)\nprint(f'Maximum value of r: \\n{a_max}\\n')\n```\n\n&gt;&gt; &lt;torch._C.Generator object at 0x10b69d210&gt;\n&gt;&gt; A random matrix: \n&gt;&gt; tensor([[-0.3748, -0.2418],\n&gt;&gt;         [-0.3827, -0.8528]])\n&gt;&gt; \n&gt;&gt; A absolute matrix: \n&gt;&gt; tensor([[0.3748, 0.2418],\n&gt;&gt;         [0.3827, 0.8528]])\n&gt;&gt; \n&gt;&gt; Inverse sine of r: \n&gt;&gt; tensor([[-0.3842, -0.2442],\n&gt;&gt;         [-0.3927, -1.0214]])\n&gt;&gt; \n&gt;&gt; Determinant of r: \n&gt;&gt; 0.22711339592933655\n&gt;&gt; \n&gt;&gt; Singlular value decomposition of r: \n&gt;&gt; torch.return_types.svd(\n&gt;&gt; U=tensor([[-0.3909, -0.9204],\n&gt;&gt;         [-0.9204,  0.3909]]),\n&gt;&gt; S=tensor([1.0111, 0.2246]),\n&gt;&gt; V=tensor([[ 0.4933,  0.8699],\n&gt;&gt;         [ 0.8699, -0.4933]]))\n&gt;&gt; \n&gt;&gt; Average and standard deviation of r: \n&gt;&gt; (tensor(0.2678), tensor(-0.4630))\n&gt;&gt; \n&gt;&gt; Maximum value of r: \n&gt;&gt; -0.24180912971496582\n\n\n\n1.3 listwise computation\n\n⌘+C```{python}\na_totalSum = torch.sum(a)\nprint(f'global sum of r: \\n{a_totalSum}\\n')\n\na_rowSum = torch.sum(a, dim=1, keepdim=True)\nprint(f'row sums of r: \\n{a_rowSum}\\n')\n\na_colSum = torch.sum(a, dim=0, keepdim=True)\nprint(f'column sums of r: \\n{a_colSum}\\n')\n```\n\n&gt;&gt; global sum of r: \n&gt;&gt; -1.852110505104065\n&gt;&gt; \n&gt;&gt; row sums of r: \n&gt;&gt; tensor([[-0.6166],\n&gt;&gt;         [-1.2355]])\n&gt;&gt; \n&gt;&gt; column sums of r: \n&gt;&gt; tensor([[-0.7575, -1.0946]])"
  },
  {
    "objectID": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#import-tensor-to-r",
    "href": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#import-tensor-to-r",
    "title": "A Conversation between R and Python on Data Analysis and Machine Learning",
    "section": "\n2 Import tensor to R",
    "text": "2 Import tensor to R\nIf you install torch package in R. You probably found that Pytorch’s tensor object can be called directly in R as torch.Tensor.\n\n⌘+C```{r}\na_inR = py$a\nclass(a_inR)\n```\n\n&gt;&gt; [1] \"torch.Tensor\"          \"torch._C.TensorBase\"   \"python.builtin.object\"\n\n\nUnfortunately, it does not allow you to manipulate this tensor object directly using R function. For example, if you do matrix operation, it will pop up error message.\n\n⌘+C```{r, error=TRUE}\na_inR * 2\n```\n\n&gt;&gt; Error in a_inR * 2: non-numeric argument to binary operator\n\n\nHowever, you can use python method in R style (change . to $) as long as you load reticulate package. Like:\n\n⌘+C```{r, error=TRUE}\na_inR$shape\na_inR$data\n```\n\n&gt;&gt; torch.Size([2, 2])\n&gt;&gt; tensor([[-0.3748, -0.2418],\n&gt;&gt;         [-0.3827, -0.8528]])\n\n\nCan we import PyTorch tensor to R and employ R to do some dirty works, and then export it back to python? The answer is Yes (partially). We can try out this workflow: tensor -&gt; np.ndarray -&gt; R matrix -&gt; …(some manipulation) -&gt; np.ndarray -&gt; tensor (see diagram below). I’m not sure if it is worthy nor it is even plausible for more complicated tensor class. Note that it may make the memory stores at least three copies of the data. More experiments needed for such work. For more information about arrays in R and python, please refer to the reticulate manual.\n\n\n\n\n\ngraph LR;\n    tensor--&gt;|np$array|np.ndarray;\n    np.ndarray--&gt;|py_to_r|R.matrix;\n    R.matrix --&gt;|some data manipulation|R.matrix;\n    R.matrix--&gt;|r_to_py|np.ndarray;\n    np.ndarray--&gt;|torch$from_numpy|tensor;\n\n\n\n\n\n\n\n⌘+C```{r}\nnp &lt;- import(\"numpy\", convert=FALSE)\na_data_inR = py_to_r(np$array(a_inR$data))\na_data_inR_revised = a_data_inR * 2\na_inR # original python object in R\na_data_inR_revised # revised matrix in R\n```\n\n&gt;&gt; tensor([[-0.3748, -0.2418],\n&gt;&gt;         [-0.3827, -0.8528]])\n&gt;&gt;            [,1]       [,2]\n&gt;&gt; [1,] -0.7496002 -0.4836183\n&gt;&gt; [2,] -0.7653229 -1.7056797\n\n\n\n⌘+C```{r, error=TRUE}\ntorch &lt;- import(\"torch\", convert=FALSE)\na_inR$data = torch$from_numpy(a_data_inR_revised)\na_inR\n```\n\n&gt;&gt; tensor([[-0.7496, -0.4836],\n&gt;&gt;         [-0.7653, -1.7057]], dtype=torch.float64)"
  },
  {
    "objectID": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#mixture-of-r-and-python-visualization",
    "href": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#mixture-of-r-and-python-visualization",
    "title": "A Conversation between R and Python on Data Analysis and Machine Learning",
    "section": "\n3 Mixture of R and Python: visualization",
    "text": "3 Mixture of R and Python: visualization\nAs mentioned above, the reticulate R package provides an R interface to Python modules, classes, and functions, which allows us to extend our graphical toolbox to all packages/modules of R and Python. Currently, we can use three methods of data visualization via mixing R code and Python code:\n\nR data + Python matlibplot module\nPython data + Python Vega-Altair module + R output\nPython data + R ggplot2 package\n\nLet’s try them out one by one. First, take iris data in R for example:\n\n⌘+C```{r}\nlibrary(tidyverse)\nlibrary(kableExtra)\niris_dat = iris[1:3]\niris_mat = matrix(unlist(iris_dat), nrow = nrow(iris), ncol = ncol(iris_dat))\ndim(iris_mat)\n```\n\n&gt;&gt; [1] 150   3\n\n\nThe column sums of iris are c(876.5, 458.6, 564.7) and the row sums of first 5 rows of iris are c(10, 9.3, 9.2, 9.2, 10).\n\n⌘+C```{r}\nglue::glue('Columns Sums: {colSums(iris_mat)}')\nglue::glue('Row Sums: {rowSums(iris_mat[1:5, ])}')\n```\n\n&gt;&gt; Columns Sums: 876.5\n&gt;&gt; Columns Sums: 458.6\n&gt;&gt; Columns Sums: 563.7\n&gt;&gt; Row Sums: 10\n&gt;&gt; Row Sums: 9.3\n&gt;&gt; Row Sums: 9.2\n&gt;&gt; Row Sums: 9.2\n&gt;&gt; Row Sums: 10\n\n\nIn Python chunk code of Rmarkdown, we can call data in R using r.{dataname}, in which {dataname} is the object name in R. Note that do not define r in python, otherwise r not longer become the R environment. Now we can load the R data frame in python. Please check reticulate package for more information about the rules of conversation between R and Python.\n\n⌘+C```{python}\nimport torch\nimport numpy as np\niris_ndarray = np.array(r.iris_mat)\niris_tensor = torch.from_numpy(iris_ndarray)\ntorch.sum(iris_tensor, dim=0, keepdim=True)\ntorch.sum(iris_tensor[:5,], dim=1, keepdim=True)\n```\n\n&gt;&gt; tensor([[876.5000, 458.6000, 563.7000]], dtype=torch.float64)\n&gt;&gt; tensor([[10.0000],\n&gt;&gt;         [ 9.3000],\n&gt;&gt;         [ 9.2000],\n&gt;&gt;         [ 9.2000],\n&gt;&gt;         [10.0000]], dtype=torch.float64)\n\n\nIn Edgar Anderson’s Iris data, we can model a prediction model given the measurements in centimeters of sepal length/width, petal length/width and the three types of species: Iris setosa, versicolor, and virginica. Each type contains 50 samples. The very first question is then “can we train a machine learning algorithm to predict whether one belongs to Setosa/Versicolor/Virginica given four characteristics?” We can fit a multinomial regression to test statistical hypotheses of the characteristics’ differences among three species:\n\n⌘+C```{r}\n#| label: tbl-coef\n#| tbl-cap: Coefficients of multinominal logistic regression\nlibrary(nnet)\nlibrary(MASS)\nlibrary(broom)\nlibrary(kableExtra)\niris_new = iris |&gt; \n  mutate(Species = relevel(Species, \"virginica\"))\n## A multinominal logistic regression with Species-setosa as reference group\ncapture.output(multinomial_fit &lt;- nnet::multinom(Species ~ . + 0, data = iris_new, \n                                                 model = TRUE),file =\"/dev/null\")\n## Print the coefficient table\nkbl(tidy(multinomial_fit, exponentiate = FALSE), digits = 2, \n    booktabs = TRUE, align = \"c\",\n    col.names = c(\"DV\", \"IV\", \"b(logit)\", \"SE\", \"t.value\", \"p.value\")) |&gt;\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\"), html_font = \"Maven Pro\")\n```\n\n\nTable 1: Coefficients of multinominal logistic regression\n\n\n\n\n\nDV\nIV\nb(logit)\nSE\nt.value\np.value\n\n\n\nsetosa\nSepal.Length\n8.11\n106.99\n0.08\n0.94\n\n\nsetosa\nSepal.Width\n13.13\n157.28\n0.08\n0.93\n\n\nsetosa\nPetal.Length\n-18.55\n76.14\n-0.24\n0.81\n\n\nsetosa\nPetal.Width\n-13.70\n34.79\n-0.39\n0.69\n\n\nversicolor\nSepal.Length\n6.33\n2.48\n2.55\n0.01\n\n\nversicolor\nSepal.Width\n6.62\n2.53\n2.62\n0.01\n\n\nversicolor\nPetal.Length\n-8.44\n3.47\n-2.43\n0.02\n\n\nversicolor\nPetal.Width\n-10.28\n3.46\n-2.97\n0.00\n\n\n\n\n\n\n\n\n\nAs shown in Table 1 , it seems that multinominal logistic regression suggests none of the four features can significantly differentiate species but this could be misleading. This is because there are underlying correlations between sepal width/length and petal width/length. Next step is we can use matplotlib module to explore the relationships among four characteristics.\n\n⌘+C```{python, out.width = \"80%\", out.height = \"100%\", dpi = 200, fig.retina = 1, error = TRUE}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport random\nimport seaborn\n\nseaborn.set(style='whitegrid'); seaborn.set_context('talk')\n\nfrom sklearn.datasets import load_iris\niris_data = load_iris()\n\nn_samples, n_features = iris_data.data.shape\n\ndef Show_Diagram(x_label,y_label,title):\n    plt.figure(figsize=(10,8))\n    plt.scatter(iris_data.data[:,x_label], iris_data.data[:,y_label], c=iris_data.target, cmap=cm.viridis, alpha = 0.5, label = iris_data.target_names)\n    plt.xlabel(iris_data.feature_names[x_label]); plt.ylabel(iris_data.feature_names[y_label]); plt.title(title)\n    plt.legend(('setosa', 'versicolor', 'virginica'))\n    plt.show();x_label = 2;y_label=3;title='Petal'\n\nShow_Diagram(0,1,'Sepal')\nShow_Diagram(2,3,'Petal')\n```\n\n&gt;&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: dlopen(/Users/jihong/venv-metal/lib/python3.11/site-packages/PIL/_imaging.cpython-311-darwin.so, 0x0002): symbol not found in flat namespace '_jpeg_resync_to_restart'\n\n\n&gt;&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: dlopen(/Users/jihong/venv-metal/lib/python3.11/site-packages/PIL/_imaging.cpython-311-darwin.so, 0x0002): symbol not found in flat namespace '_jpeg_resync_to_restart'\n\n\n&gt;&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: dlopen(/Users/jihong/venv-metal/lib/python3.11/site-packages/PIL/_imaging.cpython-311-darwin.so, 0x0002): symbol not found in flat namespace '_jpeg_resync_to_restart'\n\n\n&gt;&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'seaborn' is not defined\n\n\n&gt;&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'plt' is not defined\n\n\n&gt;&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'plt' is not defined\n\n\nAlternatively, we can use python data and module Vega-Altair to compile a interactive graphical object to a json file (.to_json()). Then we can plot it in R using as_vegaspec function.\n\n⌘+C```{python compile}\n# import altair with an abbreviated alias\nimport altair as alt\nimport pandas as pd\nlabel_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n        \niris = pd.DataFrame(\n    data= np.c_[iris_data['data'], iris_data['target']],\n    columns= iris_data['feature_names'] + ['target']\n    )\niris['target'] = iris['target'].replace(label_dict)\n\nchart1 = alt.Chart(iris).mark_point().encode(\n  alt.X('sepal length (cm):Q').scale(domain=(4,9)),\n  alt.Y('sepal width (cm):Q').scale(domain=(1.5,5)),\n  alt.Color('target:N').scale(scheme='dark2'),\n  alt.Shape('target:N')\n).interactive()\nvw = chart1.to_json()\n```\n\n\n\n⌘+C```{r display}\n#| label: fig-vega1\n#| fig-cap: An interactive plot using python altair\n#| fig-cap-location: top\n#| results: asis\nas_vegaspec(py$vw)\n```\n\nAn interactive plot using python altair\n\nAlternatively, we can use ggplot2 R package + python data to plot the scatter plots, which I prefer.\n\n⌘+C```{r}\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(dplyr)\n## A function to convert python iris to R version of iris\ntidy_python_data &lt;- function(iris_from_python) {\n  dat &lt;- iris_from_python$data\n  dat &lt;- apply(dat, 2, as.numeric)\n  colnames(dat) &lt;- iris_from_python$feature_names\n  Species = as.character(factor(iris_from_python$target, labels = iris_from_python$target_names))\n  dat = as.data.frame(cbind(dat, Species = Species))\n  dat &lt;- dat |&gt; \n    mutate(across(-Species, \\(x) as.numeric(x)))\n  dat\n}\n```\n\n\n\n⌘+C```{r}\nlibrary(emojifont)\niris_from_python = tidy_python_data(iris_from_python = py$iris_data)\nggplot(iris_from_python) +\n  geom_point(aes(x = `sepal length (cm)`, y = `sepal width (cm)`, color = Species, shape = Species), size = 2, alpha = 0.8) +\n  labs(title = 'Sepal', caption = 'Love you Melody  ♥ !') +\n  scale_color_brewer(palette = 'Dark2')\n```\n\n\n\n\n\n\n\n\n⌘+C```{r}\nggplot(iris_from_python) +\n  geom_point(aes(x = `petal length (cm)`, y = `petal width (cm)`, color = Species, shape = Species), size = 2, alpha = 0.8) +\n  labs(title = 'Petal', caption = 'Love you Melody  ♥ !') +\n  scale_color_brewer(palette = 'Dark2')\n```\n\n\n\n\n\n\n\nIt appears that its hard to differentiate versicolor with verginica in terms of sepal and petal but setosa is more smaller in petal width/length and sepal length but relative long sepal width."
  },
  {
    "objectID": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#pure-r-torch-for-r",
    "href": "posts/2023-06-20-pytorch-a-conversation-between-r-and-python-on-data-analysis-and-machine-learning/index.html#pure-r-torch-for-r",
    "title": "A Conversation between R and Python on Data Analysis and Machine Learning",
    "section": "\n4 Pure R: torch for R",
    "text": "4 Pure R: torch for R\nAlternatively, there is a R package called torch, which is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities. However, to the date I wrote this post, the version of torch is 0.11.0, which suggests that the package is still far from well developed.\n\n⌘+C```{r}\nlibrary(torch)\nlibrary(luz)\nlibrary(torchvision)\n# torch_tensor(1, device = 'cpu')\n```"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#outline",
    "href": "posts/2022-10-23-thesis-defense/index.html#outline",
    "title": "Dissertation Defence",
    "section": "0.1 Outline",
    "text": "0.1 Outline\n\n\nBackground (5 minutes)\nPerformance measures (5 minutes)\nSimulation study (15 minutes)\nEmpirical study (10 minutes)\nConclusion (5 minutes)\nDiscussion (5 minutes)"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#research-motivation",
    "href": "posts/2022-10-23-thesis-defense/index.html#research-motivation",
    "title": "Dissertation Defence",
    "section": "1.1 Research Motivation",
    "text": "1.1 Research Motivation\n\nQ-matrix is usually determined by expert judgement, so there can be uncertainty about some of its elements. Model selection methods are necessary to select the model with the “correct” Q-matrix.\nPrevious model selection methods such as information criterion and Bayes Factors are not flexible regarding checking specific aspects of data\n\n1.1.1 Posterior predictive checking (PPC)\nAdvantages of PPC\n\nPPC is a flexible tool and implemented in most Bayesian software.\n\nDrawbacks of PPC\n\nPPC is not fully Bayesian since it doesn’t take the uncertainty of observed data into account\nPPC uses data twice\n\n\nModel comparison approach for Bayesian psychometric models are not well investigated. Use global fit to evaluate fit and then use local misfit detect method to find local misfit. Then repeat it until model fits."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#research-objectives",
    "href": "posts/2022-10-23-thesis-defense/index.html#research-objectives",
    "title": "Dissertation Defence",
    "section": "1.2 Research Objectives",
    "text": "1.2 Research Objectives\n\nTo construct a novel PPMC method using limited-information model fit indices in Bayesian LCDM\nSimulation study: to determine the performance of the proposed method under different conditions and compare it to previous model checking methods\nEmpirical study: to investigate the utility of PPMC with limited-information model fit indices in real settings"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#overview-proposed-approach",
    "href": "posts/2022-10-23-thesis-defense/index.html#overview-proposed-approach",
    "title": "Dissertation Defence",
    "section": "1.3 Overview: Proposed Approach",
    "text": "1.3 Overview: Proposed Approach\n\n\n\n\n\nflowchart LR\n  style C fill:#99ff99,stroke:#333,stroke-width:2px\n  style K fill:#99ff99,stroke:#333,stroke-width:2px\n  \n  style H fill:#ff99cc,stroke:#333,stroke-width:2px\n  style M fill:#ff99cc,stroke:#333,stroke-width:2px\n  \n  style E fill:#f9f,stroke:#333,stroke-width:2px\n  style F fill:#f9f,stroke:#333,stroke-width:2px\n  style G fill:#f9f,stroke:#333,stroke-width:2px\n  \n  Z[(Data)] --&gt; A & E & I\n\n  subgraph Model 1\n  A[LCDM 1] --&gt;|Bayesian \\n estimate| B(Posterior\\n Distribution)\n  B --&gt;|sample| C(Posterior \\n Predictive \\n M2 fa:fa-star)\n  C --&gt; D{{KS Test}}\n  D --&gt; H[KS-PP-M2 fa:fa-star]\n  end\n\n  subgraph Reference: BayesNet Model\n  E[BayesNet] --&gt; |Bayesian \\n estimate| F(Posterior \\n Distribution)\n  F --&gt;|sample| G(Posterior \\n Predictive \\n M2 fa:fa-star)\n  end\n  G --&gt; D & L\n\n  subgraph Model 2\n  I[LCDM 2] --&gt;|Bayesian \\n estimate| J(Posterior\\n Distribution)\n  J --&gt;|sample| K(Posterior \\n Predictive \\n M2 fa:fa-star)\n  K --&gt; L{{KS Test}}\n  L --&gt; M[KS-PP-M2 fa:fa-star]\n  end\n\n  H & M --&gt; N{Decision}"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#performance-measures",
    "href": "posts/2022-10-23-thesis-defense/index.html#performance-measures",
    "title": "Dissertation Defence",
    "section": "1.4 Performance measures",
    "text": "1.4 Performance measures\n\nCognitive diagnostic index - item/test discrimination index\nPosterior predictive M2 - absolute fit\nKS-PP-M2 and InfoCrit (AIC/BIC/DIC/WAIC) - relative fit\n\n\nHow many information items contains to discriminate people with different attribute profiles?"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#research-questions",
    "href": "posts/2022-10-23-thesis-defense/index.html#research-questions",
    "title": "Dissertation Defence",
    "section": "2.1 Research Questions",
    "text": "2.1 Research Questions\n\nIs the proposed method appropriate for detecting model-data misfit with varied degree of Q-matrix misspecification\nCompared to information criteria, does the proposed approach have higher true positive rate (TPR) when selecting the correct model?\nHow does the overall discrimination power indicated by Cognitive Diagnostic Index affects the performance of the proposed method in selecting the model with best Q-matrix?"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#simulation-settings",
    "href": "posts/2022-10-23-thesis-defense/index.html#simulation-settings",
    "title": "Dissertation Defence",
    "section": "2.2 Simulation Settings",
    "text": "2.2 Simulation Settings\n\nGenerate simulated data sets under the LCDM framework with two main factors:\n\nsample size: 200 draws from 1000 to 2000\nattribute correlation: {0.25, 0.5}\n400 conditions in total\n\n30 items and 5 attributes\nLatent attributes: mastery status of attributes for each individual are determined by cutting attribute scores. Item parameters are randomly sampled.\n\nBased on attribute correlation, continuous attribute scores are first generated for each sample.\nThen continuous attribute scores are dichonomized by cutting the scores with the cutting scores\nFinally. observed item responses are generated with attribute status and corresponding item parameters"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#analysis-models",
    "href": "posts/2022-10-23-thesis-defense/index.html#analysis-models",
    "title": "Dissertation Defence",
    "section": "2.3 Analysis Models",
    "text": "2.3 Analysis Models\n\n\n\nBayesian Network model\nData generation model - LCDM\nModel with 3 items (10%) underspecify attributes\nModel with 6 items (20%) underspecify attributes\nModel with 3 items (10%) misspecify attributes\nModel with 6 items (20%) misspecify attributes\n\n\nQ-matrix\n\n\n\n\n\nCorrect model\n\n\n\n\n\n\n\n10% underspecify\n\n\n\n\n\n\n\n20% underspecify\n\n\n\n\n\n\n\n10% misspecify\n\n\n\n\n\n\n\n20% misspecify"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#mcmc-settings",
    "href": "posts/2022-10-23-thesis-defense/index.html#mcmc-settings",
    "title": "Dissertation Defence",
    "section": "2.4 MCMC settings",
    "text": "2.4 MCMC settings\n\nAll Bayesian models are estimated using blatent R package in R version 3.6\n4 MCMC with 4000 iterations with first 1000 discarded\nPrior distribution are set up by default of blatent.\nAll parameters estimation converged with PSRF &lt; 1.1"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#summary-of-posterior-predictive-m2-500-draws",
    "href": "posts/2022-10-23-thesis-defense/index.html#summary-of-posterior-predictive-m2-500-draws",
    "title": "Dissertation Defence",
    "section": "3.1 Summary of Posterior Predictive M2 (500 draws)",
    "text": "3.1 Summary of Posterior Predictive M2 (500 draws)\n\n\n\n\nTable 1: Mean and SD of Posterior Predictive M2 for Models across conditions\n\n\n\n\n\n\n\n\nBayesNet\nCorrect1\nUnderspecified Qmatrix\nIncorrect specified Qmatrix\n\n\n10%\n20%\n10%\n20%\n\n\n\n\nSkill Correlation is .25\n\n\n[1000,1250)\n23.23(1.94)\n21.49(2.08)\n28.92(3.19)\n33.51(3.97)\n27.92(3.36)\n30.02(3.49)\n\n\n[1250,1500)\n30.48(3.13)\n29.03(2.90)\n40.88(6.60)\n48.82(8.19)\n39.66(5.47)\n42.73(5.95)\n\n\n[1500,1750)\n39.36(3.82)\n38.30(4.15)\n57.56(8.70)\n69.60(10.70)\n53.99(8.13)\n58.41(9.03)\n\n\n[1750,2000)\n48.96(3.09)\n49.55(3.65)\n75.40(10.02)\n90.90(11.19)\n68.45(7.70)\n75.26(9.15)\n\n\nSkill Correlation is .50\n\n\n[1000,1250)\n24.83(2.97)\n24.07(2.95)\n32.25(5.36)\n36.69(5.78)\n32.59(5.58)\n34.44(5.43)\n\n\n[1250,1500)\n34.24(3.70)\n33.72(3.58)\n47.70(6.60)\n54.91(7.26)\n47.04(6.00)\n49.96(5.94)\n\n\n[1500,1750)\n43.38(3.11)\n45.76(3.96)\n63.14(7.27)\n74.70(9.73)\n65.10(7.17)\n69.42(8.05)\n\n\n[1750,2000)\n55.54(3.48)\n60.55(4.74)\n94.06(10.59)\n111.40(12.80)\n90.52(9.36)\n97.18(10.01)\n\n\n\n1 Bold font: The model with smallest average values of PP-M2.\n\n\n\n\n\n\n\n\n\n\n\nThis table presents the posterior predictive M2 results for all six models across different conditions. Upper tables is for skill correlation equals to .25. Lower table is for .5\nFor all condition, BayesNet model and Correct model have best average absolute model fit. As sample size goes above 1700, BayesNet model is best model. Higher level of Q-matrix misspecification will worsen model fit."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#path-plot-for-pp-m2",
    "href": "posts/2022-10-23-thesis-defense/index.html#path-plot-for-pp-m2",
    "title": "Dissertation Defence",
    "section": "3.2 Path plot for PP-M2",
    "text": "3.2 Path plot for PP-M2\n\nTitle: Path Plot for Average Posterior Predictive M2"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#findings",
    "href": "posts/2022-10-23-thesis-defense/index.html#findings",
    "title": "Dissertation Defence",
    "section": "3.3 Findings",
    "text": "3.3 Findings\n\nThe correct model and the BayesNet model have lowest PP-M2 (best fit).\n\nWhen sample size is relatively small, the correct model slightly fit better than the BayesNet;\nWhen sample size is relatively large, the BayesNet fits better than the correct model\n\nAs sample size increases, the difference of PP-M2 among models gets larger. In other words, the PP-M2 has asymptotically more power detecting misfit.\nThe BayesNet model has least uncertainty of model predictive accuracy in term of variations of average PP-M2\nAs more items misspecify/underspecify attributes in Q-matrix, the PP-M2 gets higher."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#compare-ks-pp-m2-to-other-methods",
    "href": "posts/2022-10-23-thesis-defense/index.html#compare-ks-pp-m2-to-other-methods",
    "title": "Dissertation Defence",
    "section": "3.4 Compare KS-PP-M2 to other methods",
    "text": "3.4 Compare KS-PP-M2 to other methods\n\n\n\n\nTable 2: Mean and SD of Model Selection Indices By Models across conditions\n\n\n\n\n\n\n\n\nModels\nInformation Criterion1,2\nKS-PP-M2\n\n\nDIC\nWAIC\nAIC\nBIC\n\n\n\n\nSkill Correlation is .25\n\n\nCorrect\nModel 1\n44,301(8,816)\n44,303(8,817)\n44,387(8,812)\n45,291(8,846)\n0.18(0.04)\n\n\nUnderspecified\nModel 2: 10%\n44,869(8,931)\n44,872(8,932)\n44,961(8,927)\n45,865(8,961)\n0.66(0.14)\n\n\nUnderspecified\nModel 3: 20%\n45,400(9,028)\n45,406(9,030)\n45,500(9,024)\n46,404(9,058)\n0.83(0.10)\n\n\nMisspecified\nModel 4: 10%\n44,842(8,905)\n44,845(8,907)\n44,932(8,902)\n45,805(8,934)\n0.60(0.13)\n\n\nMisspecified\nModel 5: 20%\n45,110(8,952)\n45,113(8,953)\n45,188(8,949)\n45,997(8,979)\n0.70(0.12)\n\n\nSkill Correlation is .50\n\n\nCorrect\nModel 1\n45,607(9,219)\n45,612(9,221)\n45,699(9,215)\n46,614(9,250)\n0.33(0.11)\n\n\nUnderspecified\nModel 2: 10%\n46,152(9,341)\n46,159(9,342)\n46,250(9,337)\n47,165(9,372)\n0.75(0.13)\n\n\nUnderspecified\nModel 3: 20%\n46,659(9,448)\n46,668(9,450)\n46,763(9,444)\n47,677(9,479)\n0.86(0.10)\n\n\nMisspecified\nModel 4: 10%\n46,172(9,326)\n46,178(9,327)\n46,268(9,322)\n47,150(9,356)\n0.75(0.13)\n\n\nMisspecified\nModel 5: 20%\n46,403(9,373)\n46,409(9,374)\n46,485(9,369)\n47,304(9,401)\n0.80(0.10)\n\n\n\n1 Bold: The model with smallest average value of model selection indice.\n\n\n2 Information Criterion & KS-PP-M2: lower values better model fit\n\n\n\n\n\n\n\n\n\n\n\nThis table shows the average values of fit indices across 200 conditions. All fit indices shows correct model has lowest values, which means best model fit."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#true-positive-rate",
    "href": "posts/2022-10-23-thesis-defense/index.html#true-positive-rate",
    "title": "Dissertation Defence",
    "section": "3.5 True Positive Rate:",
    "text": "3.5 True Positive Rate:\n\nTrue Positive Rates of choosing correct model\n\nThis table represents the average TPR for KS-PP-M2 and four information criteria for each sample size and two levels of attribute correlation.\nAll approaches have 100% power choosing the correct model among five analysis models."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#effects-of-cognitive-diagnostic-index",
    "href": "posts/2022-10-23-thesis-defense/index.html#effects-of-cognitive-diagnostic-index",
    "title": "Dissertation Defence",
    "section": "3.6 Effects of Cognitive Diagnostic Index",
    "text": "3.6 Effects of Cognitive Diagnostic Index"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#effects-of-cognitive-diagnostic-index-cont.",
    "href": "posts/2022-10-23-thesis-defense/index.html#effects-of-cognitive-diagnostic-index-cont.",
    "title": "Dissertation Defence",
    "section": "3.7 Effects of Cognitive Diagnostic Index (Cont.)",
    "text": "3.7 Effects of Cognitive Diagnostic Index (Cont.)\n\n\nThe figure shows the relationship between KS-PP-M2 along with Cognitive Diagnostic Index. As cognitive diagnostic index of data sets increase, KS-PP-M2 for each analysis models are quite stable.\nThe trend of KS-PP-M2 is also proven by regression model of CDI and models. Mean centered CDI has no signitifcant effect on KS-PP-M2 values."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#conclusion-simulation-study",
    "href": "posts/2022-10-23-thesis-defense/index.html#conclusion-simulation-study",
    "title": "Dissertation Defence",
    "section": "3.8 Conclusion (Simulation study)",
    "text": "3.8 Conclusion (Simulation study)\n\nPosterior predictive M2 statistics showed the Bayesian Network model and the correct model have best model fit.\nSimilar to information criteria, KS-PP-M2 can select data generation model from models with Q-matrix misspecification\nHigher Q-matrix misspecification, KS-PP-M2 has higher values, which suggest worse model fit.\nCompared to other methods, KS-PP-M2 has same power of selecting the better model and detecting Q-matrix misspecification under all conditions.\nCDI (test-level discrimination power) has insignificant effect on the fit statistics of the proposed method."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#research-questions-1",
    "href": "posts/2022-10-23-thesis-defense/index.html#research-questions-1",
    "title": "Dissertation Defence",
    "section": "4.1 Research Questions",
    "text": "4.1 Research Questions\n\nHow the proposed approach can be used for the model selection in real settings?\nIs the performance of the proposed approach comparable to other IC methods?"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#design",
    "href": "posts/2022-10-23-thesis-defense/index.html#design",
    "title": "Dissertation Defence",
    "section": "4.2 Design",
    "text": "4.2 Design\n\nThe Examination for Certificate of Proficiency in English (ECPE) data was used as the example data.\nOne reference model and two analysis models: (1) three-dimensional model (the best fitted model in Templin & Hoffman, 2013); (2) two-dimensional model with randomly generated Q-matrix.\nMeasures: (1) absolute fit: PP-M2, (2) relative fit: KS-PP-M2, DIC and WAIC"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#data-and-settings",
    "href": "posts/2022-10-23-thesis-defense/index.html#data-and-settings",
    "title": "Dissertation Defence",
    "section": "4.3 Data and Settings",
    "text": "4.3 Data and Settings\n\nECPE data has 2,922 test takers and 28 items.\nBayesian estimation were used with 2000 iterations and 1000 discarded burn-ins.\nPrior distribution setups are by default of blatent package"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#density-plot-of-posterior-predictive-m2",
    "href": "posts/2022-10-23-thesis-defense/index.html#density-plot-of-posterior-predictive-m2",
    "title": "Dissertation Defence",
    "section": "5.1 Density Plot of Posterior predictive M2",
    "text": "5.1 Density Plot of Posterior predictive M2\n\nNote: solid line (the BayesNet model); dotted model (the three-dimension model); dashed line (the two-dimension model)."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#model-selection-indices",
    "href": "posts/2022-10-23-thesis-defense/index.html#model-selection-indices",
    "title": "Dissertation Defence",
    "section": "5.2 Model Selection Indices",
    "text": "5.2 Model Selection Indices\n\nNote: Model 1 (three dimensional model); Model 2 (two dimensional model)."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#findings-emprical-study",
    "href": "posts/2022-10-23-thesis-defense/index.html#findings-emprical-study",
    "title": "Dissertation Defence",
    "section": "5.3 Findings (Emprical study)",
    "text": "5.3 Findings (Emprical study)\n\nAccording to the graphical checking of PP-M2, the BayesNet model is the best-fitting model, then followed by the three-dimensional model. The two-dimensioanl model has worst model fit.\nDIC, WAIC, KS-PP-M2 all suggested that the three-dimensional model is better than the two-dimensional model.\nKS-PP-M2 suggested that neither the three-dimensional model and the two-dimensional model have close model fit with BayesNet model."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#conclusion",
    "href": "posts/2022-10-23-thesis-defense/index.html#conclusion",
    "title": "Dissertation Defence",
    "section": "5.4 Conclusion",
    "text": "5.4 Conclusion\n\nIn both simulation and empirical study, posterior predictive M2 suggest BayesNet models and data generation model have close model fit statistics.\nKS statistics for posterior predictive M2 (KS-PP-M2) has same power detecting Q-matrix misspecification with other IC methods according TPR.\nDiscrimination power of data has insignificant relationship with the proposed model checking indices\nKS-PP-M2 provides graphical checking for the variation of model fit indices."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#discussion",
    "href": "posts/2022-10-23-thesis-defense/index.html#discussion",
    "title": "Dissertation Defence",
    "section": "5.5 Discussion",
    "text": "5.5 Discussion\n\nWhen comparing multiple models, varied model selection methods are recommended to report.\nIn Bayesian analysis, AIC/BIC are not fully Bayesian. They are not recommended in Bayesian framework. DIC has problems such as it may produce negative estimates of the effective number of parameters in a model and it is not defined for model with discrete parameters.\nWAIC is fully Bayesian and asymptotically equal to Bayesian cross-validation. LOO is also based on cross-validation approach.\nKS-PP-M2 approach is full Bayesian and provide uncertainty of the observed data.\nKS-PP-M2 could be a relative fit and posterior predictive M2 is a absolute fit.\nKS-PP-M2 is based on the BayesNet model\nKS-PP-M2 does not relied on likelihood function.\nKS-PP-M2 takes advatages of limited-information and potentially works for data with missing data."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#reference",
    "href": "posts/2022-10-23-thesis-defense/index.html#reference",
    "title": "Dissertation Defence",
    "section": "6.1 Reference",
    "text": "6.1 Reference\n\n\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-information-criterion",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-information-criterion",
    "title": "Dissertation Defence",
    "section": "6.2 Supplement: Information Criterion",
    "text": "6.2 Supplement: Information Criterion\n\nDIC / WAIC\n\nDIC is a somewhat Bayesian version of AIC that makes two changes, replacing the maximum likelihood estimate \\(\\theta\\) with the posterior mean and replacing k with a data-based bias correction.1\n\n\nWAIC is a more fully Bayesian approach for estimating the out-of-sample expectation, starting with the computed log pointwise posterior predictive density and then adding a correction for effective number of parameters to adjust for overfitting.\n\n\n\nDIC does not the whole posterior information and does not provide uncertainty of fit statistics.\nWAIC provides uncertainty (SE) and is popular but is not flexible to test certain aspects of data.\nPosterior predictive checking is flexible but not full Bayesian. It also has some theoretical issue.\n\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 997–1016. https://doi.org/10.1007/s11222-013-9416-2"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-ii-posterior-predictive-check",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-ii-posterior-predictive-check",
    "title": "Dissertation Defence",
    "section": "6.3 Supplement II: Posterior predictive check",
    "text": "6.3 Supplement II: Posterior predictive check\n\nSimulating replicated data under the fitted model and then comparing these to the observed data (Gelman & Hill, 2006, p. 158)\n\nAims:\n\ncheck local and global model-fit for some aspects of data they’re interested in\nprovide graphical evidence about model fit"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-iii-using-the-data-twice",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-iii-using-the-data-twice",
    "title": "Dissertation Defence",
    "section": "6.4 Supplement III: “using the data twice”",
    "text": "6.4 Supplement III: “using the data twice”\nOne critique of posterior predictive check is it uses the data twice (Blei, 2011), which means data is not only used for estimating the model but also for checking if the model fits to the data.\n\nThis is a bad idea, because it violates the likelihood principle.\nA typical way in statistics and machine learning literature:\n\nValidate the model on external data\n\nFor first solution, methods include cross-validation approach,"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-iv-model-selection-problem-exist-when",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-iv-model-selection-problem-exist-when",
    "title": "Dissertation Defence",
    "section": "6.5 Supplement IV: Model Selection Problem exist when",
    "text": "6.5 Supplement IV: Model Selection Problem exist when\n\n\nmultiple alternative models existed\nuncertaity of dimensionality\nQ-matrix misspecification\n\n\n\nThe proposed method aims to provide a fully Bayesian model selection approach or relative fit indice. This study will only focus on Q-matrix detection in model selection."
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-v-factors-of-model-selection-indices",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-v-factors-of-model-selection-indices",
    "title": "Dissertation Defence",
    "section": "6.6 Supplement V: Factors of Model selection indices",
    "text": "6.6 Supplement V: Factors of Model selection indices\n\nIn fully Bayesian framework, the posterior inference is a comparison between prior information and data. When sample size is small, the posterior information is controlled by prior information which diminish the difference of models.\n\n\nSample size\nDiscrimination information\nQ-matrix\nModel structure"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-vi-fit-measures",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-vi-fit-measures",
    "title": "Dissertation Defence",
    "section": "6.7 Supplement VI: Fit Measures",
    "text": "6.7 Supplement VI: Fit Measures\n6.7.1 Posterior Predictive M2\n\nM2 is a limited-information statistics which calculated up-to second probabilities of item responses.\n\nM2 more robust than full-information fit statistics in small sample sizes.\nPP-M2 is M2 values conditional on posterior information. Lower average values suggest better model fit.\n6.7.2 Cognitive diagnostic Index"
  },
  {
    "objectID": "posts/2022-10-23-thesis-defense/index.html#supplement-vii-ks-statistics",
    "href": "posts/2022-10-23-thesis-defense/index.html#supplement-vii-ks-statistics",
    "title": "Dissertation Defence",
    "section": "6.8 Supplement VII: KS Statistics",
    "text": "6.8 Supplement VII: KS Statistics\n\n\n\n\nMean of reference distribution:\n\n\n\n\n\nMean of target distribution:\n\n\n\n\nAdd another model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThesis Defence 2022"
  },
  {
    "objectID": "posts/2022-05-20-visualization-for-nsf-grant-on-bayesian-cognitive-diagnosis/index.html",
    "href": "posts/2022-05-20-visualization-for-nsf-grant-on-bayesian-cognitive-diagnosis/index.html",
    "title": "NSF Grant on Bayesian Cognitive Diagnosis",
    "section": "",
    "text": "The NSF grant awards can be easily searched via NSF website. Check Active Awards and type “Bayesian & Cognitive Diagnosis” into the search box. Two things I’m really interested in: (1) Who as Principal Investigator (PI) obtained most NSF awards? (2) Which institute obtained most NSF awards?\nlibrary(tidyverse)\nawards &lt;- read.csv(\"Awards.csv\")\nawards &lt;- awards |&gt;\n  filter(NSFOrganization == \"SES\") # select social economic sciences"
  },
  {
    "objectID": "posts/2022-05-20-visualization-for-nsf-grant-on-bayesian-cognitive-diagnosis/index.html#who-get-most-awards",
    "href": "posts/2022-05-20-visualization-for-nsf-grant-on-bayesian-cognitive-diagnosis/index.html#who-get-most-awards",
    "title": "NSF Grant on Bayesian Cognitive Diagnosis",
    "section": "\n1 Who get most awards",
    "text": "1 Who get most awards\n\npi_amount &lt;- awards |&gt; \n  select(PI = PrincipalInvestigator, Org = Organization, Amount = AwardedAmountToDate, Title) |&gt; \n  filter(PI!=\"\") |&gt; \n  mutate(Org = str_replace_all(Org, pattern = \" -DO NOT USE\", \"\")) |&gt; \n  mutate(Amount = str_replace_all(Amount, pattern = \"[\\\\$\\\\,]\", \"\")) |&gt; \n  mutate(Amount = as.numeric(Amount)) |&gt; \n  mutate(Title = str_replace(Title, \"    \", \" \")) |&gt; \n  mutate(Title = str_replace(Title, \"NCRN-MN: \", \"\")) |&gt; \n  mutate(Title = str_replace(Title, \"Collaborative Research: \", \"\")) |&gt; \n  mutate(Title = str_replace(Title, \"CAREER: \", \"\")) |&gt; \n  mutate(Title = sub(x = Title, pattern = '(?&lt;=.{85})', replacement = '\\n', perl = TRUE)) |&gt; \n  mutate(Title = str_replace(Title, \"^ \", \"\")) |&gt; \n  mutate(PI = paste0(PI, \"\\n\", Org))\n\npi_amount_p &lt;- pi_amount |&gt; \n  group_by(PI) |&gt; \n  summarise(\n    Amount_sum = sum(Amount),\n    Award_num = n(),\n    Title = Title[1]\n  ) |&gt; \n  mutate(PI = fct_reorder(PI, Amount_sum)) |&gt; \n  arrange(desc(PI)) |&gt; \n  head(50) |&gt;  # top 100 \n  mutate(Highlight = c(rep(1, 10), rep(0, 40)),\n         rank = row_number()) |&gt; \n  mutate(labels = paste0(round(Amount_sum/ 1e6, 1), \" M\"))\n\nggplot(pi_amount_p) +\n  geom_col(aes(x = PI, y = Amount_sum, fill = factor(Highlight))) +\n  # geom_text(aes(x = PI, y = 0, label = labels), hjust = 1, size = 3) +\n  geom_text(aes(x = PI, y = 0, label = Title), hjust = 0, size = 3, alpha = 0.8) +\n  scale_y_continuous(labels = scales::label_number(suffix = \" M\", scale = 1e-6), limits = c(NA, 4500000)) +\n  # scale_x_discrete(labels = PI, breaks = PI) +\n  scale_fill_manual(values = c(\"darkblue\", \"red\")) +\n  labs(title = \"NSF Social Economic Sciences: Top 50 PIs most amount of awards\\nfunded (2021-2022)\", \n       subtitle = \"Bayesian Cognitive Diagnosis\",\n       caption = 'SEARCH: \"Bayesian Cognitive Diagnosis\"\\nSource:https://nsf.gov/awardsearch',\n       x = \"PI & Orgnization\", y = \"Total Amount of Awards\") +\n  coord_flip() +\n  ggdark::dark_theme_gray() +\n  theme(legend.position = \"\", text = element_text(size = 10), title = element_text(hjust = -0.1))"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html",
    "href": "posts/2017-11-08-sem-homework/index.html",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "",
    "text": "This is one of my homework in Structural Equation Modeling in Fall 2017. Dr. Templin provided a excelent example showing how to perform Confirmatory Factor Analysis (CFA) using Lavaan Package. I elaborated each step as following.\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\nlibrary(lavaan)\n\nWarning: package 'lavaan' was built under R version 4.2.3\n\n#library(semPlot)\nlibrary(psych)\n\nWarning: package 'psych' was built under R version 4.2.3\n\nlibrary(knitr)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#background",
    "href": "posts/2017-11-08-sem-homework/index.html#background",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n1 Background",
    "text": "1 Background\n\n1.1 CFA on Attitude towards Inclusive Education Survey (N = 507)\nThe affective dimension of attitudes subscale includes 6 items on a 6-point likert scale (1 = Strongly Agree, 6 = Strongly Disagree), measuring teachers’ feelings and emotions associated with inclusive education:\n\nI get frustrated when I have difficulty communicating with students with a disability.\nI get upset when students with a disability cannot keep up with the day-to-day curriculum in my classroom.\nI get irritated when I am unable to understand students with a disability.\nI am uncomfortable including students with a disability in a regular classroom with other students without a disability.\nI am disconcerted that students with a disability are included in the regular classroom, regardless of the severity of the disability.\nI get frustrated when I have to adapt the curriculum to meet the individual needs of all students.\n\nThe sample size (N) is 507, which includes 6 males and 501 females. I used one-factor model as first step. All items are loaded on one general factor - affective attitude towards inclusive education. Higher response score means more positive attitude towards inclusive education.\n\ndat &lt;- read.csv(\"AttitudeForInclusiveEducation.csv\")\n# head(dat)\ndat2 &lt;- dat %&gt;% select(X,Aff.1:Aff.6)\ncolnames(dat2) &lt;- c(\"PersonID\", paste0(\"Aff\",1:6))"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#descriptive-statistics",
    "href": "posts/2017-11-08-sem-homework/index.html#descriptive-statistics",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n2 Descriptive Statistics",
    "text": "2 Descriptive Statistics\nThe descriptive statistics for all items are provided below. It appears that item 4 is the least difficult item as it has the highest mean (\\mu = 4.189, sd = 1.317); item 5 is the most difficult item as it has lowest mean score (\\mu = 3.604, sd = 1.423). All responses for each item range from 1 to 6 (1 = Strongly agree, 6 = Strongly disagree). Thus, all categories are responded. In term of item discrimination, as item 3 has the largest standard deviation (sd = 1.364) and item 6 has the smallest, item 3 has highest discrimination whearas item 6 has lowest in CTT.\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nPersonID\n1\n507\n254.000\n146.503\n254\n254.000\n188.290\n1\n507\n506\n0.000\n-1.207\n6.506\n\n\nAff1\n2\n507\n3.765\n1.337\n4\n3.779\n1.483\n1\n6\n5\n-0.131\n-0.927\n0.059\n\n\nAff2\n3\n507\n3.635\n1.335\n4\n3.636\n1.483\n1\n6\n5\n-0.026\n-0.963\n0.059\n\n\nAff3\n4\n507\n3.493\n1.364\n3\n3.472\n1.483\n1\n6\n5\n0.124\n-0.969\n0.061\n\n\nAff4\n5\n507\n4.189\n1.317\n4\n4.287\n1.483\n1\n6\n5\n-0.589\n-0.327\n0.058\n\n\nAff5\n6\n507\n3.604\n1.423\n4\n3.590\n1.483\n1\n6\n5\n0.000\n-0.939\n0.063\n\n\nAff6\n7\n507\n4.018\n1.313\n4\n4.061\n1.483\n1\n6\n5\n-0.356\n-0.733\n0.058\n\n\n\n\nItem-total correlation table was provided below. All item-total correlation coefficients are higher than 0.7, which suggests good internal consistence. Item 1 has lowest item-total correlation (r = 0.733, sd = 1.337).\n\n\nItem-total Correlation Table\n\n\nn\nraw.r\nstd.r\nr.cor\nr.drop\nmean\nsd\n\n\n\nAff1\n507\n0.733\n0.735\n0.652\n0.611\n3.765\n1.337\n\n\nAff2\n507\n0.835\n0.836\n0.806\n0.753\n3.635\n1.335\n\n\nAff3\n507\n0.813\n0.812\n0.771\n0.718\n3.493\n1.364\n\n\nAff4\n507\n0.789\n0.790\n0.742\n0.689\n4.189\n1.317\n\n\nAff5\n507\n0.774\n0.769\n0.702\n0.658\n3.604\n1.423\n\n\nAff6\n507\n0.836\n0.838\n0.805\n0.755\n4.018\n1.313\n\n\n\n\n\n2.1 Sample Correlation Matrix\nAccording to Pearson Correlation Matrix below, we can see all items have fairly high pearson correlation coefficients ranging from 0.44 to 0.72. This provides the evidence of dimensionality. Item 2 and item 3 has highest correlation coefficient(r_{23} = 0.717). The lowest correlations lies between item 1 and item 4 as well as item 1 and item 5.\n\ncor(dat2[2:7]) %&gt;% round(3) %&gt;% kable(caption = \"Pearson Correlation Matrix\")\n\n\n\nPearson Correlation Matrix\n\n\nAff1\nAff2\nAff3\nAff4\nAff5\nAff6\n\n\n\nAff1\n1.000\n0.590\n0.525\n0.448\n0.411\n0.538\n\n\nAff2\n0.590\n1.000\n0.717\n0.534\n0.553\n0.602\n\n\nAff3\n0.525\n0.717\n1.000\n0.505\n0.527\n0.608\n\n\nAff4\n0.448\n0.534\n0.505\n1.000\n0.609\n0.682\n\n\nAff5\n0.411\n0.553\n0.527\n0.609\n1.000\n0.577\n\n\nAff6\n0.538\n0.602\n0.608\n0.682\n0.577\n1.000\n\n\n\n\n\n\n\n2.2 Sample Mean and Variance\n\nmeans &lt;- dat2[,2:7] %&gt;% \n  summarise_all(funs(mean)) %&gt;% round(3) %&gt;% t() %&gt;% as.data.frame()\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nsds &lt;- dat2[,2:7] %&gt;% \n  summarise_all(funs(sd)) %&gt;% round(3) %&gt;% t() %&gt;% as.data.frame()\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\ntable1 &lt;- cbind(means,sds)\n\ncolnames(table1) &lt;- c(\"Mean\", \"SD\")\ntable1\n\n      Mean    SD\nAff1 3.765 1.337\nAff2 3.635 1.335\nAff3 3.493 1.364\nAff4 4.189 1.317\nAff5 3.604 1.423\nAff6 4.018 1.313\n\n\n\n2.3 Sample Item Response Distributions\nThose items did not exactly match normal distribution but acceptable.\n# stack data\ndat2_melted &lt;- dat2 %&gt;% gather(key, value,Aff1:Aff6) %&gt;% arrange(PersonID)\n\n# plot by variable\nggplot(dat2_melted, aes(value)) + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\", binwidth = 1) +\n  geom_density(alpha=.2, fill=\"#FF6666\") +\n  scale_x_continuous(breaks = 1:6) +\n  facet_wrap(~ key)\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead."
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#estimation-with-cfa",
    "href": "posts/2017-11-08-sem-homework/index.html#estimation-with-cfa",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n3 Estimation with CFA",
    "text": "3 Estimation with CFA\n\n3.1 One-factor Model\nOne-factor model was conducted as first step. The model has one latent facor - affective attitude and six indicators. In general, one-factor model does not provide great model fit except SRMR. The test statistics for chi-square is 75.835 (p &lt; 0.05). CFI is 0.929, which larger than 0.95 suggests good model fit. RMSEA is 0.121, which lower than 0.05 suggest good model fit. SRMR is 0.04, which lower than 0.08. The standardized factor loadings range from 0.66 to 0.8. All factor loadings are significant at the level of alpha equals 0.05.\n\nmodel1.syntax &lt;- '\n  AA =~ Aff1 + Aff2 + Aff3 + Aff4 + Aff5 + Aff6\n'\nmodel1 &lt;- cfa(model1.syntax, data = dat2,std.lv = TRUE, mimic = \"mplus\", estimator = \"MLR\")\nsummary(model1, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 14 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                           507\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               115.410      75.834\n  Degrees of freedom                                 9           9\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.522\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1573.730     960.267\n  Degrees of freedom                                15          15\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.639\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.932       0.929\n  Tucker-Lewis Index (TLI)                       0.886       0.882\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.934\n  Robust Tucker-Lewis Index (TLI)                            0.891\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4492.146   -4492.146\n  Scaling correction factor                                  1.138\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4434.440   -4434.440\n  Scaling correction factor                                  1.266\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                9020.291    9020.291\n  Bayesian (BIC)                              9096.404    9096.404\n  Sample-size adjusted Bayesian (SABIC)       9039.270    9039.270\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.153       0.121\n  90 Percent confidence interval - lower         0.129       0.101\n  90 Percent confidence interval - upper         0.178       0.142\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.149\n  90 Percent confidence interval - lower                     0.120\n  90 Percent confidence interval - upper                     0.181\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  AA =~                                                                 \n    Aff1              0.886    0.055   16.183    0.000    0.886    0.663\n    Aff2              1.078    0.046   23.284    0.000    1.078    0.808\n    Aff3              1.066    0.055   19.499    0.000    1.066    0.783\n    Aff4              0.968    0.061   15.934    0.000    0.968    0.736\n    Aff5              1.004    0.055   18.291    0.000    1.004    0.706\n    Aff6              1.057    0.049   21.644    0.000    1.057    0.805\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Aff1              3.765    0.059   63.455    0.000    3.765    2.818\n   .Aff2              3.635    0.059   61.382    0.000    3.635    2.726\n   .Aff3              3.493    0.060   57.741    0.000    3.493    2.564\n   .Aff4              4.189    0.058   71.689    0.000    4.189    3.184\n   .Aff5              3.604    0.063   57.057    0.000    3.604    2.534\n   .Aff6              4.018    0.058   68.948    0.000    4.018    3.062\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Aff1              1.000    0.081   12.285    0.000    1.000    0.560\n   .Aff2              0.617    0.066    9.283    0.000    0.617    0.347\n   .Aff3              0.719    0.089    8.089    0.000    0.719    0.387\n   .Aff4              0.794    0.095    8.390    0.000    0.794    0.459\n   .Aff5              1.014    0.090   11.226    0.000    1.014    0.501\n   .Aff6              0.605    0.068    8.898    0.000    0.605    0.351\n    AA                1.000                               1.000    1.000\n\n\n\n3.2 Local Misfit for One-factor Model\nBy looking into local misfit with residual variance-covariance matrix we can get the clues to improve the model. According to the model residuals, item 4 has relatively high positive residual covariance with item 5 and item 6. It suggests that the one-factor model underestimates the correlations among item 4, item 5 and item 6. In other words, another latent factor may be needed to explain the strong relations among item 4, 5, 6 which cannot be explained by a general Affective attitude factor.\nMoreover, modification indices below also suggest that adding the error covariances among item 4, 5 and 6 will improve chi-square much better.\nThus, I decided to add one more factor - AAE. AAE was labeled as affective attitude towards educational environment which indicated by item 4, 5, 6. The other latent factor - AAC which was indicated by item 1, 2, 3 was labeled as Affective Attitude towards communication.\n\nresid(model1)$cov %&gt;% kable(caption = \"Normalized Residual Variance-Covariance Matrix\",digits = 3)\n\n\n\nNormalized Residual Variance-Covariance Matrix\n\n\nAff1\nAff2\nAff3\nAff4\nAff5\nAff6\n\n\n\nAff1\n0.000\n0.095\n0.011\n-0.070\n-0.109\n0.006\n\n\nAff2\n0.095\n0.000\n0.153\n-0.106\n-0.034\n-0.085\n\n\nAff3\n0.011\n0.153\n0.000\n-0.128\n-0.051\n-0.041\n\n\nAff4\n-0.070\n-0.106\n-0.128\n0.000\n0.168\n0.155\n\n\nAff5\n-0.109\n-0.034\n-0.051\n0.168\n0.000\n0.015\n\n\nAff6\n0.006\n-0.085\n-0.041\n0.155\n0.015\n0.000\n\n\n\n\n\nmodificationindices(model1, standardized = TRUE,sort. = TRUE) %&gt;% slice(1:10) %&gt;% kable(caption = \"Modification Indices\", digits = 3)\n\n\n\nModification Indices\n\nlhs\nop\nrhs\nmi\nepc\nsepc.lv\nsepc.all\nsepc.nox\n\n\n\nAff2\n~~\nAff3\n55.906\n0.319\n0.319\n0.479\n0.479\n\n\nAff4\n~~\nAff6\n45.674\n0.279\n0.279\n0.403\n0.403\n\n\nAff4\n~~\nAff5\n25.673\n0.243\n0.243\n0.270\n0.270\n\n\nAff3\n~~\nAff4\n24.228\n-0.214\n-0.214\n-0.283\n-0.283\n\n\nAff2\n~~\nAff6\n22.494\n-0.194\n-0.194\n-0.318\n-0.318\n\n\nAff2\n~~\nAff4\n21.303\n-0.193\n-0.193\n-0.276\n-0.276\n\n\nAff1\n~~\nAff2\n11.974\n0.153\n0.153\n0.195\n0.195\n\n\nAff1\n~~\nAff5\n7.918\n-0.145\n-0.145\n-0.144\n-0.144\n\n\nAff1\n~~\nAff4\n4.309\n-0.096\n-0.096\n-0.108\n-0.108\n\n\nAff3\n~~\nAff6\n4.018\n-0.084\n-0.084\n-0.128\n-0.128\n\n\n\n\n\n\n\n3.3 Two-factor Model\nThe neccessity of adding another factor was tested by specifying a two-factor model.\nIn term of model fit indices, it appears that the global model fit indices are great with two-factor model (CFI = 0.986; RMSEA = 0.058; SRMR = 0.022). Ideally, two latent factors could be labeled as moderately correlated aspects of attitudes towards inclusive education. Thus, the first factor (AAC) could be labeled as how teachers feel about communicating with students with disability. The second (AAE) could be labeled as how teachers feel about evironment of inclusive education. All standardized factor loadings are statistically significant ranging from 0.676 to 0.865. The factor correlation between 2 factors is high (r = 0.838,p = 0.00).\n\nmodel2.syntax &lt;- '\n  AAC =~ Aff1 + Aff2 + Aff3\n  AAE =~ Aff4 + Aff5 + Aff6\n'\nmodel2 &lt;- cfa(model2.syntax, data = dat2, std.lv = TRUE, mimic = \"mplus\", estimator = \"MLR\")\nsummary(model2, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 19 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                           507\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                32.332      21.512\n  Degrees of freedom                                 8           8\n  P-value (Chi-square)                           0.000       0.006\n  Scaling correction factor                                  1.503\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1573.730     960.267\n  Degrees of freedom                                15          15\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.639\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.984       0.986\n  Tucker-Lewis Index (TLI)                       0.971       0.973\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.987\n  Robust Tucker-Lewis Index (TLI)                            0.975\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4450.606   -4450.606\n  Scaling correction factor                                  1.166\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4434.440   -4434.440\n  Scaling correction factor                                  1.266\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                8939.212    8939.212\n  Bayesian (BIC)                              9019.554    9019.554\n  Sample-size adjusted Bayesian (SABIC)       8959.246    8959.246\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.077       0.058\n  90 Percent confidence interval - lower         0.051       0.034\n  90 Percent confidence interval - upper         0.106       0.082\n  P-value H_0: RMSEA &lt;= 0.050                    0.046       0.268\n  P-value H_0: RMSEA &gt;= 0.080                    0.475       0.068\n                                                                  \n  Robust RMSEA                                               0.071\n  90 Percent confidence interval - lower                     0.036\n  90 Percent confidence interval - upper                     0.108\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.148\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.375\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.022       0.022\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  AAC =~                                                                \n    Aff1              0.903    0.055   16.357    0.000    0.903    0.676\n    Aff2              1.153    0.042   27.771    0.000    1.153    0.865\n    Aff3              1.117    0.051   22.079    0.000    1.117    0.820\n  AAE =~                                                                \n    Aff4              1.047    0.053   19.604    0.000    1.047    0.796\n    Aff5              1.036    0.053   19.366    0.000    1.036    0.728\n    Aff6              1.107    0.044   25.112    0.000    1.107    0.844\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  AAC ~~                                                                \n    AAE               0.838    0.028   29.829    0.000    0.838    0.838\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Aff1              3.765    0.059   63.455    0.000    3.765    2.818\n   .Aff2              3.635    0.059   61.382    0.000    3.635    2.726\n   .Aff3              3.493    0.060   57.741    0.000    3.493    2.564\n   .Aff4              4.189    0.058   71.689    0.000    4.189    3.184\n   .Aff5              3.604    0.063   57.057    0.000    3.604    2.534\n   .Aff6              4.018    0.058   68.948    0.000    4.018    3.062\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Aff1              0.970    0.083   11.753    0.000    0.970    0.543\n   .Aff2              0.449    0.057    7.860    0.000    0.449    0.253\n   .Aff3              0.607    0.084    7.244    0.000    0.607    0.327\n   .Aff4              0.635    0.085    7.504    0.000    0.635    0.367\n   .Aff5              0.950    0.090   10.539    0.000    0.950    0.470\n   .Aff6              0.497    0.061    8.191    0.000    0.497    0.288\n    AAC               1.000                               1.000    1.000\n    AAE               1.000                               1.000    1.000\n\n\n\n3.4 Local Misfit for two-factor model\nThe local misfit indices for two-factor model also suggest that the model fits data well. The largest normalized residuals is 1.215. Modification indices suggest that add covariance between item 5 and item 6. These local misfit is not theoretically defensible. Thus, the final model is two-factor model.\n\nresid(model2)$cov %&gt;% kable(caption = \"Normalized Residual Variance-Covariance Matrix\",digits = 3)\n\n\n\nNormalized Residual Variance-Covariance Matrix\n\n\nAff1\nAff2\nAff3\nAff4\nAff5\nAff6\n\n\n\nAff1\n0.000\n0.010\n-0.053\n-0.005\n-0.003\n0.105\n\n\nAff2\n0.010\n0.000\n0.014\n-0.075\n0.048\n-0.016\n\n\nAff3\n-0.053\n0.014\n0.000\n-0.076\n0.050\n0.049\n\n\nAff4\n-0.005\n-0.075\n-0.076\n0.000\n0.056\n0.019\n\n\nAff5\n-0.003\n0.048\n0.050\n0.056\n0.000\n-0.070\n\n\nAff6\n0.105\n-0.016\n0.049\n0.019\n-0.070\n0.000\n\n\n\n\n\nmodificationindices(model2, standardized = TRUE,sort. = TRUE) %&gt;% slice(1:10) %&gt;% kable(caption = \"Modification Indices\", digits = 3)\n\n\n\nModification Indices\n\nlhs\nop\nrhs\nmi\nepc\nsepc.lv\nsepc.all\nsepc.nox\n\n\n\nAff5\n~~\nAff6\n16.894\n-0.224\n-0.224\n-0.326\n-0.326\n\n\nAAC\n=~\nAff4\n16.893\n-0.578\n-0.578\n-0.439\n-0.439\n\n\nAff1\n~~\nAff6\n7.064\n0.108\n0.108\n0.155\n0.155\n\n\nAff4\n~~\nAff5\n5.977\n0.128\n0.128\n0.164\n0.164\n\n\nAAC\n=~\nAff6\n5.976\n0.368\n0.368\n0.280\n0.280\n\n\nAff1\n~~\nAff3\n5.093\n-0.112\n-0.112\n-0.146\n-0.146\n\n\nAAE\n=~\nAff2\n5.093\n-0.362\n-0.362\n-0.272\n-0.272\n\n\nAff3\n~~\nAff4\n4.045\n-0.077\n-0.077\n-0.124\n-0.124\n\n\nAff2\n~~\nAff3\n3.160\n0.119\n0.119\n0.228\n0.228\n\n\nAAE\n=~\nAff1\n3.160\n0.236\n0.236\n0.176\n0.176"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#path-diagram",
    "href": "posts/2017-11-08-sem-homework/index.html#path-diagram",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n4 Path Diagram",
    "text": "4 Path Diagram\n#semPlot::semPaths(model2, what = \"est\")"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#reliability-in-factor-analysis",
    "href": "posts/2017-11-08-sem-homework/index.html#reliability-in-factor-analysis",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n5 Reliability in Factor Analysis",
    "text": "5 Reliability in Factor Analysis\nTo get the estimates of reliabilities, Omega coefficients were calculated for each factor(\\Omega_{AAC} = 0.832, p &lt; 0.01; \\Omega_{AAE} = 0.830, p &lt; 0.01).\n\nmodel03SyntaxOmega = \"\n  # AAC loadings (all estimated)\n  AAC =~ L1*Aff1 + L2*Aff2 + L3*Aff3\n  \n  # AAE loadings (all estimated)\n  AAE =~ L4*Aff4 + L5*Aff5 + L6*Aff6\n  \n  # Unique Variances:\n  Aff1 ~~ E1*Aff1; Aff2 ~~ E2*Aff2; Aff3 ~~ E3*Aff3; Aff4 ~~ E4*Aff4; Aff5 ~~ E5*Aff5; Aff6 ~~ E6*Aff6; \n  \n  \n  # Calculate Omega Reliability for Sum Scores:\n  OmegaAAC := ((L1 + L2 + L3)^2) / ( ((L1 + L2 + L3)^2) + E1 + E2 + E3)\n  OmegaAAE := ((L4 + L5 + L6)^2) / ( ((L4 + L5 + L6)^2) + E4 + E5 + E6)\n\"\n\nmodel03EstimatesOmega = sem(model = model03SyntaxOmega, data = dat2, estimator = \"MLR\", mimic = \"mplus\", std.lv = TRUE)\nsummary(model03EstimatesOmega, fit.measures = FALSE, rsquare = FALSE, standardized = FALSE, header = FALSE)\n\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  AAC =~                                              \n    Aff1      (L1)    0.903    0.055   16.357    0.000\n    Aff2      (L2)    1.153    0.042   27.771    0.000\n    Aff3      (L3)    1.117    0.051   22.079    0.000\n  AAE =~                                              \n    Aff4      (L4)    1.047    0.053   19.604    0.000\n    Aff5      (L5)    1.036    0.053   19.366    0.000\n    Aff6      (L6)    1.107    0.044   25.112    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  AAC ~~                                              \n    AAE               0.838    0.028   29.829    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Aff1              3.765    0.059   63.455    0.000\n   .Aff2              3.635    0.059   61.382    0.000\n   .Aff3              3.493    0.060   57.741    0.000\n   .Aff4              4.189    0.058   71.689    0.000\n   .Aff5              3.604    0.063   57.057    0.000\n   .Aff6              4.018    0.058   68.948    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Aff1      (E1)    0.970    0.083   11.753    0.000\n   .Aff2      (E2)    0.449    0.057    7.860    0.000\n   .Aff3      (E3)    0.607    0.084    7.244    0.000\n   .Aff4      (E4)    0.635    0.085    7.504    0.000\n   .Aff5      (E5)    0.950    0.090   10.539    0.000\n   .Aff6      (E6)    0.497    0.061    8.191    0.000\n    AAC               1.000                           \n    AAE               1.000                           \n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    OmegaAAC          0.832    0.016   51.512    0.000\n    OmegaAAE          0.830    0.017   49.000    0.000"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#factor-scores-and-distributions",
    "href": "posts/2017-11-08-sem-homework/index.html#factor-scores-and-distributions",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n6 Factor Scores and Distributions",
    "text": "6 Factor Scores and Distributions\nThe AAC factor scores have an estimated mean of 0 with a variance of 0.88 due to the effect of the prior distribution. The SE for each person’s AAC factor score is 0.347; 95% confidence interval for AAC factor score is Score \\pm 2*0.347 = Score \\pm 0.694. The AAE factor scores have an estimated mean of 0 with a variance of 0.881 due to the effect of the prior distribution. The SE for each person’s AAC factor score is 0.357; 95% confidence interval for AAC factor score is Score \\pm 2*0.357 = Score \\pm 0.714.\nFactor Realiability for AAC is 0.892 and factor realibility for AAE is 0.887. Both factor reliability are larger than omega.\nThe resulting distribution of the EAP estimates of factor score as shown in Figure 1. Figure 2 shows the predicted response for each item as a linear function of the latent factor based on the estimated model parameters. As shown, for AAE factor, the predicted item response goes above the highest response option just before a latent factor score of 2 (i.e., 2 SDs above the mean), resulting in a ceiling effect for AAE factor, as also shown in Figure 1.\nThe extent to which the items within each factor could be seen as exchangeable was then examined via an additional set of nested model comparisons, as reported in Table 1 (for fit) and Table 2 (for comparisons of fit). Two-factor has better model fit than one-facor model. Moreover, according to chi-square difference test, two-factor is significantly better than one-factor in model fit.\n\n\n      AAC       AAE \n0.8925361 0.8867031"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#figures",
    "href": "posts/2017-11-08-sem-homework/index.html#figures",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n7 Figures",
    "text": "7 Figures\n\n7.1 Figure 1 : Factor Score Distribution\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\n\n7.2 Figure 2 : Expected Item Response Plots"
  },
  {
    "objectID": "posts/2017-11-08-sem-homework/index.html#tables",
    "href": "posts/2017-11-08-sem-homework/index.html#tables",
    "title": "How to use lavaan package to perform Confirmatory Factor Analysis",
    "section": "\n8 Tables",
    "text": "8 Tables\n\n8.1 Table 1: Model Fit Statistics Using MLR\n\n\n\n\n\n\n# Items\n# Parameters\nScaled Chi-Square\nChi-Square Scale Factor\nDF\np-value\nCFI\nRMSEA\nRMSEA Lower\nRMSEA Upper\nRMSEA p-value\n\n\n\nOne-Factor\n6\n18\n75.834\n1.522\n9\n0.000\n0.927\n0.9\n0.000\n0.05\n1.000\n\n\nTwo-Factor\n6\n19\n21.512\n1.503\n8\n0.006\n0.979\n0.9\n0.046\n0.05\n0.475\n\n\n\n\n\n\n\n8.2 Table 2: Model Comparisons\n\n\n\n\n\n\nDf\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\nOne-Factor vs. Two-Factor\n9\n49.652\n1\n0\n\n\n\n\n\n\n8.3 Table 3: Model Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnstandardized\n\n\nStandardized\n\n\n\n\nEstimate\nSE\nEstimate\n\n\n\n\nForgiveness Factor Loadings\n\n\nItem 1\n0.903\n0.055\n0.676\n\n\nItem 2\n1.153\n0.042\n0.865\n\n\nItem 3\n1.117\n0.051\n0.820\n\n\nNot Unforgiveness Factor Loadings\n\n\nItem 4\n1.047\n0.053\n0.796\n\n\nItem 5\n1.036\n0.053\n0.728\n\n\nItem 6\n1.107\n0.044\n0.844\n\n\nFactor Covariance\n\n\nFactor Covariance\n0.838\n0.028\n0.838\n\n\nItem Intercepts\n\n\nItem 1\n3.765\n0.059\n2.818\n\n\nItem 2\n3.635\n0.059\n2.726\n\n\nItem 3\n3.493\n0.060\n2.564\n\n\nItem 4\n4.189\n0.058\n3.184\n\n\nItem 5\n3.604\n0.063\n2.534\n\n\nItem 6\n4.018\n0.058\n3.062\n\n\nItem Unique Variances\n\n\nItem 1\n0.970\n0.083\n0.543\n\n\nItem 2\n0.449\n0.057\n0.253\n\n\nItem 3\n0.607\n0.084\n0.327\n\n\nItem 4\n0.635\n0.085\n0.367\n\n\nItem 5\n0.950\n0.090\n0.470\n\n\nItem 6\n0.497\n0.061\n0.288"
  },
  {
    "objectID": "posts/2019-02-19-Lasso-Regression-With-glmnet/index.html",
    "href": "posts/2019-02-19-Lasso-Regression-With-glmnet/index.html",
    "title": "Lasso Regression Example using glmnet package in R",
    "section": "",
    "text": "More details please refer to the link below: (https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#lin)\nThis post shows how to use glmnet package to fit lasso regression and how to visualize the output. The description of data is shown in here.\ndt &lt;- readRDS(url(\"https://s3.amazonaws.com/pbreheny-data-sets/whoari.rds\"))\nattach(dt)\nfit &lt;- glmnet(X, y)"
  },
  {
    "objectID": "posts/2019-02-19-Lasso-Regression-With-glmnet/index.html#visualize-the-coefficients",
    "href": "posts/2019-02-19-Lasso-Regression-With-glmnet/index.html#visualize-the-coefficients",
    "title": "Lasso Regression Example using glmnet package in R",
    "section": "\n1 Visualize the coefficients",
    "text": "1 Visualize the coefficients\n\nplot(fit)\n\n\n1.1 Label the path\n\nplot(fit, label = TRUE)\n\nThe summary table below shows from left to right the number of nonzero coefficients (DF), the percent (of null) deviance explained (%dev) and the value of \\lambda (Lambda).\nWe can get the actual coefficients at a specific \\lambda whin the range of sequence:\n\ncoeffs &lt;- coef(fit, s = 0.1) \ncoeffs.dt &lt;- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) \n\n# reorder the variables in term of coefficients\ncoeffs.dt[order(coeffs.dt$coefficient, decreasing = T),]\n\nAlso, it can allow people to make predictions at specific \\lambda with new input data:\n\nnx = matrix(rnorm(nrow(dt$X)*ncol(dt$X)), nrow = nrow(dt$X), ncol = ncol(dt$X))\npred &lt;- predict(fit, newx = nx, s = c(0.1, 0.05)) \nhead(pred, 20)\n\ncv.glmnet is the function to do cross-validation here.\n\nX &lt;- dt$X\ny &lt;- dt$y\ncv.fit &lt;- cv.glmnet(X, y)\n\nPlotting the object gives the selected \\lambda and corresponding Mean-Square Error.\n\nplot(cv.fit)\n\nWe can view the selected \\lambda’s and the corresponding coefficients, For example,\n\ncv.fit$lambda.min\ncv.fit$lambda.1se\n\nlambda.min returns the value of \\lambda that gives minimum mean cross-validated error. The other \\lambda saved is lambda.lse, which gives the most regularized model such that error is within one standard error of the minimum. To use that, we only need to replace lambda.min with lambda.lse above.\n\n# create a function to transform coefficient of glmnet and cvglmnet to data.frame\ncoeff2dt &lt;- function(fitobject, s) {\n  coeffs &lt;- coef(fitobject, s) \n  coeffs.dt &lt;- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) \n\n  # reorder the variables in term of coefficients\n  return(coeffs.dt[order(coeffs.dt$coefficient, decreasing = T),])\n}\n\ncoeff2dt(fitobject = cv.fit, s = \"lambda.min\") %&gt;% head(20)\n\n\ncoeffs.table &lt;- coeff2dt(fitobject = cv.fit, s = \"lambda.min\")\nggplot(data = coeffs.table) +\n  geom_col(aes(x = name, y = coefficient, fill = {coefficient &gt; 0})) +\n  xlab(label = \"\") +\n  ggtitle(expression(paste(\"Lasso Coefficients with \", lambda, \" = 0.0275\"))) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")"
  },
  {
    "objectID": "posts/2019-02-19-Lasso-Regression-With-glmnet/index.html#elastic-net",
    "href": "posts/2019-02-19-Lasso-Regression-With-glmnet/index.html#elastic-net",
    "title": "Lasso Regression Example using glmnet package in R",
    "section": "\n2 Elastic net",
    "text": "2 Elastic net\nAs an example, we can set \\alpha=0.2\n\nfit2 &lt;- glmnet(X, y, alpha = 0.2, weights = c(rep(1, 716), rep(2, 100)), nlambda = 20)\n\nprint(fit2, digits = 3)\n\nAccording to the default internal settings, the computations stop if either the fractional change in deviance down the path is less than 10^{-5} or the fraction of explained deviance reaches 0.999.\n\nplot(fit2, xvar = \"lambda\", label = TRUE)\n\n# plot against %deviance\nplot(fit2, xvar = \"dev\", label = TRUE)\n\n\npredict(fit2, newx = X[1:5, ], type = \"response\", s = 0.03)"
  },
  {
    "objectID": "posts/2022-04-29-power-analysis-for-sem/index.html",
    "href": "posts/2022-04-29-power-analysis-for-sem/index.html",
    "title": "Power Analysis for Structural Equation Modeling",
    "section": "",
    "text": "The statistical power of a hypothesis test is the probability of detecting an effect, if there’s a true effect presented to detect. Power, defined as the confidence in the conclusions drawn from the results, plays an important role of determining the minimum sample size (number of observation) required to detect an effect in an experiment. The power analysis of SEM is often ignored in application research partly because few guidelines exist for researchers to conduct power analysis in SEM framework. This tutorial introduces two web tools which can be used to conduct power analysis for Structural Equation Modeling (SEM). An empirical example of my own research on students’ time effect on academic motivations will also be presented for illustration. For privacy, only short name of the latent constructs are used.\nWang and Rhemtulla (2021) argued that there’re two ways of determining sample size planning in SEM: (1) traditional rule of thumbs, (2) power analysis for SEM based on parameter effect size and model misspecification. To be more specific, two ways of determining sample size planning in SEM are:\n\nRules of thumbs based absolute minimum sample size: N = 100 or N = 200\nModel Complexities based relative sample size: N = 5-10 or N = 3-6 per estimated parameters\n\nThese two ways of determining minimum sample sizes may not agree with each other and have few theoretical or empirical research support. According to the objective of hypothesis test in SEM, there are two distinct kinds of power: (1) the power of detecting model misspecification, (2) and the power of determining specific effects within the model (i.e., whether one latent variable predicts another). In this tutorial, I called the former one global power and latter one local power since the global power analyzes the model-level effect size of misfit while the local power tests the effect size of a particular model parameter.\n\n1 Global Power\nAs mentioned above, global power is a test quantifying the degree of confidence of the hypothesis test of model misspecification. There are many fit indices for power analysis for model misspecification including:\n\nSatorra and Saris’s (1985) \\chi^2 likelihood-ratio test\nMacCallum et al.’s (1996) root-mean-square error of approximation (RMSEA) tests\n\nFor the first test, I recommend using WebPower, a free web tool, to calculate global power based on Chi-square likelihood-ratio test (LRT). The test examines the misspecification by comparing the user model to the null model with the hypothesis that the hypothesized model is same as the null model. Most of cases will reject the hypothesis. WebPower tests the global power of SEM based on chi-square likelihood ratio test.\n\n\n\n\nChi-square LR test\n\n\n\nInput the chi-square test with 480.82 and vary the sample size from N=1 to N=300, we found N = 5 is enough sample size for power up to 1. Thus, normally SEM based on Chi-square LRT to null model has very low bar for misspecification. In this case, perhaps doubling the sample size (N \\times 2 = 261 \\times 2 = 522) is a more reasonable rule of thumb of minimum sample size.\n\n2 Local Power\nWang and Rhemtulla (2021) recommend a Monte Carlo simulation approach to calculate power for detecting a target effect in SEM and introduced pwrSEM, a Shiny web app to estimate the power of parameter in SEM.\n\nWang, Y. Andre, and Mijke Rhemtulla. 2021. “Power Analysis for Parameter Estimation in Structural Equation Modeling: A Discussion and Tutorial.” Advances in Methods and Practices in Psychological Science 4 (1): 2515245920918253. https://doi.org/10.1177/2515245920918253.\nLet’s take one mixture Confirmatory factor analysis with known classes (time = 0, time = 1) for example. The goal is to estimate the power of time effects on five latent constructs with sample size (N = 256).\nThe model has been fitted to the data using lavaan package beforehand. After opening up the shiny app, we can copy and paste the model syntax from lavaan to the app. Then the model specification will be visualized as following:\n\n\n\n\nModel Specification\n\n\n\nWithin this example model, InR, IdR, IGR, IER, ER are five factors measured twice. Time is a indicator variable where time = 0 represents pre-test and time = 1 represents post-test.\nThe next step is to input the model syntax and corresponding parameter estimates generated by lavaan:\n\n\n\n\nParameter estimates by lavaan\n\n\n\nThen, check the effects needed to be examined. In this case, the five time effects on ER, IER, IGR, IdR, and InR.\nThe local powers of five time effects on latent factors are as follows:\n\n\n\n\nLocal Power with sample size 256\n\n\n\nWe can vary our sample size to test how many sample sizes required for the target power of LnR regressed on Time (for example, if the goal of regression coefficient is .9, we slowly increase the sample size from 300 to 500). I tried two sample size in this case:\n\n\n\n\n\n\n\n\nLocal power with N = 300\n\n\n\n\n\nLocal power with N = 500\n\n\n\n\nAs the figures shown, based on MCMC simulation, sample size N = 300 is expected to increase the power of time effect on InR (InR ~~ time) to value 0.81 while N = 500 will increase the power to value 0.9, which is enough power for single regression coefficient effect. However, with such moderately large sample size, other time effects on ER, IRT, IGR, IdR (i.e., ER ~~ time) still stay at a low level because of the lower estimated effect size.\nThus, from this experiment, we can conclude that to make the power of time effect on latent variable InR up to 0.9, at lease around 300~500 samples are required.\n\n3 Conclusion\nTo summarize, in this post, I illustrated two types of power: (1) local power (2) global power, and how they can be calculated using pwrSEM and WebPower correspondingly. It should be noted that, they are not the only software in the market. Another newly published software is semPower R package (see Jobst, Bader, and Moshagen 2021 for details), which can also be used to provide local power based on RMSEA.\n\nJobst, Lisa J., Martina Bader, and Morten Moshagen. 2021. “A Tutorial on Assessing Statistical Power and Determining Sample Size for Structural Equation Models.” Psychological Methods, No Pagination Specified–. https://doi.org/10.1037/met0000423.\nLocal power determines the minimum sample sizes for one specific effect in SEM. Global power determines the minimum sample sizes needed for testing model misspecification. Global power will give rise to different results depending on the model misfit indices chosen. As for local power, it is important to decide on which effect needed to be examined in the model.\n\n4 Discussion\nThere are few guidelines about how and what to report local power and global power in SEM literature. Global power seems a common thing to report for all SEM paper since all SEM research need to deal with model misspecification before drawing any conclusion. Local power, on the other hand, is necessary when one effect within SEM model is of most interest and core of the study. Some may argue that power in SEM is not important as long as the model fit of SEM is acceptable. However, the acceptance of model-data fit cannot guarantee that the sample size meet the requirement of detecting specific effect. In other words, there may be some sort of false positive rate when reporting significance test in small sample setting. More research needed to investigate the relationship between power and local misfit indices to understand this problem. Please let me know your thoughts in comments below.\n\n5 Reference\n\n\n\nChi-square LR test\nModel Specification\nParameter estimates by lavaan\nLocal Power with sample size 256\nLocal power with N = 300\nLocal power with N = 500\n\n\n Back to topCitationBibTeX citation:@online{zhang2022,\n  author = {Zhang, Jihong},\n  title = {Power {Analysis} for {Structural} {Equation} {Modeling}},\n  date = {2022-04-29},\n  url = {https://www.jihongzhang.org//posts/2022-04-29-power-analysis-for-sem},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Jihong. 2022. “Power Analysis for Structural Equation\nModeling.” April 29, 2022. https://www.jihongzhang.org//posts/2022-04-29-power-analysis-for-sem."
  },
  {
    "objectID": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html",
    "href": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html",
    "title": "Introduction to Latent Attribute Network Analysis",
    "section": "",
    "text": "Network analysis is a very useful tool. This post show how to visualize the latent attribute network in Diagnostic Classification Modeling(DCM). There are a ton of R package could be used to visualize network structure.\nlibrary(CDM)\nlibrary(tidyverse)\nlibrary(network)"
  },
  {
    "objectID": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#data-preparation",
    "href": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#data-preparation",
    "title": "Introduction to Latent Attribute Network Analysis",
    "section": "\n1 Data Preparation",
    "text": "1 Data Preparation\nI will use a simulated hierachial data from CDM package. The node.list depicts the traget nodes and starting nodes. Those information could be extracted from the Q^{T}Q square matrix, in which Q is the Q matrix of the model.\nAs shown below, there are 6 latent attributes including A1, A2, A3, B1, C1, C2. The A attributes share common items and the C attributes share common items but B attribute does not share common items with other attributes. The numbers in each cell represents the number of items shared by the pair of attributes. The number of common items will be used for the weights of network edges.\n\ndata(\"data.cdm10\")\nq.matrix &lt;- data.cdm10$q.matrix\nt(q.matrix) %*% q.matrix\n\n   A1 A2 A3 B1 C1 C2\nA1  6  4  2  0  0  0\nA2  4  4  2  0  0  0\nA3  2  2  2  0  0  0\nB1  0  0  0  3  0  0\nC1  0  0  0  0  6  3\nC2  0  0  0  0  3  3\n\n## prepare the edge and node table based on t(Q)%*%Q\nedge.list = tibble(from = c(1,1,2,2,3,3,5,6), \n                   to = c(2,3,1,3,1,2,6,5), \n                   weight = c(4,2,4,2,2,2,3,3))\nnode.list = tibble(label = c(\"A1\", \"A2\", \"A3\", \"B1\", \"C1\", \"C2\")) %&gt;% rowid_to_column(\"id\")"
  },
  {
    "objectID": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#network-package",
    "href": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#network-package",
    "title": "Introduction to Latent Attribute Network Analysis",
    "section": "\n2 Network package",
    "text": "2 Network package\n\n## Network package\nlibrary(network)\nroutes_work &lt;- network(x = edge.list, vertex.attr = node.list, \n                       matrix.type = \"edgelist\", ignore.eval = FALSE)\nplot(routes_work, vertex.cex = 3, mode = \"circle\")"
  },
  {
    "objectID": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#igraph-package",
    "href": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#igraph-package",
    "title": "Introduction to Latent Attribute Network Analysis",
    "section": "\n3 igraph package",
    "text": "3 igraph package\n\n## igraph package \ndetach(package:network)\nrm(routes_work)\nlibrary(igraph)\nroutes_igraph &lt;- graph_from_data_frame(d = edge.list, vertices = node.list, directed = TRUE)\nplot(routes_igraph, edge.arrow.size = 0.5, layout = layout_with_graphopt)"
  },
  {
    "objectID": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#tidygraph-and-ggraph",
    "href": "posts/2019-10-20-introduction-to-latent-attribute-network-analysis/index.html#tidygraph-and-ggraph",
    "title": "Introduction to Latent Attribute Network Analysis",
    "section": "\n4 tidygraph and ggraph\n",
    "text": "4 tidygraph and ggraph\n\n\nlibrary(tidygraph)\nlibrary(ggraph)\nroutes_tidy &lt;- tbl_graph(nodes = node.list, edges = edge.list, directed = FALSE)\n\n\nggraph(routes_tidy, layout = \"graphopt\") + \n  geom_node_point() +\n  geom_edge_link(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_text(aes(label = label), repel = TRUE) +\n  labs(edge_width = \"Number of Common Items\") +\n  theme_graph()\n\n\n\n\n\n\n\n\n### Linear Layout\nggraph(routes_tidy, layout = \"linear\") + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_text(aes(label = label)) +\n  labs(edge_width = \"Number of Common Items\") +\n  theme_graph()"
  },
  {
    "objectID": "posts/2018-03-10-my-new-shiny-app-cut-score-consistency/index.html",
    "href": "posts/2018-03-10-my-new-shiny-app-cut-score-consistency/index.html",
    "title": "My New Shiny App Cut Score Consistency",
    "section": "",
    "text": "R Codeknitr::include_app(\"https://jihongz.shinyapps.io/Cutscore-1/\", height = 1200) \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2018-02-11-Academic-Writing/index.html",
    "href": "posts/2018-02-11-Academic-Writing/index.html",
    "title": "Academic Writing",
    "section": "",
    "text": "This post is aimed to remind myself how to write articles with Academic Writing Style. The original article is from http://libguides.usc.edu/writingguide/academicwriting."
  },
  {
    "objectID": "posts/2018-02-11-Academic-Writing/index.html#i.-the-big-picture",
    "href": "posts/2018-02-11-Academic-Writing/index.html#i.-the-big-picture",
    "title": "Academic Writing",
    "section": "1 I. The Big Picture",
    "text": "1 I. The Big Picture\nUnlike fiction or journalistic writing, the overall structure of academic writing is formal and logical. It must be cohesive and possess a logically organized flow of ideas; this means that the various parts are connected to form a unified whole. There should be narrative links between sentences and paragraphs so the readers is able to follow your argument and all sources are properly cited. The introduction should include a description of how the rest of the paper is organized."
  },
  {
    "objectID": "posts/2018-02-11-Academic-Writing/index.html#ii.-the-tone",
    "href": "posts/2018-02-11-Academic-Writing/index.html#ii.-the-tone",
    "title": "Academic Writing",
    "section": "2 II. The Tone",
    "text": "2 II. The Tone\nThe overall one refers to the attitude conveyed in a piece of writing. Throughout your paper, it is important that you present the arguments of others fairly and with an appropriate narrative tone. When presenting a position or argument that you disagree with, describe the argument accurately and without loaded or biased language (I have use “the researchers failed to…”). In academic writing, the author is expected to investigate the research problem from an authoritative point of view. You should, therefore, state the strengths of your arguments confidently, using language that is neutral, not confrontational or dismissive."
  },
  {
    "objectID": "posts/2018-02-11-Academic-Writing/index.html#iii.-diction措辞",
    "href": "posts/2018-02-11-Academic-Writing/index.html#iii.-diction措辞",
    "title": "Academic Writing",
    "section": "3 III. Diction（措辞）",
    "text": "3 III. Diction（措辞）\nDiction refers to the choice of words you use. Awareness of the words you use is important because words that have almost the same denotation can have very different connotations (字义相同，含义未必相同). This is particularly true in academic writing because words and terminology can evolve a nuanced meaning that describes a particular idea, concept, or phenomenon derived from the epistemological culture of that discipline. Therefore, use concrete words [not general] that convey a specific meaning. If this cannot be done without confusing the reader, then you need to explain what you mean within the context of how that word is used within a discipline."
  },
  {
    "objectID": "posts/2018-02-11-Academic-Writing/index.html#iv.-the-language",
    "href": "posts/2018-02-11-Academic-Writing/index.html#iv.-the-language",
    "title": "Academic Writing",
    "section": "4 IV. The language",
    "text": "4 IV. The language\nThe investigation of research problems in the social sciences is often complex and multi-dimensional. Therefore, it is important that you use unambiguous language. Well-structured paragraphs and clear topic sentences enable a reader to follow your line of thinking without difficulty. Your language should be concise, formal, and express precisely what you want it to mean. Avoid vague expressions that are not specific or precise enough for the reader to derive exact meaning [“they”, “we”, “people”, “the organization”, etc.], abbreviations like “i.e.” [“in other words”] or ‘e.g.’ [“for example”], and the use of unspecific determinate words [“super”, “very”, “incredible”]."
  },
  {
    "objectID": "posts/2018-02-11-Academic-Writing/index.html#v.-punctuation",
    "href": "posts/2018-02-11-Academic-Writing/index.html#v.-punctuation",
    "title": "Academic Writing",
    "section": "5 V. Punctuation",
    "text": "5 V. Punctuation"
  },
  {
    "objectID": "posts/2019–11-21-rethink-statistical-model/index.html",
    "href": "posts/2019–11-21-rethink-statistical-model/index.html",
    "title": "Some Thoughts After Reading ",
    "section": "",
    "text": "Last night, I read the 1st chapter of Statistical Rethinking: A Bayesian Course with Examples in R and Stan from Richard McElreath. I found this is nice book to share with my friends. The core idea of that chapter is the relationship between NULL hypothesis and statistical model. That is, should we trust the statistical models to reject the NULL hypothesis?\nIt is a long history to use statistical models to figure out what is true? What is false? People found that comparing to prove what is true, prove what is false or falsifying the hypothesis (falsification) is more straightforward. This is because even thought you found thousands of millions of cases that the hypothesis is true (“Swans are white”), it does not guarantee this hypothesis is true. However, only one case that the hypothesis is not true (“A Swan is black”) is needed to prove it wrong. Thus, scientists seek to propose a hypothesis and then falsify it.\nAnother problem related to the falsification is that hypothesis is not model. That is to say, a NULL hypothesis could have several possible process models. These process models may or may not correspond to several statistical models. If we find a statistical model that match the samples. It only indicates it could be come from several process models. These process models may or may not come from the NULL hypothesis. As the author said:\n\nAny given statistical model (M) may correspond to more than one process model (P).\nAny given hypothesis (H) may correspond to more than one process model (P).\nAny given statistical model (M) may correspond to more than one hypothesis (H).\n\nThe traditional approach is to take the “neutral” model as a null hypothesis (“Item Response is not correlated among each other”), if the data (alternative model) are not sufficiently similar to the expectation under the null (NULL model), then we say that we “reject” the null hypothesis. Another explanation is we only have one model based NULL hypothesis but we potentially have a large number of model based on non-NULL hypothesis. Some of these non-NULL hypothesis may have similar model with the one based on NULL hypothesis. If we do not think carefully, we may reject the non-NULL hypothesis rather than NULL hypothesis.\nWhat can be done? The author try to encourage use to “search for a different description of the evidence, a description under which the processes look different.” For example, use different model fit indices in factor analysis instead of one because those model fit may depict different process models and different description of the evidence.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2017-11-11-lognormal-simulation/index.html",
    "href": "posts/2017-11-11-lognormal-simulation/index.html",
    "title": "Simulation of Lognormal distribution",
    "section": "",
    "text": "This post is a try about how yo simulate lognormal distribution in R. Lognormal distribution is used a lot in cumulative data (e.x. counting), which is very similar with normal distribution except x should be larger than 0. For instance, the number of schools, the number of students. But I always have no idea about how to parameterized this distribution. I’ll update this post as I learn more about lognormal distribution…"
  },
  {
    "objectID": "posts/2017-11-11-lognormal-simulation/index.html#simulate-lognomal-distibution",
    "href": "posts/2017-11-11-lognormal-simulation/index.html#simulate-lognomal-distibution",
    "title": "Simulation of Lognormal distribution",
    "section": "1 Simulate lognomal distibution",
    "text": "1 Simulate lognomal distibution\n\n1.1 Simulation Study 1\nThis study is to simulate lognormal density distribution based on mean and sd of depedent variable (Y). My simulated mean of y is 891, and sd is 490, N (sample size) is 200000. Then use the formular below:\nmu  = log(m^2/phi) # log mean\nsigma = sqrt(log(1+v/m^2)) # log sd\nIt could calculate the log mean and log standard deviation for lognormal distribution.\nset.seed(20171108)\n\n#### Give Y mean and Y sd, simluate lognormal distribution data.\nm = 891 # geometric mean\nsd = 490 # geometric sd\nv = sd ^ 2\nphi = sqrt(v + m^2) \n\nmu    = log(m^2/phi) # log mean\nsigma = sqrt(log(1+v/m^2)) # log sd\n  \n\ny &lt;- rlnorm(n = 200000, mu, sigma) %&gt;% round(0)\nm.sim &lt;- mean(y) # should be close to 891\nsd.sim &lt;- sd(y) # should be close to 490\n\nrow1 &lt;- c(m, mu,m.sim)\nrow2 &lt;- c(sd, sigma,sd.sim)\ntable &lt;- rbind(row1, row2)\ncolnames(table) &lt;- c(\"Original\", \"Log\", \"Simulated\")\nrownames(table)  &lt;- c(\"Mean\", \"SD\")\nkable(table) \nThis is the original, log and simulated mean and sd. It could be easily found that simulated ones are very closed to original.\n\n\n\n\nOriginal\nLog\nSimulated\n\n\n\n\nMean\n891\n6.660225\n891.1468\n\n\nSD\n490\n0.514041\n490.4396\n\n\n\nplot(density(y))\nFrom the density plot below, we can see the mean of X is also close to 891."
  },
  {
    "objectID": "posts/2019-01-10-visuliaztion-of-item-information-curves-in-r/index.html",
    "href": "posts/2019-01-10-visuliaztion-of-item-information-curves-in-r/index.html",
    "title": "Visuliaztion of Item Information Curves In R",
    "section": "",
    "text": "Item Information Curve\n\nThis blog is to show a new way to display item information curves using ggridges package. The ridge plot could show the IIF plots very clear when you have a large number of items.\n\nggplot(item_information_all %&gt;% filter(item %in% 17:22)) +\n  aes(x = factor, y = item, height = info, \n      group = item, color = as.factor(item), \n      fill = as.factor(item)) +\n  ggridges::geom_ridgeline(alpha = 0.75) +\n  ggtitle(\"Peer Social Capital: Information Functions\")  +\n  theme_ridges()\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html",
    "href": "posts/2017-11-10-Linking-simulation/index.html",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "",
    "text": "This simulation study is to show how to do IRT Linking Process using mirt R Package. The simulation data includes 2 forms - Form A and Form B. These 2 forms are simulated based on 2 groups of individual, one group has 0 mean trait, another has 0.25 mean trait. Both groups have same sd."
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#look-at-the-data",
    "href": "posts/2017-11-10-Linking-simulation/index.html#look-at-the-data",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n3.1 Look at the data",
    "text": "3.1 Look at the data\nFirst of all, have a look at the data\n\nR Codelibrary(mirt)\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\nR Codelibrary(knitr)\n### Read in Raw data from Form A:\ndat &lt;- read.csv(file=\"FormA.csv\")\nglimpse(dat)\n\nRows: 5,000\nColumns: 64\n$ X   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,…\n$ V1  &lt;dbl&gt; -0.79498, -0.05589, 0.62650, -1.26832, 0.74921, 1.00922, -0.19147,…\n$ V2  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ V3  &lt;int&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, …\n$ V4  &lt;int&gt; 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, …\n$ V5  &lt;int&gt; 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, …\n$ V6  &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, …\n$ V7  &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, …\n$ V8  &lt;int&gt; 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, …\n$ V9  &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, …\n$ V10 &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, …\n$ V11 &lt;int&gt; 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, …\n$ V12 &lt;int&gt; 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ V13 &lt;int&gt; 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ V14 &lt;int&gt; 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, …\n$ V15 &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, …\n$ V16 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, …\n$ V17 &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ V18 &lt;int&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, …\n$ V19 &lt;int&gt; 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, …\n$ V20 &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, …\n$ V21 &lt;int&gt; 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ V22 &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ V23 &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, …\n$ V24 &lt;int&gt; 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, …\n$ V25 &lt;int&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ V26 &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, …\n$ V27 &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, …\n$ V28 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, …\n$ V29 &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, …\n$ V30 &lt;int&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, …\n$ V31 &lt;int&gt; 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, …\n$ V32 &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, …\n$ V33 &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, …\n$ V34 &lt;int&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, …\n$ V35 &lt;int&gt; 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ V36 &lt;int&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ V37 &lt;int&gt; 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, …\n$ V38 &lt;int&gt; 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n$ V39 &lt;int&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, …\n$ V40 &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, …\n$ V41 &lt;int&gt; 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ V42 &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ V43 &lt;int&gt; 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, …\n$ V44 &lt;int&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ V45 &lt;int&gt; 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, …\n$ V46 &lt;int&gt; 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, …\n$ V47 &lt;int&gt; 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ V48 &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ V49 &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, …\n$ V50 &lt;int&gt; 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, …\n$ V51 &lt;int&gt; 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ V52 &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, …\n$ V53 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ V54 &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ V55 &lt;int&gt; 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ V56 &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, …\n$ V57 &lt;int&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, …\n$ V58 &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, …\n$ V59 &lt;int&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, …\n$ V60 &lt;int&gt; 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, …\n$ V61 &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, …\n$ V62 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, …\n$ V63 &lt;int&gt; 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, …"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#plot-the-density-of-true-theta-of-group-a",
    "href": "posts/2017-11-10-Linking-simulation/index.html#plot-the-density-of-true-theta-of-group-a",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n3.2 Plot the density of true \\theta of Group A",
    "text": "3.2 Plot the density of true \\theta of Group A\nFrom the density function, mu_{\\theta} is 0, sd_{\\theta} is 1.\n\nR Codeplot(density(dat$V1), main = \"Group A True Trait Density\", \n     xlab=expression(theta) )"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#ctt-table",
    "href": "posts/2017-11-10-Linking-simulation/index.html#ctt-table",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n3.3 CTT Table",
    "text": "3.3 CTT Table\nCTT table could provide a brief description of table. 2 key valables in the table is item difficulty (item.diff) calculated by item means P(y=1) and item discrimination (item.disc), which is item-total correlation.\n\n3.3.1 Clean data\nBy cleaning, data has 60 items including 50 unique item (from item1 to item50) and 10 common items (from item51 to item60). The sample size is 5000.\n\nR Codedat_cali &lt;- dat %&gt;% select(V3:V52, V54:V63)\ncolnames(dat_cali) &lt;- paste0(\"item\",1:60)\nN &lt;- nrow(dat_cali)\nn &lt;- ncol(dat_cali)\n\n\n\n3.3.2 Classical Test Theory\nThen calculate the CTT table for Form A. The item discrimination and difficulty could be compared between Form A and Form B. Because the relationship between trait and total score is non-linear, so there is effect of shrinkage.\n\nR Code# item stats\n## item discrimnation\nitem.disc &lt;- apply(dat_cali, 2, function(x) cor(x, rowSums(dat_cali, na.rm = TRUE)))\n\n## item difficulty\nitem.diff &lt;- colMeans(dat_cali)\n\n## item response frequency\nitem.freq &lt;- reduce(lapply(dat_cali, table), bind_rows)\n\nCTT &lt;- cbind(item.disc, item.diff, item.freq)\n\nkable(CTT, digits = 3, caption = \"CTT Table for Form A\")\n\n\nCTT Table for Form A\n\n\nitem.disc\nitem.diff\n0\n1\n\n\n\nitem1\n0.365\n0.761\n1197\n3803\n\n\nitem2\n0.464\n0.833\n835\n4165\n\n\nitem3\n0.457\n0.539\n2305\n2695\n\n\nitem4\n0.471\n0.524\n2380\n2620\n\n\nitem5\n0.385\n0.221\n3893\n1107\n\n\nitem6\n0.400\n0.563\n2183\n2817\n\n\nitem7\n0.261\n0.457\n2717\n2283\n\n\nitem8\n0.264\n0.365\n3176\n1824\n\n\nitem9\n0.331\n0.560\n2198\n2802\n\n\nitem10\n0.505\n0.431\n2843\n2157\n\n\nitem11\n0.306\n0.354\n3228\n1772\n\n\nitem12\n0.528\n0.560\n2199\n2801\n\n\nitem13\n0.362\n0.550\n2251\n2749\n\n\nitem14\n0.427\n0.761\n1196\n3804\n\n\nitem15\n0.433\n0.241\n3793\n1207\n\n\nitem16\n0.377\n0.458\n2710\n2290\n\n\nitem17\n0.524\n0.545\n2275\n2725\n\n\nitem18\n0.351\n0.578\n2112\n2888\n\n\nitem19\n0.429\n0.509\n2455\n2545\n\n\nitem20\n0.330\n0.307\n3465\n1535\n\n\nitem21\n0.399\n0.403\n2984\n2016\n\n\nitem22\n0.418\n0.609\n1957\n3043\n\n\nitem23\n0.356\n0.354\n3232\n1768\n\n\nitem24\n0.460\n0.670\n1651\n3349\n\n\nitem25\n0.388\n0.507\n2465\n2535\n\n\nitem26\n0.209\n0.310\n3451\n1549\n\n\nitem27\n0.242\n0.476\n2618\n2382\n\n\nitem28\n0.495\n0.502\n2492\n2508\n\n\nitem29\n0.441\n0.481\n2593\n2407\n\n\nitem30\n0.450\n0.731\n1344\n3656\n\n\nitem31\n0.601\n0.399\n3006\n1994\n\n\nitem32\n0.396\n0.682\n1588\n3412\n\n\nitem33\n0.584\n0.613\n1933\n3067\n\n\nitem34\n0.489\n0.557\n2216\n2784\n\n\nitem35\n0.467\n0.759\n1203\n3797\n\n\nitem36\n0.312\n0.367\n3166\n1834\n\n\nitem37\n0.410\n0.778\n1109\n3891\n\n\nitem38\n0.443\n0.571\n2143\n2857\n\n\nitem39\n0.519\n0.755\n1224\n3776\n\n\nitem40\n0.280\n0.327\n3365\n1635\n\n\nitem41\n0.413\n0.582\n2088\n2912\n\n\nitem42\n0.338\n0.395\n3024\n1976\n\n\nitem43\n0.430\n0.634\n1830\n3170\n\n\nitem44\n0.348\n0.382\n3088\n1912\n\n\nitem45\n0.370\n0.530\n2351\n2649\n\n\nitem46\n0.286\n0.337\n3314\n1686\n\n\nitem47\n0.456\n0.403\n2985\n2015\n\n\nitem48\n0.340\n0.397\n3014\n1986\n\n\nitem49\n0.310\n0.291\n3544\n1456\n\n\nitem50\n0.435\n0.357\n3214\n1786\n\n\nitem51\n0.283\n0.821\n895\n4105\n\n\nitem52\n0.272\n0.324\n3382\n1618\n\n\nitem53\n0.474\n0.531\n2347\n2653\n\n\nitem54\n0.298\n0.336\n3318\n1682\n\n\nitem55\n0.427\n0.546\n2271\n2729\n\n\nitem56\n0.471\n0.667\n1667\n3333\n\n\nitem57\n0.410\n0.470\n2652\n2348\n\n\nitem58\n0.483\n0.433\n2834\n2166\n\n\nitem59\n0.363\n0.232\n3841\n1159\n\n\nitem60\n0.376\n0.666\n1670\n3330"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#final-calibration-of-form-a",
    "href": "posts/2017-11-10-Linking-simulation/index.html#final-calibration-of-form-a",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n3.4 Final Calibration of Form A",
    "text": "3.4 Final Calibration of Form A\n\n3.4.1 Model Specification\n\nR CodeSPECS &lt;- mirt.model('F = 1-60\n                    PRIOR = (1-60, a1, lnorm, 0, 1),\n                    (1-60,  d,  norm, 0, 1),\n                    (1-60,  g,  norm, -1.39,1)')\nmod_A3PL &lt;- mirt(data=dat_cali, model=SPECS, itemtype='3PL')\nparms_a &lt;- coef(mod_A3PL, simplify=TRUE, IRTpars = TRUE)$items\n\n\n\nR Codea_A &lt;- parms_a[,1] \nb_A &lt;- parms_a[,2] \nc_A &lt;- parms_a[,3] \ntheta_A &lt;- fscores(mod_A3PL,method=\"EAP\",\n                   full.scores=TRUE, full.scores.SE=TRUE,\n                   scores.only=TRUE)\n\n\nUsing 3-PL for irt model of form A. Extracting the cofficients (a, b, c) of IRT. The model-implied theta was outputed, whose mean is 0.1314938\nThe plot below suggest that SE is low when theta is close to mean, but low theta and high theta has large SE.\n\nR Codeplot(theta_A[,1],theta_A[,2])"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#final-calibration-of-form-a-1",
    "href": "posts/2017-11-10-Linking-simulation/index.html#final-calibration-of-form-a-1",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n4.1 Final Calibration of Form A",
    "text": "4.1 Final Calibration of Form A\n\n4.1.1 Model Specification of B\n\nR CodeSPECS2 &lt;- mirt.model('F = 1-60\n                    PRIOR = (1-60, a1, lnorm, 0, 1),\n                    (1-60,  d,  norm, 0, 1),\n                    (1-60,  g,  norm, -1.39,1)')\nmod_B3PL &lt;- mirt(data=dat_cali2, model=SPECS, itemtype='3PL')\nparms_b &lt;- coef(mod_B3PL, simplify=TRUE, IRTpars = TRUE)$items\n\n\n\nR Codea_B &lt;- parms_b[,1] \nb_B &lt;- parms_b[,2] \nc_B &lt;- parms_b[,3] \ntheta_B &lt;- fscores(mod_B3PL,method=\"EAP\",\n                   full.scores=TRUE, full.scores.SE=TRUE,\n                   scores.only=TRUE)\nhead(theta_B, 20) %&gt;% kable(digits = 3, caption = \"Model-implied Theta of B\")\n\n\nModel-implied Theta of B\n\nF\nSE_F\n\n\n\n0.933\n0.214\n\n\n0.193\n0.220\n\n\n-0.783\n0.293\n\n\n0.865\n0.208\n\n\n-0.233\n0.261\n\n\n-0.217\n0.239\n\n\n0.890\n0.215\n\n\n1.369\n0.232\n\n\n-0.875\n0.306\n\n\n-0.411\n0.266\n\n\n1.091\n0.223\n\n\n-0.638\n0.261\n\n\n-1.253\n0.321\n\n\n-0.784\n0.261\n\n\n0.436\n0.228\n\n\n0.135\n0.211\n\n\n-1.841\n0.491\n\n\n-0.491\n0.259\n\n\n0.065\n0.225\n\n\n-0.339\n0.240"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#b-plot",
    "href": "posts/2017-11-10-Linking-simulation/index.html#b-plot",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n4.2 b-plot",
    "text": "4.2 b-plot\nThe relationship between b parameters of A and B reflect the latent traits of A and B:\n\n\\theta_{A} = \\theta_{B} - 0.25 \\\\\n\\sigma_A^2 = \\sigma_B^2\n thus, b_B -b_A should also be -0.25. the estimated difference of b is calculate by the mean of b parametes of Form A’s common items and that of Form B’s common items, which is -0.2756443. Thus, it is very close to difference of true traits.\n\nR Code### b-plot\nplot(b_A[51:60],b_B[51:60],\n     main=paste0(\"r =\", round(cor(b_A[51:60],b_B[51:60]),5)), \n     xlab = \"b_A\",\n     ylab = \"b_B\"\n     )\n\n\n\n\n\n\nR Code# mean(b_B[51:60])-mean(b_A[51:60])"
  },
  {
    "objectID": "posts/2017-11-10-Linking-simulation/index.html#a-plot",
    "href": "posts/2017-11-10-Linking-simulation/index.html#a-plot",
    "title": "Simulation Study of Linking Using Mirt",
    "section": "\n4.3 a-plot",
    "text": "4.3 a-plot\nBecause \\theta_B - \\theta_A = 0.25, so a parametes are: \na_A / a_B = 1\n\nThe true ratio of (means of) a parameters is 1.016284, which is very close to 1. SD of A and B are both close to 1.\n\nR Code### a-plot\nplot(a_A[51:60],a_B[51:60],\n     main=round(cor(a_A[51:60],a_B[51:60]),5)\n     )\n\n\n\n\n\n\nR Code# mean(a_A[51:60]) / mean(a_B[51:60]) \n\n\n#SDs of b-values across forms\nSD_bA &lt;- sd(b_A[51:60])\nSD_bB &lt;- sd(b_B[51:60])\n\nMean_bA &lt;- mean(b_A[51:60])\nMean_bB &lt;- mean(b_B[51:60])"
  },
  {
    "objectID": "posts/2019-04-19-make-a-game-in-r/index.html",
    "href": "posts/2019-04-19-make-a-game-in-r/index.html",
    "title": "Make a Game in R",
    "section": "",
    "text": "Recently I found a interesting R package call nessy which allows you to create a simple game driven by shiny. Thus. I tried a little bit about this package. Making a interactive app in R is promising in the files like teaching, presentation and visualization.\n\nFinally, I created the following shiny app:\n\nlibrary(nessy)\nlibrary(shinyjs)\n\njscode &lt;- \"shinyjs.closeWindow = function() { window.close(); }\"\nui &lt;- cartridge(\n  title = \"{Memorize the Names!}\",\n  subtitle = \"Do you have some names to memorize in few minutes? Try this game!\",\n  container_with_title(\n    title = \"Names you want to memorize (i.e. Jonathan, Lesa)\"\n  ),\n  container_with_title(\n    title = \"Add a Name\",\n    text_input(id = \"name1\", label = \"Name\", placeholder = \"Jonathan Templin\"),\n    text_input(id = \"key\", label = \"Keys\", placeholder = \"Iowa/DCM\"),\n    htmlOutput(\"namelist\"),\n    button_primary(id = \"add\", \"Add\")\n  ),\n  button_success(id = \"play\", \"Play the Game\"),\n  useShinyjs(),\n  extendShinyjs(text = jscode, functions = c(\"closeWindow\")),\n  button_error(id = \"close\", \"Close Window\"),\n  \n  # Game pages\n  uiOutput(\"gamepage\")\n)\n\n\nserver &lt;- function(input, output, session) {\n  names &lt;- reactiveValues(\n    oldnames = \"\",\n    allnames = NULL,\n    allkeys = NULL\n  )\n  \n  observeEvent(input$add, {\n    names$oldnames = paste(names$oldnames, \"&lt;br&gt;\", input$name1, \"  &lt;==&gt;  \", input$key)\n    names$allnames = c(names$allnames, input$name1)\n    names$allkeys = c(names$allkeys, input$key)\n    output$namelist &lt;- renderText(names$oldnames)\n  })\n  \n  observeEvent(input$play, {\n    selectedkey &lt;- sample(names$allkeys, 1)\n    selectedname &lt;- names$allnames[names$allkeys == selectedkey]\n    output$gamepage &lt;- renderUI({\n      container_with_title(\n        paste(\"Key:\", selectedkey),\n        text_input(id = \"guessname\", label = \"Guess a Name\", placeholder = \"Jonathan\")\n      )\n    })\n  })\n\n\n  observeEvent(input$close, {\n    js$closeWindow()\n    stopApp()\n  })\n  \n}\n\nshiny::shinyApp(ui, server)\n\nThe game is like this:\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-07-04-data-visualization-for-survey-data/index.html",
    "href": "posts/2023-07-04-data-visualization-for-survey-data/index.html",
    "title": "Data visualization for survey data",
    "section": "",
    "text": "Me: I don’t know much about survey R package. Can you tell me know about core features of this package?\n\nSure, the survey package in R is designed specifically for analysis of data from complex surveys. It provides functions for descriptive statistics and general regression models for survey data that includes design features such as clustering, stratification, and weighting.\nHere are some of the core features of the survey package:\n\nDescriptive Statistics: The package provides functions for computing descriptive statistics on survey data, including mean, total, and quantiles.\nRegression Models: The package provides a variety of model fitting functions for binary and multi-category response, count data, survival data, and continuous response.\nDesign Effects: It allows calculation of design effects for complex survey designs.\nPost-stratification and Raking: The package allows for adjusting the sampling weights to match known population margins.\nSubpopulation Analysis: It includes functions for correctly handling analyses that are limited to a subset of the population (a subpopulation).\nVariance Estimation: The survey package supports multiple methods of variance estimation, including Taylor series linearization, replication weights, and subbootstrap.\n\nRemember that before you can use these functions, you will need to define a survey design object that specifies the features of your survey’s design (like the sampling method, strata, clustering, and weights).\nHere’s an example of how you might use it to calculate the mean of a variable from a survey:\n\nR Code# Load the necessary package\nlibrary(survey)\n\nmydata &lt;- iris\n\n# Define the survey design\ndes &lt;- svydesign(ids = ~1, data = mydata, weights = ~Sepal.Length)\n\n# Calculate the mean of a variable\nmean &lt;- svymean(~Species, design = des)\n\nmean\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nvariables\nFormula or data frame specifying the variables measured in the survey. If NULL, the data argument is used.\n\n\nids\nFormula or data frame specifying cluster ids from largest level to smallest level, ~0 or ~1 is a formula for no clusters.\n\n\nprobs\nFormula or data frame specifying cluster sampling probabilities\n\n\n\n\n\nPlease replace mydata, weight, and variable with your actual data frame, weight column, and the variable you’re interested in, respectively.\nRemember, working with survey data can be complex due to the design features of surveys. The survey package in R provides a robust set of tools for dealing with this complexity."
  },
  {
    "objectID": "posts/2023-07-04-data-visualization-for-survey-data/index.html#r-package---survey",
    "href": "posts/2023-07-04-data-visualization-for-survey-data/index.html#r-package---survey",
    "title": "Data visualization for survey data",
    "section": "",
    "text": "Me: I don’t know much about survey R package. Can you tell me know about core features of this package?\n\nSure, the survey package in R is designed specifically for analysis of data from complex surveys. It provides functions for descriptive statistics and general regression models for survey data that includes design features such as clustering, stratification, and weighting.\nHere are some of the core features of the survey package:\n\nDescriptive Statistics: The package provides functions for computing descriptive statistics on survey data, including mean, total, and quantiles.\nRegression Models: The package provides a variety of model fitting functions for binary and multi-category response, count data, survival data, and continuous response.\nDesign Effects: It allows calculation of design effects for complex survey designs.\nPost-stratification and Raking: The package allows for adjusting the sampling weights to match known population margins.\nSubpopulation Analysis: It includes functions for correctly handling analyses that are limited to a subset of the population (a subpopulation).\nVariance Estimation: The survey package supports multiple methods of variance estimation, including Taylor series linearization, replication weights, and subbootstrap.\n\nRemember that before you can use these functions, you will need to define a survey design object that specifies the features of your survey’s design (like the sampling method, strata, clustering, and weights).\nHere’s an example of how you might use it to calculate the mean of a variable from a survey:\n\nR Code# Load the necessary package\nlibrary(survey)\n\nmydata &lt;- iris\n\n# Define the survey design\ndes &lt;- svydesign(ids = ~1, data = mydata, weights = ~Sepal.Length)\n\n# Calculate the mean of a variable\nmean &lt;- svymean(~Species, design = des)\n\nmean\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nvariables\nFormula or data frame specifying the variables measured in the survey. If NULL, the data argument is used.\n\n\nids\nFormula or data frame specifying cluster ids from largest level to smallest level, ~0 or ~1 is a formula for no clusters.\n\n\nprobs\nFormula or data frame specifying cluster sampling probabilities\n\n\n\n\n\nPlease replace mydata, weight, and variable with your actual data frame, weight column, and the variable you’re interested in, respectively.\nRemember, working with survey data can be complex due to the design features of surveys. The survey package in R provides a robust set of tools for dealing with this complexity."
  },
  {
    "objectID": "posts/2023-07-04-data-visualization-for-survey-data/index.html#an-empirical-example",
    "href": "posts/2023-07-04-data-visualization-for-survey-data/index.html#an-empirical-example",
    "title": "Data visualization for survey data",
    "section": "\n2 An empirical example",
    "text": "2 An empirical example\nThe example I used here is a tody data exacted from a real data about eating disorders. The sample size is 500.\nThe measurement data contains 12 items, each ranging from 0 to 3. The demographic data contains 6 variables: age, gender, race, birthplace, height, weight. The very first thing is to visualize the characteristics of the samples to have a big picture of respondents.\n\nR Codeknitr::opts_chunk$set(echo = TRUE, message=FALSE, warnings=FALSE, include = TRUE)\nlibrary(here)\nlibrary(glue)\nlibrary(readr)\nlibrary(bruceR)\nlibrary(xtable)\nlibrary(survey)\nlibrary(formattable) # format styles of table \nlibrary(reshape2)\nlibrary(tidyverse)\nlibrary(ggtext) \nlibrary(kableExtra)\noptions(knitr.kable.NA = '')\nmycolors = c(\"#4682B4\", \"#B4464B\", \"#B4AF46\", \n             \"#1B9E77\", \"#D95F02\", \"#7570B3\",\n             \"#E7298A\", \"#66A61E\", \"#B4F60A\")\nsoftcolors = c(\"#B4464B\", \"#F3DCD4\", \"#ECC9C7\", \n               \"#D9E3DA\", \"#D1CFC0\", \"#C2C2B4\")\nmykbl &lt;- function(x, ...){\n  kbl(x, digits = 2, ...) |&gt; kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) }\n\n\n\nR CodedatList &lt;- readRDS(here::here(\"posts/2023-07-04-data-visualization-for-survey-data/Example_Data.RDS\"))\nstr(datList)\n\n\n\n2.1 Descriptive statistics\nWe can use multiple R tools for descriptive statistics. bruceR is one of them.\n\nR Codedescription &lt;- datList$description\nbruceR::Freq(dplyr::select(description, gender:birthplace), \n             varname = \"gender\")\n\n\n\nR CodefreqTblVars = c(\"gender\", \"race\", \"birthplace\")\nfreqTable &lt;- function(tbl, var) {\n  tbl |&gt; as.data.frame() |&gt; \n    tibble::rownames_to_column(\"Levels\") |&gt; \n    dplyr::mutate(Variable = var)\n}\nfreqTableComb = NULL\nfor (var in freqTblVars) {\n  tbl = bruceR::Freq(dplyr::select(description, gender:birthplace), varname = var)\n  freqTableComb = rbind(freqTableComb, freqTable(tbl = tbl, var = var))\n  freqTableComb &lt;- freqTableComb |&gt; \n    relocate(\"Variable\")\n}\n\n\n\nR Codemykbl(freqTableComb)\n\n\nOr we can use survey package for descriptive analysis\n\nR Codelibrary(survey)\ndexample = svydesign(ids = ~1,\n                     data = datList$measurement)\nsummary(dexample)\n\n## summay statistics for all measurement indicators\nvars &lt;- colnames(datList$measurement)\nsvymean(make.formula(vars), design = dexample, na.rm = TRUE)\nsvytotal(make.formula(vars), design = dexample, na.rm = TRUE)\n\n\n\n2.2 Stacked barplot for survey data responses\n\nR Codesurvey = datList$measurement\nsurvey &lt;- survey |&gt; \n  mutate(ID = 1:nrow(survey)) |&gt; \n  mutate(across(starts_with(\"EDEQS\"), \\(x) factor(x, levels = 0:3))) |&gt; \n  pivot_longer(starts_with(\"EDEQS\"), names_to = \"items\", values_to = \"values\") |&gt; \n  group_by(items) |&gt; \n  dplyr::count(values) |&gt; \n  dplyr::mutate(perc = n/sum(n) * 100)\n\np = ggplot(survey) +\n  geom_col(aes(y = factor(items, levels = paste0(\"EDEQS\", 1:12)),\n               x = perc,\n               fill = values), \n           position = position_stack(reverse = TRUE)) +\n  labs(y = \"\", x = \"Proportion (%)\", title = \"N and proportion of responses for items\")\n\np = p + geom_text(aes(y = factor(items, levels = paste0(\"EDEQS\", 1:12)),\n                  x = perc, group = items,\n                  label = ifelse(n &gt;= 50, paste0(n, \"(\", round(perc, 1), \"%)\"), \"\")), \n              size = 3, color = \"white\",\n              position = position_stack(reverse = TRUE, vjust = 0.5))\np = p + scale_fill_manual(values = mycolors)\np\n\n\nWe can clearly identify item 7 has highest proportion of level 0, and needed to be theoretically justified."
  },
  {
    "objectID": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html",
    "href": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html",
    "title": "[Workshop] Creating Academic Blog via R",
    "section": "",
    "text": "Michael Clark has a great workshop in his blog introducing distill package. I suggest readers check it out first."
  },
  {
    "objectID": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#resource",
    "href": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#resource",
    "title": "[Workshop] Creating Academic Blog via R",
    "section": "",
    "text": "Michael Clark has a great workshop in his blog introducing distill package. I suggest readers check it out first."
  },
  {
    "objectID": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#requirement",
    "href": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#requirement",
    "title": "[Workshop] Creating Academic Blog via R",
    "section": "\n2 Requirement",
    "text": "2 Requirement\nFollowing software is necessary to replicate the steps in this post:\n\nGithub account\nRstudio and R\n\ndistill R package\n\nTo get started with Github, please refer to the official website. To install distill package, run install.packages(\"distill\") in your R console."
  },
  {
    "objectID": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#procedure",
    "href": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#procedure",
    "title": "[Workshop] Creating Academic Blog via R",
    "section": "\n3 Procedure",
    "text": "3 Procedure\nTo create a website with the URL &lt;username.github.io&gt;, create the directory name with same name and same Github repo name. For example, my github repo name is “jihongz.github.io”, and it is published in http://jihongz.github.io.\n\n\n\n\nFlowchart for creating website\n\n\n\nTo have your personal website, you need to have all require files for the website. The basic files for Github Pages based website with distill includes:\n\n_site.yml\nindex.Rmd\n.nojekyll\n_posts\n\n\n\nExample of file structure used in my github website"
  },
  {
    "objectID": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#new-post",
    "href": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#new-post",
    "title": "[Workshop] Creating Academic Blog via R",
    "section": "\n4 New Post",
    "text": "4 New Post\nTo create a new post, use distill::create_post() function with the name of the post. The function will create a new directory “2022-XX-XX-post-name” in the _posts folder in which there exists a Rmd file with same name. The Rmd file will be your content of the new post.\n\ndistill::create_post(title = \"Tutorial of creating personal blog using distill package and Github Pages\")\n\n\n\nFiles in your post folder\n\nOpen the Rmd file and make sure the header of your Rmd file is like this:\n---\ntitle: \"[Tutorial] Creating Academic Blog\"\ndescription: |\n  This post reviews the procedure of creating Github Pages Website  using distill package and Github Pages in a step-to-step way.\nauthor:\n  - name: Zhang Jihong\n    url: {}\ndate: 2022-04-24\noutput:\n  distill::distill_article:\n    self_contained: false\n---\nAfter editting the Rmd file, knit the fill and then build website using Build Website button in the Build pane of Rstudio.\n\n\nThe Build button in Rstudio\n\nIt will then pop up your updated website in a window.\n\n\nPreview of updated website\n\nHowever, the website is not published yet. To publish the website, upload the files to Github. Following is a example of pushing the local files to the Github server via command line.\n\ngit add *\ngit commit -m \"create a new post\"\ngit push\n\n\n4.1 Settings of Github Repo\nGo to the Github Repo &gt; Settings &gt; Pages\n\n\nGithub Settings\n\nIn the webpage, choose _docs and you will see your website is published.\n\n\nchange settings"
  },
  {
    "objectID": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#reference",
    "href": "posts/2022-04-24-tutorial-of-creating-personal-blog-using-distill-package-and-github-pages/index.html#reference",
    "title": "[Workshop] Creating Academic Blog via R",
    "section": "\n5 Reference",
    "text": "5 Reference\nLearn more about using Distill at https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2024-01-14-how-to-use-uark-hpc/index.html",
    "href": "posts/2024-01-14-how-to-use-uark-hpc/index.html",
    "title": "How to set up High Performance Computing of University of Arkansas",
    "section": "",
    "text": "Arkansas High Performance Computing Center (AHPCC) is available for research and instructional use to faculty and students of any Arkansas university and their research collaborators. There is no charge for use of our computing resources.\nTo use the HPC, an AHPCC account must be requested through Internal Account Request Form. Please see here for more information about AHPPC inventory."
  },
  {
    "objectID": "posts/2024-01-14-how-to-use-uark-hpc/index.html#general-information",
    "href": "posts/2024-01-14-how-to-use-uark-hpc/index.html#general-information",
    "title": "How to set up High Performance Computing of University of Arkansas",
    "section": "",
    "text": "Arkansas High Performance Computing Center (AHPCC) is available for research and instructional use to faculty and students of any Arkansas university and their research collaborators. There is no charge for use of our computing resources.\nTo use the HPC, an AHPCC account must be requested through Internal Account Request Form. Please see here for more information about AHPPC inventory."
  },
  {
    "objectID": "posts/2024-01-14-how-to-use-uark-hpc/index.html#connect-to-hpc",
    "href": "posts/2024-01-14-how-to-use-uark-hpc/index.html#connect-to-hpc",
    "title": "How to set up High Performance Computing of University of Arkansas",
    "section": "\n2 Connect to HPC",
    "text": "2 Connect to HPC\nAs long as you have an AHPCC account, you can connect to HPC through SSH. For Windows users, you can use PuTTY to connect to HPC. For Mac and Linux users, you can use the terminal to connect to HPC. The command is:\nssh [loginname]@hpc-portal2.hpc.uark.edu\nIf your account was successfully setted up, passwords will be required. After you enter your password, you will be connected to HPC.\n\n\nLogin Screenshot\n\nNote: Pinnacle is a new resource at the University of Arkansas in 2019. It consists of 100 Intel based nodes with 20 NVIDIA V100 GPU nodes enabling data science and machine learning and 8 big memory nodes with 768 Gb ram/each for projects requiring a large memory footprint.\n\n2.1 SSH login without password\n\nGenerate a pair of authentication keys in your local machine and do not enter a passphrase using the following codes:\n\nssh-keygen -t rsa   \nPlease note that make the passphrase empty:\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/[username]/.ssh/id_rsa): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/[username]/.ssh/id_rsa\nYour public key has been saved in /Users/[username]/.ssh/id_rsa.pub\n\nIn your local machine, type in following commands to copy local public key to the hpc server.\n\nscp ~/.ssh/id_rsa.pub [loginname]@hpc-portal2.hpc.uark.edu:/home/[loginname]/.ssh/authorized_keys\n\nNow you should be able to login the hpc login node without password:\n\nssh [loginname]@hpc-portal2.hpc.uark.edu"
  },
  {
    "objectID": "posts/2024-01-14-how-to-use-uark-hpc/index.html#moving-data",
    "href": "posts/2024-01-14-how-to-use-uark-hpc/index.html#moving-data",
    "title": "How to set up High Performance Computing of University of Arkansas",
    "section": "\n3 Moving data",
    "text": "3 Moving data\nFor moving data files from local machine to HPC, type in following codes on your local machine:\nscp program.c loginname@hpc-portal2.hpc.uark.edu:/home/loginname/\nTo copy an entire directory tree src using SCP, use -r for recursive:\nscp -r src loginname@hpc-portal2.hpc.uark.edu:/home/loginname/\nSimilarly, for moving data files from HPC to local machine, type in following codes on your local machine:\nscp -r loginname@hpc-portal2.hpc.uark.edu:/home/loginname/src ./"
  },
  {
    "objectID": "posts/2024-01-14-how-to-use-uark-hpc/index.html#jobs-submission",
    "href": "posts/2024-01-14-how-to-use-uark-hpc/index.html#jobs-submission",
    "title": "How to set up High Performance Computing of University of Arkansas",
    "section": "\n4 Jobs submission",
    "text": "4 Jobs submission\nThere are multiple steps to submit the R file to cluster to run.\nFirst, we need to determine the computing nodes we want to use. Please refer to this link for detailed information about HPC equipment. A general ‘submitting’ command is like this:\nsbatch -q q06h32c -l walltime=1:00 -l nodes=1:ppn=32 example.sh\n\n\n\n\n\n\nNote\n\n\n\nsbatch command aims to submit a job with the job file example.sh. The command above submitted the job to the q06h32c queue with a wall-time of 1 minute requesting all 32 cores on 1 node.\n\n\nSecond, create a job file with .sh extension. Here’s a simple example of a job file example.sh that can tell HPC how to run your R code:\n#!/bin/bash\n#SBATCH --job-name=mpi\n#SBATCH --output=zzz.slurm\n#SBATCH --partition comp06\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=32\n#SBATCH --time=6:00:00\nmodule load gcc/11.2.1 mkl/19.0.5 R/4.2.2\n\nRscript HelloWorld/example.R\n\n\n\n\n\n\nNote\nWhere Line 8 loaded all required modules:\n\ngcc and mkl are required for R package installation (Note: To the date, gcc/11.2.1 is the latest version of gcc than can compile the cmdstanr successfully). Please see here for more details.\nRscript is the bash command to execute R file on HPC. HelloWorld/example.R is the path of your R script.\nAnything behind the #SBATCH are options for the SLURM scheduler. Please see following summary or view it online:\n\n\n\n\n\n\nDownload summary.pdf\n\n\nThird, you may want to use R interactively to test your R code is running well. Use the following bash command to start a brand new R in terminal:\nsrun -N1 -n1 -c1 -p cloud72 -q cloud -t 2:00:00 --pty /bin/bash\n\n\n\n\n\n\nNote\n\n\n\nThis command will redirect to cloud72 queue, which includes virtual machines and containers, usually single processor, 72 hour limit, 3 nodes.\n\n\n\n\n\n\n\n\n\nslurm commands\ncompatibility commands\nMeaning\n\n\n\nsbatch\nqsub\nsubmit &lt;job file&gt;\n\n\nsrun\nqsub -I\nsubmit interactive job\n\n\nsqueue\nqstat\nlist all queued jobs\n\n\nsqueue -u -rfeynman\nqstat -u rfeynman\nlist queued jobs for user rfeynman\n\n\nscancel\nqdel\ncancel &lt;job#&gt;\n\n\nsinfo\nshownodes -l -n;qstat -q\nnode status;list of queues\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor slurm, please use the commands in the 1st column. Then you should be able to start R in an interactive job and install required packages. If you load R/4.2.2 module, those packages installed via an interactive job will be stored at $HOME$/R/x86_64-pc-linux-gnu/4.2/. See here for more details about interactive job.\n\n\nFinnally, the whole workflow of job submission is as follows:\nc1331:jzhang:~/HelloWorld$ cat exampleJob.sh\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=2\n#SBATCH --job-name=Exp0\nmodule purge\nmodule load os/el7 gcc/9.3.1 mkl/19.0.5 R/4.2.2\ncd $SLURM_SUBMIT_DIR\nRscript example.R\nc1331:jzhang:~/HelloWorld$ rm slurm-334001.out\nc1331:jzhang:~/HelloWorld$ sbatch exampleJob.sh\nSubmitted batch job 334002\nc1331:jzhang:~/HelloWorld$ ls\nexampleJob.sh  example.R  gene_up.txt  MS_entrez_id_alldata.txt  slurm-334002.out\nc1331:jzhang:~/HelloWorld$"
  },
  {
    "objectID": "posts/2024-01-14-how-to-use-uark-hpc/index.html#checking-job",
    "href": "posts/2024-01-14-how-to-use-uark-hpc/index.html#checking-job",
    "title": "How to set up High Performance Computing of University of Arkansas",
    "section": "\n5 Checking job",
    "text": "5 Checking job\nsqueue -u loginname\nBelow shows the information of all running/down task for the queue comp06 (there are 9 Jobs but only 7 are running). Note that comp06 and comp72 queues share the same nodes, both belonging to pinnacle cluster. There are 49 public standard compute nodes. Thus, if there are 49 running jobs in both queues, then your job has to be waitting until some jobs finished.\nFor example, below shows all 47 running jobs in comp06/comp72 queues. Some have been running like 2 days. There are 72 hours limit though for all computation nodes.\nsqueue -p comp06,comp72 -t R\n JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n 362406    comp06    measr   jzhang  R    1:04:24      1 c1501\n 362385    comp06 sys/dash ashmitau  R    2:07:25      1 c1402\n 362309    comp06 sys/dash    igorf  R    4:14:32      1 c1402\n 362361    comp06 03a_iqtr amatthew  R      55:02      1 c1420\n 362311    comp06  mystery cdgoolsb  R    4:09:10      1 c1410\n 362310    comp06  mystery cdgoolsb  R    4:09:40      1 c1410\n 362308    comp06  mystery cdgoolsb  R    4:14:41      1 c1410\n 362454    comp72 test_str pradeepk  R      11:03      1 c1512\n 362150    comp72   cv0_rf    igorf  R 1-00:43:47      1 c1410\n 362151    comp72   cv1_rf    igorf  R 1-00:43:47      1 c1410\n 362152    comp72   cv2_rf    igorf  R 1-00:43:47      1 c1410\n 362137    comp72 sys/dash maghamoh  R 1-03:12:27      1 c1410\n 362340    comp72  cv1_pls    igorf  R    1:00:24      1 c1512\n 362341    comp72  cv2_pls    igorf  R    1:00:24      1 c1512\n 362339    comp72  cv0_pls    igorf  R    1:04:24      1 c1501\n 360997    comp72  TiS2-ph bothinah  R 2-04:46:42      1 c1609\n 360877    comp72 Temp_230 sppoudel  R      54:55      1 c1419\n 360875    comp72 Temp_220 sppoudel  R    6:19:42      1 c1415\n 360876    comp72 Temp_225 sppoudel  R    6:19:42      1 c1509\n 354260    comp72 peep19-0   djo001  R   11:27:37      1 c1506\n 354262    comp72 peep20-0   djo001  R   11:27:37      1 c1514\n 354263    comp72 peep21-0   djo001  R   11:27:37      1 c1515\n 351991    comp72 peep16-0   djo001  R   12:31:45      1 c1507\n 351988    comp72 peep18-2   djo001  R   14:00:48      1 c1603\n 351987    comp72 peep17-2   djo001  R   14:07:24      1 c1519\n 360873    comp72 Temp_210 sppoudel  R   14:49:25      1 c1408\n 360874    comp72 Temp_215 sppoudel  R   14:49:25      1 c1418\n 351989    comp72 peep18-0   djo001  R   14:49:55      1 c1516\n 351990    comp72 peep17-0   djo001  R   14:49:55      1 c1518\n 351986    comp72 peep16-2   djo001  R   15:36:01      1 c1605\n 360824    comp72 SmNiO3-2 paillard  R   16:23:51      4 c[1405-1406,1503,1508]\n 360871    comp72 Temp_205 sppoudel  R   16:23:51      1 c1511\n 360821    comp72 SmNiO3-2 paillard  R 1-01:41:15      4 c[1412-1413,1513,1517]\n 360869    comp72 Temp_200 sppoudel  R 1-01:41:15      1 c1606\n 360868    comp72 Temp_195 sppoudel  R 1-14:30:28      1 c1504\n 349719    comp72  peep8-2   djo001  R 1-16:02:28      1 c1608\n 360867    comp72 Temp_190 sppoudel  R 1-20:18:00      1 c1610\n 360818    comp72 SmNiO3-2 paillard  R 1-20:18:01      4 c[1404,1409,1502,1505]\n 360866    comp72 Temp_180 sppoudel  R 2-04:52:43      1 c1411\n 349718    comp72  peep9-2   djo001  R 2-05:37:18      1 c1604\n 349717    comp72 peep10-2   djo001  R 2-05:51:48      1 c1520\n 349715    comp72 peep12-2   djo001  R 2-09:11:29      1 c1417\n 349716    comp72 peep11-2   djo001  R 2-09:11:29      1 c1607\n 349714    comp72 peep13-2   djo001  R 2-09:30:18      1 c1510\n 338160    comp72 INT3-WT- dgirodat  R 2-10:20:18      1 c1407\n 338164    comp72 C1069T-p dgirodat  R 2-10:20:18      1 c1414\nWhen you want to get the worst case scenario estimate of when your waiting jobs will start, you can always run following command,\nsqueue -u [loginname] --start\n\n5.1 Queues\nMost frequently used queues are from pinnacle cluster.\nBelow is a list of queues in the pinnacle cluster. The number after the queue is the time limit for a running job. For example, comp72 has 72 hour limits while comp06 has only 6 hour limit, but they share same nodes. Thus, for efficiency, maybe use comp01 for quick examination of coding and use comp72 for time consuming jobs.\ncomp72/06/01: standard compute nodes, 72/6/1 hour limit, 42/46/48 nodes\ngpu72/06:     gpu nodes: 72/6 hour limit, 19 nodes\nagpu72/06:    a100 gpu nodes: 72/6 hour limit\nhimem72/06:   768 GB nodes, 72/6 hour limit, 6 nodes\npubcondo06:   condo nodes all-user use, 6 hour limit, various constraints required, 25 nodes\npcon06:       same as pubcondo06, shortened name for easier printout, use this going forward\ncloud72:      virtual machines and containers, usually single processor, 72 hour limit, 3 nodes\ncondo:        condo nodes, no time limit, authorization required, various constraints required, 25 nodes\ntres72/06:    reimaged trestles nodes, 72/06 hour limit, 126 nodes\nrazr72/06:    reimaged razor nodes, 72 hour limit, in progress\nHere’s some useful information regarding selecting queue from https://hpcwiki.uark.edu/doku.php?id=pinnacle_usage\n\nGenerally the nodes are reserved for the most efficient use, especially for expensive features such as GPU and extra memory. Pinnacle compute nodes are very busy (comp.. and himem.. partitions) are reserved for scalable programs that can use all 32/24 cores (except for the cloud partition, and condo usage by the owner). Cores are allocated by the product of ntasks-per-node x cpus-per-task. Exceptions: (1) serial/single core jobs that use more memory than available on Razor/Trestles (64 to 192 GB) (2) multiple jobs submitted together that use a whole node, such as 4 x 8 cores (3) two jobs on one high-memory node (2 x 12 cores) that each use more than 192 GB (and less than 384 GB so that they can run on the himem node)\n\n\n5.2 Troubleshooting\n\n\nRevise .bashrc so that the ssh cannot login?\nYou potential can try ctrl+c to avoid the ssh to execute bashrc. Try multiple time if not succeed. See here for reference.\n\n\n\n\n\nLogin Screenshot"
  },
  {
    "objectID": "posts/2020-05-25-study-notes-gt-package-and-format-table/index.html",
    "href": "posts/2020-05-25-study-notes-gt-package-and-format-table/index.html",
    "title": "Study Notes: gt package and format table",
    "section": "",
    "text": "A introduction about gt package is here\nknitr::opts_chunk$set(echo = TRUE, \n                      warning = FALSE, \n                      message = FALSE,\n                      fig.align = \"default\", \n                      eval = TRUE)\n\nlibrary(gt)\nsuppressMessages(library(tidyverse))\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3"
  },
  {
    "objectID": "posts/2020-05-25-study-notes-gt-package-and-format-table/index.html#basics-of-gt",
    "href": "posts/2020-05-25-study-notes-gt-package-and-format-table/index.html#basics-of-gt",
    "title": "Study Notes: gt package and format table",
    "section": "\n1 Basics of gt\n",
    "text": "1 Basics of gt\n\nA basic gt table can be created as so\n\ndata(\"iris\")\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\niris %&gt;% \n  head() %&gt;% \n  gt()\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\nYou can add row names (rowname_col argument) and add group names (groupname_col argument) into the table:\n\niris %&gt;% \n  arrange(desc(Sepal.Length)) %&gt;% # 6 types of iris with largest sepal length\n  mutate(Rank = paste0(\"ID\", 1:nrow(.))) %&gt;% \n  head(20) %&gt;% \n  gt(groupname_col = \"Species\", \n     rowname_col = \"Rank\")\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\nvirginica\n\n\nID1\n7.9\n3.8\n6.4\n2.0\n\n\nID2\n7.7\n3.8\n6.7\n2.2\n\n\nID3\n7.7\n2.6\n6.9\n2.3\n\n\nID4\n7.7\n2.8\n6.7\n2.0\n\n\nID5\n7.7\n3.0\n6.1\n2.3\n\n\nID6\n7.6\n3.0\n6.6\n2.1\n\n\nID7\n7.4\n2.8\n6.1\n1.9\n\n\nID8\n7.3\n2.9\n6.3\n1.8\n\n\nID9\n7.2\n3.6\n6.1\n2.5\n\n\nID10\n7.2\n3.2\n6.0\n1.8\n\n\nID11\n7.2\n3.0\n5.8\n1.6\n\n\nID12\n7.1\n3.0\n5.9\n2.1\n\n\nID15\n6.9\n3.2\n5.7\n2.3\n\n\nID16\n6.9\n3.1\n5.4\n2.1\n\n\nID17\n6.9\n3.1\n5.1\n2.3\n\n\nID19\n6.8\n3.0\n5.5\n2.1\n\n\nID20\n6.8\n3.2\n5.9\n2.3\n\n\nversicolor\n\n\nID13\n7.0\n3.2\n4.7\n1.4\n\n\nID14\n6.9\n3.1\n4.9\n1.5\n\n\nID18\n6.8\n2.8\n4.8\n1.4\n\n\n\n\n\n\n\nNext, the boarder could be added into the table:\n\niris %&gt;% \n  arrange(desc(Sepal.Length)) %&gt;% # 6 types of iris with largest sepal length\n  mutate(Rank = paste0(\"ID\", 1:nrow(.))) %&gt;% \n  head(20) %&gt;% \n  gt(groupname_col = \"Species\", \n     rowname_col = \"Rank\") %&gt;% \n  ########################### \n  # Below is changed\n  ###########################\n  tab_style( # tab_style to change style of cells, \n    # cells_borders provides the formatting\n    # locations tells it where add black borders to all column labels\n    style = list(\n      cell_borders(\n        sides = \"left\",\n        color = \"black\",\n        weight = px(1.2)\n      )\n    ),\n    locations = list(\n      cells_body(\n        columns = colnames(iris)\n      )\n    )\n  ) %&gt;% \n  # Add botton line below the column names\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"bottom\",\n        color = \"black\",\n        weight = px(3)\n      )\n    ),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n      )\n    )\n  )\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\nvirginica\n\n\nID1\n7.9\n3.8\n6.4\n2.0\n\n\nID2\n7.7\n3.8\n6.7\n2.2\n\n\nID3\n7.7\n2.6\n6.9\n2.3\n\n\nID4\n7.7\n2.8\n6.7\n2.0\n\n\nID5\n7.7\n3.0\n6.1\n2.3\n\n\nID6\n7.6\n3.0\n6.6\n2.1\n\n\nID7\n7.4\n2.8\n6.1\n1.9\n\n\nID8\n7.3\n2.9\n6.3\n1.8\n\n\nID9\n7.2\n3.6\n6.1\n2.5\n\n\nID10\n7.2\n3.2\n6.0\n1.8\n\n\nID11\n7.2\n3.0\n5.8\n1.6\n\n\nID12\n7.1\n3.0\n5.9\n2.1\n\n\nID15\n6.9\n3.2\n5.7\n2.3\n\n\nID16\n6.9\n3.1\n5.4\n2.1\n\n\nID17\n6.9\n3.1\n5.1\n2.3\n\n\nID19\n6.8\n3.0\n5.5\n2.1\n\n\nID20\n6.8\n3.2\n5.9\n2.3\n\n\nversicolor\n\n\nID13\n7.0\n3.2\n4.7\n1.4\n\n\nID14\n6.9\n3.1\n4.9\n1.5\n\n\nID18\n6.8\n2.8\n4.8\n1.4"
  },
  {
    "objectID": "posts/2023-06-26-moving-to-quarto/index.html",
    "href": "posts/2023-06-26-moving-to-quarto/index.html",
    "title": "Moving My Website to Quarto",
    "section": "",
    "text": "Since June 25, 2023, I start to move my website from hugo/rmarkdown/blogdown/wowchemy to quarto website. It is difficult to explain why I spend so much time on that. Perhaps because some reasons same as other bloggers:\n(1) Quarto is the next generation of rmarkdown.\n(2) Quarto is independent with R or Rstudio.\n(3) Quarto has a clean file tree for website building.\n(4) Quarto has cleaner command than hugo (i.e., `quarto publish netlify` for publishing website etc.).\nThis blog serves as a note how I customize quarto website bit by bit. It will not be a comprehensive tutorial (much online resources exists) but contain be some tricks."
  },
  {
    "objectID": "posts/2023-06-26-moving-to-quarto/index.html#useful-links",
    "href": "posts/2023-06-26-moving-to-quarto/index.html#useful-links",
    "title": "Moving My Website to Quarto",
    "section": "\n1 Useful links",
    "text": "1 Useful links\n\nQuarto official documentation is always the best place to starts with.\n\n\n\n\n\n\n\nNicola Rennie’s blog is very inspiring. Nice font setting and background. I feel like Hugo Apéro is the best hugo theme in the market.\n\n\n\nYihui’s blog. As the creator of blogdown, Yihui talked about why not transfer to Quarto :P I am a old player of hugo/blogdown, but I am not good at customizing the website from the scratch. Thus, quarto suits my needs very well. For those who is very experienced at JS, hugo is still a better choice.\n\n\n\nFrank Harrel’s blog: R workflow."
  },
  {
    "objectID": "posts/2023-06-26-moving-to-quarto/index.html#tricks-of-quarto",
    "href": "posts/2023-06-26-moving-to-quarto/index.html#tricks-of-quarto",
    "title": "Moving My Website to Quarto",
    "section": "\n2 Tricks of Quarto",
    "text": "2 Tricks of Quarto\n\n2.1 Code highlighting\nThe very first trick is using format &gt; html &gt; code-fold: true settings in YAML metadata in _quarto.yml (global) or .qmd (local) to hide the code block as |&gt; Code, like:\n\n⌘+Clibrary(kableExtra)\n\nWarning: package 'kableExtra' was built under R version 4.2.3\n\n⌘+Ckbl(head(iris)) |&gt; \n  kable_styling(\n    html_font = \"Ysabeau Office\",\n    font_size = 11, full_width = TRUE)\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\nNote that the code block can be numbered and added with a left border using code-block-bg: true and code-block-border-left: \"#31BAE9\"\nCode block now also can be shown with language #| echo: fenced. As the code chunk shown below, {r} is explicitly presented. I don’t hate #| as the new way of chuck option setup but when I test the code chuck, I found there is a space between #| and option keys. For example, it should be #| echo: fenced rather than #|echo:fenced. Otherwise, quarto will ignore chunk options.\n\n```{r}\n#| eval: false\n#| code-fold: false\nkableExtra::kbl(head(iris)) |&gt; \n  kableExtra::kable_styling(\n    html_font = \"Ysabeau Office\",\n    bootstrap_options = c('striped', 'hover'),\n    font_size = 10, full_width = TRUE)\n```\n\n\n⌘+C```{python}\n#| eval: false\n#| code-fold: show\nimport numpy as np\niris = np.array(iris)\n```\n\n\n\n2.2 Visual model\nVisual model is one of the most appealing feature for Quarto but I am far from a expert in that. It looks like a visual model version of markdown editor (so called WYSIWYM editing, What you see is What you mean) but slightly different. The shortcut of switching between source and visual model is Cmd + Shift + F4 (Thanks to Yihui’s JS for pretty keyboard styling).\nI list some most frequently used keyboard for visual mode editing:\n\nInsert Hyperlink: Cmd + K\nAdd footnote1: Shift+Cmd+F7\nInsert picture: Shift +Cmd +I\nInsert code chunk2: Alt +Cmd +I\nEdit attribute3: F4\n\n1 Like this.2 By default, it will insert R code chunk. If another language is frequently, use Tools &gt; Modify keyboard shortcuts &gt; filter 'Insert'. There should be a place for setting shortcut for other language.3 Click `F4` in any place in the div works.Please also refer to the official document for more shortcuts and features. One thing I found very interesting is single quote mark ` . First, I’m not sure how to escape this in visual model. Second, if you close quote before typing content, like ``, visual mode will escape both quote marks. However, if you type in left quote mark then content and then close quote (i.e., type in`something first, and type closing quote mark), it can quote content correctly.\nLast but not least, perhaps just type / in rstudio is a better choice as it will pop up a list of elements to choose."
  },
  {
    "objectID": "posts/2023-06-26-moving-to-quarto/index.html#footnotes",
    "href": "posts/2023-06-26-moving-to-quarto/index.html#footnotes",
    "title": "Moving My Website to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\nLike this.↩︎\nBy default, it will insert R code chunk. If another language is frequently, use Tools &gt; Modify keyboard shortcuts &gt; filter 'Insert'. There should be a place for setting shortcut for other language.↩︎\nClick `F4` in any place in the div works.↩︎"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOnline Homework\n\n\n\n\n\n\n\n\n\n\n\nLecture 01\n\n\nCourse and Bayesian Analysis Introduction\n\n\n\n\n\n\n\n\n\n\n\nLecture 02\n\n\nIntroduction to Bayesian Concepts\n\n\n\n\n\n\n\n\n\n\n\nLecture 03\n\n\nLinear Regression Model with Stan\n\n\n\n\n\n\n\n\n\n\n\nLecture 04\n\n\nLinear Regression Model with Stan II\n\n\n\n\n\n\n\n\n\n\n\nLecture 05\n\n\nBayesian Model fit and comparison\n\n\n\n\n\n\n\n\n\n\n\nLecture 06\n\n\nGeneralized Measurement Models: An Introduction\n\n\n\n\n\n\n\n\n\n\n\nDanmu Cloud for Bilibili\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize font family of Visual Mode in Rstudio and in Zotero\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up High Performance Computing of University of Arkansas\n\n\n\nJan 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nESRM 6553: Advanced Multivariate Analysis\n\n\nSpring 2024, Mondays, 5:00-7:45PM, Classroom GRAD 0229\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData visualization for survey data\n\n\n\nJul 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial for forester R package\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOne Template for Quarto Report\n\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMoving My Website to Quarto\n\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA Conversation between R and Python on Data Analysis and Machine Learning\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Class Model: Batch Mplus using R on Mac\n\n\n\nJun 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDissertation Defence\n\n\nTITLE: A novel method for model selection in Bayesian Diagnostic Classification Modeling\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNSF Grant on Bayesian Cognitive Diagnosis\n\n\nData visualization using ggplot2\n\n\n\nMay 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping Academia Institute’s Grant Fundings using R\n\n\n\n\n\n\nMay 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPower Analysis for Structural Equation Modeling\n\n\n\nApr 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n[Workshop] Creating Academic Blog via R\n\n\nThis post reviews the procedure of creating Github Pages Website using distill package and Github Pages in a step-to-step way.\n\n\n\nApr 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Care Data Analysis with Apple Watch\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nStudy Notes: gt package and format table\n\n\n\nMay 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Latent Attribute Network Analysis\n\n\n\nOct 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Country own the most Liberty Nobel Prizes? France? Ireland?\n\n\n\nMay 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nMake a Game in R\n\n\n\nApr 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduce gganimate for Psychometric\n\n\n\nFeb 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nLasso Regression Example using glmnet package in R\n\n\n\nFeb 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nVisuliaztion of Item Information Curves In R\n\n\n\nJan 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n[Manual]Using Jags and R2jags in R\n\n\n\nSep 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduce Descrepancy Measures\n\n\n\nSep 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do Data Cleaning in R\n\n\n\nSep 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMy New Shiny App Cut Score Consistency\n\n\n\nMar 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Profile Analysis using MCLUST (in R)\n\n\n\nNov 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nOne Example of Measurement Invariance\n\n\n\nNov 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation Study of Linking Using Mirt\n\n\n\nNov 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use lavaan package to perform Confirmatory Factor Analysis\n\n\n\nOct 19, 2017\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2019-05-20-nobel-prizes-visualization/index.html",
    "href": "posts/2019-05-20-nobel-prizes-visualization/index.html",
    "title": "Which Country own the most Liberty Nobel Prizes? France? Ireland?",
    "section": "",
    "text": "It is of my interest to use ggplot package to visualize some fun data. In this post, I tried to play with a Nobel Prizes Data including countries, prize year, each prize. The goal is to plot a cumulative traceplot of Liberty Nobel Prizes for top 10 countries.\n\n\n1 Load Packages\ntidyverse package include some very useful tools such as ggplot2, tidyr and dplyr.\n\nlibrary(tidyverse)\nlibrary(LaCroixColoR)\nlibrary(ggthemes)\nlibrary(ggimage)\n\nggimage package was used to add country flags to ggplot layer. LaCroixColoR package used for selecting different colors.\n\n2 Data\nImport the data.\n\nnobel_winners &lt;- read_csv(\"data_2019-05-14.csv\",\n                          col_types = \"dccccdccDccccccDcc\")\nstr(nobel_winners)\n\nspc_tbl_ [969 × 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ prize_year          : num [1:969] 1901 1901 1901 1901 1901 ...\n $ category            : chr [1:969] \"Chemistry\" \"Literature\" \"Medicine\" \"Peace\" ...\n $ prize               : chr [1:969] \"The Nobel Prize in Chemistry 1901\" \"The Nobel Prize in Literature 1901\" \"The Nobel Prize in Physiology or Medicine 1901\" \"The Nobel Peace Prize 1901\" ...\n $ motivation          : chr [1:969] \"\\\"in recognition of the extraordinary services he has rendered by the discovery of the laws of chemical dynamic\"| __truncated__ \"\\\"in special recognition of his poetic composition, which gives evidence of lofty idealism, artistic perfection\"| __truncated__ \"\\\"for his work on serum therapy, especially its application against diphtheria, by which he has opened a new ro\"| __truncated__ NA ...\n $ prize_share         : chr [1:969] \"1/1\" \"1/1\" \"1/1\" \"1/2\" ...\n $ laureate_id         : num [1:969] 160 569 293 462 463 1 161 571 294 464 ...\n $ laureate_type       : chr [1:969] \"Individual\" \"Individual\" \"Individual\" \"Individual\" ...\n $ full_name           : chr [1:969] \"Jacobus Henricus van 't Hoff\" \"Sully Prudhomme\" \"Emil Adolf von Behring\" \"Jean Henry Dunant\" ...\n $ birth_date          : Date[1:969], format: \"1852-08-30\" \"1839-03-16\" ...\n $ birth_city          : chr [1:969] \"Rotterdam\" \"Paris\" \"Hansdorf (Lawice)\" \"Geneva\" ...\n $ birth_country       : chr [1:969] \"Netherlands\" \"France\" \"Prussia (Poland)\" \"Switzerland\" ...\n $ gender              : chr [1:969] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ organization_name   : chr [1:969] \"Berlin University\" NA \"Marburg University\" NA ...\n $ organization_city   : chr [1:969] \"Berlin\" NA \"Marburg\" NA ...\n $ organization_country: chr [1:969] \"Germany\" NA \"Germany\" NA ...\n $ death_date          : Date[1:969], format: \"1911-03-01\" \"1907-09-07\" ...\n $ death_city          : chr [1:969] \"Berlin\" \"Châtenay\" \"Marburg\" \"Heiden\" ...\n $ death_country       : chr [1:969] \"Germany\" \"France\" \"Germany\" \"Switzerland\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   prize_year = col_double(),\n  ..   category = col_character(),\n  ..   prize = col_character(),\n  ..   motivation = col_character(),\n  ..   prize_share = col_character(),\n  ..   laureate_id = col_double(),\n  ..   laureate_type = col_character(),\n  ..   full_name = col_character(),\n  ..   birth_date = col_date(format = \"\"),\n  ..   birth_city = col_character(),\n  ..   birth_country = col_character(),\n  ..   gender = col_character(),\n  ..   organization_name = col_character(),\n  ..   organization_city = col_character(),\n  ..   organization_country = col_character(),\n  ..   death_date = col_date(format = \"\"),\n  ..   death_city = col_character(),\n  ..   death_country = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nunique(nobel_winners$category)\n\n[1] \"Chemistry\"  \"Literature\" \"Medicine\"   \"Peace\"      \"Physics\"   \n[6] \"Economics\" \n\n\ntribble function allows to create a table by columns. ~country will generate a new columns.\n\ndf_countrycode &lt;-\n  tribble(~country, ~code, \n          \"United States of America\", \"US\",\n          \"Germany\", \"DE\",\n          \"United Kingdom\", \"GB\",\n          \"France\", \"FR\",\n          \"Japan\", \"JP\",\n          \"Netherlands\", \"NL\",\n          \"Sweden\", \"SE\",\n          \"Russia\", \"RU\",\n          \"Canada\", \"CA\",\n          \"Austria\", \"AT\",\n          \"Spain\", \"ES\",\n          \"Denmark\", \"DK\",\n          \"Ireland\", \"IE\",\n          \"Italy\", \"IT\"\n          )\n\n\n3 Tables\npull() function is similar to use [] to return the value of one column.\n\ncountries &lt;- \n  nobel_winners %&gt;% \n  filter(category == \"Literature\") %&gt;% \n  count(birth_country, sort = T) %&gt;% \n  head(n = 10) %&gt;% # Top 10 countries\n  pull(birth_country)\n\nAdding some columns such as the total number of Liberty Prizes (n_prize), the first year of prize, the last year and the cumulative prizes.\n\nnobel_countries &lt;- \n  nobel_winners %&gt;% \n  filter(birth_country %in% countries, category == \"Literature\") %&gt;% \n  select(prize_year, birth_country) %&gt;% \n  arrange(prize_year) %&gt;% \n  group_by(birth_country) %&gt;% \n  mutate(n_prize = n(),\n         first_prize = min(prize_year),\n         last_prize = max(prize_year),\n         cum = row_number()\n  ) %&gt;% \n  ungroup()\nnobel_countries\n\n# A tibble: 58 × 6\n   prize_year birth_country n_prize first_prize last_prize   cum\n        &lt;dbl&gt; &lt;chr&gt;           &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1       1901 France             11        1901       2014     1\n 2       1904 France             11        1901       2014     2\n 3       1904 Spain               5        1904       1989     1\n 4       1909 Sweden              7        1909       2011     1\n 5       1915 France             11        1901       2014     3\n 6       1916 Sweden              7        1909       2011     2\n 7       1917 Denmark             4        1917       1944     1\n 8       1917 Denmark             4        1917       1944     2\n 9       1921 France             11        1901       2014     4\n10       1922 Spain               5        1904       1989     2\n# ℹ 48 more rows\n\n\n\nnobel_countries &lt;- \n  nobel_countries %&gt;% \n  filter(cum == 1) %&gt;% \n  mutate(cum = 0) %&gt;% \n  bind_rows(nobel_countries) %&gt;% \n  arrange(prize_year, cum) %&gt;% \n  mutate(birth_country = fct_reorder(birth_country, n_prize, .desc = TRUE))\nnobel_countries\n\n# A tibble: 68 × 6\n   prize_year birth_country n_prize first_prize last_prize   cum\n        &lt;dbl&gt; &lt;fct&gt;           &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1       1901 France             11        1901       2014     0\n 2       1901 France             11        1901       2014     1\n 3       1904 Spain               5        1904       1989     0\n 4       1904 Spain               5        1904       1989     1\n 5       1904 France             11        1901       2014     2\n 6       1909 Sweden              7        1909       2011     0\n 7       1909 Sweden              7        1909       2011     1\n 8       1915 France             11        1901       2014     3\n 9       1916 Sweden              7        1909       2011     2\n10       1917 Denmark             4        1917       1944     0\n# ℹ 58 more rows\n\n\n\nfirst_last_nobel &lt;- \n  nobel_countries %&gt;% \n  select(birth_country, n_prize, first_prize, last_prize) %&gt;% \n  mutate(birth_country = as.character(birth_country)) %&gt;% \n  distinct() %&gt;% \n  left_join(df_countrycode, by=c(\"birth_country\" = \"country\"))\nfirst_last_nobel\n\n# A tibble: 10 × 5\n   birth_country            n_prize first_prize last_prize code \n   &lt;chr&gt;                      &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;\n 1 France                        11        1901       2014 FR   \n 2 Spain                          5        1904       1989 ES   \n 3 Sweden                         7        1909       2011 SE   \n 4 Denmark                        4        1917       1944 DK   \n 5 Ireland                        3        1923       1969 IE   \n 6 Italy                          5        1926       1997 IT   \n 7 Germany                        4        1929       1972 DE   \n 8 United States of America       9        1930       2016 US   \n 9 United Kingdom                 6        1932       2005 GB   \n10 Russia                         4        1933       1970 RU   \n\n\n\n4 Plot\n\npp1 &lt;- ggplot(nobel_countries) +\n  aes(x = prize_year, y = cum, group = birth_country) +\n  geom_line(aes(color = birth_country)) +\n  geom_point(data = first_last_nobel, y = 0,\n             aes(x = first_prize, color =birth_country)) +\n  geom_flag(data = first_last_nobel, size = 0.03, asp= 2,\n            aes(x= last_prize, y = n_prize, image = code)) +\n  scale_color_manual(values = lacroix_palette(\"PassionFruit\", n = 10, type = \"continuous\")) +\n  scale_y_continuous(limits = c(NA, 12), breaks = c(0,3,6,9,12)) +\n  labs(title = \"Number of Liberty Nobel Prizes by Countries\",\n       color = NULL,\n       caption = \"Source: The Nobel Prize\\n@_abichat for #TidyTuesday\") +\n  theme_wsj(color = \"gray\") +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(size = 10, family = \"Georgia\"),\n        plot.title = element_text(size = 18, family = \"Andale Mono\"),\n        legend.text =  element_text(family = \"Georgia\")\n        )\n\npp1\n\n\n\n\n\n\n# ggsave(\"~/图片/plot_2019-05-14.png\", width = 29, height = 21, units = \"cm\", dpi = \"retina\")\n\n\n5 Animation\nFinally, let’s use gganimate package to add some animation.\n\nlibrary(gganimate)\npp1 +\n  transition_reveal(prize_year)\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2021-07-04-visualization-for-process-data/index.html",
    "href": "posts/2021-07-04-visualization-for-process-data/index.html",
    "title": "Visualization for Process Data",
    "section": "",
    "text": "This tutorial aims to explore various types of tools of visualizing the process data.\nBefore diving into the main text, I found one trick to git pull one repo but ignore the local changes is:"
  },
  {
    "objectID": "posts/2021-07-04-visualization-for-process-data/index.html#load-packages",
    "href": "posts/2021-07-04-visualization-for-process-data/index.html#load-packages",
    "title": "Visualization for Process Data",
    "section": "1 Load Packages",
    "text": "1 Load Packages\nlibrary(ProcData)\nlibrary(tidyverse)\nlibrary(RColorBrewer) # for color pallett"
  },
  {
    "objectID": "posts/2021-07-04-visualization-for-process-data/index.html#a-little-about-the-toy-data",
    "href": "posts/2021-07-04-visualization-for-process-data/index.html#a-little-about-the-toy-data",
    "title": "Visualization for Process Data",
    "section": "2 A little about the toy data",
    "text": "2 A little about the toy data\nA dataset containing the response processes and binary response outcomes of 16763 respondents. seqs is an object of class “proc” containing the action sequences and the time sequences of the respondents and responses is binary responses of 16763 respondents. The order of the respondents matches that in seqsß.\nstr(cc_data, max.level = 2)\n## List of 2\n##  $ seqs     :List of 2\n##   ..$ action_seqs:List of 16763\n##   ..$ time_seqs  :List of 16763\n##   ..- attr(*, \"class\")= chr \"proc\"\n##  $ responses: Named int [1:16763] 0 1 1 1 0 0 0 0 0 0 ...\n##   ..- attr(*, \"names\")= chr [1:16763] \"ARE000000200039\" \"ARE000000200051\" \"ARE000000300079\" \"ARE000000400093\" ...\nhead(cc_data$seqs$action_seqs, n = 3)\n## $ARE000000200039\n##  [1] \"start\"    \"0_0_0\"    \"1_2_-2\"   \"2_2_2\"    \"2_2_2\"    \"2_2_2\"   \n##  [7] \"2_2_2\"    \"2_2_2\"    \"2_2_-2\"   \"2_2_-2\"   \"2_-2_-2\"  \"-2_-2_-2\"\n## [13] \"-2_-2_-2\" \"-2_-2_-2\" \"-2_-2_-2\" \"-2_-2_-2\" \"-2_-2_0\"  \"-2_-2_0\" \n## [19] \"-2_-2_0\"  \"-2_0_1\"   \"-2_0_1\"   \"-2_0_1\"   \"-2_0_1\"   \"-2_0_1\"  \n## [25] \"0_0_1\"    \"0_0_1\"    \"0_0_1\"    \"0_0_1\"    \"0_0_1\"    \"0_0_1\"   \n## [31] \"0_0_1\"    \"0_0_1\"    \"0_0_1\"    \"0_0_1\"    \"0_0_1\"    \"end\"     \n## \n## $ARE000000200051\n##  [1] \"start\"    \"reset\"    \"-1_0_0\"   \"-1_-1_0\"  \"-1_-1_-1\" \"-1_0_0\"  \n##  [7] \"-1_0_0\"   \"reset\"    \"2_0_0\"    \"reset\"    \"0_2_0\"    \"reset\"   \n## [13] \"0_0_2\"    \"reset\"    \"0_1_0\"    \"reset\"    \"0_-1_0\"   \"reset\"   \n## [19] \"-1_0_0\"   \"reset\"    \"end\"     \n## \n## $ARE000000300079\n## [1] \"start\" \"1_1_1\" \"reset\" \"0_0_1\" \"reset\" \"0_1_0\" \"reset\" \"1_0_0\" \"end\""
  },
  {
    "objectID": "posts/2021-07-04-visualization-for-process-data/index.html#data-transformation",
    "href": "posts/2021-07-04-visualization-for-process-data/index.html#data-transformation",
    "title": "Visualization for Process Data",
    "section": "3 Data Transformation",
    "text": "3 Data Transformation\n## actions\ndt1 &lt;- cc_data$seqs$action_seqs[1:30]\n## time stamps\ndt2 &lt;- cc_data$seqs$time_seqs[1:30]\n\n## x轴为时间轴，y轴为不同的observations\ndt1_long &lt;- mapply(function(x, y) data.frame(ID = y, action = x) , dt1, names(dt1), SIMPLIFY = FALSE)\ndt1_long &lt;- Reduce(rbind, dt1_long)\n\ndt2_long &lt;- mapply(function(x, y) data.frame(ID = y, time = x) , dt2, names(dt2), SIMPLIFY = FALSE)\ndt2_long &lt;- Reduce(rbind, dt2_long)\n\ndt_full &lt;- cbind(dt1_long, time = dt2_long[,2]) %&gt;% \n  group_by(ID) %&gt;% \n  mutate(time_upper = lead(time)) %&gt;% \n  ungroup() %&gt;% \n  mutate(time_upper = ifelse(is.na(time_upper), time, time_upper), action = as.factor(action))\nhead(dt_full)\n## # A tibble: 6 x 4\n##   ID              action  time time_upper\n##   &lt;chr&gt;           &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 ARE000000200039 start    0         49.3\n## 2 ARE000000200039 0_0_0   49.3       55.9\n## 3 ARE000000200039 1_2_-2  55.9       61.7\n## 4 ARE000000200039 2_2_2   61.7       62.6\n## 5 ARE000000200039 2_2_2   62.6       63.2\n## 6 ARE000000200039 2_2_2   63.2       63.5"
  },
  {
    "objectID": "posts/2021-07-04-visualization-for-process-data/index.html#data-visualization",
    "href": "posts/2021-07-04-visualization-for-process-data/index.html#data-visualization",
    "title": "Visualization for Process Data",
    "section": "4 Data Visualization",
    "text": "4 Data Visualization\nset.seed(1234)\nn &lt;- 30 # 30 colors\nqual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]\ncol_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))\nline_color = sample(col_vector, n)\n\nggplot(aes(x = time, y = ID, col = action), data = dt_full) +\n  geom_point(size = 2)+\n  geom_linerange(aes(xmin = time, xmax= time_upper), linetype = 1, size = 1.5)+\n  scale_color_manual(values = col_vector, name = \"\") +\n  labs(y = \"\", x = \"Time Length\") +\n  theme(legend.position=\"\")"
  },
  {
    "objectID": "posts/2021-01-31-ppmc-cfa/index.html",
    "href": "posts/2021-01-31-ppmc-cfa/index.html",
    "title": "[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis",
    "section": "",
    "text": "Posterior Predictive Model Checking (PPMC) is frequently used for model fit evaluation in Bayesian Confirmatory Factor Analysis (BCFA). In standard PPMC procedures, model misfit is quantified by the location of a ML-based estimate to the predictive distribution of a statistic for a model. When the ML-based point estimate is far away from the center of the density of the posterior predictive distribution, model fit is poor. One main critique of such standard PPMC procedures is the strong link to the ML-based point estimates of the observed data. Not included in this approach, however, is how variable the ML-based point estimates are and their use in general as the reference point for Bayesian analyses. We propose a new method of PPMC based on the Posterior Predictive distribution of Bayesian saturated model for BCFA models. The method uses the predictive distribution from parameters of the posterior distribution of the saturated model as reference to detect the local misfit of hypothesized models. The results of the simulation study suggest that the saturated model PPMC approach was an accurate method of determining local model misfit and could be used for model comparison. A real example is also provided in this study."
  },
  {
    "objectID": "posts/2021-01-31-ppmc-cfa/index.html#abstract",
    "href": "posts/2021-01-31-ppmc-cfa/index.html#abstract",
    "title": "[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis",
    "section": "",
    "text": "Posterior Predictive Model Checking (PPMC) is frequently used for model fit evaluation in Bayesian Confirmatory Factor Analysis (BCFA). In standard PPMC procedures, model misfit is quantified by the location of a ML-based estimate to the predictive distribution of a statistic for a model. When the ML-based point estimate is far away from the center of the density of the posterior predictive distribution, model fit is poor. One main critique of such standard PPMC procedures is the strong link to the ML-based point estimates of the observed data. Not included in this approach, however, is how variable the ML-based point estimates are and their use in general as the reference point for Bayesian analyses. We propose a new method of PPMC based on the Posterior Predictive distribution of Bayesian saturated model for BCFA models. The method uses the predictive distribution from parameters of the posterior distribution of the saturated model as reference to detect the local misfit of hypothesized models. The results of the simulation study suggest that the saturated model PPMC approach was an accurate method of determining local model misfit and could be used for model comparison. A real example is also provided in this study."
  },
  {
    "objectID": "posts/2021-01-31-ppmc-cfa/index.html#citation",
    "href": "posts/2021-01-31-ppmc-cfa/index.html#citation",
    "title": "[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis",
    "section": "\n2 Citation",
    "text": "2 Citation\nZhang, J., Templin, J., & Mintz, C. E. (2021, February 9). A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis. https://doi.org/10.31234/osf.io/rf64x"
  },
  {
    "objectID": "posts/2021-01-31-ppmc-cfa/index.html#download",
    "href": "posts/2021-01-31-ppmc-cfa/index.html#download",
    "title": "[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis",
    "section": "\n3 Download",
    "text": "3 Download\nThe document can be viewed and downloaded here."
  },
  {
    "objectID": "posts/2018-09-11-model-checking-in-dcm/index.html",
    "href": "posts/2018-09-11-model-checking-in-dcm/index.html",
    "title": "Introduce Descrepancy Measures",
    "section": "",
    "text": "This Blog is the notes for my recent project about reliability and model checking. Next I want to organize a little about one important concept in model checking - discrepancy measures.\n\n\n1 Descrepancy Measures\n\n\\chi^2 measures for item-pairs (Chen & Thissen, 1997) \nX^2_{jj'}=\\sum_{k=0}^{1} \\sum_{k'=0}^{1} \\frac{(n_{kk'}-E(n_{kk'}))^2}{E(n_{kk'})}\n\n\nG^2 for item pairs\n\nG^2_{jj'}=-2\\sum_{k=0}^{1} \\sum_{k'=0}^{1} \\ln \\frac{E(n_{kk'})}{n_{kk'}}\n\n\nmodel-based covariance (MBC; Reckase, 1997) \nCOV_{jj'} = \\frac{\\sum_{i=1}^{N}(X_{ij}-\\overline{X_j})(X_{ij'}-\\overline{X_{j'}}) }{N} \\\\\nMBC_{jj'} = \\frac{\\sum_{i=1}^{N}(X_{ij}-E(X_{ij}))(X_{ij'}-E(X_{ij'}))}{N}\n\nQ_3 (Yen, 1993) \nQ_{3jj'} = r_{e_{ij}e_{ij'}}\n where r refers to the correlation, e_{ij} = X_{ij} - E(X_{ij}), and E(X_{ij})\nResidual Item Covariance (Fu et al., 2005) \nRESIDCOV_{jj'} = \\frac{[(n_{11})(n_{00})-(n{10})(n_{01})]}{N^2} - \\frac{[E(n_{11})E(n_{00})-E(n_{10})E(n_{01})]}{E(N^2)}\n\nnatural log of the odds ratio (Agresti, 2002) \nLN(OR_{jj'})= \\ln[\\frac{(n_{11})(n_{00})}{(n_{10})(n_{01})}] = \\ln(n_{11}) +\\ln(n_{00})+\\ln(n_{10}) +\\ln(n_{01})\n\nstandardized log odds ratio residual (Chen & Thissen, 1997) \nSTDLN(OR_{jj'})-RESID =  \\frac\n{\\ln[\\frac{n_{11}n_{00}}{n_{10}n_{01}}]-\\ln[\\frac{E(n_{11})E(n_{00})}{E(n_{10})E(n_{01})}]}\n{\\sqrt{\\frac{1}{n_{11}}+\\frac{1}{n_{10}}+\\frac{1}{n_{01}}+\\frac{1}{n_{00}}}}\n\nMantel-Haenszel statistic (MH; Agresti, 2002; Sinharay et al., 2006) \nMH_{jj'} = \\frac{\\sum_rn_{11r}n_{00r}/n_r}{\\sum_rn_{10r}n_{01r}/n_r}\n where counts of examinees with a response pattern are conditional on rest score r, defined as the total test score excluding items j and j’.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-02-20-Danmu-Analysis-Bilibili/index.html",
    "href": "posts/2024-02-20-Danmu-Analysis-Bilibili/index.html",
    "title": "Danmu Cloud for Bilibili",
    "section": "",
    "text": "Figure 1: Word Cloud\n\n\n\n \n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2017-11-23-Latent-Profile-Analysis/index.html",
    "href": "posts/2017-11-23-Latent-Profile-Analysis/index.html",
    "title": "Latent Profile Analysis using MCLUST (in R)",
    "section": "",
    "text": "Hi there! This is Jihong. This is a webpage folked from JOSHUA M. ROSENBERG. It aims to provid a very clear example about how to conduct Latent Profile Analysis using MCLUST in r.\n\n1 Import data and load packages\n\nlibrary(tidyverse)\nlibrary(mclust)\nlibrary(hrbrthemes) # typographic-centric ggplot2 themes\n\n\ndata(\"iris\")\ndf &lt;- select(iris, -Species) # 4 variables\n\nexplore_model_fit &lt;- function(df, n_profiles_range = 1:9, \n                              model_names = c(\"EII\", \"VVI\", \"EEE\", \"VVV\")) {\n    x &lt;- mclustBIC(df, G = n_profiles_range, modelNames = model_names)\n    y &lt;- x %&gt;%\n        as.data.frame.matrix() %&gt;%\n        rownames_to_column(\"n_profiles\") %&gt;%\n        rename(`Constrained variance, fixed covariance` = EII, \n               `Freed variance, fixed covariance` = VVI,\n               `Constrained variance, constrained covariance` = EEE,\n               `Freed variance, freed covariance` = VVV)\n    y\n}\n\nfit_output &lt;- explore_model_fit(df, n_profiles_range = 1:6)\n\nlibrary(forcats)\n\nto_plot &lt;- fit_output %&gt;%\n    gather(`Covariance matrix structure`, val, -n_profiles) %&gt;% \n    mutate(\n      `Covariance matrix structure` = as.factor(`Covariance matrix structure`\n                                                ),\n      val = abs(val)) \n# this is to make the BIC values positive (to align with more common formula / interpretation of BIC)\n\n\nggplot(to_plot, aes(x = n_profiles, y = val, color = `Covariance matrix structure`, group = `Covariance matrix structure`)) +\n    geom_line() +\n    geom_point() +\n    ylab(\"BIC (smaller value is better)\") +\n    theme_ipsum_rc()\n\n\n\n\n\n\n\nFrom red to purple, the models become less constrained (more free). It appears that a two or three profile (mixture component) model with freely-estimated residual variances and covariances, or a four profile model with constrained residual covariances and variances, fit best (based on interpreting the BIC).\nGiven this, we can fit (and inspect) a model, say, the three profile model with freely-estimated residual variance and covariances.\n\ncreate_profiles_mclust &lt;- function(df,\n                                   n_profiles, \n                                   variance_structure = \"freed\",\n                                   covariance_structure = \"freed\"){\n    \n    if (variance_structure == \"constrained\" & covariance_structure == \"fixed\") {\n        \n        model_name &lt;- \"EEI\"\n        \n    } else if (variance_structure == \"freed\" & covariance_structure == \"fixed\") {\n        \n        model_name &lt;- \"VVI\"\n        \n    } else if (variance_structure == \"constrained\" & covariance_structure == \"constrained\") {\n        \n        model_name &lt;- \"EEE\"\n        \n    } else if (variance_structure == \"freed\" & covariance_structure == \"freed\") {\n        \n        model_name &lt;- \"VVV\"\n        \n    } else if (variance_structure == \"fixed\") {\n        \n        stop(\"variance_structure cannot equal 'fixed' using this function; change this to 'constrained' or 'freed' or try one of the models from mclust::Mclust()\")\n        \n    } \n    \n    x &lt;- Mclust(df, G = n_profiles, modelNames = model_name)\n    \n    print(summary(x))\n    \n    dff &lt;- bind_cols(df, classification = x$classification)\n    \n    proc_df &lt;- dff %&gt;%\n        mutate_at(vars(-classification), scale) %&gt;%\n        group_by(classification) %&gt;%\n        summarize_all(funs(mean)) %&gt;%\n        mutate(classification = paste0(\"Profile \", 1:n_profiles)) %&gt;%\n        mutate_at(vars(-classification), function(x) round(x, 3)) %&gt;%\n        rename(profile = classification)\n    \n    return(proc_df)\n    \n}\n\nm3 &lt;- create_profiles_mclust(df, 3, variance_structure = \"freed\", covariance_structure = \"freed\")\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 3\ncomponents: \n\n log-likelihood   n df       BIC       ICL\n      -180.1858 150 44 -580.8396 -584.0522\n\nClustering table:\n 1  2  3 \n50 45 55 \n\n\nWe can then plot the mean values for the variables used to estimate the model for each of the two profiles. Of course, there are other models that we may want to inspect with different covariance matrix structures or profile numbers.\n\nm3 %&gt;%\n    gather(key, val, -profile) %&gt;% \n    ggplot(aes(x = profile, y = val, fill = key, group = key)) +\n    geom_col(position = \"dodge\") +\n    ylab(\"Z-score\") +\n    xlab(\"\") +\n    scale_fill_discrete(\"\") +\n    theme_ipsum_rc()\n\n\n\n\n\n\n\nOne big question: Are the residual covariance structures correctly specified? There are a lot of possible specifications (see help file here). I think they are right, based on their definitions, inspecting their covariance matrices, and inspecting their plots. But they might not be.\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-02-10-Customize-Rstudio-Theme/index.html",
    "href": "posts/2024-02-10-Customize-Rstudio-Theme/index.html",
    "title": "Customize font family of Visual Mode in Rstudio and in Zotero",
    "section": "",
    "text": "Screenshot for customized material theme\n\n\n\nVisual model of quarto and rmarkdown is very easy to use. Though users can change font family for Source mode in Rstudio by Options menu, there is no direct way change font family for Visual model. However, by customizing the rstudio theme file—“.rstheme”, users can change font size/family/margin of Visual mode.\n\nFirst of all, users can install some customized rstudio themes from github. For example, following guidelines here.\nSecond, on Mac, copy one .rstheme file (~/.config/rstudio/themes) and rename it to your customized file (in my case, jihongMaterials.rstheme).\n➜  rstudio ls   \ndictionaries       rstudio-prefs.json themes\n➜  rstudio cd themes \n➜  themes ls\nmaterial-lighter.rstheme   material-palenight.rstheme material.rstheme\n➜  themes cp material.rstheme jihongMaterial.rstheme\n➜  themes code jihongMaterial.rstheme        \nThird, open up the .rstheme file and add the following code in the beginning of the file:\n\n\njihongMaterial.rstheme\n\n/* rs-theme-name: jihongMaterial */\n/* rs-theme-is-dark: TRUE */\n\n.pm-proportional-font {\n  font-family: \"Apple SD Gothic Neo\", serif !important;\n  font-size: 16pt !important;\n}\n\n.pm-editing-root-node {\n    padding-left: calc((100% - 1300px)/2) !important;\n    padding-right: calc((100% - 1300px)/2) !important;\n}\n\nNote that .pm-proportional-font controls the font in Visual model. Users can change the font family/size just like CSS file.\nNow, restart your Rstudio, select your theme from `Global Options`. You should find the user-specified font family in your Visual Model\nHere is the my customized file: jihongMaterial.rstheme\nNote:\n\nPlease look over the website for more details."
  },
  {
    "objectID": "posts/2024-02-10-Customize-Rstudio-Theme/index.html#rstudio-visual-editor-font",
    "href": "posts/2024-02-10-Customize-Rstudio-Theme/index.html#rstudio-visual-editor-font",
    "title": "Customize font family of Visual Mode in Rstudio and in Zotero",
    "section": "",
    "text": "Screenshot for customized material theme\n\n\n\nVisual model of quarto and rmarkdown is very easy to use. Though users can change font family for Source mode in Rstudio by Options menu, there is no direct way change font family for Visual model. However, by customizing the rstudio theme file—“.rstheme”, users can change font size/family/margin of Visual mode.\n\nFirst of all, users can install some customized rstudio themes from github. For example, following guidelines here.\nSecond, on Mac, copy one .rstheme file (~/.config/rstudio/themes) and rename it to your customized file (in my case, jihongMaterials.rstheme).\n➜  rstudio ls   \ndictionaries       rstudio-prefs.json themes\n➜  rstudio cd themes \n➜  themes ls\nmaterial-lighter.rstheme   material-palenight.rstheme material.rstheme\n➜  themes cp material.rstheme jihongMaterial.rstheme\n➜  themes code jihongMaterial.rstheme        \nThird, open up the .rstheme file and add the following code in the beginning of the file:\n\n\njihongMaterial.rstheme\n\n/* rs-theme-name: jihongMaterial */\n/* rs-theme-is-dark: TRUE */\n\n.pm-proportional-font {\n  font-family: \"Apple SD Gothic Neo\", serif !important;\n  font-size: 16pt !important;\n}\n\n.pm-editing-root-node {\n    padding-left: calc((100% - 1300px)/2) !important;\n    padding-right: calc((100% - 1300px)/2) !important;\n}\n\nNote that .pm-proportional-font controls the font in Visual model. Users can change the font family/size just like CSS file.\nNow, restart your Rstudio, select your theme from `Global Options`. You should find the user-specified font family in your Visual Model\nHere is the my customized file: jihongMaterial.rstheme\nNote:\n\nPlease look over the website for more details."
  },
  {
    "objectID": "posts/2024-02-10-Customize-Rstudio-Theme/index.html#zotero-editor-font",
    "href": "posts/2024-02-10-Customize-Rstudio-Theme/index.html#zotero-editor-font",
    "title": "Customize font family of Visual Mode in Rstudio and in Zotero",
    "section": "2 Zotero Editor Font",
    "text": "2 Zotero Editor Font\n\n\n\nScreenshot for Zotero windown\n\n\n➜  mkdir /Users/[username]/Library/Application\\ Support/Zotero/Profiles/[xxxxxxx].default/Chrome\n➜  cd /Users/[username]/Library/Application\\ Support/Zotero/Profiles/[xxxxxxx].default/Chrome \n➜  touch userChrome.css\n➜  vim userChrome.css \n➜  open userChrome.css \nTo change the font family for Zotero window, create a new folder “Chrome” in above path. Next, create a userChrome.css in that folder. A template for css file is here. Add font-family setting in the first several lines (line3-5).\nIf you want to use my theme file, you can cope&paste following syntax into your userChrome.css file.\n\n\nuserChrome.css\n\n\n/*:::::::::::::::::::::::::::::::::::::::::::::::::::: Zotero :::::::::::::::::::::::::::::::::::::::::::::::::::*/\n* {border: 0 !important; \n  font-family: 'Baloo 2', Source Sans Pro, Fira Sans, SF Pro Text, -apple-system, sans-serif ! important;\n}\n\n/*change background color and text color of zotero toolbar and menubar*/\n#zotero-tb, #zotero-toolbar, #navigator-toolbox, #tab-bar-container { background: #323234 !important;\ncolor: #FFFFFF !important; }\n#zotero-toolbar {border-bottom: 1px solid #1d1d1d !important;}\n\n/*change color of menu items to white*/\n#navigator-toolbox menu { color: white !important }\n\n/*change color of menu items in drop down menus back to black*/\n#manage-attachments-menu label, #developer-menu label, #layout-menu label, #note-font-size-menu label, #font-size-menu label, #debug-output-menu label, #new-item label { color: black !important; }\n\n/*Fixed New Item showing in white*/\n#navigator-toolbox menu:first-child { color: unset !important; }\n\n/*Change color of tags selector below collections tree*/\n#zotero-tag-selector-container {  border-bottom: 1px solid #1d1d1d !important;\n  border-top:1px solid #1d1d1d !important;}\n\n#zotero-tag-selector-container input { -moz-appearance: none !important;\n  background: #474749 !important;\n  border: 1px solid #1d1d1d !important;\n  color: white !important; }\n\n.tag-selector-list-container {-moz-appearance: none !important;\n  background: #474749 !important;\n  color: white !important; }\n\n/* for Zotero 6*/\n.tag-selector-item:hover { background: #BBCEF1 !important;\n    color: black !important; }\n\n.tag-selector-item {\n  background: #474749 !important;\ncolor:white ; }\n\n.tag-selector-item.selected { background: #7E7E7E !important;\n    font-weight: bold !important; }\n\n.tag-selector-item.selected:hover { background: #BBCEF1 !important;\n    font-weight: bold !important; }\n\n#zotero-tb-search { -moz-appearance: none !important;\n  background-color: #474749 !important;\n  color: white !important;\n  border: 1px solid #1d1d1d !important; }\n\n#zotero-pane { background: #323234 !important;\ncolor: black !important; }\n\n/*change background and color of collections and items panes*/\ntreechildren { background: #323234 !important;\ncolor: #FFFFFF !important; }\n\n/*for Zotero 6*/\n.virtualized-table .row{background: #323234 !important; color: #FFFFFF !important;}\n.virtualized-table .row.highlighted {background: #BBCEF1 !important; color: black !important;}\n.virtualized-table .row.selected {background: #474749 !important; border: 0 !important;\nborder-bottom: 1px solid #1d1d1d !important;}\n.virtualized-table:not(:focus) .row.selected {background: #474749 !important; border: 0 !important;\nborder-bottom: 1px solid #1d1d1d !important;}\n.row:hover { background: #7E7E7E !important;\ncolor: #FFFFFF !important; border: 0 !important; }\n\n\n.tabs {  background: #474749 !important;\n  color: #FFFFFF !important;\nborder: 0px !important; }\n\n.tab {-moz-appearance: none !important;\ncolor: #7E7E7E !important;\nbackground: #474749 !important;\nborder-left: 0px solid rgba(127,127,127) !important;\nborder-right: 0px solid rgba(127,127,127) !important;}\n\n.tab:hover { background: #7E7E7E !important;\ncolor: #FFFFFF !important;\nborder: 0px !important; }\n\n.tab:not(:focus){ -moz-appearance: none !important;\ncolor: #7E7E7E ;\nbackground: #474749 ;\nborder-top: 0px solid #0a84ff !important;\nborder-left: 1px solid rgba(127,127,127) !important;\nborder-right: 0px solid rgba(127,127,127) !important;\nborder-radius: 0px !important;\nborder-bottom-left-radius: 0px !important;\nborder-bottom-right-radius: 0px !important; }\n\n.tab.selected:not(.highlighted){ color: #FFFFFF !important;\n  background: #323234 !important;\n  border-top: 3px solid #0a84ff !important;\n  border-left: 0px solid rgba(127,127,127) !important;\n  border-right: 0px solid #1d1d1d !important;\n  border-radius: 0px !important;\n  border-bottom-left-radius: 0px !important;\n  border-bottom-right-radius: 0px !important; }\n\n.tab-close:hover { background: #7E7E7E !important; }\n\n/*PDF reader notes list*/\n.notes-list-container { background: #323234 !important;\ncolor: #ffffff !important; }\n\n.note-row {background: #474749 !important;\n  color: #ffffff !important; }\n\n.note-row .date {color: #ffffff !important; }\n\n.more-row { background: #474749 !important;\n  color: #ffffff !important; }\n\n.note-row:hover, .more-row:hover {background: #7E7E7E !important;\ncolor: #FFFFFF !important; }\n\n/*collections tree background*/\n#zotero-collections-tree { color: #ffffff; background: #323234 !important; }\n/*increase spacing between rows in collections and items and add a border to separate panes (not working on zotero 6)*/\n/*#zotero-collections-tree treechildren::-moz-tree-row { height: 30px !important; }\n.virtualized-table { rowHeight: 30px !important; } */\n/*change background and color of collections when hovering*/\n#zotero-collections-tree treechildren::-moz-tree-row(hover) { background: #7E7E7E !important;\ncolor: #FFFFFF !important; border: 0 !important; }\n\n/*change color of selected item in Collections pane (My library or folders) */\n#zotero-collections-tree treechildren::-moz-tree-cell(selected) { background: #474749 !important; border: 0 !important;\nborder-bottom: 1px solid #1d1d1d !important; }\n\n#zotero-collections-tree treechildren::-moz-tree-cell-text(selected) { color: #FFFFFF !important; }\n\n#zotero-collections-tree treechildren { border-right: 1px solid #1d1d1d !important ; }\n/*for Zotero 6*/\n#zotero-collections-tree  .virtualized-table {border-right: 1px solid #1d1d1d !important ; }\n/*increase spacing between rows in collections and items (-moz-tree-row not working in zotero 6)*/\n/*#zotero-items-tree treechildren::-moz-tree-row { height: 30px !important; }*/\n#zotero-items-tree { border-right: 1px solid #1d1d1d !important;\nborder-bottom: 1px solid #1d1d1d !important; \ncolor: #ffffff; background: #323234 !important; }\n\n/*change background and color of items when hovering*/\n#zotero-items-tree treechildren::-moz-tree-row(hover) { background: #7E7E7E !important;\ncolor: #FFFFFF !important; border: 0 !important; }\n\n/*change background and color of items when selecting*/\n#zotero-items-tree treechildren::-moz-tree-row(selected) { background: #474749 !important;\ncolor: #FFFFFF !important; border: 0 !important;\nborder-bottom: 1px solid #1d1d1d !important; }\n#zotero-items-tree treechildren::-moz-tree-cell-text(selected) { color: #FFFFFF !important; border: 0 !important; }\n\n/*change background and color of columns field selector*/\n#zotero-items-tree treecol { -moz-appearance: none !important;\n  background: #474749 !important;\ncolor: #FFFFFF !important;\nborder-bottom: 1px solid #1d1d1d !important;\nborder-right: 1px solid #1d1d1d !important; }\n\n/*fix columns field selector for Zotero 6*/\n#zotero-items-tree .virtualized-table-header { -moz-appearance: none !important;\n  background: #474749 !important;\ncolor: #FFFFFF !important;\nborder-bottom: 1px solid #1d1d1d !important;\nborder-right: 1px solid #1d1d1d !important; }\n\n#zotero-items-tree .virtualized-table-header .cell:hover { background: #7E7E7E !important;\ncolor: #FFFFFF !important;}\n\n.treecol-image { -moz-appearance: none !important;\n  background: #474749 !important;\ncolor: #FFFFFF !important;\nborder-bottom: 1px solid #1d1d1d !important; }\n\n/*Change color of fields in item pane (clicky items) when hovering */\n#zotero-item-pane-content .zotero-clicky:hover\n{   color: black !important;\n  border-radius: 0px !important; }\n\n#better-bibtex-citekey-display { color: white !important; }\n\n/*change color of buttons, but messes with .zotero-clicky-minus and .zotero-clicky-plus in Tags tab*/\n\n/*Correct Minus and plus clicky buttons */\n#zotero-item-pane-content .zotero-clicky-minus:hover { color: transparent !important;}\n#zotero-item-pane-content .zotero-clicky-plus:hover { color: transparent !important;}\n\n/*tabs in Preferences dialog - well more like all the tabs except the ones I'll specify later*/\ntabpanel { color: black !important;\nbackground: #7E7E7E !important; }\n\ntab[selected=\"true\"] { color: black !important;\nbackground: #7E7E7E !important;\nborder-top: 3px solid #0a84ff !important; }\n\ntab { -moz-appearance: none !important;\ncolor: #eee !important;\nbackground: #474749 !important;\nborder-top: 0px solid black !important;\nborder-left: 0px solid black !important;\nborder-right: 1px solid #1d1d1d !important;\nborder-radius: 0px !important;\nborder-bottom-left-radius: 0px !important;\nborder-bottom-right-radius: 0px !important; }\n\ntab:hover { color: black !important;\nbackground: #E0E8F6 !important;\nbox-shadow: inset 0px 0px 0px !important;\nborder-radius: 0px 0px 0 0 !important; }\n\n/*change items in item pane besides tabs*/\n#zotero-duplicates-merge-pane &gt; groupbox &gt; .groupbox-body { background: #323234 !important;\n  color: #FFFFFF !important;\n  border: 0 !important; }\n\n#zotero-item-pane-content &gt; groupbox &gt; .groupbox-body { border: 0 !important; background: #323234 !important;\ncolor: #FFFFFF !important; }\n\n/*change color of retraction details background*/\n#retraction-details { background: #7f0000 !important; }\n\n/*background of item-pane-content (Zotero 6)*/\n.zotero-view-tabbox, .zotero-item-pane-content { background: #323234 !important; color: #FFFFFF !important; }\n\n/*tabs in Item pane*/\n#zotero-item-pane-content tabpanel { background: #474749 !important;\n  color: white !important; }\n\n/*fixed color in drop-down tags: see https://css-tricks.com/solved-with-css-dropdown-menus/*/\n#tags-box-container ul li ul { background: #7E7E7E !important; color: white !important; }\n#tags-box-container input { -moz-appearance: none !important;\n  background-color: #7E7E7E !important;\n  color: white !important;\n  border: 1px solid #1d1d1d !important;}\n\n/*In PDF reader tab pane*/\n.zotero-view-tabbox  .tags-box-container ul li ul { background: #7E7E7E !important; color: white !important; }\n.zotero-view-tabbox  .tags-box-container input { -moz-appearance: none !important;\n    background-color: #7E7E7E !important;\n    color: white !important;\n    border: 1px solid #1d1d1d !important;}\n\n.zotero-view-tabbox tab[selected=\"true\"]{ background: #474749 !important;\nborder-top: 3px solid #0a84ff !important;}\n\n.zotero-view-tabbox tab { -moz-appearance: none !important;\ncolor: #eee !important;\nbackground: #323234 !important;\nborder-top: 0px solid #0a84ff !important;\nborder-left: 0px solid rgba(127,127,127,0.2) !important;\nborder-right: 1px solid #1d1d1d !important;\nborder-radius: 0px !important;\nborder-bottom-left-radius: 0px !important;\nborder-bottom-right-radius: 0px !important; }\n\n.zotero-view-tabbox tab:hover{ color: #FFFFFF !important;\nbackground: #7E7E7E !important; }\n\n.zotero-view-tabbox tabpanel { background: #474749 !important;\n  color: #FFFFFF !important; }\n  .zotero-view-tabbox .zotero-clicky:hover { color: black !important; }\n\n#zotero-item-pane-content tab[selected=\"true\"]{ background: #474749 !important;\nborder-top: 3px solid #0a84ff !important;}\n\n#zotero-item-pane-content tab { -moz-appearance: none !important;\ncolor: #eee !important;\nbackground: #323234 !important;\nborder-top: 0px solid #0a84ff !important;\nborder-left: 0px solid rgba(127,127,127,0.2) !important;\nborder-right: 1px solid #1d1d1d !important;\nborder-radius: 0px !important;\nborder-bottom-left-radius: 0px !important;\nborder-bottom-right-radius: 0px !important; }\n\n#zotero-item-pane-content tab:hover{ color: #FFFFFF !important;\nbackground: #7E7E7E !important;\nbox-shadow: inset 0px 0px 0px !important;\nborder-radius: 0px 0px 0 0 !important; }\n\n/*define color of item type field*/\n#item-type-menu { -moz-appearance: none;\n    color: white !important;\n    height: 1.5em !important;\n    min-height: 1.5em !important;\n    padding: 0 0 0 2px !important;\n    margin: 1px 5px 0 1px !important;\n    max-height: 1.5em !important;\n    border: 1px solid transparent;\n    background-color: transparent; }\n\n#item-type-menu:hover { color: black !important; \nbackground: #BBCEF1 !important; }\n\n/*define color of date-field-status*/\n#zotero-date-field-status { color: #ffffff !important;\n    padding: 0 10px 0 1px !important; }\n\n/*preferences dialog*/\n#zotero-prefpane-general, #zotero-prefpane-sync, #zotero-prefpane-export, #zotero-prefpane-advanced-keys-tab, #wordProcessors, #zotero-quickCopy-format, #sync-reset-form, #zotero-export-options { color: black !important;\nbackground: #7E7E7E !important; }\n#zotero-prefpane-general title { background: #474749 !important;\ncolor: #FFFFFF !important; }\n\nprefwindow { color: black !important;\nbackground: #7E7E7E !important; }\n\nprefwindow .chromeclass-toolbar { color: #FFFFFF !important;\nbackground: #323234 !important; }\n\n/*about dialog*/\n#aboutcontent { background: #474749 !important;\n  color: white !important;\n    padding: 10px; }\n\n#version { color: white !important; }\n\n/*advanced search dialog*/\n#zotero-advanced-search-dialog #zotero-search-box-controls { color: black !important;\nbackground: #7E7E7E !important; }\n\n/*Note editor - ongoing*/\n#zotero-note-editor linksbox { background-color: #474749 !important;\ncolor: white !important; }\n\n#zotero-note-window h1 { color: white;\nfont-size: 1.6em;\npadding-bottom: none; }\n\n#zotero-note-window h2 { color: white;\nfont-size: 1.4em;\nfont-weight: bold;\npadding-bottom: none;\nborder-bottom: none; }\n\n#zotero-note-window h3 { color: white;\nfont-size: 1.2em; }\n\n#zotero-note-window p { text-indent: 1.75em; }\n\n#zotero-note-window ul, ol { padding-left: 1.75em; }\n\n#zotero-note-window blockquote { margin-top: 0;\nmargin-bottom: 0;\nmargin-left: 0;\npadding-left: 1.55em;\nborder-left: 3px solid darkgrey;\ncolor: white (255, 248, 248); }\n\n#zotero-note-window blockquote p { text-indent: 0; }\n\n\n\n\nScreenshot for customized material theme\nScreenshot for Zotero windown"
  },
  {
    "objectID": "posts/2017-11-20-Longitudinal-Effect-of-Motivation/index.html",
    "href": "posts/2017-11-20-Longitudinal-Effect-of-Motivation/index.html",
    "title": "Parental Involvement and Children Motivation",
    "section": "",
    "text": "Parental Involvement"
  },
  {
    "objectID": "posts/2017-11-20-Longitudinal-Effect-of-Motivation/index.html#what-is-parental-involvement",
    "href": "posts/2017-11-20-Longitudinal-Effect-of-Motivation/index.html#what-is-parental-involvement",
    "title": "Parental Involvement and Children Motivation",
    "section": "1 What is Parental Involvement?",
    "text": "1 What is Parental Involvement?\nParent involvement is considered as one of the most influential factors in education researches. Parents choose specific forms of involvement in response to the specific requests for involvement from children and the school.\nThe indicators of parent involvement in education vary considerably across studies, most of which treat parental involvement as a unidimensional construct. Some other study identified four dimensions of parental involvement (Sui-Chu, 1996). Some early studies conceived parental involvement as involving parents in school activities; More recent studies have emphasized parents’ actions at home, such as discussing their children’s experiences at school and helping children with their schoolwork.\n\n1.1 Relationship with Academic Achievement\nOne meta analysis of parental involvement and students’ academic achievement (Fan, 2001) suggest that the vast proportion of the literature in this area is qualitative and non-empirical. Among the empirical studies that have investigated the issue quantitatively, there appear to be considerable inconsistencies. The study indicates that a small to moderate, and piratically meaningful, relationship between parental involvement and academic achievement. Parental aspiration/expectation for children’s education achievement has the strongest relationship, whereas parental home supervision has the weakest relationship with students’ academic achievement. In addition, the relationship is stronger for global indicator (e.g. GPA) of academic achievement than a subject-specific indicator (e.g., math grade). Some other study also found that parents’ participation at school had a moderate effect on reading achievement, but a negligible effect on mathematics achievement.\n\n\n1.2 Relationship with Academic Motivation"
  },
  {
    "objectID": "posts/2017-11-20-Longitudinal-Effect-of-Motivation/index.html#academic-motivation",
    "href": "posts/2017-11-20-Longitudinal-Effect-of-Motivation/index.html#academic-motivation",
    "title": "Parental Involvement and Children Motivation",
    "section": "2 Academic Motivation",
    "text": "2 Academic Motivation"
  },
  {
    "objectID": "posts/2018-09-12-manual-learn-jags-in-r/index.html",
    "href": "posts/2018-09-12-manual-learn-jags-in-r/index.html",
    "title": "[Manual]Using Jags and R2jags in R",
    "section": "",
    "text": "This post is aimed to introduce the basics of using jags in R programming. Jags is a frequently used program for conducting Bayesian statistics.Most of information below is borrowed from Jeromy Anglim’s Blog. I will keep editing this post if I found more resources about jags."
  },
  {
    "objectID": "posts/2018-09-12-manual-learn-jags-in-r/index.html#what-is-jags",
    "href": "posts/2018-09-12-manual-learn-jags-in-r/index.html#what-is-jags",
    "title": "[Manual]Using Jags and R2jags in R",
    "section": "1 What is JAGS?",
    "text": "1 What is JAGS?\nJAGS stands for Just Another Gibbs Sampler. To quote the program author, Martyn Plummer, “It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation…” It uses a dialect of the BUGS language, similar but a little different to OpenBUGS and WinBUGS."
  },
  {
    "objectID": "posts/2018-09-12-manual-learn-jags-in-r/index.html#installation",
    "href": "posts/2018-09-12-manual-learn-jags-in-r/index.html#installation",
    "title": "[Manual]Using Jags and R2jags in R",
    "section": "2 Installation",
    "text": "2 Installation\nTo run jags with R, There is an interface with R called rjags. 1. Download and install Jags based on your operating system. 2. Install additional R packages: type install.packages(c(“rjags”,”coda”)) in R console. rjags is to interface with JAGS and coda is to process MCMC output."
  },
  {
    "objectID": "posts/2018-09-12-manual-learn-jags-in-r/index.html#jags-examples",
    "href": "posts/2018-09-12-manual-learn-jags-in-r/index.html#jags-examples",
    "title": "[Manual]Using Jags and R2jags in R",
    "section": "3 JAGS Examples",
    "text": "3 JAGS Examples\nThere are a lot of examples online. The following provides links or simple codes to JAGS code.\n\nJustin Esarey\n\nAn entire course on Bayesian Statistics with examples in R and JAGS. It includes 10 lectures and each lecture lasts around 2 hours. The content is designed for a social science audience and it includes a syllabus linking with Simon Jackman’s text. The videos are linked from above or available direclty on YouTube.\n\nJeromy Anglim\n\nThe author of this blog also provides a few examples. He shared the codes on his github account\n\nJohn Myles White\n\nA course on statistical models that is under development with JAGS scripts on github.\nSimple introductory examples of fitting a normal distribution, linear regression, and logistic regression\nA follow-up post demonstrating the use of the coda package with rjags to perform MCMC diagnostics.\n\nA simple simulation sample:\n\nFirst, simulate the Data:\n\nlibrary(R2jags)\nn.sim &lt;- 100; set.seed(123)\nx1 &lt;- rnorm(n.sim, mean = 5, sd = 2)\nx2 &lt;- rbinom(n.sim, size = 1, prob = 0.3)\ne &lt;- rnorm(n.sim, mean = 0, sd = 1)\n\nNext, we create the outcome y based on coefficients b_1 and b_2 for the respective predictors and an intercept a:\n\nb1 &lt;- 1.2\nb2 &lt;- -3.1\na &lt;- 1.5\ny &lt;- b1*x1 + b2*x2 + e\n\nNow, we combine the variables into one dataframe for processing later:\n\nsim.dat &lt;- data.frame(y, x1, x2)\n\nAnd we create and summarize a (frequentist) linear model fit on these data:\n\nfreq.mod &lt;- lm(y ~ x1 + x2, data = sim.dat)\nsummary(freq.mod)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sim.dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3432 -0.6797 -0.1112  0.5367  3.2304 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.34949    0.28810   1.213    0.228    \nx1           1.13511    0.05158  22.005   &lt;2e-16 ***\nx2          -3.09361    0.20650 -14.981   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9367 on 97 degrees of freedom\nMultiple R-squared:  0.8772,    Adjusted R-squared:  0.8747 \nF-statistic: 346.5 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n3.1 Beyesian Model\n\nbayes.mod &lt;- function() {\n for(i in 1:N){\n y[i] ~ dnorm(mu[i], tau)\n mu[i] &lt;- alpha + beta1 * x1[i] + beta2 * x2[i]\n }\n alpha ~ dnorm(0, .01)\n beta1 ~ dunif(-100, 100)\n beta2 ~ dunif(-100, 100)\n tau ~ dgamma(.01, .01)\n}\n\nNow define the vectors of the data matrix for JAGS:\n\ny &lt;- sim.dat$y\nx1 &lt;- sim.dat$x1\nx2 &lt;- sim.dat$x2\nN &lt;- nrow(sim.dat)\n\nRead in the data frame for JAGS\n\nsim.dat.jags &lt;- list(\"y\", \"x1\", \"x2\", \"N\")\n\nDefine the parameters whose posterior distributions you are interested in summarizing later:\n\nbayes.mod.params &lt;- c(\"alpha\", \"beta1\", \"beta2\")\n\nSetting up starting values\n\nbayes.mod.inits &lt;- function(){\n list(\"alpha\" = rnorm(1), \"beta1\" = rnorm(1), \"beta2\" = rnorm(1))\n}\n\n# inits1 &lt;- list(\"alpha\" = 0, \"beta1\" = 0, \"beta2\" = 0)\n# inits2 &lt;- list(\"alpha\" = 1, \"beta1\" = 1, \"beta2\" = 1)\n# inits3 &lt;- list(\"alpha\" = -1, \"beta1\" = -1, \"beta2\" = -1)\n# bayes.mod.inits &lt;- list(inits1, inits2, inits3)\n\n\n\n3.2 Fitting the model\n\nset.seed(123)\nbayes.mod.fit &lt;- jags(data = sim.dat.jags, inits = bayes.mod.inits,\n  parameters.to.save = bayes.mod.params, n.chains = 3, n.iter = 9000,\n  n.burnin = 1000, model.file = bayes.mod)\n\nmodule glm loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 4\n   Total graph size: 511\n\nInitializing model\n\n\n\n\n3.3 Diagnostics\n\nprint(bayes.mod.fit)\n\nInference for Bugs model at \"/var/folders/9t/ryz7lf_s7ts720p_lwdttfhm0000gn/T//RtmppxyFg2/model157ce57d94444.txt\", fit using jags,\n 3 chains, each with 9000 iterations (first 1000 discarded), n.thin = 8\n n.sims = 3000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nalpha      0.362   0.293  -0.205   0.166   0.358   0.562   0.958 1.009   250\nbeta1      1.133   0.053   1.025   1.099   1.134   1.169   1.236 1.009   250\nbeta2     -3.090   0.205  -3.496  -3.231  -3.090  -2.950  -2.685 1.002  1700\ndeviance 271.830   2.899 268.167 269.718 271.122 273.198 279.223 1.000  3000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 4.2 and DIC = 276.0\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nplot(bayes.mod.fit)\n\ntraceplot(bayes.mod.fit)"
  },
  {
    "objectID": "posts/2019-10-08-handout-for-smep-poster/index.html",
    "href": "posts/2019-10-08-handout-for-smep-poster/index.html",
    "title": "SMEP 2019 Poster Session Materials",
    "section": "",
    "text": "See the handout and download the poster here.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#todays-lecture-objectives",
    "title": "Lecture 04",
    "section": "Today’s Lecture Objectives",
    "text": "Today’s Lecture Objectives\n\nMaking Stan Syntax Shorter\n\nComputing Functions of Model Parameters\nDownload R file DietDataExample2.R",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#in-previous-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#in-previous-class",
    "title": "Lecture 04",
    "section": "In previous class…",
    "text": "In previous class…\n\n\n\nR code for data read inlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(tidyr)\nlibrary(dplyr)\ncolor_scheme_set('brightblue')\ndat &lt;- read.csv(here::here(\"posts\", \"2024-01-12-syllabus-adv-multivariate-esrm-6553\", \"Lecture03\", \"Code\", \"DietData.csv\"))\ndat$DietGroup &lt;- factor(dat$DietGroup, levels = 1:3)\ndat$HeightIN60 &lt;- dat$HeightIN - 60\nkableExtra::kable( rbind(head(dat), tail(dat)) ) |&gt; kableExtra::kable_classic_2() |&gt; \n  kableExtra::kable_styling(full_width = F, font_size = 15)\n\n\n\n\n\nRespondent\nDietGroup\nHeightIN\nWeightLB\nHeightIN60\n\n\n\n1\n1\n1\n56\n140\n-4\n\n\n2\n2\n1\n60\n155\n0\n\n\n3\n3\n1\n64\n143\n4\n\n\n4\n4\n1\n68\n161\n8\n\n\n5\n5\n1\n72\n139\n12\n\n\n6\n6\n1\n54\n159\n-6\n\n\n25\n25\n3\n70\n259\n10\n\n\n26\n26\n3\n52\n201\n-8\n\n\n27\n27\n3\n59\n228\n-1\n\n\n28\n28\n3\n64\n245\n4\n\n\n29\n29\n3\n65\n241\n5\n\n\n30\n30\n3\n72\n269\n12\n\n\n\n\n\n\n\n\nIntroduce the empty model\nExample: Post-Diet Weights\n\nWeightLB (Dependent Variable): The respondents’ weight in pounds\nHeightIN: The respondents’ height in inches\nDietGroup: 1, 2, 3 representing the group to which a respondent was assigned\n\n\nThe empty model has two parameters to be estimated: (1) \\(\\beta_0\\), (2) \\(\\sigma_e\\)\n\nThe posterior mean/median of \\(\\beta_0\\) should be mean of WeightLB\nThe posterior mean/median of \\(\\sigma_e\\) should be sd of WeightLB",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#making-stan-code-short-and-efficient",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#making-stan-code-short-and-efficient",
    "title": "Lecture 04",
    "section": "Making Stan Code Short and Efficient",
    "text": "Making Stan Code Short and Efficient\nThe Stan syntax from our previous model was lengthy:\n\nA declared variable for each parameter\nThe linear combination of coefficients by multiplying predictors\n\nStan has built-in features to shorten syntax:\n\nMatrices/Vectors\nMatrix products\nMultivariate distribution (initially for prior distributions)\nBuilt-in Functions (sum() better than +=)\n\nNote: if you are interested in Efficiency tuning in Stan, look at this Charpter for more details.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#linear-models-without-matrices",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#linear-models-without-matrices",
    "title": "Lecture 04",
    "section": "Linear Models without Matrices",
    "text": "Linear Models without Matrices\nThe linear model from our example was:\n\\[\n\\text{WeightLB}_p = \\beta_0 + \\beta_1 \\text{HeightIN}_p + \\beta_2 \\text{Group2}_p + \\beta_3\\text{Group3}_p \\\\\n+\\beta_4 \\text{HeightIN}_p\\text{Group2}_p \\\\\n+\\beta_5 \\text{HeightIN}_p\\text{Group3}_p \\\\\n+ e_p\n\\]\nwith:\n\n\\(\\text{Group2}_p\\) the binary indicator of person \\(p\\) being in group 2\n\\(\\text{Group}3_p\\) the binary indicator of person \\(p\\) being in group 3\n\\(e_p \\sim N(0, \\sigma_e)\\)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#linear-models-with-matrices",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#linear-models-with-matrices",
    "title": "Lecture 04",
    "section": "Linear Models with Matrices",
    "text": "Linear Models with Matrices\n\n\nModel (predictor) matrix with the size 30 (rows) \\(\\times\\) 6 (columns)\n\\[\n\\mathbf{X} = \\begin{bmatrix}1 & -4 & 0 & 0 & 0 & 0\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 12 & 0 & 1 & 0 & 12 \\end{bmatrix}\n\\]\n\nCoefficients vectors with the size 6 (rows) \\(\\times\\) 1 (column):\n\\[\n\\mathbf{\\beta} =\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4 \\\\\n\\beta_5 \\\\\n\\end{bmatrix}\n\\]\n\n\n\nFullModelFormula = as.formula(\"WeightLB ~ HeightIN60 + DietGroup + HeightIN60*DietGroup\")\nmodel.matrix(FullModelFormula, data = dat) |&gt; head()\n\n  (Intercept) HeightIN60 DietGroup2 DietGroup3 HeightIN60:DietGroup2\n1           1         -4          0          0                     0\n2           1          0          0          0                     0\n3           1          4          0          0                     0\n4           1          8          0          0                     0\n5           1         12          0          0                     0\n6           1         -6          0          0                     0\n  HeightIN60:DietGroup3\n1                     0\n2                     0\n3                     0\n4                     0\n5                     0\n6                     0",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#linear-models-with-matrices-cont.",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#linear-models-with-matrices-cont.",
    "title": "Lecture 04",
    "section": "Linear Models with Matrices (Cont.)",
    "text": "Linear Models with Matrices (Cont.)\nWe then rewrite the equation from\n\\[\n\\text{WeightLB}_p = \\beta_0 + \\beta_1 \\text{HeightIN}_p + \\beta_2 \\text{Group2}_p + \\beta_3\\text{Group3}_p \\\\\n+\\beta_4 \\text{HeightIN}_p\\text{Group2}_p \\\\\n+\\beta_5 \\text{HeightIN}_p\\text{Group3}_p \\\\\n+ e_p\n\\]\nto:\n\\[\n\\mathbf{WeightLB} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}\n\\]\nWhere:\n\n\\(\\mathbf{WeightLB}\\) is the vector of outcome (N \\(\\times\\) 1)\n\\(\\mathbf{X}\\) is the model (predictor) matrix (N \\(\\times\\) P for P - 1 predictors)\n\\(\\boldsymbol{\\beta}\\) is the coefficients vector (P \\(\\times\\) 1)\n\\(\\mathbf{e}\\) is the vector for residuals (N \\(\\times\\) 1)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#vectorize-prior-distributions",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#vectorize-prior-distributions",
    "title": "Lecture 04",
    "section": "Vectorize prior distributions",
    "text": "Vectorize prior distributions\nPreviously, we defined a normal distribution for each regression coefficient \\[\n\\beta_0 \\sim normal(0, 1) \\\\\n\\vdots \\\\\n\\beta_p \\sim normal(0, 1)\n\\]\n\nThey are all univariate normal distribution\nIssue: Each parameter had a prior that was independent of the other parameter; then the correlation between betas is low and cannot be changed.\n\nFor example, the code shows two betas with univariate normal distribution have low correlation (r = -0.025)\n\nset.seed(1234)\nbeta0 = rnorm(100, 0, 1)\nbeta1 = rnorm(100, 0, 1)\ncor(beta0, beta1)\n\n[1] -0.02538285",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#vectorize-prior-distributions-cont.",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#vectorize-prior-distributions-cont.",
    "title": "Lecture 04",
    "section": "Vectorize prior distributions (Cont.)",
    "text": "Vectorize prior distributions (Cont.)\nWhen combining all parameters into a vector, a natural extension is a multivariate normal distribution, so that the betas have a pre-defined correlation strength\n\nThe syntax shows the two betas generated by the multivariate normal distribution with correlation of .5\n\n\nset.seed(1234)\nsigma_of_betas = matrix(c(1, 0.5, 0.5, 1), ncol = 2)\nbetas = mvtnorm::rmvnorm(100, mean = c(0, 0), sigma = sigma_of_betas)\nbeta0 = betas[,1]\nbeta1 = betas[,2]\ncor(beta0, beta1)\n\n[1] 0.5453899\n\n\nBack to the stan code, we need to specify:\n\nMean vector of betas (meanBeta; size P \\(\\times\\) 1)\n\nPut all prior means for those coefficients into a vector\n\n\nCovariance matrix for betas (covBeta; size P \\(\\times\\) P)\n\nPut all prior variances into the diagonal; zeros for off diagonal; ’cause we are not sure the potential correlation between betas",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#syntax-changes-data-section",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#syntax-changes-data-section",
    "title": "Lecture 04",
    "section": "Syntax Changes: Data Section",
    "text": "Syntax Changes: Data Section\n\n\n\n\nOld syntax without matrix:\n\n\ndata{\n    int&lt;lower=0&gt; N;\n    vector[N] weightLB;\n    vector[N] height60IN;\n    vector[N] group2;\n    vector[N] group3;\n    vector[N] heightXgroup2;\n    vector[N] heightXgroup3;\n}\n\n\n\n\n\nNew syntax with matrix:\n\n\n\ndata{\n  int&lt;lower=0&gt; N;         // number of observations\n  int&lt;lower=0&gt; P;         // number of predictors (plus column for intercept)\n  matrix[N, P] X;         // model.matrix() from R \n  vector[N] weightLB;     // outcome\n  real sigmaRate;         // hyperparameter: rate parameter for residual standard deviation\n}",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#syntax-changes-parameters-section",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#syntax-changes-parameters-section",
    "title": "Lecture 04",
    "section": "Syntax Changes: Parameters Section",
    "text": "Syntax Changes: Parameters Section\n\n\n\n\nOld syntax without matrix:\n\n\n\nparameters {\n  real beta0;\n  real betaHeight;\n  real betaGroup2;\n  real betaGroup3;\n  real betaHxG2;\n  real betaHxG3;\n  real&lt;lower=0&gt; sigma;\n}\n\n\n\n\nNew syntax with matrix:\n\n\n\nparameters {\n  vector[P] beta;         // vector of coefficients for Beta\n  real&lt;lower=0&gt; sigma;    // residual standard deviation\n}",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#syntax-changes-prior-distributions-definition",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#syntax-changes-prior-distributions-definition",
    "title": "Lecture 04",
    "section": "Syntax Changes: Prior Distributions Definition",
    "text": "Syntax Changes: Prior Distributions Definition\n\n\n\n\nOld syntax without matrix:\n\n\n\nmodel {\n  beta0 ~ normal(0,100);\n  betaHeight ~ normal(0,100);\n  betaGroup2 ~ normal(0,100);\n  betaGroup3 ~ normal(0,100);\n  betaHxG2 ~ normal(0,100);\n  betaHxG3 ~ normal(0,100);\n  sigma ~ exponential(.1); // prior for sigma\n  weightLB ~ normal(\n    beta0 + betaHeight * height60IN + betaGroup2 * group2 + \n    betaGroup3*group3 + betaHxG2*heightXgroup2 +\n    betaHxG3*heightXgroup3, sigma);\n}\n\n\nNew syntax with matrix:\n\n\n\nmulti_normal() is the multivariate normal sampling in Stan, similar to rmvnorm() in R; For uninformative, we did not need to specify\n\nexponential() is the exponential distribution sampling in Stan, similar to rexp() in R\n\n\n\nmodel {\n  sigma ~ exponential(sigmaRate);         // prior for sigma\n  weightLB ~ normal(X*beta, sigma);       // linear model\n}",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#compare-results-and-computational-time",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#compare-results-and-computational-time",
    "title": "Lecture 04",
    "section": "Compare results and computational time",
    "text": "Compare results and computational time\n\n\n\nCodemod_full_old &lt;- cmdstan_model(\"Code/FullModel_Old.stan\")\ndata_full_old &lt;- list(\n  N = nrow(dat),\n  weightLB = dat$WeightLB,\n  height60IN = dat$HeightIN60,\n  group2 = as.numeric(dat$DietGroup == 2),\n  group3 = as.numeric(dat$DietGroup == 3),\n  heightXgroup2 = as.numeric(dat$DietGroup == 2) * dat$HeightIN60,\n  heightXgroup3 = as.numeric(dat$DietGroup == 3) * dat$HeightIN60\n)\nfit_full_old &lt;- mod_full_old$sample(\n  data = data_full_old,\n  seed = 1234,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 0\n)\n\n\n\nfit_full_old$summary()[, -c(9, 10)]\n\n# A tibble: 8 × 8\n  variable      mean  median    sd   mad     q5     q95  rhat\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 lp__       -76.7   -76.3   2.17  2.00  -80.8  -73.8    1.00\n2 beta0      148.    148.    3.26  3.20  142.   153.     1.00\n3 betaHeight  -0.370  -0.378 0.495 0.488  -1.17   0.437  1.00\n4 betaGroup2 -24.2   -24.1   4.59  4.52  -31.6  -16.4    1.00\n5 betaGroup3  81.2    81.1   4.31  4.36   74.6   88.5    1.00\n6 betaHxG2     2.46    2.45  0.690 0.678   1.34   3.59   1.00\n7 betaHxG3     3.56    3.55  0.658 0.648   2.48   4.66   1.00\n8 sigma        8.26    8.10  1.23  1.13    6.52  10.5    1.00\n\n\n\n\nCodemod_full_new &lt;- cmdstan_model(\"Code/FullModel_New.stan\")\nFullModelFormula = as.formula(\"WeightLB ~ HeightIN60 + DietGroup + HeightIN60*DietGroup\")\nX = model.matrix(FullModelFormula, data = dat)\ndata_full_new &lt;- list(\n  N = nrow(dat),\n  P = ncol(X),\n  X = X, \n  weightLB = dat$WeightLB,\n  sigmaRate = 0.1\n)\nfit_full_new &lt;- mod_full_new$sample(\n  data = data_full_new,\n  seed = 1234,\n  chains = 4,\n  parallel_chains = 4\n)\n\n\n\nfit_full_new$summary()[, -c(9, 10)]\n\n# A tibble: 8 × 8\n  variable    mean  median    sd   mad     q5     q95  rhat\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 lp__     -76.6   -76.3   2.15  2.00  -80.6  -73.9    1.00\n2 beta[1]  148.    148.    3.27  3.23  142.   153.     1.00\n3 beta[2]   -0.374  -0.372 0.483 0.479  -1.16   0.414  1.00\n4 beta[3]  -24.1   -24.2   4.59  4.45  -31.6  -16.5    1.00\n5 beta[4]   81.3    81.3   4.44  4.40   74.1   88.5    1.00\n6 beta[5]    2.47    2.48  0.683 0.676   1.32   3.56   1.00\n7 beta[6]    3.57    3.57  0.646 0.643   2.52   4.62   1.00\n8 sigma      8.25    8.10  1.26  1.20    6.50  10.5    1.00\n\n\n\n\nThe differences between two method:\n\n\nbetaGroup3 has the largest differences between two methods\n\n\ncbind(fit_full_old$summary()[,1], fit_full_old$summary()[, -c(1, 9, 10)] - fit_full_new$summary()[, -c(1, 9, 10)])\n\n    variable         mean     median            sd          mad        q5\n1       lp__ -0.007672400  0.0063000  0.0242926098  0.007931910 -0.115880\n2      beta0  0.071524500  0.1155000 -0.0135811270 -0.029652000 -0.102200\n3 betaHeight  0.004062205 -0.0067305  0.0121231572  0.008502340 -0.018165\n4 betaGroup2 -0.013670300  0.0513000 -0.0005405792  0.069904590 -0.037665\n5 betaGroup3 -0.103896750 -0.2107500 -0.1281290904 -0.041068020  0.468720\n6   betaHxG2 -0.019823355 -0.0341300  0.0072874047  0.001823598  0.020645\n7   betaHxG3 -0.010911025 -0.0213900  0.0116900063  0.005040840 -0.041349\n8      sigma  0.011585872  0.0005500 -0.0326167204 -0.068377512  0.021184\n          q95          rhat\n1  0.03527000  2.381442e-03\n2  0.00740000  1.491287e-04\n3  0.02306265  1.225973e-03\n4  0.07863000  4.621964e-05\n5 -0.04479000 -4.035145e-05\n6  0.02261500  5.967744e-04\n7  0.03879350  6.457172e-05\n8 -0.00486500  2.857860e-04",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#compare-computational-time",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#compare-computational-time",
    "title": "Lecture 04",
    "section": "Compare computational time",
    "text": "Compare computational time\n\nThe Stan code with matrix has faster computation:\n\n\n\n\nfit_full_old$time()\n\n$total\n[1] 0.2029581\n\n$chains\n  chain_id warmup sampling total\n1        1  0.054    0.043 0.097\n2        2  0.063    0.050 0.113\n3        3  0.057    0.053 0.110\n4        4  0.054    0.044 0.098\n\n\n\n\nfit_full_new$time()\n\n$total\n[1] 0.16032\n\n$chains\n  chain_id warmup sampling total\n1        1  0.030    0.028 0.058\n2        2  0.036    0.033 0.069\n3        3  0.036    0.033 0.069\n4        4  0.032    0.030 0.062\n\n\n\n\n\n\nPros: With matrices, there is less syntax to write\n\nModel is equivalent\nMore efficient for sampling (sample from matrix space)\nMore flexible: modify matrix elements in R instead of individual parameters in Stan\n\n\n\nCons: Output, however, is not labeled with respect to parameters\n\nMay have to label output",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#computing-functions-of-parameters",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#computing-functions-of-parameters",
    "title": "Lecture 04",
    "section": "Computing Functions of Parameters",
    "text": "Computing Functions of Parameters\n\n\nOften, we need to compute some linear or non-linear function of parameters in a linear model\n\nMissing effects - beta for diet group 2 and 3\nModel fit indices: \\(R^2\\)\nTransformed effects - residual variance \\(\\sigma^2\\)\n\n\nIn non-Bayesian (frequentist) analyses, there are often formed with the point estimates of parameters (with standard errors - second derivative of likelihood function)\n\nFor Bayesian analyses, however, we seek to build the posterior distribution for any function of parameters\n\nThis means applying the function to all posterior samples\nIt is especially useful when you want to propose your new statistic",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#wrapping-up",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#wrapping-up",
    "title": "Lecture 04",
    "section": "Wrapping up",
    "text": "Wrapping up\nToday we further added generated quantities into our Bayesian toolset:\n\nHow to make Stan use less syntax using matrices\nHow to form posterior distributions for functions of parameters\n\nWe will use both of these features in psychometric models.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#next-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture04/Lecture04.html#next-class",
    "title": "Lecture 04",
    "section": "Next Class",
    "text": "Next Class\n\nBayesian Model fit\nBayesian Model Comparison\n\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "Posts",
      "2024 01 12 Syllabus Adv Multivariate Esrm 6553",
      "Lecture04",
      "Lecture 04"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#todays-lecture-objectives",
    "title": "Lecture 02",
    "section": "Today’s Lecture Objectives",
    "text": "Today’s Lecture Objectives\n\nBayes’ Theorem\nLikelihood function, Posterior distribution\nHow to report posterior distribution of parameters\nBayesian update\n\nbut, before we begin… Music: Good Bayesian\nMusic: Good Bayesian\nAnil Seth",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#quiz-what-is-bayesian",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#quiz-what-is-bayesian",
    "title": "Lecture 02",
    "section": "Quiz: What is Bayesian?",
    "text": "Quiz: What is Bayesian?\n\nWhat are key components of Bayesian models?\n\nLikelihood function - data\nPrior distribution - belief / previous evidences of parameters\nPosterior distribution - updated information of parameters given our data and model\nPosterior predictive distribution - future / predicted data\n\nWhat are the differences between Bayesian with Frequentist analysis?\n\nprior distribution: Bayesian\nhypothesis of fixed parameters: frequentist\nestimation process: MCMC vs. MLE\nposterior distribution vs. point estimates of parameters\ncredible interval (plausibility of the parameters having those values) vs. confidence interval (the proportion of infinite samples having the fixed parameters)",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#bayes-theorem-how-bayesian-statistics-work",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#bayes-theorem-how-bayesian-statistics-work",
    "title": "Lecture 02",
    "section": "Bayes’ Theorem: How Bayesian Statistics Work",
    "text": "Bayes’ Theorem: How Bayesian Statistics Work\nBayesian methods rely on Bayes’ Theorem\n\\[\nP(\\theta | Data) = \\frac{P(Data|\\theta)P(\\theta)}{P(Data)} \\propto P(Data|\\theta) P(\\theta)\n\\]\nWhere:\n\nP: probability distribution function (PDF)\n\\(P(\\theta|Data)\\) : the posterior distribution of parameter \\(\\theta\\), given the observed data\n\\(P(Data|\\theta)\\): the likelihood function (conditional distributin) of the observed data, given the parameters\n\\(P(\\theta)\\): the prior distribution of parameter \\(\\theta\\)\n\\(P(Data)\\): the marginal distribution of the observed data",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#step-1-choose-the-likelihood-function-model",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#step-1-choose-the-likelihood-function-model",
    "title": "Lecture 02",
    "section": "Step 1: Choose the Likelihood function (Model)",
    "text": "Step 1: Choose the Likelihood function (Model)\n\nThe Likelihood function \\(P(X|p_1)\\) follows Binomial distribution of 3 succuess out of 5 samples:\n\\[P(X|p_1) = \\prod_{i =1}^{N=5} p_1^{X_i}(1-p_1)^{X_i}\n\\\\= (1-p_i) \\cdot p_i \\cdot (1-p_i) \\cdot p_i \\cdot p_i\\]\nQuestion here: WHY use Bernoulli (Binomial) Distribution (feat. Jacob Bernoulli, 1654-1705)?\nMy answer: Bernoulli dist. has nice statistical probability. “Nice” means making totally sense in normal life– a common belief. For example, the \\(p_1\\) value that maximizes the Bernoulli-based likelihood function is \\(Mean(X)\\), and the \\(p_1\\) values that minimizes the Bernoulli-based likelihood function is 0 or 1",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#step-2-choose-the-prior-distribution-for-p_1",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#step-2-choose-the-prior-distribution-for-p_1",
    "title": "Lecture 02",
    "section": "Step 2: Choose the Prior Distribution for \\(p_1\\)",
    "text": "Step 2: Choose the Prior Distribution for \\(p_1\\)\nWe must now pick the prior distribution of \\(p_1\\):\n\\[\nP(p_1)\n\\]\n\nCompared to likelihood function, we have much more choices. Many distributions to choose from\nTo choose prior distribution, think about what we know about a “fair” die.\n\nthe probability of rolling a “1” is about \\(\\frac{1}{6}\\)\nthe probabilities much higher/lower than \\(\\frac{1}{6}\\) are very unlikely\n\nLet’s consider a Beta distribution:\n\\[\np_1 \\sim Beta(\\alpha, \\beta)\n\\]",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#step-3-the-posterior-distribution",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#step-3-the-posterior-distribution",
    "title": "Lecture 02",
    "section": "Step 3: The Posterior Distribution",
    "text": "Step 3: The Posterior Distribution\nChoose a Beta distribution as the prior distribution of \\(p_1\\) is very convenient:\n\nWhen combined with Bernoulli (Binomial) data likelihood, the posterior distribution (\\(P(p_1|Data)\\)) can be derived analytically\nThe posterior distribution is also a Beta distribution\n\n\\(\\alpha' = \\alpha + \\sum_{i=1}^{N}X_i\\) (\\(\\alpha'\\) is parameter of the posterior distribution)\n\\(\\beta' = \\beta + (N - \\sum_{i=1}^{N}X_i)\\) (\\(\\beta'\\) is parameter of the posterior distribution)\n\nThe Beta distribution is said to be a conjugate prior in Bayesian analysis: A prior distribution that leads to posterior distribution of the same family\n\nPrior and Posterior distribution are all Beta distribution",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#install-cmdstan",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#install-cmdstan",
    "title": "Lecture 02",
    "section": "Install CmdStan",
    "text": "Install CmdStan\ncmdstanr Installation cmdstanr Users’ Guide",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#next-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#next-class",
    "title": "Lecture 02",
    "section": "Next Class",
    "text": "Next Class\nWe will talk more about how Bayesian model works. Thank you.",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#bonus-conjugacy-of-beta-distribution",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#bonus-conjugacy-of-beta-distribution",
    "title": "Lecture 02",
    "section": "Bonus: Conjugacy of Beta Distribution",
    "text": "Bonus: Conjugacy of Beta Distribution\nWhen the binomial likelihood is multiplied by the beta prior, the result is proportional to another beta distribution:\n\n\\(\\text{Posterior} \\propto \\theta^x (1 - \\theta)^{n-x} \\times \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}\\)\nSimplifies to: \\(\\theta^{x + \\alpha - 1} (1 - \\theta)^{n - x + \\beta - 1}\\)\n\nThis is the kernel of a beta distribution with updated parameters \\(\\alpha' = x + \\alpha\\) and \\(\\beta' = n - x + \\beta\\). The fact that the posterior is still a beta distribution is what makes the beta distribution a conjugate prior for the binomial likelihood.\n\nHuman langauge: Both beta (prior) and binomial (likelihood) are so-called “exponetial family”. The muliplication of them is still a “exponential family” distribution.",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#bayesian-updating",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#bayesian-updating",
    "title": "Lecture 02",
    "section": "Bayesian updating",
    "text": "Bayesian updating\nWe can use the posterior distribution as a prior!\n1. data {0, 1, 0, 1, 1} with the prior hyperparameter {2, 6} -&gt; posterior parameter {5, 9}\n2. new data {1, 1, 1, 1, 1} with the prior hyperparameter {5, 9} -&gt; posterior parameter {10, 9}\n3. one more new data {0, 0, 0, 0, 0} with the prior hyperparameter {10, 9} -&gt; posterior parameter {10, 14}",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#wrapping-up",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture02/Lecture02.html#wrapping-up",
    "title": "Lecture 02",
    "section": "Wrapping up",
    "text": "Wrapping up\nToday is a quick introduction to Bayesian Concept\n\nBayes’ Theorem: Fundamental theorem of Bayesian Inference\nPrior distribution: What we know about the parameter before seeing the data\n\nhyperparameter: parameter of the prior distribution\nUninformative prior: Prior distribution that does not convey any information about the parameter\nInformative prior: Prior distribution that conveys information about the parameter\nConjugate prior: Prior distribution that makes the posterior distribution the same family as the prior distribution\n\nLikelihood: What the data tell us about the parameter\n\nLikelihood function: Probability of the data given the parameter\nLikelihood principle: All the information about the data is contained in the likelihood function\nLikelihood is a sort of “scientific judgment” on the generation process of the data\n\nPosterior distribution: What we know about the parameter after seeing the data\n\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553",
    "crumbs": [
      "Home",
      "ESRM6553",
      "Lecture 02"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#course-topics",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#course-topics",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "1.1 Course Topics",
    "text": "1.1 Course Topics\n\nIntroduction to Bayesian Statistics\n\nBayes Theorem;\nProbability distributions, such as likelihood, priors and posteriors;\nTheoretical foundations of Bayesian inference;\n\nBayesian analysis in Stan\n\nBasic Stan coding;\nR package cmdstanr ;\nEfficient programming using vectorization;\n\nBayesian Inference and Computational Methods\n\nMarkov Chain Monte Carlo (MCMC) estimation procedures, such as Gibbs/Hamiltonian sampling;\nMCMC sampling settings, such as burnins, warmups, and MCMC chains;\n\nBayesian Modeling Evaluation\n\nModel specification, estimation, and testing;\nModel fit and model comparison, such as convergence diagnostics, posterior predictive checks, information criteria;\n\nBayesian Modeling\n\nEstimating and making inferences from psychometric models such as Confirmatory Factor Analysis (CFA) or Item Response Theory (IRT) models;\n\nAdvanced Topics in Bayesian Multivariate Analysis\n\nBayesian networks;\nMultilevel/Hierarchical models, mixture models;\nMissing data, non-normal data;"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#course-description",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#course-description",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "1.2 Course Description",
    "text": "1.2 Course Description\nThis course offers an in-depth exploration of multivariate statistics within the context of Bayesian inferences. Bayesian statistics have been widely used in public health, education, and psychology. Bayesian techniques are increasingly used in Artificial Intelligence and Brain models for decision-making under uncertainty. Designed for graduate students in educational statistics and research methods, it focuses on the theoretical underpinnings and practical applications of Bayesian approaches in psychometric modeling. Prerequisites include basic knowledge of multivariate statistics and psychometrics."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#course-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#course-objectives",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "1.3 Course Objectives",
    "text": "1.3 Course Objectives\nUpon completion of ESRM 6554 - Adv. Multivariate, students will:\n\nComprehend fundamental concepts and principles of Bayesian multivariate analysis;\nArticulate the rationale of Bayesian approaches to data analysis and statistical inference;\nCompare Bayesian inference to MLE;\nDevelop conceptual and mathematical Bayesian literacy, as well as computer software skills (e.g., R, Stan, or JAGS) required to conduct Bayesian data analyses in educational research;\nGain technical foundations necessary to be contributors to applied and methodological research that use Bayesian methods;\nConduct analyses on empirical data, interpret results, and communicate work in written and oral presentations."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#prerequisite-knowledge",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#prerequisite-knowledge",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "1.4 Prerequisite Knowledge",
    "text": "1.4 Prerequisite Knowledge\nIt is assumed that students have has solid statistical training up to and including topics in multivariate statistics (ESRM 6413, 6423, and 6453). In addition, it is assumed you are familiar with R programming (python or SAS are fine). SPSS may not be sufficient for this course.\n\nLectures for theoretical understanding.\nHands-on sessions with statistical software.\nGroup discussions and presentations.\nResearch project guidance."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#how-to-be-successful-in-this-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#how-to-be-successful-in-this-class",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "1.5 How to Be Successful in This Class",
    "text": "1.5 How to Be Successful in This Class\n\nCome to class ready to learn.\nComplete the out-of-class exercises prior to each class.\nIf you become confused or don’t fully grasp a concept, ask for help from your instructor\nKnow what is going on: keep up with email, course announcements, and the course schedule.\nTry to apply the information to your area of interest — if you have a cool research idea, come talk to me!"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#required-materials",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#required-materials",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "2.1 Required Materials",
    "text": "2.1 Required Materials\n\nPrimary Text: Richard McElreath (2019), Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Free to download it online.\nPrimary Text: Levy, Mislevy (2016), Bayesian Psychometric Modeling. Chapters will be uploaded before class."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#optional-materials",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#optional-materials",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "2.2 Optional Materials",
    "text": "2.2 Optional Materials\n\nKaplan, D. (2014), Bayesian Statistics for the Social Sciences. New York: Guilford Press.\nGelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2020), Bayesian Data Analysis 3rd edition. Chapman and Hall.\nAndrew Gelman’s Website for an unfiltered, stream of consciousness Bayesian commentary\nSupplementary Texts:\n\nStan User’s Guide (PDF)\nStan User’s Guide (HTML)"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#software",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#software",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "2.3 Software",
    "text": "2.3 Software\n\nR and R packages (tidyverse)\nStan is gaining in popularity and has an avid user community. To use Stan in R, you need to download RStan or cmdstanr package. Here is the tutorial of installing RStan or click here for installing cmdstanr package.\n(Optional) Mplus, JAGS, WinBUGS"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#online-homework",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#online-homework",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "3.1 Online Homework",
    "text": "3.1 Online Homework\nOnline Homework Portal"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#projects",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#projects",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "3.2 Projects",
    "text": "3.2 Projects\nStudents will complete a project utilizing your knowledge learnt from the class. You may work individually. I will provide data and questions for this project OR you can use data that is of interest to you in your GA position or dissertation research. The primary objective of the research project is to facilitate the application and understanding of concepts learned in this course.\nThere will be a short project proposal due around week 15 - it can be sooner if you want to get started early. Required for this proposal is an NCME-type conference proposal (800 words maximum). Please see Individual Paper Presentations for more details\nTypical components of the research proposal include:\n\nTitle (no more than 12 words)\nSummary of research (no more than 800 words)\n\nBackground of research\nResearch questions/hypotheses\nMethod (Data, Analysis Plan)\nPreliminary findings\nReferences/Table/Figure"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#in-class-short-quiz",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#in-class-short-quiz",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "3.3 In-Class Short Quiz",
    "text": "3.3 In-Class Short Quiz\nAt the commencement of each class session, a brief quiz consisting of one to three questions will be administered. These quizzes are intended for formative assessment purposes only and will not contribute to your overall score. However, regular attendance is essential, as it will ensure full credit in the final grading process."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#grading",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#grading",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "3.4 Grading",
    "text": "3.4 Grading\n\nBrief quiz: 60%\nProject Presentation: 20%\nProject Proposal: 20%"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#ai-statement",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#ai-statement",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "4.1 AI Statement",
    "text": "4.1 AI Statement\nSpecific permissions will be provided to students regarding the use of generative artificial intelligence tools on certain graded activities in this course. In these instances, I will communicate explicit permission as well as expectations and any pertinent limitations for use and attribution. Without this permission, the use of generative artificial intelligence tools in any capacity while completing academic work submitted for credit, independently or collaboratively, will be considered academic dishonesty and reported to the Office of Academic Initiatives and Integrity."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#academic-integrity",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#academic-integrity",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "4.2 Academic Integrity",
    "text": "4.2 Academic Integrity\nYou are responsible for reading and understanding the University of Arkansas’ Academic Integrity Policy. You are expected to complete all assignments and exams with the highest level of integrity. Any form of academic dishonesty will result in a failing grade for the course and will be reported to the Office of Academic Integrity. If you have any questions about what constitutes academic dishonesty, please ask me."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#emergency-preparedness",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#emergency-preparedness",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "4.3 Emergency Preparedness",
    "text": "4.3 Emergency Preparedness\nThe University of Arkansas is committed to providing a safe and healthy environment for study and work. In that regard, the university has developed a campus safety plan and an emergency preparedness plan to respond to a variety of emergency situations. The emergency preparedness plan can be found at emergency.uark.edu. Additionally, the university uses a campus-wide emergency notification system, UARKAlert, to communicate important emergency information via email and text messaging. To learn more and to sign up: http://safety.uark.edu/emergency-preparedness/emergency-notification-system/"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#inclement-weather",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#inclement-weather",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "4.4 Inclement Weather",
    "text": "4.4 Inclement Weather\nEach faculty member is responsible for determining whether or not to cancel class due to inclement weather. If you have any questions about whether or not class will be held, please contact me. If I cancel class, I will notify you via email and/or Blackboard. In general, students need to know how and when they will be notified in the event that class is cancelled for weather-related reasons. Please see here for more information."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#academic-support",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#academic-support",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "4.5 Academic Support",
    "text": "4.5 Academic Support\nA complete list and brief description of academic support programs can be found on the University’s Academic Support site, along with links to the specific services, hours, and locations. Faculty are encouraged to be familiar with these programs and to assist students with finding an using the support services that will help them be successful. Please see here for more information."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#religious-holidays",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#religious-holidays",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "4.6 Religious Holidays",
    "text": "4.6 Religious Holidays\nThe university does not observe religious holidays; however Campus Council has passed the following resolution concerning individual observance of religious holidays and class attendance:\n\nWhen members of any religion seek to be excused from class for religious reasons, they are expected to provide their instructors with a schedule of religious holidays that they intend to observe, in writing, before the completion of the first week of classes."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#applied-statistics",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#applied-statistics",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "6.1 Applied Statistics",
    "text": "6.1 Applied Statistics\n\nBayesian Data Analysis, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#cognitive-science",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#cognitive-science",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "6.2 Cognitive Science",
    "text": "6.2 Cognitive Science\n\nAn Introduction to Bayesian Data Analysis for Cognitive Science, by Bruno Nicenboim, Daniel Schad, and Shravan Vasishth)"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#footnotes",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/index.html#footnotes",
    "title": "ESRM 6553: Advanced Multivariate Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSR: Statistical Rethinking 2ed Edition by Richard McElreath↩︎\nBDA: Bayesian Data Analysis by Gelman et al. (2021)↩︎\nBPM: Bayesian Psychometric Modeling by Levy, Mislevy (2016)↩︎"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#todays-lecture-objectives",
    "title": "Lecture 06",
    "section": "",
    "text": "Introduce measurement (psychometric) models in general\nDescribe the steps needed in a psychometric model analysis\nDive deeper into the observed-variables modeling aspect",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#measurement-model-analysis-steps",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#measurement-model-analysis-steps",
    "title": "Lecture 06",
    "section": "\n1.1 Measurement Model Analysis Steps",
    "text": "1.1 Measurement Model Analysis Steps\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nsubgraph \"Measurement Procedure\"\n  subgraph Modeling\n  direction LR\n  Start --&gt; id1\n  id1([Specify model]) --&gt; id2([\"`Specify scale identification \n  method for latent variables`\"])\n  id2 --&gt; id3([Estimate model])\n  id3 --&gt; id4{Adequate fit indices}\n  id4 -- No --&gt; id1\n  end\n  Modeling -- Model fit acceptable --&gt; one\n  subgraph one[\"Evaluation: Measurement Model with Auxiliary Components\"]\n    direction LR\n    id5([\"Score estimation \n    and secondary analyses with scores\"]) --&gt; id6([Item evaluation])\n    id6 --&gt; id7([Scale construction])\n    id7 --&gt; id8([Equating])\n    id8 --&gt; id9([Measure Invariance/Differential item functioning])\n    id9 --&gt; End\n  end\nend\nstyle Start fill:#f9f,stroke:#333,stroke-width:5px\nstyle End fill:#bbf,stroke:#333,stroke-width:5px",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#components-of-a-measurement-model",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#components-of-a-measurement-model",
    "title": "Lecture 06",
    "section": "\n1.2 Components of a Measurement Model",
    "text": "1.2 Components of a Measurement Model\nThere are two components of a measurement model\nTheory (what we cannot see but assume its existence):\n\n\nLatent variable(s)\n\nOther effects as needed by a model\n\n\nRandom effects (e.g., initial status and slopes in Latent Growth Model)\nTestlet effects (e.g., a group-level variation among items)\nEffects of observed variables (e.g., gender differences, DIF, Measurement Invariance)\n\n\n\n\n\nData (what we can see and we assume generated by theory):\n\n\n\nOutcomes\n\nAn assumed distribution for each outcome\nA key statistic of outcome for the model (e.g., mean, sd)\nA link function\n\n\n\n\n\n\n1.2.1 General form for measurement model (SEM, IRT):\n\nf(E(\\mathbf{Y}\\mid\\Theta)) = \\boldsymbol{\\mu} +\\Theta\\Lambda^T\n\nand\n\n\\Lambda_j = Q \\odot \\boldsymbol{\\lambda_j}\n\nAssume N as sample size, P as number of factors, J as number of items. Then,\n\n\n\nf(): link function. CFA: identity link; IRT: logistic/probit link\n\nE(\\mathbf{Y}\\mid\\Theta): Expected/Predicted values of outcomes\n\n\\Theta: latent factor scores matrix (N \\times P)\n\n\\Lambda: A factor loading matrix (J \\times P)\n\n\\Lambda_j: jth row vector of factor loading matrix\n\\textbf{Q}: Q-matrix represents the connections between items and latent variables\n\\boldsymbol{\\lambda}_j: a vector of factor loading vectors for item j\n\n\n\n\\mu: item intercepts (J \\times 1)\n\n\n\n1.2.2 Example 1 with general form\n\n\nLet’s consider a measurement model with only one latent variable and five items:\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart TD\n  id1((θ)) --&gt; id2[\"Y1\"]\n  id1 --&gt; id3[\"Y2\"]\n  id1 --&gt; id4[\"Y3\"]\n  id1 --&gt; id5[\"Y4\"]\n  id1 --&gt; id6[\"Y5\"]\n\n\n\n\n\n\nThe model shows:\n\n\nOne latent variable (\\theta)\nFive observed variables (\\mathbf{Y} = \\{Y_1, Y_2, Y_3, Y_4, Y_5\\})\n\n\n\nThen,\n\n\\Theta = \\begin{bmatrix} \\theta_1, \\\\\\theta_2,\\\\ \\cdots,\\\\ \\theta_N \\end{bmatrix}\n\\Lambda^T = \\begin{bmatrix}\\lambda_1, \\lambda_2, ..., \\lambda_5 \\end{bmatrix}\n\n\n\n\n1.2.3 Example 2 with general form\n\n\nLet’s consider a measurement model with only two latent variables and five items:\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart TD\n  theta1((θ1)) --&gt; id2[\"Y1\"]\n  theta1 --&gt; id3[\"Y2\"]\n  theta1 --&gt; id4[\"Y3\"]\n  theta1 --&gt; id5[\"Y4\"]\n  theta1 --&gt; id6[\"Y5\"]\n  theta2((θ2)) --&gt; id2[\"Y1\"]\n  theta2 --&gt; id3[\"Y2\"]\n  theta2 --&gt; id4[\"Y3\"]\n  theta2 --&gt; id5[\"Y4\"]\n  theta2 --&gt; id6[\"Y5\"]\n\n\n\n\n\n\nThe model shows:\n\n\nTwo latent variables (\\theta_1, \\theta_2)\nFive observed variables (\\mathbf{Y} = \\{Y_1, Y_2, Y_3, Y_4, Y_5\\})\n\n\n\nThen,\n\n\\Theta = \\begin{bmatrix} \\theta_{1,1}, \\theta_{1,2}\\\\\\theta_{2,1}, \\theta_{2,2}\\\\ \\cdots,\\cdots \\\\ \\theta_{N, 1}, \\theta_{N,2} \\end{bmatrix} \\sim [0, \\Sigma]\n\\Lambda^T = \\begin{bmatrix}\\lambda_{1,1}, \\lambda_{1,2}, ..., \\lambda_{1,5}\\\\\\lambda_{2,1}, \\lambda_{2,2}, ..., \\lambda_{2,5}\\end{bmatrix}\n\n\n\n\n1.2.4 Example 3 with general form\n\n\nLet’s consider a measurement model with only two latent variables and five items:\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart TD\n  theta1((θ1)) --&gt; id2[\"Y1\"]\n  theta1 --&gt; id3[\"Y2\"]\n  theta1 --&gt; id4[\"Y3\"]\n  theta2((θ2)) --&gt; id4[\"Y3\"]\n  theta2 --&gt; id5[\"Y4\"]\n  theta2 --&gt; id6[\"Y5\"]\n\n\n\n\n\n\nThe model shows:\n\n\nTwo latent variables (\\theta_1, \\theta_2)\nFive observed variables (\\mathbf{Y} = \\{Y_1, Y_2, Y_3, Y_4, Y_5\\})\n\n\n\nThen,\n\n\\Theta = \\begin{bmatrix} \\theta_{1,1}, \\theta_{1,2}\\\\\\theta_{2,1}, \\theta_{2,2}\\\\ \\cdots,\\cdots \\\\ \\theta_{N, 1}, \\theta_{N,2} \\end{bmatrix} \\sim [0, \\Sigma]\n\\Lambda^T = \\begin{bmatrix}\\lambda_{1,1}, \\lambda_{1,2}, \\lambda_{1,3}, 0,0\\\\ 0, 0, \\lambda_{2,3}, \\lambda_{2,4}, \\lambda_{2,5}\\end{bmatrix}\nNote that we only limit our model to main-effect models. Interaction effects of factors introduce more complexity.\nIt is difficulty to specify factor loadings with 0s directly\n\n\n\n\n1.2.5 Item-specific form\nFor each item j:\n\n\\mathbf{Y_j} \\sim N(\\mu_j+ \\boldsymbol{\\lambda}_{j}Q_j\\Theta, \\psi_j)",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#bayesian-view-latent-variables",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#bayesian-view-latent-variables",
    "title": "Lecture 06",
    "section": "\n1.3 Bayesian view: latent variables",
    "text": "1.3 Bayesian view: latent variables\nLatent variables in Bayesian are built by following specification:\n\n\nWhat are their distributions? (normal distribution or others)\n\nFor example, \\theta_i values for one person and \\theta values for samples. Factor score \\theta is a mixture distribution of distributions of each individual’s factor score \\theta_i\nBut, in MLE/WSLMV, we do not estimate mean and sd of each individual’s factor score for model to be converged\n\n\n\n\n⌘+Clibrary(tidyverse)\nset.seed(12)\nN = 15\nndraws = 200\nFS = matrix(NA, nrow = N, ndraws)\nFS_p = rnorm(N)\nFS_p = FS_p[order(FS_p)]\nfor (i in 1:N) {\n  FS[i,] = rnorm(ndraws, mean = FS_p[i], sd = 1)\n}\nFS_plot &lt;- as.data.frame(t(FS))\ncolnames(FS_plot) &lt;- paste0(\"Person\", 1:N)\nFS_plot &lt;- FS_plot |&gt; pivot_longer(everything(), names_to = \"Person\", values_to = \"Factor Score\")\nFS_plot$Person &lt;- factor(FS_plot$Person, levels = paste0(\"Person\", 1:N))\nggplot() +\n  geom_density(aes(x = `Factor Score`, fill = Person, col = Person ), alpha = .5, data = FS_plot)  +\n  geom_density(aes(x = FS_p))\n\n\n\n\n\n\n\n\n\nMultidimensionality\n\nHow many factors to be measured?\nIf \\geq 2 factors, we specify mean vectors and variance-covariance matrix\nTo link latent variables with observed variables, we need to create a indicator matrix of coefficient/effects of latent variable on items.\n\nIn diagnostic modeling and multidimensional IRT, we call it Q-matrix",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#simulation-study-1",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#simulation-study-1",
    "title": "Lecture 06",
    "section": "\n1.4 Simulation Study 1",
    "text": "1.4 Simulation Study 1\n\nLet’s perform a small simulation study to see how to perform factor analysis in naive Stan.\nThe model specification is a two-factor with each measured by 3 items. In total, there are 6 items with continuous responses. Sample size is 1000.\n\n\n\nModel Specification\n\n\n\n1.4.1 Data Simulation\n\n\n\n\n⌘+Cset.seed(1234)\nN &lt;- 1000\nJ &lt;- 6\n# parameters\npsi &lt;- .3 # factor correlation\nsigma &lt;- .1 # residual varaince\nFS &lt;- mvtnorm::rmvnorm(N, mean = c(0, 0), sigma = matrix(c(1, psi, psi, 1), 2, 2, byrow = T))\nLambda &lt;- matrix(\n  c(\n    0.7, 0,\n    0.5, 0,\n    0.3, 0,\n    0, 0.7,\n    0, 0.5,\n    0, 0.3\n  ), 6, 2,\n  byrow = T\n)\nmu &lt;- matrix(rep(0.1, J), nrow = 1, byrow = T)\nresidual &lt;- mvtnorm::rmvnorm(N, mean = rep(0, J), sigma = diag(sigma^2, J))\nY &lt;- t(apply(FS %*% t(Lambda), 1, \\(x) x + mu)) + residual\nQ &lt;- matrix(\n  c(\n    1, 0,\n    1, 0,\n    1, 0,\n    0, 1,\n    0, 1,\n    0, 1\n  ), 6, 2,\n  byrow = T\n)\nloc &lt;- Q |&gt;\n  as.data.frame() |&gt;\n  rename(`1` = V1, `2` = V2) |&gt; \n  rownames_to_column(\"Item\") |&gt;\n  pivot_longer(c(`1`, `2`), names_to = \"Theta\", values_to = \"q\") |&gt; \n  mutate(across(Item:q, as.numeric)) |&gt; \n  filter(q == 1) |&gt; \n  as.matrix()\n\n\n\n⌘+Chead(Y)\n\n           [,1]        [,2]         [,3]       [,4]        [,5]        [,6]\n[1,] -0.8030701 -0.48545470 -0.256368448  0.2829436 -0.02007136  0.02274601\n[2,]  0.4271083  0.50926367  0.270175274 -1.5916892 -1.05896321 -0.64570802\n[3,]  0.4803976  0.40621781  0.122951997  0.5982296  0.42932001  0.29298711\n[4,] -0.3292829 -0.18227739  0.002010588 -0.3921847 -0.20918407  0.19121259\n[5,] -0.4938891 -0.21694927 -0.251691793 -0.5544642 -0.28519849 -0.42496295\n[6,] -0.3946768  0.01379814 -0.164355671 -0.7838352 -0.38385332 -0.29007225\n\n\n\n1.4.2 Strategies for Stan: factor loadings\n\n\nWe will iterate over item response function across each item\nTo benefit from the efficiency of vectorization, we specify a vector of factor loadings with length 6\n\n\\{\\lambda_1, \\lambda_2, \\cdots, \\lambda_6 \\}\nOptionally, a matrix with number of items by number of factor for \\lambdas can be specified like \\lambda_{11} and \\lambda_{62}, that introduces flexibility but complexity\n\n\nWe should have a location table telling stan the information about which factor each factor loading belong to\n\nFor example, \\lambda_{1} belongs to factor 1 and \\lambda_4 belong to factor 2\n\n\n\n\n\n⌘+Cloc\n\n     Item Theta q\n[1,]    1     1 1\n[2,]    2     1 1\n[3,]    3     1 1\n[4,]    4     2 1\n[5,]    5     2 1\n[6,]    6     2 1\n\n\n\n1.4.3 Strategies for Stan: prior distribution and hyperparameters\n\n\n\n\nresidual variances for items: \\sigma \\sim \\text{exponential}(sigmaRate)\n\nsigmaRate is set to 1\n\n\nitem intercepts: \\mu \\sim \\text{MVN}(meanMu, covMu)\n\nmeanMu is set to a vector of 0s with length 6\ncovMu is set to a diagonol matrix of 1000 with 6 \\times 6\n\n\nfactor scores: \\Theta \\sim \\text{MVN}(meanTheta, corrTheta)\n\nmeanTheta is set to a vector of 0s with length 2\ncorrTheta \\sim lkj\\_corr(eta) and eta is set to 1\nOptionally, L \\sim lkj\\_corr\\_cholesky(eta) and corrTheta = LL’\n\n\nfactor loadings: \\Lambda \\sim \\text{MVN}(meanLambda, covLambda)\n\nmeanLambda is set to a vector of 0s with length 6 (number of items)\ncovLambdais set to a matrix of \\begin{pmatrix}1000, 0,\\cdots,0\\\\ \\cdots\\\\0, 0, \\cdots1000\\end{pmatrix}\n\n\n\n\n\n\nQuestion: what about factor correlation \\psi?\nSee this blog and Stan’s reference for Choleskey decomposition\nFor eta = 1, the distribution is uniform over the space of all correlation matrices\n\n\n\n\n1.4.4 Data Structure and Data Block\n\n\n\n\n⌘+C\nLecture06.R\n\ndata_list &lt;- list(\n  N = 1000, # number of subjects/observations\n  J = J, # number of items\n  K = 2, # number of latent variables,\n  Y = Y,\n  Q = Q,\n  # location/index of lambda\n  kk = loc[,2],\n  #hyperparameter\n  sigmaRate = .01,\n  meanMu = rep(0, J),\n  covMu = diag(1000, J),\n  meanTheta = rep(0, 2),\n  eta = 1,\n  meanLambda = rep(0, J),\n  covLambda = diag(1000, J)\n)\n\n\n\n\n\n\n⌘+C\nsimulation_loc.stan\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  int&lt;lower=0&gt; J; // number of items\n  int&lt;lower=0&gt; K; // number of latent variables\n  matrix[N, J] Y; // item responses\n  \n  //location/index of lambda\n  array[J] int&lt;lower=0&gt; kk;\n  \n  //hyperparameter\n  real&lt;lower=0&gt; sigmaRate;\n  vector[J] meanMu;\n  matrix[J, J] covMu;      // prior covariance matrix for coefficients\n  vector[K] meanTheta;\n  \n  vector[J] meanLambda;\n  matrix[J, J] covLambda;      // prior covariance matrix for coefficients\n  \n  real&lt;lower=0&gt; eta; // LKJ shape parameters\n}\n\n\n\n\n\n\n1.4.5 Parameter and Transformed Parameter block\n\n\n⌘+C\nsimulation_loc.stan\n\nparameters {\n  vector[J] mu;                      // item intercepts\n  vector&lt;lower=0,upper=1&gt;[J] lambda; // factor loadings\n  vector&lt;lower=0&gt;[J] sigma;          // the unique residual standard deviation for each item\n  matrix[N, K] theta;                // the latent variables (one for each person)\n  cholesky_factor_corr[K] L;         // L of factor correlation matrix\n}\ntransformed parameters{\n  matrix[K,K] corrTheta = multiply_lower_tri_self_transpose(L);\n}\n\n\n\nNote that L is the Cholesky decomposition factor of factor correlation matrix\n\n\nIt is also a lower triangle matrix\nuse cholesky_factor_corr[K] to declare this parameter\nSometime, parameters are those that are easy to sampling, transformed parameters block is used to transform your parameters to those that are difficulty to direct sampling.\n\n\n\n1.4.6 Model block\n\n\n\n\n⌘+C\nLecture06.R\n\ndata_list &lt;- list(\n  N = 1000, # number of subjects/observations\n  J = J, # number of items\n  K = 2, # number of latent variables,\n  Y = Y,\n  Q = Q,\n  # location/index of lambda\n  kk = loc[,2],\n  #hyperparameter\n  sigmaRate = .01,\n  meanMu = rep(0, J),\n  covMu = diag(1000, J),\n  meanTheta = rep(0, 2),\n  eta = 1,\n  meanLambda = rep(0, J),\n  covLambda = diag(1000, J)\n)\n\n\n\n\n\n\n⌘+C\nsimulation_loc.stan\n\nmodel {\n  mu ~ multi_normal(meanMu, covMu);\n  sigma ~ exponential(sigmaRate);   \n  lambda ~ multi_normal(meanLambda, covLambda);\n  L ~ lkj_corr_cholesky(eta);\n  for (i in 1:N) {\n    theta[i,] ~ multi_normal(meanTheta, corrTheta);\n  }\n  for (j in 1:J) { // loop over each item response function\n    Y[,j] ~ normal(mu[j]+lambda[j]*theta[,kk[j]], sigma[j]);\n  }\n}\n\n\n\n\n\nNote that kk[j] selects which factor to be multiplied dependent on factor loading’s index. That why we have a location matrix of factor loadings. Theta in loc table is kk.\n\n⌘+Cloc\n\n     Item Theta q\n[1,]    1     1 1\n[2,]    2     1 1\n[3,]    3     1 1\n[4,]    4     2 1\n[5,]    5     2 1\n[6,]    6     2 1\n\n\n\n1.4.7 Generated quantities block\n\n\n⌘+C\nsimulation_loc.stan\n\ngenerated quantities {\n  vector[N * J] log_lik;\n  matrix[N, J] temp;\n  matrix[N, J] Y_rep;\n  vector[J] Item_Mean_rep;\n  for (i in 1:N) {\n    for (j in 1:J) {\n      temp[i, j] = normal_lpdf(Y[i, j] | mu[j]+lambda[j]*theta[i,kk[j]],  sigma[j]); \n    }\n  }\n  log_lik = to_vector(temp);\n  for (j in 1:J) {\n    Y_rep[,j] = to_vector(normal_rng(mu[j]+lambda[j]*theta[,kk[j]], sigma[j]));\n    Item_Mean_rep[j] = mean(Y_rep[,j]);\n  }\n}\n\n\n\nTo obtain leave-one-out (LOO) model fitting, we need to generate log-likelihood:\n\nlog_lik includes both person information and item information in factor analysis and IRT\nlog_lik must be a vector in Stan\nThus, the length of log-likelihood is a vector of length N \\times J\n\nTo conduct posterior predictive model checking, we need to generate simulation data sets: Y_rep\n\nY_rep can be generated using normal_rng and posterior draws of parameters\nItem_Mean_rep were generated to compared to observed item means\n\n\n1.4.8 Model Estimation\nHere, my MCMC estimation is set to:\n\n\nFour MCMC chains that are running parallel\nA seed as 1234 for replication\nThe warmup iteration is 1000 and the sampling iteration is 2000. Thus, the total iteration is 3000.\nSince we have 4 chains, the total iteration for summary statistics is 12000 iterations.\nThe total estimation time is 220s (3.6 minutes)\n\n\n#| eval: false\nmod_cfa_twofactor &lt;- cmdstan_model(here::here(\"posts\", \"2024-01-12-syllabus-adv-multivariate-esrm-6553\", \"Lecture06\", \"Code\", \"simulation_loc.stan\"))\nfit_cfa_twofactor &lt;- mod_cfa_twofactor$sample(\n  data = data_list,\n  seed = 1234,\n  chains = 4,\n  parallel_chains = 4, \n  iter_sampling = 2000,\n  iter_warmup = 1000\n)\n\n⌘+Cfit_cfa_twofactor$time()\n\n$total\n[1] 220.9985\n\n$chains\n  chain_id  warmup sampling   total\n1        1  30.640   51.569  82.209\n2        2 178.157   42.665 220.822\n3        3  31.094   43.514  74.608\n4        4  29.858   43.240  73.098\n\n\n\n1.4.9 MCMC Result &gt; Model diagnostic &gt; PPMC\nAll items have PPP of item mean close to 0.5, suggesting great model-data fitting.\n\n\n\n⌘+CItem_Mean_rep_mat &lt;- fit_cfa_twofactor$draws(\"Item_Mean_rep\", format = 'matrix')\nItem_Mean_obs &lt;- colMeans(Y)\nPPP &lt;- rep(NA, J)\n# colMeans(Item_Mean_rep_mat)\nfor (item in 1:J) {\n  PPP[item] &lt;- mean(as.numeric(Item_Mean_rep_mat[,item]) &gt; Item_Mean_obs[item])\n}\nPPP\n\n[1] 0.491250 0.505750 0.497500 0.499875 0.503250 0.486500\n\n⌘+Cdata.frame(\n  Item = factor(1:J, levels = 1:J),\n  PPP = PPP\n) |&gt; \n  ggplot() +\n  geom_col(aes(x = Item, y = PPP)) + \n  geom_hline(aes(yintercept = .5), col = 'red', size = 1.3) + \n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n⌘+C\nPosterior predictive distribution of item means\n\nobs_mean &lt;- data.frame(\n  Item = paste0(\"Item_Mean_rep[\", 1:6,\"]\"),\n  y = Item_Mean_obs\n)\nItem_Mean_rep_mat |&gt; as.data.frame() |&gt; \n  pivot_longer(everything(), names_to = \"Item\", values_to = \"Posterior\") |&gt; \n  ggplot() +\n  geom_density(aes(x = Posterior)) +\n  geom_vline(aes(xintercept = y), data = obs_mean, col ='red') +\n  facet_wrap(~ Item, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.10 MCMC Result &gt; Model diagnostic &gt; LOO\nWe firt examined the max/mean PSRF (rhat) for convergence. This is also called Gelman and Rubin diagnosis. The maximum RSRF is 1.03 suggesting MCMC estimation converged.\nThen, we examined the LOO with PSIS. According to Pareto K diagnostic, most log-likelihood estimation suggests good reliability.\nQuestion: Why we have 8000 \\times 6000 log-likelihood elements? hints: our information in data\n\n⌘+Cfit_cfa_twofactor$summary(\n  variables = NULL,\n  \"rhat\"\n) |&gt; \n summarise(mean_rhat = mean(rhat, na.rm = T),\n           max_rhat = max(rhat, na.rm = T))\n\n# A tibble: 1 × 2\n  mean_rhat max_rhat\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      1.00     1.03\n\n\n\n⌘+Cloo_res &lt;- fit_cfa_twofactor$loo(variables = 'log_lik', save_psis = TRUE, cores = 2)\nprint(loo_res)\n\n\nComputed from 8000 by 6000 log-likelihood matrix\n\n         Estimate    SE\nelpd_loo   4074.8  54.5\np_loo      1844.1  29.4\nlooic     -8149.5 108.9\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     3888  64.8%   677       \n (0.5, 0.7]   (ok)       1559  26.0%   174       \n   (0.7, 1]   (bad)       532   8.9%   12        \n   (1, Inf)   (very bad)   21   0.4%   13        \nSee help('pareto-k-diagnostic') for details.\n\n\n\n1.4.11 MCMC Results &gt; Estimation &gt; Factor Loadings\n\n\nBenchmark model using lavaan\n\n⌘+C# lavaan\nmod &lt;- \"\nF1 =~ I1 + I2 + I3\nF2 =~ I4 + I5 + I6\n\"\ndat &lt;- as.data.frame(Y)\ncolnames(dat) &lt;- paste0('I', 1:6)\nfit &lt;- cfa(mod, data = dat, std.lv = TRUE)\nlavaan::parameterestimates(fit) |&gt; filter(op == \"=~\")\n\n  lhs op rhs   est    se      z pvalue ci.lower ci.upper\n1  F1 =~  I1 0.720 0.016 43.685      0    0.688    0.752\n2  F1 =~  I2 0.514 0.012 43.082      0    0.491    0.538\n3  F1 =~  I3 0.310 0.008 40.922      0    0.295    0.325\n4  F2 =~  I4 0.673 0.015 43.699      0    0.643    0.703\n5  F2 =~  I5 0.483 0.011 42.975      0    0.461    0.505\n6  F2 =~  I6 0.293 0.007 40.248      0    0.279    0.307\n\n\n\nModel 1 using Stan\n\n⌘+Cfit_cfa_twofactor$summary(\"lambda\") |&gt; select(variable,mean, median, sd, q5, q95)\n\n# A tibble: 6 × 6\n  variable   mean median      sd    q5   q95\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 lambda[1] 0.723  0.722 0.0175  0.694 0.752\n2 lambda[2] 0.516  0.516 0.0127  0.495 0.537\n3 lambda[3] 0.311  0.311 0.00791 0.298 0.324\n4 lambda[4] 0.674  0.674 0.0151  0.649 0.699\n5 lambda[5] 0.484  0.484 0.0111  0.466 0.502\n6 lambda[6] 0.294  0.294 0.00727 0.282 0.306\n\n\n\n⌘+Cfit_cfa_twofactor$summary(\"lambda\", \\(x) quantile(x, c(.025, .975)))\n\n# A tibble: 6 × 3\n  variable  `2.5%` `97.5%`\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 lambda[1]  0.689   0.757\n2 lambda[2]  0.492   0.541\n3 lambda[3]  0.296   0.326\n4 lambda[4]  0.645   0.704\n5 lambda[5]  0.462   0.505\n6 lambda[6]  0.280   0.308\n\n\n\n\nWe can easily notice how consistent between estimates of lavaan with Stan :\n\n\n\nest in lavaan corresponds to mean or median in stan . Their difference is around .001 - .002.\n\nse in lavaan corresponds to sd in stan\n\n\nci.lower and ci.upper in lavaan is for 95% confidence interval, so they have larger range than q5 and q95 in Stan, which is for 90% Credible Interval\n90% CI is the default setting, but we can calculate Bayesian 95% Credible Interval using $summary()\n\n\n\n\n1.4.12 MCMC Results &gt; Visualization &gt; Factor Loadings\n\n⌘+Cbayesplot::color_scheme_set(\"viridis\")\nbayesplot::mcmc_trace(fit_cfa_twofactor$draws(), regex_pars = \"lambda\")\n\n\n\n\n\n\n\n\n1.4.13 MCMC Results &gt; Estimation &gt; Factor Correlation\nThe factor correlation (Psi = .3) is represented by the non-diagonal elements of factor correlation.\nThere are two ways to check estimation of factor correlation:\n\nSince we use LKJ sampling with L Cholesky factor, recall that L is lower triangle of factor correlation.\n\n\n⌘+Cfit_cfa_twofactor$summary(\"L[2,1]\")\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 L[2,1]   0.318  0.318 0.0283 0.0286 0.272 0.365  1.00    1836.    4208.\n\n\n\nWe can also look at the elements of factor correlation matrix\n\n\n⌘+Cfit_cfa_twofactor$summary(c(\"corrTheta[1,2]\", \"corrTheta[2,1]\"))\n\n# A tibble: 2 × 10\n  variable        mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 corrTheta[1,2] 0.318  0.318 0.0283 0.0286 0.272 0.365  1.00    1836.    4208.\n2 corrTheta[2,1] 0.318  0.318 0.0283 0.0286 0.272 0.365  1.00    1836.    4208.\n\n\n\n1.4.14 MCMC Results &gt; Visualization &gt; Factor Correlation\n\nWe can also visual inspect trace plots of MCMC draws of L\n\nWhat type of error leads to the deviation between posterior values of L and true value.\n\n\n⌘+Cbayesplot::mcmc_trace(fit_cfa_twofactor$draws(\"L[2,1]\"))\n\n\n\n\n\n\n\n\n1.4.15 MCMC Results &gt; Estimation &gt; Item Intercepts\nItem intercepts are set to .1. Let’s see what Bayesian model recovers the true values\n\n⌘+Cfit_cfa_twofactor$summary(\"mu\")\n\n# A tibble: 6 × 10\n  variable   mean median      sd     mad     q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mu[1]    0.106  0.106  0.0247  0.0231  0.0673 0.147  1.03     83.2     122.\n2 mu[2]    0.106  0.106  0.0177  0.0167  0.0777 0.135  1.03     85.8     144.\n3 mu[3]    0.101  0.101  0.0109  0.0102  0.0836 0.119  1.03     88.6     130.\n4 mu[4]    0.0842 0.0844 0.0208  0.0217  0.0488 0.118  1.02    128.      328.\n5 mu[5]    0.0966 0.0968 0.0151  0.0157  0.0707 0.121  1.02    129.      348.\n6 mu[6]    0.0989 0.0990 0.00952 0.00983 0.0828 0.114  1.02    138.      401.\n\n\nWe notices item 4 is little off with true values.\n\n⌘+Cbayesplot::mcmc_trace(fit_cfa_twofactor$draws(\"mu\"))\n\n\n\n\n\n\n\n\n1.4.16 MCMC Results &gt; Estimation &gt; Residual variances\n\n\n⌘+Cfit_cfa_twofactor$summary(\"sigma\")\n\n# A tibble: 6 × 10\n  variable   mean median      sd     mad     q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 sigma[1] 0.107  0.108  0.00688 0.00674 0.0961 0.118  1.01     747.    1009.\n2 sigma[2] 0.0995 0.0995 0.00412 0.00412 0.0928 0.106  1.00    1178.    2166.\n3 sigma[3] 0.0961 0.0960 0.00254 0.00251 0.0919 0.100  1.00    5741.    6069.\n4 sigma[4] 0.0991 0.0993 0.00676 0.00644 0.0877 0.110  1.00     738.    1131.\n5 sigma[5] 0.0962 0.0962 0.00408 0.00416 0.0894 0.103  1.00    1221.    2231.\n6 sigma[6] 0.0998 0.0998 0.00256 0.00260 0.0957 0.104  1.00    6468.    5918.\n\n\n\n⌘+Cbayesplot::mcmc_trace(fit_cfa_twofactor$draws(\"sigma\"))",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#simulation-study-2",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#simulation-study-2",
    "title": "Lecture 06",
    "section": "\n1.5 Simulation Study 2",
    "text": "1.5 Simulation Study 2\n\n\n\n\n\nTo illustrate how to model a more complex CFA with cross-loadings, let’s generate a new simulation data with test length 7 and two latent variables\nThe mean structure of model is like this, each latent variable was measured by 4 items. The other parameters are as same as Model 1.\n\n\n\n\nMean Structure of Model 2\n\n\n\n1.5.1 Strategies for Model 2\nThe general idea is that we assign a uninformative prior distribution for “main” factor loadings, and assign informative “close-to-zero” prior distribution for “zero” factor loadings:\n\n\n1.5.2 Strategies for Model 2 (Cont.)\n\n\n\nHave a more detailed location matrix for factor loading matrix\n\n\n⌘+Cas.data.frame(Lambda2)\n\n   V1  V2\n1 0.7 0.0\n2 0.5 0.0\n3 0.3 0.0\n4 0.5 0.5\n5 0.0 0.7\n6 0.0 0.5\n7 0.0 0.3\n\n\n\nWe can transform the information of factor loading into:\n\n\n⌘+C## Transform Q to location index\nloc2 &lt;- Q2 |&gt;\n  as.data.frame() |&gt;\n  rename(`1` = V1, `2` = V2) |&gt; \n  rownames_to_column(\"Item\") |&gt;\n  pivot_longer(c(`1`, `2`), names_to = \"Theta\", values_to = \"q\") |&gt; \n  mutate(across(Item:q, as.numeric)) |&gt; \n  mutate(q = -q + 2) |&gt; \n  as.matrix()\nas.data.frame(loc2)\n\n   Item Theta q\n1     1     1 1\n2     1     2 2\n3     2     1 1\n4     2     2 2\n5     3     1 1\n6     3     2 2\n7     4     1 1\n8     4     2 1\n9     5     1 2\n10    5     2 1\n11    6     1 2\n12    6     2 1\n13    7     1 2\n14    7     2 1\n\n\n\n\nWhere Item represents the index of item a factor loading belongs to, and Theta represents the index of latent variables a factor loading belongs to.\nIn Stan’s data block, we denote Item of location matrix as jj and Theta as kk . q represents two types of prior distributions for factor loadings.\n\n\n\n⌘+C\nsimulation_exp2.stan\n\ndata {\n  ...\n  int&lt;lower=0&gt; R; // number of rows in location matrix\n  array[R] int&lt;lower=0&gt;jj;\n  array[R] int&lt;lower=0&gt;kk;\n  array[R] int&lt;lower=0&gt;q;\n  ...\n}\n\n\n\n\n\n\n1.5.3 Data Structure and Data Block\n\n\n\n\n⌘+C\nLecture06.R\n\ndata_list2 &lt;- list(\n  N = 1000, # number of subjects/observations\n  J = J2, # number of items\n  K = 2, # number of latent variables,\n  Y = Y2,\n  Q = Q2,\n  # location of lambda\n  R = nrow(loc2),\n  jj = loc2[,1],\n  kk = loc2[,2],\n  q = loc2[,3],\n  #hyperparameter\n  meanSigma = .1,\n  scaleSigma = 1,\n  meanMu = rep(0, J2),\n  covMu = diag(10, J2),\n  meanTheta = rep(0, 2),\n  corrTheta = matrix(c(1, .3, .3, 1), 2, 2, byrow = T)\n)\n\n\n\n\n\n\n⌘+C\nsimulation_exp2.stan\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  int&lt;lower=0&gt; J; // number of items\n  int&lt;lower=0&gt; K; // number of latent variables\n  matrix[N, J] Y; // item responses\n  \n  int&lt;lower=0&gt; R; // number of rows in location matrix\n  array[R] int&lt;lower=0&gt;jj;\n  array[R] int&lt;lower=0&gt;kk;\n  array[R] int&lt;lower=0&gt;q;\n  \n  //hyperparameter\n  real&lt;lower=0&gt; meanSigma;\n  real&lt;lower=0&gt; scaleSigma;\n  vector[J] meanMu;\n  matrix[J, J] covMu;      // prior covariance matrix for coefficients\n  vector[K] meanTheta;\n  matrix[K, K] corrTheta;\n  \n}\n\n\n\n\n\nNote that for the simplicity of estimation, I specified the factor correlation matrix as fixed. If you are interested in estimating factor correlation, you can refer to the previous model using LKJ sampling.\n\n1.5.4 Parameters block\n\n\n⌘+C\nsimulation_exp2.stan\n\nparameters {\n  vector&lt;lower=0,upper=1&gt;[J] mu;\n  matrix&lt;lower=0&gt;[J, K] lambda;\n  vector&lt;lower=0,upper=1&gt;[J] sigma; // the unique residual standard deviation for each item\n  matrix[N, K] theta;                // the latent variables (one for each person)\n  //matrix[K, K] corrTheta; // not use corrmatrix to avoid duplicancy of validation\n}\n\n\n\nFor parameters block, the only difference is we specify lambda as a matrix with J \\times K, which is 7 \\times 2 in our case.\n\n1.5.5 Model block\nAs you can see, in Model block, we need to use if_else in Stan to specify factor loadings in different locations\n\n\nFor type 1 (green), we specify a uninformative normal distribution\nFor type 2 (red), we specify a informative shrinkage priors.\nFor univariate residual, we can simply use cauchy or exponential prior\nThe item response function estimate each item response using \\mu+ \\Lambda * \\Theta as kernal and \\sigma as variation\n\n\n\n\n⌘+C\nsimulation_exp2.stan\n\nmodel {\n  mu ~ multi_normal(meanMu, covMu);\n  // specify lambda's regulation\n  for (r in 1:R) {\n    if (q[r] == 1){\n      lambda[jj[r], kk[r]] ~ normal(0, 10);\n    }else{// student_t(nu, mu, sigma)\n      lambda[jj[r], kk[r]] ~ student_t(1, 0, 0.01);\n    }\n  }\n  //corrTheta ~ lkj_corr(eta);\n  for (i in 1:N) {\n    theta[i,] ~ multi_normal(meanTheta, corrTheta);\n  }\n  for (j in 1:J) {\n    sigma[j] ~ cauchy(meanSigma, scaleSigma);                   // Prior for unique standard deviations\n    Y[,j] ~ normal(mu[j]+to_row_vector(lambda[j,])*theta', sigma[j]);\n  }\n}\n\n\n\n\n1.5.6 Model Result &gt; R-hat\nWe set up the MCMC as follows:\n\n\n3000 warmups + 3000 samplings = 6000 iterations\nTotal execution time: 202.7 seconds for my computers but it takes 20 minutes or more if I specify inproper priors\n4 chains\nR hat is acceptable suggesting convergence\n\n\n\n⌘+Cfit_cfa_exp2 &lt;- mod_cfa_exp2$sample(\n  data = data_list2,\n  seed = 1234,\n  chains = 4,\n  parallel_chains = 4, \n  iter_sampling = 3000,\n  iter_warmup = 3000\n)\n\n\n\n⌘+Cfit_cfa_exp2$summary(\n  variables = NULL,\n  \"rhat\"\n) |&gt; \n summarise(mean_rhat = mean(rhat, na.rm = T),\n           max_rhat = max(rhat, na.rm = T))\n\n# A tibble: 1 × 2\n  mean_rhat max_rhat\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      1.00     1.04\n\n\n\n1.5.7 MCMC Results &gt; Estimation &gt; Factor Loadings\n\n⌘+Cfit_cfa_exp2$summary('lambda') |&gt; select(variable, mean, median, sd, q5, q95)\n\n# A tibble: 14 × 6\n   variable       mean  median      sd       q5    q95\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 lambda[1,1] 0.714   0.714   0.0164  0.687    0.741 \n 2 lambda[2,1] 0.509   0.509   0.0119  0.489    0.529 \n 3 lambda[3,1] 0.305   0.305   0.00761 0.293    0.318 \n 4 lambda[4,1] 0.515   0.514   0.0130  0.493    0.536 \n 5 lambda[5,1] 0.00724 0.00582 0.00597 0.000607 0.0186\n 6 lambda[6,1] 0.00795 0.00726 0.00503 0.00128  0.0171\n 7 lambda[7,1] 0.00476 0.00422 0.00324 0.000559 0.0107\n 8 lambda[1,2] 0.00606 0.00479 0.00523 0.000414 0.0163\n 9 lambda[2,2] 0.00797 0.00734 0.00483 0.00142  0.0168\n10 lambda[3,2] 0.00618 0.00586 0.00355 0.000989 0.0126\n11 lambda[4,2] 0.491   0.491   0.0122  0.471    0.511 \n12 lambda[5,2] 0.680   0.680   0.0152  0.655    0.706 \n13 lambda[6,2] 0.482   0.482   0.0110  0.464    0.500 \n14 lambda[7,2] 0.285   0.285   0.00694 0.274    0.296 \n\n\n\n⌘+CLambda2\n\n     [,1] [,2]\n[1,]  0.7  0.0\n[2,]  0.5  0.0\n[3,]  0.3  0.0\n[4,]  0.5  0.5\n[5,]  0.0  0.7\n[6,]  0.0  0.5\n[7,]  0.0  0.3",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#wrapping-up",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#wrapping-up",
    "title": "Lecture 06",
    "section": "\n1.6 Wrapping up",
    "text": "1.6 Wrapping up\nWe simulated two models: one is 2-factor model without cross-loadings; another is 2-factor model with cross-loadings.\nIn real setting, the Bayesian modeling could be challenging because\n\n\nPrior distributions are unsure\n\nBad prior may leads to unconverge; So try multiple priors\n\nGood priors will have nice converge very early (i.e, 500 or 1000 samples)\n\n\nMCMC sampling is computationally intensive, and you may not sure how many iterations are enough\n\nHard to come up with a strategy of model building\n\nFor example, “location matrix and different priors” is a strategy I prefer\nIt may not works for any problems for cross-loadings\n\n\n\nYou may try blavaan or other wrap-up package for Bayesian CFA, it saves some time for model building\n\nBut you have no idea how they set up MCMC\n\n\n\n\nAll of these topics will be with us when we start model complicated models in our future lecture.",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#next-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#next-class",
    "title": "Lecture 06",
    "section": "\n1.7 Next Class",
    "text": "1.7 Next Class\n\nMore strategies about measurement models with Stan",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#other-materials",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#other-materials",
    "title": "Lecture 06",
    "section": "\n1.8 Other materials",
    "text": "1.8 Other materials\n\nJonathan Templin’s Website\nWinston-Salem’s Website\nRick Farouni’s Website",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#reference",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html#reference",
    "title": "Lecture 06",
    "section": "\n1.9 Reference",
    "text": "1.9 Reference",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "posts/2023-06-28-a-tutorial-for-forester-r-package/index.html",
    "href": "posts/2023-06-28-a-tutorial-for-forester-r-package/index.html",
    "title": "A tutorial for forester R package",
    "section": "",
    "text": "Workflow of forester\nR Codeknitr::opts_chunk$set(echo = TRUE, message=FALSE, warnings=FALSE, include = FALSE)\noptions(knitr.kable.NA = '')\nlibrary('tidyverse')\nlibrary('forester')\nmycolors = c(\"#4682B4\", \"#B4464B\", \"#B4AF46\", \n             \"#1B9E77\", \"#D95F02\", \"#7570B3\",\n             \"#E7298A\", \"#66A61E\", \"#B4F60A\")\nsoftcolors = c(\"#B4464B\", \"#F3DCD4\", \"#ECC9C7\", \n               \"#D9E3DA\", \"#D1CFC0\", \"#C2C2B4\")\nmykbl &lt;- function(x, ...){\n  kbl(x, digits = 2, ...) |&gt; kable_styling(bootstrap_options = c(\"striped\", \"condensed\")) \n}\nAs contributors of the R package said in their github website:\nIn this blog, I will introduce forester package as a case study using a simulation example and a real example.\ngraph TD;\n    A[data check]--&gt;B;\n    A--&gt;C;\n    B--&gt;D;\n    C--&gt;D;"
  },
  {
    "objectID": "posts/2023-06-28-a-tutorial-for-forester-r-package/index.html#useful-links",
    "href": "posts/2023-06-28-a-tutorial-for-forester-r-package/index.html#useful-links",
    "title": "A tutorial for forester R package",
    "section": "\n1 Useful links",
    "text": "1 Useful links\n\nOne of the contributor, Hubert Ruczynski, wrote up a detailed tutorial published in Mar 1, 2023."
  },
  {
    "objectID": "posts/2023-06-28-a-tutorial-for-forester-r-package/index.html#an-example",
    "href": "posts/2023-06-28-a-tutorial-for-forester-r-package/index.html#an-example",
    "title": "A tutorial for forester R package",
    "section": "\n2 An example",
    "text": "2 An example\nFirst of all, forester::check_data is a very convenient function to glimpse the data for data analysis. By default it will output a “CHECK DATA REPORT”, including some data quality checking list, meanwhile it will also do Spearman correlations.\n\nR Codecheck_dat_res &lt;- forester::check_data(iris, 'Species')\n\n -------------------- CHECK DATA REPORT -------------------- \n \nThe dataset has 150 observations and 5 columns, which names are: \nSepal.Length; Sepal.Width; Petal.Length; Petal.Width; Species; \n\nWith the target value described by a column Species.\n\n✔ No static columns. \n\n✔ No duplicate columns.\n\n✔ No target values are missing. \n\n✔ No predictor values are missing. \n\n✔ No issues with dimensionality. \n\n✖ Strongly correlated, by Spearman rank, pairs of numerical values are: \n \n Sepal.Length - Petal.Length: 0.87;\n Sepal.Length - Petal.Width: 0.82;\n Petal.Length - Petal.Width: 0.96;\n\n✖ These observations migth be outliers due to their numerical columns values: \n 16 ;\n\n✖ Multilabel classification is not supported yet. \n\n✔ Columns names suggest that none of them are IDs. \n\n✔ Columns data suggest that none of them are IDs. \n\n -------------------- CHECK DATA REPORT END -------------------- \n \n\n\n\nR Codestr(check_dat_res$str)\n\n chr [1:38] \" -------------------- **CHECK DATA REPORT** -------------------- \" ..."
  },
  {
    "objectID": "posts/2019-02-22-Play-With-gganimate/index.html",
    "href": "posts/2019-02-22-Play-With-gganimate/index.html",
    "title": "Introduce gganimate for Psychometric",
    "section": "",
    "text": "A new R packge (gganimate ) provides some new features for animation in R. Its big advantage is it could make use of ggplot API and embeded into ggplot. Next, I will use a sample data to show the example. Then I will use some real educational data to explore a little bit what we can do in psychometric area."
  },
  {
    "objectID": "posts/2019-02-22-Play-With-gganimate/index.html#load-the-packages-requried",
    "href": "posts/2019-02-22-Play-With-gganimate/index.html#load-the-packages-requried",
    "title": "Introduce gganimate for Psychometric",
    "section": "\n1.1 1.0 Load the packages requried",
    "text": "1.1 1.0 Load the packages requried"
  },
  {
    "objectID": "posts/2019-02-22-Play-With-gganimate/index.html#prepare-the-data",
    "href": "posts/2019-02-22-Play-With-gganimate/index.html#prepare-the-data",
    "title": "Introduce gganimate for Psychometric",
    "section": "\n1.2 1.1 prepare the data",
    "text": "1.2 1.1 prepare the data\n\ndata(\"austres\")\ndt &lt;- data.frame(x=as.matrix(austres), date=time(austres))\ndt$y &lt;- rnorm(nrow(dt))\ndt$date  &lt;- as.numeric(dt$date)\n\n\np &lt;- ggplot(dt,\n            aes(y = y, x =x)) +\n  geom_line(alpha = 0.7) +\n  geom_point()\np\n\n\n\n\n\n\n\n\n1.2.1 transition_reveal: gradually reveal the data\n\np + transition_reveal(date)\n\n\n\n\n\n\n\n\ndt$g &lt;- rep( c(\"red\", \"green\", \"blue\"), length.out = nrow(dt))\n\n\np_bygroup &lt;- ggplot(dt,\n            aes(y = y, x =x, col = g)) +\n  geom_line(alpha = 0.7) +\n  geom_point()\np_bygroup\n\n\n\n\n\n\n\n\np_bygroup + transition_reveal(date)\n\n\n\n\n\n\n\n\n1.2.2"
  },
  {
    "objectID": "projects/Initialize-R-Project/index.html",
    "href": "projects/Initialize-R-Project/index.html",
    "title": "Initialize R Project",
    "section": "",
    "text": "A screenshot of files for Project\n\nA great file management is important when revisiting project file when getting feedback from reviewers. I created a R file to generate the folders required for a project. Typically, I follow following step:\n\nCreate a empty folder in your project file “Project A”\nOpen rstudio, File &gt; New Project ... then select Project A, you should find a .Rproj file in the folder. Open it.\nThen, create a new R file named “InitializeProject.R”, then copy and run following codes in the file.\nFinnally, you should have similar folders in Project A\n\n\n\n\n\nInitializeProject.R\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\ngetwd()\n\nfolders &lt;- c(\n  \"00_Code\",# for R/Stan/Python syntax files\n  \"01_OriginalData\", # for original data\n  \"02_TransformedData\", # for transformed data, intermedian data\n  \"03_Figure\", # for output figure\n  \"03_Table\", # for output table (.csv)\n  \"04_Modeling\", # sometimes model results are large (MCMC), for models' results\n  \"05_Manuscript\",  # for ppt, pdf, word\n  \"97_Reference\", # for pdf files which have tutorials\n  \"98_TempData\" # for temporate files which will not be used \n)\n\nfs::dir_create(\n  path = folders\n)\n\n# Library Load\nlibrary_load &lt;- function(){\n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"tidyverse\",\n    \"modeltime\",\n    \"psychonetrics\"\n  )\n}\n\n\nlib_funs &lt;- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Code/library_load.R\"\n)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jihong Zhang, Dr.",
    "section": "",
    "text": "Email\n  \n  \n    \n     Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     ORCID\n  \n\n      \nI currently hold the position of Assistant Professor in the Educational Statistics and Research Methods (ESRM) Program at the University of Arkansas. Previously, I served as a postdoctoral fellow in the School of Social Science at the Chinese University of Hong Kong. My academic journey includes a rigorous doctoral training in the Educational Measurement and Statistics (EMS) program at the University of Iowa. My primary research focus was on Psychological and Bayesian Network, Bayesian latent variable modeling, Item Response Theory modeling, and other advanced psychometric methodologies.\nMy expertise lies in the application of sophisticated statistical models, including machine learning and Bayesian networks. My research endeavors are primarily aimed at assisting researchers in the social sciences to comprehend their data more effectively, interpret results accurately, and utilize the potent tools of statistics judiciously. My work is characterized by a commitment to enhancing the methodological understanding and application of statistics in educational research and beyond."
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Jihong Zhang, Dr.",
    "section": "Teaching",
    "text": "Teaching\n\n\n\n\n\n\n\n\n\n\nESRM 6553: Advanced Multivariate Analysis\n\n\nSpring 2024, Mondays, 5:00-7:45PM, Classroom GRAD 0229\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nJihong Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Jihong Zhang, Dr.",
    "section": "Recent posts",
    "text": "Recent posts\n\n\n\n\n\n\n\n\nCustomize font family of Visual Mode in Rstudio and in Zotero\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up High Performance Computing of University of Arkansas\n\n\n\nJan 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nESRM 6553: Advanced Multivariate Analysis\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData visualization for survey data\n\n\n\nJul 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial for forester R package\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOne Template for Quarto Report\n\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMoving My Website to Quarto\n\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA Conversation between R and Python on Data Analysis and Machine Learning\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Class Model: Batch Mplus using R on Mac\n\n\n\nJun 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDissertation Defence\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNSF Grant on Bayesian Cognitive Diagnosis\n\n\n\nMay 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping Academia Institute’s Grant Fundings using R\n\n\n\nMay 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPower Analysis for Structural Equation Modeling\n\n\n\nApr 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n[Workshop] Creating Academic Blog via R\n\n\n\nApr 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Care Data Analysis with Apple Watch\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n[Pre-print] A Model Comparison Approach to Posterior Predictive Model Checks in Bayesian Confirmatory Factor Analysis\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nStudy Notes: gt package and format table\n\n\n\nMay 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Latent Attribute Network Analysis\n\n\n\nOct 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Country own the most Liberty Nobel Prizes? France? Ireland?\n\n\n\nMay 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nMake a Game in R\n\n\n\nApr 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduce gganimate for Psychometric\n\n\n\nFeb 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nLasso Regression Example using glmnet package in R\n\n\n\nFeb 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nVisuliaztion of Item Information Curves In R\n\n\n\nJan 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n[Manual]Using Jags and R2jags in R\n\n\n\nSep 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduce Descrepancy Measures\n\n\n\nSep 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do Data Cleaning in R\n\n\n\nSep 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMy New Shiny App Cut Score Consistency\n\n\n\nMar 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Profile Analysis using MCLUST (in R)\n\n\n\nNov 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nOne Example of Measurement Invariance\n\n\n\nNov 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation Study of Linking Using Mirt\n\n\n\nNov 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use lavaan package to perform Confirmatory Factor Analysis\n\n\n\nOct 19, 2017\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\nSee all"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture06/Lecture06.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Introduce measurement (psychometric) models in general\nDescribe the steps needed in a psychometric model analysis\nDive deeper into the observed-variables modeling aspect",
    "crumbs": [
      "<span class='menutext-home'>Home</span><span class='text-line'></span>",
      "ESRM6553-Adv.Multivariate",
      "Lecture 06"
    ]
  },
  {
    "objectID": "dashboards/template.html#column",
    "href": "dashboards/template.html#column",
    "title": "MMM 2023 Archive",
    "section": "Column",
    "text": "Column"
  },
  {
    "objectID": "dashboards/template.html#column-1",
    "href": "dashboards/template.html#column-1",
    "title": "MMM 2023 Archive",
    "section": "Column",
    "text": "Column\n\n\n\n\n\n\nRow\n\n\n$icon\n[1] \"arrow-repeat\"\n\n$color\n[1] \"danger\"\n\n$value\n[1] 50\n\n\n\n\n$icon\n[1] \"chat\"\n\n$value\n[1] 5"
  },
  {
    "objectID": "projects/2024-02-20-Danmu-Analysis-Bilibili/index.html",
    "href": "projects/2024-02-20-Danmu-Analysis-Bilibili/index.html",
    "title": "Danmu Cloud for Bilibili",
    "section": "",
    "text": "Figure 1: Word Cloud\n\n\n\n \n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#todays-lecture-objectives",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#todays-lecture-objectives",
    "title": "Lecture 07",
    "section": "0.2 Today’s Lecture Objectives",
    "text": "0.2 Today’s Lecture Objectives\n\nQuickly go through our R code file so far\nShow different modeling specifications for different types of item response data\nShow how parameterization differs for standardized latent variables vs. marker item scale identification"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#example-data-conspiracy-theories",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#example-data-conspiracy-theories",
    "title": "Lecture 07",
    "section": "0.3 Example Data: Conspiracy Theories",
    "text": "0.3 Example Data: Conspiracy Theories\n\nToday’s example is from a bootstrap resample of 177 undergraduate students at a large state university in the Midwest.\nThe survey was a measure of 10 questions about their beliefs in various conspiracy theories that were being passed around the internet in the early 2010s\nAll item responses were on a 5-point Likert scale with:\n\nStrong Disagree\nDisagree\nNeither Agree nor Disagree\nAgree\nStrongly Agree\n\nThe purpose of this survey was to study individual beliefs regarding conspiracies.\nOur purpose in using this instrument is to provide a context that we all may find relevant as many of these conspiracies are still prevalent."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#next-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#next-class",
    "title": "Lecture 07",
    "section": "Next Class",
    "text": "Next Class\n\nMore strategies about measurement models with Stan"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#other-materials",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#other-materials",
    "title": "Lecture 07",
    "section": "Other materials",
    "text": "Other materials\n\nJonathan Templin’s Website\nWinston-Salem’s Website\nRick Farouni’s Website"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#reference",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#reference",
    "title": "Lecture 07",
    "section": "Reference",
    "text": "Reference\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theory-q1-q5-built-by-dr.-jonathan-templin",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theory-q1-q5-built-by-dr.-jonathan-templin",
    "title": "Lecture 07",
    "section": "Conspiracy Theory Q1-Q5 (Built by Dr. Jonathan Templin)",
    "text": "Conspiracy Theory Q1-Q5 (Built by Dr. Jonathan Templin)\nQuestions:\n\n\nThe U.S. invasion of Iraq was not part of a campaign to fight terrorism, but was driven by oil companies and Jews in the U.S. and Israel.\nCertain U.S. government officials planned the attacks of September 11, 2001 because they wanted the United States to go to war in the Middle East.\nPresident Barack Obama was not really born in the United States and does not have an authentic Hawaiian birth certificate.\nThe current financial crisis was secretly orchestrated by a small group of Wall Street bankers to extend the power of the Federal Reserve and further their control of the world’s economy.\nVapor trails left by aircraft are actually chemical agents deliberately sprayed in a clandestine program directed by government officials.\n\n\n\n\n\n\n\n\nPS.\n\n\nSome of them I even heart of in my home country…"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theory-q6-q10",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theory-q6-q10",
    "title": "Lecture 07",
    "section": "0.5 Conspiracy Theory Q6-Q10",
    "text": "0.5 Conspiracy Theory Q6-Q10\n\nBillionaire George Soros is behind a hidden plot to destabilize the American government, take control of the media, and put the world under his control.\nThe U.S. government is mandating the switch to compact fluorescent light bulbs because such lights make people more obedient and easier to control.\nGovernment officials are covertly Building a 12-lane “NAFTA superhighway” that runs from Mexico to Canada through America’s heartland.\nGovernment officials purposely developed and spread drugs like crack-cocaine and diseases like AIDS in order to destroy the African American community.\nGod sent Hurricane Katrina to punish America for its sins."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#observed-variables-with-normal-distributions",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#observed-variables-with-normal-distributions",
    "title": "Lecture 07",
    "section": "1.1 Observed Variables with Normal Distributions",
    "text": "1.1 Observed Variables with Normal Distributions\nA psychometric model posits that one or more hypothesized latent variable(s) is the common cause that can predict a person’s response to observed items:\n\nOur hypothesized latent variable: Tendency to Believe in Conspiracies (\\theta_p)\nAs we have only one variable, the model structure is called Unidimensional\nAll 10 items are considered as outcomes of the latent variable in the model\nIn today’s class, we assume all item response follow a normal distribution:\n\nThis is the assumption underlying confirmatory factor analysis (CFA) models\nThis assumption is tenuous at best"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#normal-distribution-linear-regression",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#normal-distribution-linear-regression",
    "title": "Lecture 07",
    "section": "1.2 Normal Distribution: Linear Regression",
    "text": "1.2 Normal Distribution: Linear Regression\nA typical linear regression is like\n\nY_p =\\beta_0 +\\beta_1 X_p + e_p\n\nwith e_p\\sim N(0, \\sigma_e)\nIf we replace X_p with latent variable \\theta_p, and replace \\beta as factor loading \\lambda\nWe can get the linear regression function (IRF) for each item\n\nY_{p1} =\\mu_{0} +\\lambda_{1} \\theta_p + e_{p1}; \\ \\ \\ \\ \\ e_{p1}\\sim N(0, \\psi_1^2)\nY_{p2} =\\mu_{2} +\\lambda_{2} \\theta_p + e_{p2}; \\ \\ \\ \\ \\ e_{p2}\\sim N(0, \\psi_{2}^2)\nY_{p3} =\\mu_{3} +\\lambda_{3} \\theta_p + e_{p3}; \\ \\ \\ \\ \\ e_{p3}\\sim N(0, \\psi_{3}^2)\nY_{p4} =\\mu_{4} +\\lambda_{4} \\theta_p + e_{p4}; \\ \\ \\ \\ \\ e_{p4}\\sim N(0, \\psi_{4}^2)\nY_{p5} =\\mu_{5} +\\lambda_{5} \\theta_p + e_{p5}; \\ \\ \\ \\ \\ e_{p5}\\sim N(0, \\psi_{5}^2)\nY_{p6} =\\mu_{6} +\\lambda_{6} \\theta_p + e_{p6}; \\ \\ \\ \\ \\ e_{p6}\\sim N(0, \\psi_{6}^2)\nY_{p7} =\\mu_{7} +\\lambda_{7} \\theta_p + e_{p7}; \\ \\ \\ \\ \\ e_{p7}\\sim N(0, \\psi_{7}^2)\nY_{p8} =\\mu_{8} +\\lambda_{8} \\theta_p + e_{p8}; \\ \\ \\ \\ \\ e_{p8}\\sim N(0, \\psi_{8}^2)\nY_{p9} =\\mu_{9} +\\lambda_{9} \\theta_p + e_{p9}; \\ \\ \\ \\ \\ e_{p9}\\sim N(0, \\psi_{9}^2)\nY_{p10} =\\mu_{10} +\\lambda_{10} \\theta_p + e_{p10}; \\ \\ \\ \\ \\ e_{p10}\\sim N(0, \\psi_1{0}^2)"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#measurement-model-parameters",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#measurement-model-parameters",
    "title": "Lecture 07",
    "section": "Measurement Model Parameters",
    "text": "Measurement Model Parameters\n\n\n\n\\(\\mu_i\\): Item intercept\n\nInterpretation: the expected score on the item i when \\(\\theta_p =0\\)\n\n\n\n\n\\(\\lambda_i\\): Factor loading (CFA) or Item discrimination (Item Response Theory)\n\nThe change in the expected score of an item for a one-unit increase in belief in conspiracy\n\n\n\\(\\psi_i^2\\): Unique variance 1\n\n\nIn Stan, we will specify \\(\\psi_e\\); the unique standard deviation"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#interpretation-of-measurement-model-parameters",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#interpretation-of-measurement-model-parameters",
    "title": "Lecture 07",
    "section": "Interpretation of Measurement Model Parameters",
    "text": "Interpretation of Measurement Model Parameters\n\n\n\\(\\mu_i\\): Item intercept\n\nInterpretation: the expected score on the item i when \\(\\theta_p =0\\)\nHigher Item Intercept suggests more likely to agree with conspiracy for people with average level of belief in conspiracy\nSo it is also called item easiness in item response theory (IRT)\n\n\\(\\lambda_i\\): Factor loading (CFA) or Item discrimination (Item Response Theory)\n\nThe change in the expected score of an item for a one-unit increase in belief in conspiracy\n\n\\(\\psi_i^2\\): Unique variance1\n\nThe variance of the residuals (sum of square of the expect score minus observed score)\nHigher values suggests more deviation between prediction and observed scores\n\n\n\nIn Stan, we will specify \\(\\psi_e\\); the unique standard deviation"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#measurement-model-identification",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#measurement-model-identification",
    "title": "Lecture 07",
    "section": "1.4 Measurement Model Identification",
    "text": "1.4 Measurement Model Identification\nWhen we specify measurement model, we need to choose on scale identification method for latent variable\n\nAssume latent variable is normal distribution\nOr, maker item has factor loading as “1”\n\nIn this study, we assume \\theta_p \\sim N(0,1) which allows us to estimate all item parameters of the model\n\nThis is what we call a standardization identification method\nFactor scores are like Z-scores"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#last-class",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#last-class",
    "title": "Lecture 07",
    "section": "",
    "text": "We used two simulation data sets to introduce how Stan can be used for factor analysis\n\n\nA simple structure\nA structure with cross-loading\n\n\nWhat we left is how to interpret parameters in factor analysis\nWe also need to refresh our memory of R coding and Stan coding we’ve learnt so far"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#implementing-normal-outcomes-in-stan",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#implementing-normal-outcomes-in-stan",
    "title": "Lecture 07",
    "section": "1.5 Implementing Normal Outcomes in Stan",
    "text": "1.5 Implementing Normal Outcomes in Stan\nRecall that we can use matrix operation to make Stan estimate psychometric models with normal outcomes:\n\nThe model (predictor) matrix cannot be used\n\nThis is because the latent variable will be sampled so that the model matrix cannot be formed as a constant\n\nThe data will be imported as a matrix\n\nMore than one outcome means more than one column vector of data\n\nThe parameters will be specified as vectors of each type\n\nEach item will have its own set of parameters\nImplications for the use of prior distributions"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#stans-data-block",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#stans-data-block",
    "title": "Lecture 07",
    "section": "1.6 Stan’s data Block",
    "text": "1.6 Stan’s data Block\n\n\n⌘+C\ndata {\n\n1  int&lt;lower=0&gt; nObs;                 // number of observations\n2  int&lt;lower=0&gt; nItems;               // number of items\n  matrix[nObs, nItems] Y;            // item responses in a matrix\n\n  vector[nItems] meanMu;\n3  matrix[nItems, nItems] covMu;      // prior covariance matrix for coefficients\n\n  vector[nItems] meanLambda;         // prior mean vector for coefficients\n4  matrix[nItems, nItems] covLambda;  // prior covariance matrix for coefficients\n\n5  vector[nItems] psiRate;            // prior rate parameter for unique standard deviations\n}\n\n\n\n1\n\nnObs is 177, declared as integer with lower bound as 0\n\n2\n\nnItems is 11, declared as integer with lower bound as 0\n\n3\n\nmeanMu as covMu are prior mean and covariance matrix for \\mu_i\n\n4\n\nmeanLambda and covLambda are prior mean and covariance matrix for \\lambda_i\n\n5\n\npsiRate is prior rate parameter for \\psi_i"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#stans-parameter-block",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#stans-parameter-block",
    "title": "Lecture 07",
    "section": "1.7 Stan’s parameter Block",
    "text": "1.7 Stan’s parameter Block\n\n\n⌘+C\nparameters {\n  vector[nObs] theta;                // the latent variables (one for each person)\n  vector[nItems] mu;                 // the item intercepts (one for each item)\n  vector[nItems] lambda;             // the factor loadings/item discriminations (one for each item)\n  vector&lt;lower=0&gt;[nItems] psi;       // the unique standard deviations (one for each item)   \n}\n\n\nHere, the parameterization of \\lambda (factor loadings / item discrimination) can lead to problems in estimation\n\nThe issue: \\lambda_i \\theta_p = (-\\lambda_i) (-\\theta_p)\n\nDepending on the random starting values of each of these parameters (per chain), a given chain may converge to a different region\n\nTo demonstrate, we will start with different random number seed\n\nCurrently using 09102022: works fine\nChange to 25102022: big problem"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theory-q1-q51",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theory-q1-q51",
    "title": "Lecture 07",
    "section": "0.4 Conspiracy Theory Q1-Q51",
    "text": "0.4 Conspiracy Theory Q1-Q51\n\nThe U.S. invasion of Iraq was not part of a campaign to fight terrorism, but was driven by oil companies and Jews in the U.S. and Israel.\nCertain U.S. government officials planned the attacks of September 11, 2001 because they wanted the United States to go to war in the Middle East.\nPresident Barack Obama was not really born in the United States and does not have an authentic Hawaiian birth certificate.\nThe current financial crisis was secretly orchestrated by a small group of Wall Street bankers to extend the power of the Federal Reserve and further their control of the world’s economy.\nVapor trails left by aircraft are actually chemical agents deliberately sprayed in a clandestine program directed by government officials."
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#response-distribution",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#response-distribution",
    "title": "Lecture 07",
    "section": "0.6 Response Distribution",
    "text": "0.6 Response Distribution\n\n\n⌘+C\nlibrary(tidyverse)\nlibrary(here)\nself_color &lt;- c(\"#DB7093\", \"#AFEEEE\", \"#3CB371\", \"#9370DB\", \"#FFD700\")\nroot_dir &lt;- \"posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code\"\ndat &lt;- read.csv(here(root_dir, 'conspiracies.csv'))\nitemResp &lt;- dat[,1:10]\ncolnames(itemResp) &lt;- paste0('item', 1:10)\nitemResp |&gt; \n  rownames_to_column(\"ID\") |&gt; \n  pivot_longer(-ID, names_to = \"Item\", values_to = \"Response\") |&gt; \n  mutate(Item = factor(Item, levels = paste0('item', 1:10)),\n         Response = factor(Response, levels = 1:5)) |&gt; \n  ggplot() +\n  geom_bar(aes(x = Response, fill = Response, group = Response), \n           position = position_stack()) +\n  facet_wrap(~ Item, nrow = 3, ncol = 4) +\n  theme_classic() +\n  scale_fill_manual(values = self_color)\n\n\n\n\n\n\n\n\n\n⌘+C\nconspiracyItems = itemResp"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theories-assumed-latent-variable",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#conspiracy-theories-assumed-latent-variable",
    "title": "Lecture 07",
    "section": "0.7 Conspiracy Theories: Assumed Latent Variable",
    "text": "0.7 Conspiracy Theories: Assumed Latent Variable\nFor today’s lecture, we will assume each of the 10 items measures one single latent variable\n\\theta: tendency to believe in conspiracy theories\n\nHigher value of \\theta suggests more likelihood of believing in conspiracy theories\nLet’s denote this latent variable as \\theta_p for individual p\n\np is the index for person with p = \\{1, \\cdots, P\\}\n\nWe will assume this latent variable is:\n\nContinuous\nNormally distribution: \\theta_p \\sim N(\\mu_\\theta, \\sigma_\\theta)\n\nWe will make different assumptions about the response distribution to show how prior settings affect results\n\n\nAcross all people, we will denote the set of vector of latent variable as\n\n\n\\Theta = \\begin{bmatrix}\\theta_1, \\cdots, \\theta_P\\end{bmatrix}^T"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#interpretation-of-parameters",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#interpretation-of-parameters",
    "title": "Lecture 07",
    "section": "1.3 Interpretation of Parameters",
    "text": "1.3 Interpretation of Parameters\n\n\\mu_1: Item intercept\n\nInterpretation: the expected score on the item i when \\theta_p=0\nHigher Item intercept suggests more likely to believe in conspiracy for people with average level of conspiracy belief\nSo it is also called item easiness in item response theory (IRT)\n\n\\lambda_i: Factor loading or Item discrimination\n\nThe change in the expected score of an item for a one-unit increase in belief in conspiracy\n\n\\psi_i^2: Unique variance2\n\n2 In Stan, we will specify \\psi_e: the unique standard deviation"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#stans-model-block",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#stans-model-block",
    "title": "Lecture 07",
    "section": "1.8 Stan’s model Block",
    "text": "1.8 Stan’s model Block\n\n\n⌘+C\nmodel {\n  lambda ~ multi_normal(meanLambda, covLambda); // Prior for item discrimination/factor loadings\n  mu ~ multi_normal(meanMu, covMu);             // Prior for item intercepts\n  psi ~ exponential(psiRate);                   // Prior for unique standard deviations\n  \n  theta ~ normal(0, 1);                         // Prior for latent variable (with mean/sd specified)\n  \n  for (item in 1:nItems){\n    Y[,item] ~ normal(mu[item] + lambda[item]*theta, psi[item]);\n  }\n}\n\n\nThe loop here conducts the model via item response function (IRF) for each item:\n\nAssumption of conditional independence enables this\n\nNon-independence would need multivariate normal model\n\nThe item mean is set by the conditional mean of the model\n\nThe item SD is set by the unique variance parameter\n\nThe loop puts each item’s parameters into the question"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#choosing-prior-distributions-for-parameters",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#choosing-prior-distributions-for-parameters",
    "title": "Lecture 07",
    "section": "1.9 Choosing Prior Distributions for Parameters",
    "text": "1.9 Choosing Prior Distributions for Parameters\nThere is not uniform agreement about the choices of prior distributions for item parameters\n\nWe will use uninformative priors on each to begin\n\nAfter first model analysis, we will discuss these choices and why they were made\n\nFor now:\n\nItem intercepts: \\mu_i \\sim N(0, \\sigma_{\\mu_i}^2 = 1000)\nFactor loadings / item discrimination: \\lambda_i \\sim N(0, \\sigma^2_{\\lambda_i}=1000)\nUnique standard deviations: \\psi_i \\sim \\text{exponential}(r_{\\psi_i} = .01)"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#prior-density-function-plots",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#prior-density-function-plots",
    "title": "Lecture 07",
    "section": "1.10 Prior Density Function Plots",
    "text": "1.10 Prior Density Function Plots\n\n\nPrior distribution for item intercepts, factor loadings\nN(0, 1000)\n\n\n⌘+C\nset.seed(1234)\ndata.frame(\n  x = seq(0, 5, .001),\n  y = dnorm(x = seq(0, 5, .001), mean = 0, sd = sqrt(1000))\n) |&gt; \n  ggplot(aes(x=x, y =y)) +\n  geom_path(linewidth = 1.3) +\n  labs(x = \"\", y = \"Probability\") +\n  theme_classic() +\n  theme(text = element_text(size = 25))\n\n\n\n\n\n\n\n\n\n\nPrior distribution of unique standard deviations\nExp(.01)\n\n\n⌘+C\ndata.frame(\n  x = seq(0, 2, .001),\n  y = dexp(x = seq(0, 2, .001), rate = .01)\n  ) |&gt; \n  ggplot(aes(x=x, y =y)) +\n  geom_path(linewidth = 1.3) +\n  labs(x = \"\", y = \"Probability\") +\n  theme_classic() +\n  theme(text = element_text(size = 25))"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#rs-data-list-object",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#rs-data-list-object",
    "title": "Lecture 07",
    "section": "1.11 R’s Data List Object",
    "text": "1.11 R’s Data List Object\n\n\n⌘+C\nnObs = nrow(conspiracyItems)\nnItems = ncol(conspiracyItems)\n\n# item intercept hyperparameters\nmuMeanHyperParameter = 0\nmuMeanVecHP = rep(muMeanHyperParameter, nItems)\n\nmuVarianceHyperParameter = 1000\nmuCovarianceMatrixHP = diag(x = muVarianceHyperParameter, nrow = nItems)\n\n# item discrimination/factor loading hyperparameters\nlambdaMeanHyperParameter = 0\nlambdaMeanVecHP = rep(lambdaMeanHyperParameter, nItems)\n\nlambdaVarianceHyperParameter = 1000\nlambdaCovarianceMatrixHP = diag(x = lambdaVarianceHyperParameter, nrow = nItems)\n\n# unique standard deviation hyperparameters\npsiRateHyperParameter = .01\npsiRateVecHP = rep(psiRateHyperParameter, nItems)\n\nmodelCFA_data = list(\n  nObs = nObs,\n  nItems = nItems,\n  Y = conspiracyItems, \n  meanMu = muMeanVecHP,\n  covMu = muCovarianceMatrixHP,\n  meanLambda = lambdaMeanVecHP,\n  covLambda = lambdaCovarianceMatrixHP,\n  psiRate = psiRateVecHP\n)"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#running-the-model-in-stan",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#running-the-model-in-stan",
    "title": "Lecture 07",
    "section": "1.12 Running the model in Stan",
    "text": "1.12 Running the model in Stan\nThe total number of parameters is 207.\n\n177 person parameters (\\theta_1 to \\theta_{177})\n10 estimated parameters for item intercepts (\\mu_{1-10}), factor loadings (\\lambda_{1-10}), and unique standard deviation (\\psi_{1-10}).\n\n\n\n\n\n\n⌘+C\nmodelCFA_samples$metadata()$model_params\n\n\n  [1] \"lp__\"       \"theta[1]\"   \"theta[2]\"   \"theta[3]\"   \"theta[4]\"  \n  [6] \"theta[5]\"   \"theta[6]\"   \"theta[7]\"   \"theta[8]\"   \"theta[9]\"  \n [11] \"theta[10]\"  \"theta[11]\"  \"theta[12]\"  \"theta[13]\"  \"theta[14]\" \n [16] \"theta[15]\"  \"theta[16]\"  \"theta[17]\"  \"theta[18]\"  \"theta[19]\" \n [21] \"theta[20]\"  \"theta[21]\"  \"theta[22]\"  \"theta[23]\"  \"theta[24]\" \n [26] \"theta[25]\"  \"theta[26]\"  \"theta[27]\"  \"theta[28]\"  \"theta[29]\" \n [31] \"theta[30]\"  \"theta[31]\"  \"theta[32]\"  \"theta[33]\"  \"theta[34]\" \n [36] \"theta[35]\"  \"theta[36]\"  \"theta[37]\"  \"theta[38]\"  \"theta[39]\" \n [41] \"theta[40]\"  \"theta[41]\"  \"theta[42]\"  \"theta[43]\"  \"theta[44]\" \n [46] \"theta[45]\"  \"theta[46]\"  \"theta[47]\"  \"theta[48]\"  \"theta[49]\" \n [51] \"theta[50]\"  \"theta[51]\"  \"theta[52]\"  \"theta[53]\"  \"theta[54]\" \n [56] \"theta[55]\"  \"theta[56]\"  \"theta[57]\"  \"theta[58]\"  \"theta[59]\" \n [61] \"theta[60]\"  \"theta[61]\"  \"theta[62]\"  \"theta[63]\"  \"theta[64]\" \n [66] \"theta[65]\"  \"theta[66]\"  \"theta[67]\"  \"theta[68]\"  \"theta[69]\" \n [71] \"theta[70]\"  \"theta[71]\"  \"theta[72]\"  \"theta[73]\"  \"theta[74]\" \n [76] \"theta[75]\"  \"theta[76]\"  \"theta[77]\"  \"theta[78]\"  \"theta[79]\" \n [81] \"theta[80]\"  \"theta[81]\"  \"theta[82]\"  \"theta[83]\"  \"theta[84]\" \n [86] \"theta[85]\"  \"theta[86]\"  \"theta[87]\"  \"theta[88]\"  \"theta[89]\" \n [91] \"theta[90]\"  \"theta[91]\"  \"theta[92]\"  \"theta[93]\"  \"theta[94]\" \n [96] \"theta[95]\"  \"theta[96]\"  \"theta[97]\"  \"theta[98]\"  \"theta[99]\" \n[101] \"theta[100]\" \"theta[101]\" \"theta[102]\" \"theta[103]\" \"theta[104]\"\n[106] \"theta[105]\" \"theta[106]\" \"theta[107]\" \"theta[108]\" \"theta[109]\"\n[111] \"theta[110]\" \"theta[111]\" \"theta[112]\" \"theta[113]\" \"theta[114]\"\n[116] \"theta[115]\" \"theta[116]\" \"theta[117]\" \"theta[118]\" \"theta[119]\"\n[121] \"theta[120]\" \"theta[121]\" \"theta[122]\" \"theta[123]\" \"theta[124]\"\n[126] \"theta[125]\" \"theta[126]\" \"theta[127]\" \"theta[128]\" \"theta[129]\"\n[131] \"theta[130]\" \"theta[131]\" \"theta[132]\" \"theta[133]\" \"theta[134]\"\n[136] \"theta[135]\" \"theta[136]\" \"theta[137]\" \"theta[138]\" \"theta[139]\"\n[141] \"theta[140]\" \"theta[141]\" \"theta[142]\" \"theta[143]\" \"theta[144]\"\n[146] \"theta[145]\" \"theta[146]\" \"theta[147]\" \"theta[148]\" \"theta[149]\"\n[151] \"theta[150]\" \"theta[151]\" \"theta[152]\" \"theta[153]\" \"theta[154]\"\n[156] \"theta[155]\" \"theta[156]\" \"theta[157]\" \"theta[158]\" \"theta[159]\"\n[161] \"theta[160]\" \"theta[161]\" \"theta[162]\" \"theta[163]\" \"theta[164]\"\n[166] \"theta[165]\" \"theta[166]\" \"theta[167]\" \"theta[168]\" \"theta[169]\"\n[171] \"theta[170]\" \"theta[171]\" \"theta[172]\" \"theta[173]\" \"theta[174]\"\n[176] \"theta[175]\" \"theta[176]\" \"theta[177]\" \"mu[1]\"      \"mu[2]\"     \n[181] \"mu[3]\"      \"mu[4]\"      \"mu[5]\"      \"mu[6]\"      \"mu[7]\"     \n[186] \"mu[8]\"      \"mu[9]\"      \"mu[10]\"     \"lambda[1]\"  \"lambda[2]\" \n[191] \"lambda[3]\"  \"lambda[4]\"  \"lambda[5]\"  \"lambda[6]\"  \"lambda[7]\" \n[196] \"lambda[8]\"  \"lambda[9]\"  \"lambda[10]\" \"psi[1]\"     \"psi[2]\"    \n[201] \"psi[3]\"     \"psi[4]\"     \"psi[5]\"     \"psi[6]\"     \"psi[7]\"    \n[206] \"psi[8]\"     \"psi[9]\"     \"psi[10]\""
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#model-resuls",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#model-resuls",
    "title": "Lecture 07",
    "section": "Model Resuls",
    "text": "Model Resuls\n\nChecking convergence with \\(\\hat R\\) (PSRF):\n\nThe maximum of \\(\\hat R\\) is\n\n\n\n\n[1] 1.00334\n\n\n\nItem Response Results\n\n\n\n\n \n  \n    variable \n    mean \n    median \n    sd \n    mad \n    q5 \n    q95 \n    rhat \n    ess_bulk \n    ess_tail \n  \n \n\n  \n    mu[1] \n    2.36 \n    2.36 \n    0.09 \n    0.09 \n    2.22 \n    2.50 \n    1 \n    1367.36 \n    2823.03 \n  \n  \n    mu[2] \n    1.95 \n    1.95 \n    0.09 \n    0.09 \n    1.81 \n    2.09 \n    1 \n    1055.36 \n    2376.43 \n  \n  \n    mu[3] \n    1.87 \n    1.87 \n    0.08 \n    0.08 \n    1.74 \n    2.01 \n    1 \n    1209.55 \n    2367.33 \n  \n  \n    mu[4] \n    2.01 \n    2.01 \n    0.08 \n    0.09 \n    1.87 \n    2.14 \n    1 \n    1081.83 \n    2686.73 \n  \n  \n    mu[5] \n    1.98 \n    1.98 \n    0.09 \n    0.08 \n    1.84 \n    2.12 \n    1 \n    821.40 \n    1759.97 \n  \n  \n    mu[6] \n    1.89 \n    1.89 \n    0.08 \n    0.08 \n    1.76 \n    2.01 \n    1 \n    817.01 \n    1839.81 \n  \n  \n    mu[7] \n    1.72 \n    1.72 \n    0.08 \n    0.08 \n    1.59 \n    1.85 \n    1 \n    1065.73 \n    2404.71 \n  \n  \n    mu[8] \n    1.84 \n    1.84 \n    0.07 \n    0.07 \n    1.72 \n    1.96 \n    1 \n    845.53 \n    1718.28 \n  \n  \n    mu[9] \n    1.80 \n    1.80 \n    0.09 \n    0.09 \n    1.66 \n    1.95 \n    1 \n    1074.97 \n    2372.27 \n  \n  \n    mu[10] \n    1.52 \n    1.52 \n    0.08 \n    0.08 \n    1.38 \n    1.65 \n    1 \n    1506.57 \n    2666.10 \n  \n  \n    lambda[1] \n    0.74 \n    0.74 \n    0.08 \n    0.08 \n    0.60 \n    0.88 \n    1 \n    2774.39 \n    4171.35 \n  \n  \n    lambda[2] \n    0.87 \n    0.87 \n    0.08 \n    0.08 \n    0.75 \n    1.00 \n    1 \n    1893.32 \n    3822.33 \n  \n  \n    lambda[3] \n    0.80 \n    0.80 \n    0.08 \n    0.08 \n    0.68 \n    0.93 \n    1 \n    2352.69 \n    4185.55 \n  \n  \n    lambda[4] \n    0.84 \n    0.84 \n    0.08 \n    0.07 \n    0.72 \n    0.97 \n    1 \n    2128.91 \n    4208.40 \n  \n  \n    lambda[5] \n    1.00 \n    1.00 \n    0.07 \n    0.07 \n    0.89 \n    1.12 \n    1 \n    1343.00 \n    2760.32 \n  \n  \n    lambda[6] \n    0.90 \n    0.90 \n    0.06 \n    0.06 \n    0.80 \n    1.01 \n    1 \n    1256.83 \n    2822.40 \n  \n  \n    lambda[7] \n    0.77 \n    0.76 \n    0.07 \n    0.07 \n    0.65 \n    0.88 \n    1 \n    1922.80 \n    3637.63 \n  \n  \n    lambda[8] \n    0.85 \n    0.85 \n    0.06 \n    0.06 \n    0.76 \n    0.96 \n    1 \n    1348.54 \n    2607.39 \n  \n  \n    lambda[9] \n    0.86 \n    0.86 \n    0.08 \n    0.08 \n    0.74 \n    1.00 \n    1 \n    2033.57 \n    3537.83 \n  \n  \n    lambda[10] \n    0.67 \n    0.67 \n    0.08 \n    0.08 \n    0.55 \n    0.80 \n    1 \n    3476.29 \n    4664.41 \n  \n  \n    psi[1] \n    0.89 \n    0.89 \n    0.05 \n    0.05 \n    0.81 \n    0.98 \n    1 \n    14162.28 \n    6228.69 \n  \n  \n    psi[2] \n    0.73 \n    0.73 \n    0.04 \n    0.04 \n    0.67 \n    0.81 \n    1 \n    12697.11 \n    6128.65 \n  \n  \n    psi[3] \n    0.78 \n    0.78 \n    0.04 \n    0.04 \n    0.71 \n    0.86 \n    1 \n    13849.89 \n    6613.76 \n  \n  \n    psi[4] \n    0.76 \n    0.76 \n    0.04 \n    0.04 \n    0.69 \n    0.83 \n    1 \n    13945.87 \n    6457.77 \n  \n  \n    psi[5] \n    0.54 \n    0.54 \n    0.04 \n    0.04 \n    0.49 \n    0.61 \n    1 \n    8672.12 \n    7261.97 \n  \n  \n    psi[6] \n    0.50 \n    0.50 \n    0.03 \n    0.03 \n    0.45 \n    0.56 \n    1 \n    9470.43 \n    6372.91 \n  \n  \n    psi[7] \n    0.69 \n    0.68 \n    0.04 \n    0.04 \n    0.62 \n    0.75 \n    1 \n    12904.57 \n    6743.24 \n  \n  \n    psi[8] \n    0.48 \n    0.48 \n    0.03 \n    0.03 \n    0.43 \n    0.53 \n    1 \n    9533.77 \n    6643.96 \n  \n  \n    psi[9] \n    0.78 \n    0.78 \n    0.05 \n    0.05 \n    0.71 \n    0.86 \n    1 \n    12473.18 \n    5860.73 \n  \n  \n    psi[10] \n    0.84 \n    0.84 \n    0.05 \n    0.05 \n    0.77 \n    0.92 \n    1 \n    14582.38 \n    6001.16 \n  \n\n\n\n\n\n\n\n\n\nhttps://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#model-results",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#model-results",
    "title": "Lecture 07",
    "section": "1.14 Model Results",
    "text": "1.14 Model Results\n\nChecking convergence with \\hat R (PSRF):\n\nThe maximum of \\hat R is\n\n\n\n\n[1] 1.00334\n\n\n\nItem Response Results\n\n\n\n\n \n  \n    variable \n    mean \n    median \n    sd \n    mad \n    q5 \n    q95 \n    rhat \n    ess_bulk \n    ess_tail \n  \n \n\n  \n    mu[1] \n    2.36 \n    2.36 \n    0.09 \n    0.09 \n    2.22 \n    2.50 \n    1 \n    1367.36 \n    2823.03 \n  \n  \n    mu[2] \n    1.95 \n    1.95 \n    0.09 \n    0.09 \n    1.81 \n    2.09 \n    1 \n    1055.36 \n    2376.43 \n  \n  \n    mu[3] \n    1.87 \n    1.87 \n    0.08 \n    0.08 \n    1.74 \n    2.01 \n    1 \n    1209.55 \n    2367.33 \n  \n  \n    mu[4] \n    2.01 \n    2.01 \n    0.08 \n    0.09 \n    1.87 \n    2.14 \n    1 \n    1081.83 \n    2686.73 \n  \n  \n    mu[5] \n    1.98 \n    1.98 \n    0.09 \n    0.08 \n    1.84 \n    2.12 \n    1 \n    821.40 \n    1759.97 \n  \n  \n    mu[6] \n    1.89 \n    1.89 \n    0.08 \n    0.08 \n    1.76 \n    2.01 \n    1 \n    817.01 \n    1839.81 \n  \n  \n    mu[7] \n    1.72 \n    1.72 \n    0.08 \n    0.08 \n    1.59 \n    1.85 \n    1 \n    1065.73 \n    2404.71 \n  \n  \n    mu[8] \n    1.84 \n    1.84 \n    0.07 \n    0.07 \n    1.72 \n    1.96 \n    1 \n    845.53 \n    1718.28 \n  \n  \n    mu[9] \n    1.80 \n    1.80 \n    0.09 \n    0.09 \n    1.66 \n    1.95 \n    1 \n    1074.97 \n    2372.27 \n  \n  \n    mu[10] \n    1.52 \n    1.52 \n    0.08 \n    0.08 \n    1.38 \n    1.65 \n    1 \n    1506.57 \n    2666.10 \n  \n  \n    lambda[1] \n    0.74 \n    0.74 \n    0.08 \n    0.08 \n    0.60 \n    0.88 \n    1 \n    2774.39 \n    4171.35 \n  \n  \n    lambda[2] \n    0.87 \n    0.87 \n    0.08 \n    0.08 \n    0.75 \n    1.00 \n    1 \n    1893.32 \n    3822.33 \n  \n  \n    lambda[3] \n    0.80 \n    0.80 \n    0.08 \n    0.08 \n    0.68 \n    0.93 \n    1 \n    2352.69 \n    4185.55 \n  \n  \n    lambda[4] \n    0.84 \n    0.84 \n    0.08 \n    0.07 \n    0.72 \n    0.97 \n    1 \n    2128.91 \n    4208.40 \n  \n  \n    lambda[5] \n    1.00 \n    1.00 \n    0.07 \n    0.07 \n    0.89 \n    1.12 \n    1 \n    1343.00 \n    2760.32 \n  \n  \n    lambda[6] \n    0.90 \n    0.90 \n    0.06 \n    0.06 \n    0.80 \n    1.01 \n    1 \n    1256.83 \n    2822.40 \n  \n  \n    lambda[7] \n    0.77 \n    0.76 \n    0.07 \n    0.07 \n    0.65 \n    0.88 \n    1 \n    1922.80 \n    3637.63 \n  \n  \n    lambda[8] \n    0.85 \n    0.85 \n    0.06 \n    0.06 \n    0.76 \n    0.96 \n    1 \n    1348.54 \n    2607.39 \n  \n  \n    lambda[9] \n    0.86 \n    0.86 \n    0.08 \n    0.08 \n    0.74 \n    1.00 \n    1 \n    2033.57 \n    3537.83 \n  \n  \n    lambda[10] \n    0.67 \n    0.67 \n    0.08 \n    0.08 \n    0.55 \n    0.80 \n    1 \n    3476.29 \n    4664.41 \n  \n  \n    psi[1] \n    0.89 \n    0.89 \n    0.05 \n    0.05 \n    0.81 \n    0.98 \n    1 \n    14162.28 \n    6228.69 \n  \n  \n    psi[2] \n    0.73 \n    0.73 \n    0.04 \n    0.04 \n    0.67 \n    0.81 \n    1 \n    12697.11 \n    6128.65 \n  \n  \n    psi[3] \n    0.78 \n    0.78 \n    0.04 \n    0.04 \n    0.71 \n    0.86 \n    1 \n    13849.89 \n    6613.76 \n  \n  \n    psi[4] \n    0.76 \n    0.76 \n    0.04 \n    0.04 \n    0.69 \n    0.83 \n    1 \n    13945.87 \n    6457.77 \n  \n  \n    psi[5] \n    0.54 \n    0.54 \n    0.04 \n    0.04 \n    0.49 \n    0.61 \n    1 \n    8672.12 \n    7261.97 \n  \n  \n    psi[6] \n    0.50 \n    0.50 \n    0.03 \n    0.03 \n    0.45 \n    0.56 \n    1 \n    9470.43 \n    6372.91 \n  \n  \n    psi[7] \n    0.69 \n    0.68 \n    0.04 \n    0.04 \n    0.62 \n    0.75 \n    1 \n    12904.57 \n    6743.24 \n  \n  \n    psi[8] \n    0.48 \n    0.48 \n    0.03 \n    0.03 \n    0.43 \n    0.53 \n    1 \n    9533.77 \n    6643.96 \n  \n  \n    psi[9] \n    0.78 \n    0.78 \n    0.05 \n    0.05 \n    0.71 \n    0.86 \n    1 \n    12473.18 \n    5860.73 \n  \n  \n    psi[10] \n    0.84 \n    0.84 \n    0.05 \n    0.05 \n    0.77 \n    0.92 \n    1 \n    14582.38 \n    6001.16"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#running-the-model-in-stan-1",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html#running-the-model-in-stan-1",
    "title": "Lecture 07",
    "section": "1.13 Running the model in Stan",
    "text": "1.13 Running the model in Stan\nDifferent seed will have different initial values that may leads to convergence.\n\ncmdstanr sampling call:\n\n\n\n⌘+C\nmodelCFA_samples = modelCFA_stan$sample(\n  data = modelCFA_data,\n  seed = 09102022,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 2000\n)\n\n\n\nThe running time is about 2 seconds on my Macbook\n\n\n\n$total\n[1] 2.199301\n\n$chains\n  chain_id warmup sampling total\n1        1  0.707    1.340 2.047\n2        2  0.697    1.340 2.037\n3        3  0.708    1.341 2.049\n4        4  0.682    1.341 2.023\n\n\n\nNote: Typically, longer chains are needed for larger models like this\n\nThese will become even more longer when we use non-normal distributions for observed data"
  },
  {
    "objectID": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html",
    "href": "posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Lecture07.html",
    "title": "Lecture 07",
    "section": "",
    "text": "We used two simulation data sets to introduce how Stan can be used for factor analysis\n\n\nA simple structure\nA structure with cross-loading\n\n\nWhat we left is how to interpret parameters in factor analysis\nWe also need to refresh our memory of R coding and Stan coding we’ve learnt so far"
  }
]