---
title: "Lecture 05: ANOVA Comparisons and Contrasts"
subtitle: "Experimental Design in Education"
date: "2025-02-17"
execute: 
  eval: true
  echo: true
  message: false
  warning: false
format: 
  html:
    code-tools: true
    code-line-numbers: false
    code-fold: false
    number-offset: 1
    fig.width: 10
    fig-align: center
    message: false
    grid:
      sidebar-width: 350px
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503"
    slide-number: c/t
    tbl-colwidths: auto
    scrollable: true
    output-file: slides-index.html
    mermaid:
      theme: forest  
---

::: objectives
[Class Outline]{.redcolor .bigger}

-   Planned Contrast
-   Example: Group Comparison - STEM vs. Non-STEM Groups
    -   ANOVA-style t-statistics and Regression-style Coding Schema in R
-   Effect sizes.
:::

# Planned Contrasts

## Definition: Planned Contrasts

-   Pre-defined:

    -   Unlike post-hoc tests, planned contrasts are determined **before** looking at the data, meaning the researcher has a specific [[hypothesis about which groups to compare]{.underline}]{.bigger}.
    -   **Definition**: Planned contrasts are hypothesis-driven comparisons made before data collection.

-   Weights assigned:

    -   To perform a planned contrast, each group is assigned a numerical "weight" which reflects its role in the comparison, with the [weights usually summing to zero]{.underline}.

    $$
    D = weights * means
    $$

    -   **D**: unscalled group differences given coding scheme

## Example of Planned Contrasts

-   **Imagine** a study comparing the effects of three different study methods (**A, B, C**) on test scores.

    -   One planned contrast might be to compare the average score of method A (considered the "experimental" method) against the combined average of methods B and C (considered the "control" conditions),

    -   Testing the hypothesis that method A leads to significantly higher scores than the traditional methods.

    -   $H_0: \mu_{A} = \frac{\mu_B+\mu_C}{2}$, we also call this **complex contrast**

-   **When** to use planned contrasts:

    -   When you have a clear theoretical basis for predicting specific differences between groups in your study.
    -   When you are only interested in a few specific comparisons, not all possible pairwise comparisons.

## What Does Each Contrast Tell Us?

-   Each contrast is a mean comparison (via t-test).
-   **Simple** contrast (pairwise) compares two individual group means.
-   **Complex** contrast compares a combination of group means.
-   Must be theoretically justified for meaningful interpretation.

## Simple vs. Complex Comparisons

-   **Simple Comparison:** Two groups directly compared.
    -   Example: $H_0: \mu_2 = \mu_3$
-   **Complex Comparison:** Combines means of multiple groups.
    -   Example: $H_0: \frac{(\mu_1 + \mu_2)}{2} = \mu_3$
    -   Example: $H_0: \frac{(\mu_1 + \mu_2 + \mu_3)}{3} = \frac{(\mu_4 + \mu_5)}{2}$

::: callout-note
We should not test all possible combinations of groups. Instead, justify your comparison plan before performing statistic analysis.
:::

## Today's focus: Complex Comparisons

-   We perform ominous test in last lecture, which gives us simple contrasts
    -   which provides all pairwise group comparisons (simple contrasts)
-   Today we focus more on **complex contrasts**.
    -   **Helmert** contrast: Compares each mean to the mean of subsequent groups.
    -   **Sum (Deviation)** contrast: each group compared to grand mean
    -   **Polynomial** contrast: Tests for trends in ordered data.
-   By default, R uses **Treatment** contrasts: each group compared to the reference group
    -   Question: is "Treatment constrast" simple or complex constrast
    -   G1 vs. Treatment (reference)
    -   G2 vs. Treatment (reference)
    -   ....

## Orthogonal vs. Non-Orthogonal Contrasts

-   **Orthogonal Contrasts:** Independent from each other, sum of product of weights equals zero.

-   **Non-Orthogonal Contrasts:** Not independent, lead to inflated Type I error rates.

    ::: callout-note
    Orthogonal contrasts allow clear interpretation without **redundancy**.
    :::

-   Orthogonal contrasts follows a series of group comparisons that does not overlap variances.

## Orthogonal contrasts from variances: no redundency

[Hermert contrast for example]{.redcolor}

-   With a logical control group, a good first contrast compares all treatment groups to the one control group.
-   To get each level of IV alone you should have one less contrast than your number of IV levels (3 levels = 2 contrasts)
-   Once an IV level appears by itself, it shouldn’t reappear in subsequent contrasts

```{dot}
//| echo: FALSE
//| fig-width: 15
digraph VarianceDiagram {
    rankdir=LR;
    
    A [label="Total Variance Explained", shape=box, style=filled, fillcolor=tomato, fontcolor=blue, fontsize=20, width=2.5, height=1.5];
    B [label="Variance for G1 and G2", shape=box, style=filled, fillcolor=lightgrey, fontcolor=red, fontsize=16, width=2.0, height=1];
    C [label="Variance for G1", shape=box, style=filled, fillcolor=lightblue, fontcolor=red];
    D [label="Variance for G2", shape=box, style=filled, fillcolor=lightblue, fontcolor=red];
    E [label="Variance for G3", shape=box, style=filled, fillcolor=lightgrey, fontcolor=red];

    A -> B;
    B -> C;
    B -> D;
    A -> E;
}
```

## Example of Orthogonal Contrasts

::::: columns
::: {.column width="50%"}
-   Contrast 1: g3 vs. (g1, g2) ![](image/g3vsg12.jpg){width="90%"}
:::

::: {.column width="50%"}
-   Contrast 2: g1 vs. g2 ![](image/g1vsg2.jpg){width="90%"}
:::
:::::

## Orthogonal Planned Contrasts

-   If the same exact combination of means is not found in more than one contrast, the contrasts are independent (orthogonal)
    -   Check this by ensuring that the product of the weights across all contrasts sum to zero
-   For a orthogonal comparison, **contrasts are independent with each other:**
    -   We weight the means included on each side of the contrast
    -   Each contrast has a sum of weights as 0
    -   Groups not in the contrast get a weight of 0
-   Why does independence matter?
    -   [Type I error rate is unaffected by independent (orthogonal) contrasts]{.underline}
    -   Interpretation of contrasts is cleaner because contrasts aren’t related (you’ve isolated effects)

| Group | Contrast 1 | Contrast 2 | Product |
|-------|------------|------------|---------|
| G1    | +1         | -1         | -1      |
| G2    | +1         | +1         | +1      |
| G3    | -2         | 0          | 0       |
| Sum   | 0          | 0          | 0       |

## Contrasts' Independence checking in R

```{r}
contras <- matrix(
  c(1, 1, -2,
    -1, 1, 0), ncol = 2
)
contras
t(contras[,1]) %*% contras[,2] ## the cross-product of two constrasts should be zero
crossprod(contras) ## if non-diagnonal elements are zero = orthogonal
cor(contras) ## or correlation matrix is identity matrix
```

## Computing Planned Contrasts

-   Formula for contrast value: $C = c_1\mu_1 + c_2\mu_2 + \dots + c_k\mu_k$
-   Test statistic: $t = \frac{C}{\sqrt{MSE \sum \frac{c_i^2}{n_i}}}$
    -   $MSE$: Mean Square Error from ANOVA
    -   $c_i$: Contrast coefficients
    -   $n_i$: Sample size per group

# Example - STEM vs. Non-STEM Groups

## Background

-   Hypothesis: STEM students have different **growth mindset scores**(score) than non-STEM students.
-   Weights assigned:
    -   STEM (Engineering, Chemistry): $+\frac{1}{2}$
    -   Non-STEM (Education, Political Sci, Psychology): $-\frac{1}{3}$
-   Compute contrast value and test using t-statistic.

## Set Contrasts in R

```{r}
#| code-fold: true
library(tidyverse)
library(kableExtra)
library(here)
# Set seed for reproducibility
set.seed(42)
dt <- read.csv(here("teaching/2025-01-13-Experiment-Design/Lecture05","week5_example.csv"))
options(digits = 5)
summary_tbl <- dt |> 
  group_by(group) |> 
  summarise(
    N = n(),
    Mean = mean(score),
    SD = sd(score),
    shapiro.test.p.values = shapiro.test(score)$p.value
  )
kable(summary_tbl)
```

-   HOV Assumption: Levene's Test

```{r}
aov_fit <- aov(score ~ group, data = dt)
car::leveneTest(aov_fit) |> as.data.frame() |> kable()
```

Even though assumption checkings did not pass using original categorical levels, we may be still interested in different group contrasts.

------------------------------------------------------------------------

### Complex Contrast Matrix

-   There are multiple "canned" contrasts: Helmert, Sum (Effective Coding), Treatment

[For example, Helmert Four contrasts:]{.redcolor}

1.  g1 vs. g2: $\mu_{Engineering} = \mu_{Education}$
2.  $\frac{g1+g2}{2}$ vs. g3: $\mu_{non-Chemistry} = \mu_{Chemistry}$
3.  $\frac{g1+g2+g3}{3}$ vs. g4: $\mu_{non-Political} = \mu_{Political}$
4.  $\frac{g1+g2+g3+g4}{4}$ vs. g5: $\mu_{non-Psychology} = \mu_{Psychology}$

Summary Statistics:

```{r}
#| code-fold: true
dt$group <- factor(dt$group, levels = c("g1", "g2", "g3", "g4", "g5"))
groups <- levels(dt$group)
cH <- contr.helmert(groups) # pre-defined four contrasts
colnames(cH) <- paste0("Ctras", 1:4)
summary_ctras_tbl <- cbind(summary_tbl, cH)
kable(summary_ctras_tbl)
```

------------------------------------------------------------------------

[Orthogonal contrast matrix]{.redcolor}

```{r}
apply(cH, 2, sum)
crossprod(cH) # diagonal -- columns are orthogonal
```

```{r}
summary(aov(score ~ group, dt))
```

## Planed Contrasts and Coding Schema

-   The relationship between planned contrasts in ANOVA and coding in regression lies in how categorical variables are represented and interpreted in statistical models.

-   Both approaches aim to test specific hypotheses about group differences, but their implementation varies based on the framework

    -   ANOVA focuses on partitioning variance,
    -   while regression interprets categorical predictors through coding schemes.

## ANOVA: t-value formula for Defined Contrast Matrix

$t = \frac{C}{\sqrt{MSE \sum \frac{c_i^2}{n_i}}}$

```{r}
Sum_C2_n <- colSums(cH^2 / summary_tbl$N)
C <- crossprod(summary_tbl$Mean, cH)
MSE <- 5.0
t <- as.numeric(C / sqrt(MSE * Sum_C2_n))
t
tibble(
  t_value = t,
  p_value = pt(t, df = 135) ## p-values
)
```

-   g1 vs. g2: We reject the null and determine that the mean of the Education is different from the mean of Engineering in their growth mindset scores (p = 0.531).

-   $\frac{g1+g2}{2}$ vs. g3: We retain the null and determine that the mean of the Chemistry is not significant different from the mean of Education and Engineering in their growth mindset scores (p = 0.531).

------------------------------------------------------------------------

### Helmert Contrast

Remember that Planned Contrast: g1 vs. g2 from Helmert Contrast:

-   t-value: -2.495
-   p-value: 0.0069
-   df: 134

```{r}
contrasts(dt$group) <- "contr.helmert"
fit_helmert <- lm(score ~ group, dt)
contr.helmert(levels(dt$group))
summary(fit_helmert)$coefficients |> round(3)
```

------------------------------------------------------------------------

### Planned Contrast Connected to Linear Regression

-   Planned contrast can be done using `linear regression` + `contrasts`

-   Let's look at the default contrasts plan: treatment contrasts == dummy coding

```{r}
#| output-location: column
## treatment contrast matrix 
attributes(C(dt$group, treatment, 4))$contrasts
```

```{r}
#| output-location: column

## sum contrast matrix 
attributes(C(dt$group, sum, 4))$contrasts
```

```{r}
#| output-location: column
attributes(C(dt$group, helmert, 4))$contrasts
```

```{r}
#| output-location: column
crossprod(attributes(C(dt$group, treatment, 4))$contrasts)
```

------------------------------------------------------------------------

### Treatment Contrasts

::::: columns
::: {.column width="\"50%"}
-   For treatment contrasts, four dummy variables are created to compared:

    -   G1 (ref) vs. G2
    -   G1 (ref) vs. G3
    -   G1 (ref) vs. G4
    -   G1 (ref) vs. G5
:::

::: {.column width="\"50%"}
-   `Intercept`: G1's mean
-   `group2`: G2 vs. G1
-   `group3`: G3 vs. G1
-   `group4`: G4 vs. G1
-   `group5`: G5 vs. G1
:::
:::::

```{r}
#| output-location: column
library(multcomp)
contrasts(dt$group) <- "contr.treatment"
fit <- lm(score ~ group, dt)
unique(cbind(model.matrix(fit), group = dt$group))
```

```{r}
#| output-location: column
summary(fit)$coefficients
```

------------------------------------------------------------------------

### Sum Contrasts

-   Another type of coding is **effect coding**. In R, the corresponding contrast type are the so-called **sum contrasts**.

-   A detailed post about sum contrasts can be found [here](https://learnb4ss.github.io/learnB4SS/articles/contrasts.html)

-   With sum contrasts the reference level is in fact the grand mean.

    -   $\frac{g1+g2+g3+g4+g5}{5}$ vs. g1/g2/g3/g4: the difference between mean score of g1 with grand mean across all five groups

```{r}
#| output-location: column
contrasts(dt$group) <- "contr.sum"
fit2 <- lm(score ~ group, dt)
contr.sum(levels(dt$group))
```

```{r}
#| output-location: column
summary(fit2)$coefficients
```

```{r}
#| output-location: column
mean(dt$score) # (Intercept) grand mean
```

```{r}
#| output-location: column
tibble(
  Label = paste0("group", 1:4),
  Estimate = summary_tbl$Mean[1:4] - mean(dt$score) 
)
```

## Effect Coding (Deviation Coding)

-   In modern statistics, Regression-style coding is statistically equivalent as ANOVA-style contrast matrix.
    -   Equivalent to ANOVA-style contrasts. ([we will use this in R to reproduce ANOVA-style contrast matrix]{.redcolor})
-   Compares each level to the grand mean.

::: callout-note
Effect coding is a method of encoding categorical variables in regression models, similar to dummy coding, but with a different interpretation of the resulting coefficients. It is particularly useful when researchers want to compare each level of a categorical variable to the overall mean rather than to a specific reference category.
:::

------------------------------------------------------------------------

### **1. Definition and Representation**

In effect coding, categorical variables are transformed into numerical variables, typically using values of -1, 0, and 1. The key difference from dummy coding is that the reference category is represented by -1 instead of 0, and the coefficients indicate deviations from the grand mean.

For a categorical variable with `k` levels, effect coding requires `k-1` coded variables. If we have a categorical variable X with three levels: $A, B, C$, the effect coding scheme could be:

| Category      | $X_1$ | $X_2$ |
|---------------|-------|-------|
| A             | 1     | 0     |
| B             | 0     | 1     |
| C (reference) | -1    | -1    |

The last category ($C$) is the reference group, coded as -1 for all indicator variables.

------------------------------------------------------------------------

### **2. Interpretation of Coefficients**

::::: columns
::: {.column width="40%"}
When effect coding is used in a regression model:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
$$

-   $X_1$ and $X_2$ are coded varaibles. They have no much meaning, but their coefficients are **important**
-   $\beta_0$ represents the **grand mean** of $Y$ across all categories.
-   $\beta_1$ and $\beta_2$ represent the **deviation** of categories $A$ and $B$ from the grand mean.
-   The reference group ($C$) does not have a separate coefficient; instead, its deviation can be inferred as $-(\beta_1 + \beta_2)$.
:::

::: {.column width="60%"}
```{r}
#| code-fold: true
#| eval: true
#| fig-height: 8
library(ggplot2)
# Create a data frame for text labels
text_data <- data.frame(
  x = rep(0.25, 3),  # Repeating the same x-coordinate
  y = c(0.3, 0.7, 0.9),  # Different y-coordinates
  label = c("C: beta[0] - beta[1] - beta[2]", 
            "A: beta[0] + 1*'×'*beta[1] + 0*'×'*beta[2]", 
            "B: beta[0] + 0*'×'*beta[1] + 1*'×'*beta[2]")  # Labels
)

# Create an empty ggplot with defined limits
ggplot() +
  geom_text(data = text_data, aes(x = x, y = y, label = label), parse = TRUE, size = 11) +
  # Add a vertical line at x = 0.5
  # geom_vline(xintercept = 0.5, color = "blue", linetype = "dashed", linewidth = 1) +
  # Add two horizontal lines at y = 0.3 and y = 0.7
  geom_hline(yintercept = c(0.35, 0.75, 0.95), color = "red", linetype = "solid", linewidth = 1) +
  geom_hline(yintercept = 0.5, color = "grey", linetype = "solid", linewidth = 1) +
  geom_text(aes(x = .25, y = .45, label = "grand mean of Y"), color = "grey", size = 11) +
  # Set axis limits
  xlim(0, 1) + ylim(0, 1) +
  labs(y = "Y", x = "") +
  # Theme adjustments
  theme_minimal() +
  theme(text = element_text(size = 20))
```
:::
:::::

------------------------------------------------------------------------

### **3. Comparison to Dummy Coding**

-   **Dummy Coding**: Compares each category to a **specific reference category** (e.g., comparing A and B to C).

| Category      | $X_1$ | $X_2$ |
|---------------|-------|-------|
| A             | 1     | 0     |
| B             | 0     | 1     |
| C (reference) | 0     | 0     |

-   **Effect Coding**: Compares each category to the **grand mean** rather than a single reference category.

------------------------------------------------------------------------

### **4. Use Cases**

Effect coding is beneficial when:

-   There is no natural baseline category, and comparisons to the **overall mean** are more meaningful.
-   Researchers want to maintain **sum-to-zero constraints** for categorical variables in linear models.
-   In ANOVA-style analyses, where main effects and interaction effects are tested under an equal-weight assumption.

------------------------------------------------------------------------

### **5. Implementation in R**

Effect coding can be set in R using the `contr.sum` function:

```{r}
#| eval: false
X <- factor(c("A", "B", "C"))
contrasts(X) <- contr.sum(3) # set up effect coding in R
model <- lm(Y ~ X, data = mydata) # use linear regression to mimic ANOVA-style results
summary(model)
```

## Self-defined Contrast

-   Extended Example 2 : Assume now that I think the average of the STEM groups is different than the average of the non-STEM groups

### Method 1: Calculation by Hand

```{r}
#| echo: false
(summary_tbl_ext <- cbind(summary_tbl, Contrasts = c(1/2, -1/3, 1/2, -1/3, -1/3)))
```

$$
H_0: \frac{\mu_{Engineering}+\mu_{Chemistry}}{2} = \frac{\mu_{Education}+\mu_{PoliSci}+\mu_{Psychology}}{3}
$$

weighted mean difference:

$$
C = c_1\mu_{Eng}+c_2\mu_{Edu}+c_3\mu_{Chem}+c_4\mu_{PoliSci}+c_5\mu_{Psych}\\
= \frac{1}{2}*4.25+(-\frac13)*2.75+(\frac12)*3.54+(-\frac13)*3.85+(-\frac13)*2.02\\
= 1.0173
$$

```{r}
(C <- sum(summary_tbl_ext$Contrasts*summary_tbl_ext$Mean))
```

$$
\sum\frac{c^2}{n} = \frac{(\frac12)^2}{28}+\frac{(-\frac13)^2}{28}+\frac{(\frac12)^2}{28}+\frac{(-\frac13)^2}{28}+\frac{(-\frac13)^2}{28}
$$

```{r}
(Sum_C2_n <- sum(summary_tbl_ext$Contrasts^2 / summary_tbl$N))
(MSE = sum((residuals(aov(score ~ group, dt)))^2) / (nrow(dt) - 5))
(t = as.numeric(C / sqrt(MSE * Sum_C2_n)))
```

$$
t = \frac{C}{\sqrt{MSE*\sum\frac{c^2}{n} }} = \frac{1.0173}{\sqrt{5.0011*0.029762}}=2.6368
$$

```{r}
pt(t, df = 135, lower.tail = FALSE) * 2
```

------------------------------------------------------------------------

### Method 2: Linear Regression Contrasts by R

```{r}
# set first contrast
contrasts(dt$group) <- matrix(
  c(1/2, -1/3, 1/2, -1/3, -1/3)
)
fit_extended <- lm(score ~ group, dt)
unique(model.matrix(fit_extended))[, 1:2]
```

```{r}
#| output-line-numbers: "3"
#| class-output: highlight
summary(fit_extended)$coefficient |> round(3)
```

# Effect Sizes

## What is Effect Sizes

-   Effect size measures the magnitude of an effect beyond statistical significance.
    -   Put simply: a p-value is partially dependent on sample size and does not give us any insight into the strength of the relationship
    -   Lower p-value → just increase sample size
-   Provides context for interpreting practical significance.
    -   In scientific experiments, it is often useful to know not only whether an experiment has a statistically significant effect, but also the size (magnitude) of any observed effects.
-   Common measures: Eta squared ($\eta^2$), Omega squared ($\omega^2$), Cohen’s d.

::: callout-note
Many psychology journals require the reporting of effect sizes
:::

## Eta Squared

-   $\eta^2$: Proportion of total variance explained by the independent variable.
-   Formula: $\eta^2 = \frac{SS_{Model}}{SS_{Total}}$
-   Interpretation:
    -   Small: 0.01, Medium: 0.06, Large: 0.14

```{r}
(F_table <- as.data.frame(anova(fit)))
```

```{r}
(eta_2 <- F_table$`Sum Sq`[1] / sum(F_table$`Sum Sq`))
```

[Interpretation: 11.69% of variance in the DV is due to group differences.]{.redcolor}

## Drawbacks of Eta Squared

1.  **As you add more variables to the model, the proportion explained by any one variable will automatically decrease.**
    -   This makes it hard to compare the effect of a single variable in different studies.
    -   Partial Eta Squared solves this problem. There, the denominator is not the total variation in Y, but the unexplained variation in Y plus the variation explained just by that IV.
        -   Any variation explained by other IVs is removed from the denominator.
    -   In a one-way ANOVA, Eta Squared and Partial Eta Squared will be equal, but this isn’t true in models with more than one independent variable (factorial ANOVA).
2.  **Eta Squared is a biased measure of population variance explained (although it is accurate for the sample).**
    -   It always overestimates it. This bias gets very small as sample size increases, but for small samples an unbiased effect size measure is Omega Squared.

## Omega Square

-   Omega Squared ($\omega^2$) has the same basic interpretation but uses unbiased measures of the variance components.
    -   Because it is an unbiased estimate of population variances, Omega Squared is always smaller than Eta Squared. \## Omega Squared ($\omega^2$)
-   Unbiased estimate of effect size, preferred for small samples.
-   Formula: $\omega^2 = \frac{SS_{Model} - df_{Model} \cdot MSE}{SS_{Total} + MSE}$
-   Interpretation follows $\eta^2$ scale but slightly smaller values.

```{r}
F_table
```

```{r}
#| results: hold
attach(F_table) #<1>
(Omega_2 <- (`Sum Sq`[1] - Df[1] * MSE) / (sum(`Sum Sq`) + MSE)) #<2>
detach(F_table)
```

1.  Attach data set so that you can directly call the columns without "\$"
2.  The formula of Omega square

## Effect Size for Planned Contrasts

-   Correlation-based effect size: $r = \sqrt{\frac{t^2}{t^2 + df}} = \sqrt{\frac{F}{F + df}}$
-   Example: For $t = 2.49, df = 135$: $r = \sqrt{\frac{2.49^2}{2.49^2 + 135}} = 0.21$
    -   Small to moderate effect.

```{r}
(coef_tbl <- as.data.frame(summary(fit)$coefficients))
```

```{r}
#| results: hold
attach(coef_tbl)
round(sqrt(`t value`^2 / (`t value`^2 + 135)), 3)
detach(coef_tbl)
```

-   Shows a small to moderate positive relationship between g1 with g5.

## Cohen’s d and Hedges’ g

-   Used for simple mean comparisons.
-   Cohen’s d formula: $d = \frac{M_1 - M_2}{SD_{pooled}}$
-   Hedges’ g corrects for small sample bias.
-   Guidelines:
    -   Small: 0.2, Medium: 0.5, Large: 0.8

## Guideline of Effect Size

![](images/clipboard-1482285190.png)

-   For our example: there is a significant effect of academic program on growth mindset scores (F(4,135)=4.47).
-   Academic program explains 11.69% of variance in growth mindset scores. This is a large medium to large effect ($\eta^1$ = 0.1169).

## Summary

-   Planned contrasts allow hypothesis-driven mean comparisons.
-   Orthogonal contrasts maintain Type I error control.
-   Effect sizes help interpret the importance of results.
-   Combining planned contrasts with effect size measures enhances statistical analysis.
