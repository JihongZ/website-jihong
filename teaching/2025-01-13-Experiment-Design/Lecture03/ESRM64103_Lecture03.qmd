---
title: "Lecture 03: Introduction to One-way ANOVA"
subtitle: "Experimental Design in Education"
date: "2025-08-18"
execute: 
  eval: false
  echo: true
  warning: false
format: 
  html:
    code-tools: true
    code-line-numbers: false
    code-fold: false
    number-offset: 1
    fig.width: 10
    fig-align: center
    message: false
    grid:
      sidebar-width: 350px
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503"
    slide-number: c/t
    tbl-colwidths: auto
    scrollable: true
    output-file: slides-index.html
    mermaid:
      theme: forest  
---

## Lecture Outline

-   Homework assignment
-   Introduction to ANOVA
    -   Why use ANOVA instead of multiple t-tests?
    -   Logic and components of ANOVA
    -   Steps and assumptions in ANOVA
    -   Example scenario and real-world applications
-   Performing ANOVA in R
    -   Checking homogeneity of variance
    -   Post-hoc analysis
    -   Using weights in ANOVA
    -   Where to find weights for ANOVA
-   Conclusion

# Homework 1

## Let's walk through HW1

# One-Way ANOVA

## Introduction

-   Overview of ANOVA and its applications.
-   Used for comparing means across multiple groups.
-   Explanation of why ANOVA is essential in statistical analysis.

## ANOVA Basics

-   **Analysis of Variance (ANOVA)** is a statistical method for comparing means across three or more groups.
-   When comparing only 2 groups, a t-test is typically used (though mathematically equivalent to ANOVA with 2 groups).
-   When comparing 3 or more groups, ANOVA using the F-test is the appropriate method to avoid inflating Type I error.

## Why Use ANOVA Instead of Multiple t-tests?

1.  **Computational efficiency**: As the number of groups increases, the number of pairwise comparisons grows rapidly (k groups require k(k-1)/2 comparisons).
2.  **Controls Type I error**: Multiple t-tests inflate the family-wise error rate. For example, with 4 groups and α = 0.05, conducting 6 t-tests results in an actual error rate of approximately 0.26, not 0.05.
3.  **Omnibus test**: ANOVA provides a single test to determine if any group differences exist before conducting pairwise comparisons.
4.  **Statistical power**: ANOVA is more powerful than multiple t-tests when properly applied with post-hoc corrections.

## Logic of ANOVA

-   ANOVA compares group means by analyzing [variance components]{.underline}.
-   Two independent variance estimates:
    1.  **Between-group variability**: Differences among group means (reflects treatment effect)
    2.  **Within-group variability**: Differences within each group (reflects random error or chance)
-   If the between-group variability is substantially larger than the within-group variability, we conclude that groups differ significantly.

## Components of ANOVA

1.  **Total Sum of Squares** ($SS_{total}$): Total variability in the outcome variable across all observations.
2.  **Sum of Squares Between** ($SS_{between}$): Variability attributable to differences between group means (systematic variation).
3.  **Sum of Squares Within** ($SS_{within}$): Variability within groups, representing random error (unsystematic variation).
    -   Key relationship: $SS_{total} = SS_{between} + SS_{within}$

------------------------------------------------------------------------

### If $SS_{between}$ \> $SS_{within}$ ...

```{r}
#| eval: true
#| echo: false
library(ggplot2)
library(dplyr)
set.seed(1234)
data <- data.frame(
  group = rep(c("G1", "Control"), each = 20),
  score = c(rnorm(20, mean = 20, sd = 1), rnorm(20, mean = 25, sd = 1))
)
mean_tbl <- data |> 
  group_by(group) |> 
  summarise(G_means = mean(score),
            G_max = max(score),
            G_min = min(score))
ggplot(data) +
  geom_point(aes(x = group, y = score, color = group)) +
  geom_point(aes(x = group, y = G_means, color = group), data = mean_tbl, size = 5) +
  geom_errorbar(aes(ymin = G_min, ymax = G_max, x = group), 
                 data = mean_tbl, width = .05,
                color = "green2", linewidth = 1.3,
                position = position_nudge(x = -.1)) +
  annotate(geom = "errorbar", 
           ymin = min(mean_tbl$G_means[2]), 
           ymax = max(mean_tbl$G_means[1]), 
           x = 1.5, width = .05, linewidth = 1.3,
           color = "royalblue") +
  theme_classic() +
  labs(title = "Scatter Plot of Scores by Group", x = "Group", y = "Score", caption = "SD = 1, Mean = {25, 20}")
```

#### R output:

```{r}
#| eval: true
#| echo: false
anova_result <- aov(score ~ group, data = data)
res <-  summary(anova_result)
res
```

**Interpretation**: With small within-group variability (SD = 1) and clear separation between groups (means of 20 and 25), the F-statistic is large and highly significant. The between-group variance substantially exceeds the within-group variance, providing strong evidence that the groups differ.

------------------------------------------------------------------------

### If $SS_{between}$ \< $SS_{within}$ ...

```{r}
#| eval: true
#| echo: false
set.seed(1234)
data <- data.frame(
  group = rep(c("G1", "Control"), each = 20),
  score = c(rnorm(20, mean = 20, sd = 5), 
            rnorm(20, mean = 25, sd = 5))
)
mean_tbl <- data |> 
  group_by(group) |> 
  summarise(G_means = mean(score),
            G_max = max(score),
            G_min = min(score))
ggplot(data) +
  geom_point(aes(x = group, y = score, color = group)) +
  geom_point(aes(x = group, y = G_means, color = group), data = mean_tbl, size = 5) +
  geom_errorbar(aes(ymin = G_min, ymax = G_max, x = group), 
                 data = mean_tbl, width = .05,
                color = "green2", linewidth = 1.3,
                position = position_nudge(x = -.1)) +
  annotate(geom = "errorbar", 
           ymin = min(mean_tbl$G_means[2]), 
           ymax = max(mean_tbl$G_means[1]), 
           x = 1.5, width = .05, linewidth = 1.3,
           color = "royalblue") +
  theme_classic() +
  labs(title = "Scatter Plot of Scores by Group", x = "Group", y = "Score", caption = "SD = 5, Mean = {25, 20}")
```

#### R output:

```{r}
#| eval: true
#| echo: false
anova_result <- aov(score ~ group, data = data)
res <-  summary(anova_result)
res
```

**Interpretation**: With larger within-group variability (SD = 5), the overlap between groups increases. Even though the between-group variance remains the same, it is now smaller relative to the within-group variance. This results in a smaller F-statistic and reduced statistical power to detect group differences.

## Practical Steps in One-way ANOVA

1.  Compute the total variability in the outcome variable.
2.  Partition the total variability into between-group (model) and within-group (error) components.
3.  Calculate the F-statistic: $F_{obs} = \frac{SS_{between}/df_{between}}{SS_{within}/df_{within}}=  \frac{MS_{between}}{MS_{within}}$
4.  Construct the ANOVA table, set the alpha level (typically 0.05), and draw conclusions based on the p-value.
5.  Check the homogeneity of variance assumption (Levene's test or Bartlett's test).
6.  If the overall F-test is significant, conduct post-hoc pairwise comparisons (e.g., Tukey HSD) to identify which specific groups differ.

## Assumptions of ANOVA

1.   Homogeneity of variance (homoscedasticity).
2.   Independence of observations.
3.   Normality of residuals.

## Homogeneity of Variance

-   **Assumption**: ANOVA assumes that the variance of the outcome variable is approximately equal across all groups (homoscedasticity).
-   **Why it matters**: When variances differ substantially across groups, the F-test can become either too liberal (inflated Type I error) or too conservative (reduced power).
-   **Checking the assumption**: Use visual inspection (boxplots), Levene's test, or Bartlett's test to assess variance equality before interpreting ANOVA results.

::: callout-note
## We will talk more details about this in the next lecture
:::

## Methods to Check Homogeneity of Variance

-   **Levene’s Test**: Tests for equal variances across groups.
-   **Bartlett’s Test**: Specifically tests for homogeneity in normally distributed data.
-   **Visual Inspection**: Boxplots can help assess variance equality.
-   Graph: Example of equal and unequal variance in boxplots.

## When Homogeneity of Variance Is Violated

1. **Welch's ANOVA** (Welch's F-test)
   - Specifically designed for unequal variances
   - Adjusts degrees of freedom to account for heteroscedasticity
   - Generally preferred first option when HoV is violated

2. **Brown-Forsythe Test**
   - Another robust alternative to traditional ANOVA
   - Less affected by unequal variances than standard F-test

3. **Non-parametric Alternatives**
   - Kruskal-Wallis test (non-parametric equivalent to one-way ANOVA)
   - Does not assume equal variances or normality
   - Tests for differences in distributions rather than means

# Example: One-way ANOVA in R

## Example Scenario

-   **Research Aim**: Investigating the effect of a teaching intervention on children's [verbal acquisition]{.underline}.
-   **IV (Factor):** Intervention groups (G1, G2, G3, Control).
-   **DV:** Verbal acquisition scores.
-   **Hypotheses:**
    -   $H_0$: $\mu_{Control} = \mu_{G1} = \mu_{G2} = \mu_{G3}$
    -   $H_A$: At least two group means differ.

## Performing ANOVA in R

### Load Libraries

```{r}
#| eval: true
library(ggplot2)
library(car) # used for leveneTest; install.packages("car")
```

### Generate Sample Data

```{r}
set.seed(1234)
rnorm(10, 20, 10) # generate 10 data points from a normal distribution with mean = 20 and SD = 10
```

**Explanation**: The `rnorm()` function generates random values from a normal distribution. The `set.seed()` function ensures reproducibility by fixing the random number generation sequence.

-   Context: This example focuses on whether three different teaching methods (labeled as G1, G2, G3) affect students' test scores.

-   In total, 40 students are assigned to three teaching groups and one control group. Each group has 10 students.

```{r}
#| eval: true
set.seed(1234)
# Create dataset with EQUAL variances (SD = 5 for all groups)
data <- data.frame(
  group = rep(c("G1", "G2", "G3", "Control"), each = 10),
  score = c(rnorm(10, 20, 5), rnorm(10, 25, 5), rnorm(10, 30, 5), rnorm(10, 22, 5))
)
# Create dataset with UNEQUAL variances (SD varies from 0.1 to 10 across groups)
data_unequal <- data.frame(
  group = rep(c("G1", "G2", "G3", "Control"), each = 10),
  score = c(rnorm(10, 20, 10), rnorm(10, 25, 5), rnorm(10, 30, 1), rnorm(10, 22, .1))
)
```

**Note**: We create two datasets to demonstrate the importance of homogeneity of variance:
- `data`: Equal variances (homoscedastic) - meets ANOVA assumptions
- `data_unequal`: Unequal variances (heteroscedastic) - violates ANOVA assumptions

------------------------------------------------------------------------

### Conduct ANOVA Test

```{r}
#| eval: true
# ANOVA with equal variances
anova_result <- aov(score ~ group, data = data)
summary(anova_result)
# ANOVA with unequal variances
anova_result2 <- aov(score ~ group, data = data_unequal)
summary(anova_result2)
```

**Comparison**:
- Both ANOVAs show significant F-statistics (p < 0.05), indicating group differences
- However, the unequal variance dataset violates ANOVA assumptions, making these results potentially unreliable
- For `data_unequal`, we should use Welch's ANOVA instead: `oneway.test(score ~ group, data = data_unequal)`

## Checking Homogeneity of Variance (HoV)

### Method 1: Visual Inspection

#### Equal Variances across groups

```{r}
#| eval: true
ggplot(data, aes(x = group, y = score)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Equal Variance: Boxplot of Scores by Group", x = "Group", y = "Score")
```

------------------------------------------------------------------------

#### Unequal Variances across groups

```{r}
#| eval: true
ggplot(data_unequal, aes(x = group, y = score)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Unequal Variances: Boxplot of Scores by Group", x = "Group", y = "Score")
```

------------------------------------------------------------------------

### Method 2: Using Bartlett’s Test

```{r}
#| eval: true
#| results: hold

cat("Equal group:\n")
bartlett.test(score ~ group, data = data)
cat("Unequal group:\n" )
bartlett.test(score ~ group, data = data_unequal)
```

**Interpretation**:

- **Equal variance data**: The Bartlett test is non-significant (p > 0.05), indicating we have no evidence against the null hypothesis. We can assume the groups have homogeneous variances.
- **Unequal variance data**: The Bartlett test is significant (p < 0.05), indicating strong evidence that variances differ across groups. This violates the homogeneity of variance assumption, and we should consider using Welch's ANOVA or other robust alternatives.

------------------------------------------------------------------------

### Method 3: Using Levene’s Test

```{r}
#| eval: true
leveneTest(score ~ group, data = data)
leveneTest(score ~ group, data = data_unequal)
```

**Interpretation**:

- **Equal variance data**: Levene's test shows p > 0.05, confirming homogeneity of variance across groups.
- **Unequal variance data**: Levene's test shows p < 0.05, indicating significant differences in variances. This suggests the homogeneity assumption is violated and alternative methods should be considered.

### When to use each test


- Use Levene's Test (more frequently used) when:

  - Data may not be normally distributed
  - Dealing with outliers in the dataset
  - Working with small sample sizes
  - Conducting preliminary tests for ANOVA with non-normal data
  - General robustness is prioritized over power
  
- Use Bartlett's Test when:

  - Data is confirmed to be normally distributed
  - No significant outliers are present
  - Larger sample sizes are available
  - Maximum statistical power is desired (under normality)
  - Preliminary testing for parametric procedures with normal data

## Post-hoc Analysis

### Tukey Honest Significant Differences (HSD) Test

-   **Purpose**: Conducts all possible pairwise comparisons between group means while controlling the family-wise error rate at the specified alpha level (typically 0.05).

-   **Why it's needed**: Simple pairwise t-tests after a significant ANOVA inflate the Type I error rate. Tukey HSD adjusts for multiple comparisons to maintain the overall error rate.

-   **How it works**: Creates confidence intervals for all pairwise mean differences using the studentized range distribution, which accounts for the number of groups being compared.

-   **Interpretation**: If a confidence interval does not include zero, or if p adj < 0.05, the pair of groups differs significantly.

```{r}
#| eval: true
tukey_result <- TukeyHSD(anova_result)
print(tukey_result$group)
```

**Interpretation**: The Tukey HSD test provides pairwise comparisons between all groups while controlling for family-wise error rate. The output shows:

- **diff**: The difference in means between each pair of groups
- **lwr & upr**: The 95% confidence interval for the difference
- **p adj**: The adjusted p-value for multiple comparisons

Pairs with p adj < 0.05 indicate statistically significant differences between those groups.

------------------------------------------------------------------------

```{r}
#| eval: true
plot(tukey_result)
```

## Interpreting Results

### ANOVA Statistics

```         
           Df Sum Sq Mean Sq F value   Pr(>F)    
group        3  724.1  241.37   11.45 2.04e-05 ***
Residuals   36  759.2   21.09                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

```{r}
#| eval: true
#| code-fold: true

library(dplyr)
data |> 
  group_by(group) |> 
  summarise(
    Mean = mean(score),
    SD = sd(score)
  )
```

-   A one-way analysis of variance (ANOVA) was conducted to examine the effect of teaching method on students' test scores. The results indicated a statistically significant difference in test scores across the four groups, (F(3, 36) = 11.45, p < .001). Post-hoc comparisons using the Tukey HSD test revealed that the Interactive method (G3) (M = 28.06, SD = 3.33) resulted in significantly higher scores than the Traditional method (M = 18.17, SD = 4.47) with p < .001. However, no significant difference was found between G1 and the Control group (p = .99). These results suggest that certain teaching methods can significantly improve student performance compared to traditional approaches.

## Real-world Applications of ANOVA

-   Experimental designs in psychology.
-   Clinical trials in medicine.
-   Market research and A/B testing.
-   Example case studies.

## Using Weights in ANOVA

-   In some cases, observations may have different levels of reliability or importance.
-   Weighted ANOVA allows us to account for these differences by assigning weights.
-   Example: A study where some groups have higher variance and should contribute less to the analysis.

## Correct Approach to Sampling Weights

### Basic Principle
- **Inverse weighting**: Weight = 1 / (Selection probability)
- Clusters with larger samples have higher selection probabilities
- Therefore, they receive smaller weights

### Example
- If Cluster A has 1000 people and you sampled 100, selection probability = 100/1000 = 0.1
- If Cluster B has 100 people and you sampled 50, selection probability = 50/100 = 0.5
- Weight for Cluster A = 1/0.1 = 10
- Weight for Cluster B = 1/0.5 = 2

### Why This Works
- Oversampled groups (higher sampling fraction) get downweighted
- Undersampled groups (lower sampling fraction) get upweighted
- This corrects the disproportionate representation in your sample

## Example: Applying Weights in `aov()`

```{r}
#| eval: true
weights <- c(rep(1, 10), rep(2, 10), rep(0.5, 10), rep(1.5, 10))
anova_weighted <- aov(score ~ group, data = data, weights = weights)
summary(anova_weighted)
```

**Interpretation**:

-   The weights modify the influence of each observation in the model. In this example, G2 observations receive double weight (2.0), G3 observations receive half weight (0.5), and G1 and Control receive standard weights.
-   Notice how the F-statistic and p-value change compared to the unweighted analysis, reflecting the differential contribution of each group.
-   This approach is useful when data reliability varies across groups or when correcting for sampling design effects.

## Where Do We Get Weights for ANOVA?

-   Weights can be derived from:
    -   **Large-scale assessments**: Different student groups may have varying reliability in measurement.
    -   **Survey data**: Unequal probability of selection can be adjusted using weights.
    -   **Experimental data**: Measurement error models may dictate different weight assignments.

## Example: Using Weights in Large-Scale Assessments

-   Consider an educational study where test scores are collected from schools of varying sizes.
-   Larger schools may contribute more observations but should not dominate the analysis.
-   Weighting adjusts for this imbalance:

```{r}
#| eval: true
weights <- ifelse(data$group == "LargeSchool", 0.5, 1)
anova_weighted <- aov(score ~ group, data = data, weights = weights)
summary(anova_weighted)
```

**Interpretation**:

-   In this example, we apply conditional weighting where observations from "LargeSchool" receive half the weight (0.5) compared to other groups (weight = 1).
-   This prevents larger schools from dominating the analysis due to their overrepresentation in the sample.
-   The weighted analysis ensures fair representation across schools of different sizes, providing more generalizable results.

## Conclusion and Interpretation

-   Review results and discuss findings.
-   Key takeaways from the analysis.


