{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'How to use Julia in Quarto'\n",
        "\n",
        "date: 'Mar 15 2024'\n",
        "categories:\n",
        "  - Julia\n",
        "  - Quarto\n",
        "toc: true\n",
        "number-sections: true\n",
        "execute: \n",
        "  eval: true\n",
        "  output: false\n",
        "format: \n",
        "  html: \n",
        "    code-summary: 'Code'\n",
        "    code-fold: false\n",
        "    code-line-numbers: false\n",
        "jupyter: julia-1.10\n",
        "---\n",
        "\n",
        "\n",
        "## Previous posts\n",
        "\n",
        "This [post](posts/2021-08-30-gradient-descent-via-julia/index.md) illustrates how to use Julia to create a gradient descent algorithm. What has not been introduced, however, is how to perform the data analysis using Julia in Quarto. This post will illustrate the workflow step by step.\n",
        "\n",
        "## Initial Setup\n",
        "\n",
        "First of all, refer to this [Quarto.org](https://quarto.org/docs/computations/julia.html), [JuliaHub](https://help.juliahub.com/juliahub/stable/tutorials/quarto/), and Patrick Altmeyer's [post](https://www.paltmeyer.com/blog/posts/tips-and-tricks-for-using-quarto-with-julia/). The first step is to install following components:\n",
        "\n",
        "1.  IJulia\n",
        "2.  Revise.jl\n",
        "3.  Jupyter Cache\n",
        "\n",
        "``` {.bash filename=\"Terminal\"}\n",
        "using Pkg\n",
        "Pkg.add(\"IJulia\")\n",
        "Pkg.add(\"Revise\")\n",
        "using Conda\n",
        "Conda.add(\"jupyter-cache\")\n",
        "```\n",
        "\n",
        "Second, when you create the new quarto document, make sure the `yaml` header contains the `jupyter` item. For example, the yaml of this post is:\n",
        "\n",
        "``` yaml\n",
        "title: 'How to use Julia in Quarto'\n",
        "\n",
        "date: 'Mar 10 2024'\n",
        "categories:\n",
        "  - Julia\n",
        "  - Quarto\n",
        "format: \n",
        "  html: \n",
        "    code-summary: 'Code'\n",
        "    code-fold: false\n",
        "    code-line-numbers: false\n",
        "jupyter: julia-1.6\n",
        "```\n",
        "\n",
        "After the installation, you should be able to run the julia code in quarto like:\n"
      ],
      "id": "c8ccddd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "print(\"Hello World!\")"
      ],
      "id": "939faf9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import dataset\n"
      ],
      "id": "3fc0f3aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import packages\n",
        "using DataFrames\n",
        "using CSV\n",
        "# load in the diamonds.csv\n",
        "diamonds = DataFrame(CSV.File(\"diamonds.csv\"))"
      ],
      "id": "e0f9eb7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "first(diamonds, 7)"
      ],
      "id": "9fd3f1ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Statistical Modeling\n",
        "\n",
        "Following the previous post, we can easily model a generalized linear regression using `GLM` module:\n"
      ],
      "id": "eaf8bb4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "using GLM\n",
        "lm_fit = lm(@formula(price ~ depth), diamonds)"
      ],
      "id": "153cf9a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do some more advanced measurement - Factor analysis:\n"
      ],
      "id": "bff4cf58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "using MultivariateStats\n",
        "# only sample first 300 cases and four variables\n",
        "Xtr = diamonds[1:300 , [:x, :y, :z]]\n",
        "# with each observation in a column\n",
        "Xtr = Matrix(Xtr)' # somehow the data matrix has size of (d, n), which is the trasponse of data matrix in R \n",
        "# train a one-factor model\n",
        "M = fit(FactorAnalysis, Xtr; maxoutdim=1, method=:em)"
      ],
      "id": "99a13342",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can refer to this [doc](https://multivariatestatsjl.readthedocs.io/en/latest/fa.html) for more details for parameter estimation of factor analysis\n"
      ],
      "id": "76b4665f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "loadings(M)"
      ],
      "id": "3913f460",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's quickly compare the results of `lavaan`\n",
        "\n",
        "``` r\n",
        "library(ggplot2)\n",
        "library(lavaan)\n",
        "data('diamonds')\n",
        "X = diamonds[1:300, c('x', 'y', 'z')]\n",
        "fa_model = \"\n",
        "F1 =~ x + y + z\n",
        "\"\n",
        "fit = cfa(fa_model, data = X, std.lv = TRUE)\n",
        "coef(fit)[1:3] # factor loading\n",
        "    F1=~x     F1=~y     F1=~z \n",
        "0.7802245 0.7673664 0.4752576 \n",
        "```\n",
        "\n",
        "## Some Popular Julia Package\n",
        "\n",
        "### Flux\n",
        "\n",
        "Flux is an elegant approach to machine learning. It's a 100% pure-Julia stack, and provides lightweight abstractions on top of Julia's native GPU and AD support. Flux makes the easy things easy while remaining fully hackable. See more details in [Juliahub](https://juliahub.com/ui/Packages/General/Flux).\n"
      ],
      "id": "dc686197"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "using Flux, Plots\n",
        "data = [([x], 2x-x^3) for x in -2:0.1f0:2]\n",
        "\n",
        "model = Chain(Dense(1 => 23, tanh), Dense(23 => 1, bias=false), only)\n",
        "\n",
        "optim = Flux.setup(Adam(), model)\n",
        "for epoch in 1:1000\n",
        "  Flux.train!((m,x,y) -> (m(x) - y)^2, model, data, optim)\n",
        "end\n",
        "\n",
        "plot(x -> 2x-x^3, -2, 2, legend=false)\n",
        "scatter!(x -> model([x]), -2:0.1f0:2)"
      ],
      "id": "1231ee95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| code-fold: true\n",
        "# This will prompt if neccessary to install everything, including CUDA:\n",
        "using Flux, CUDA, Statistics, ProgressMeter\n",
        "\n",
        "# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\n",
        "noisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}\n",
        "truth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}\n",
        "\n",
        "# Define our model, a multi-layer perceptron with one hidden layer of size 3:\n",
        "model = Chain(\n",
        "    Dense(2 => 3, tanh),   # activation function inside layer\n",
        "    BatchNorm(3),\n",
        "    Dense(3 => 2),\n",
        "    softmax) |> gpu        # move model to GPU, if available\n",
        "\n",
        "# The model encapsulates parameters, randomly initialised. Its initial output is:\n",
        "out1 = model(noisy |> gpu) |> cpu                                 # 2×1000 Matrix{Float32}\n",
        "\n",
        "# To train the model, we use batches of 64 samples, and one-hot encoding:\n",
        "target = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix\n",
        "loader = Flux.DataLoader((noisy, target) |> gpu, batchsize=64, shuffle=true);\n",
        "# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)\n",
        "\n",
        "optim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.\n",
        "\n",
        "# Training loop, using the whole data set 1000 times:\n",
        "losses = []\n",
        "@showprogress for epoch in 1:1_000\n",
        "    for (x, y) in loader\n",
        "        loss, grads = Flux.withgradient(model) do m\n",
        "            # Evaluate model and loss inside gradient context:\n",
        "            y_hat = m(x)\n",
        "            Flux.crossentropy(y_hat, y)\n",
        "        end\n",
        "        Flux.update!(optim, model, grads[1])\n",
        "        push!(losses, loss)  # logging, outside gradient context\n",
        "    end\n",
        "end\n",
        "\n",
        "optim # parameters, momenta and output have all changed\n",
        "out2 = model(noisy |> gpu) |> cpu  # first row is prob. of true, second row p(false)\n",
        "\n",
        "mean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!"
      ],
      "id": "f839d84d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "using Plots  # to draw the above figure\n",
        "\n",
        "p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\n",
        "p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\n",
        "p_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title=\"Trained network\", legend=false)\n",
        "\n",
        "plot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))"
      ],
      "id": "4043454f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "plot(losses; xaxis=(:log10, \"iteration\"),\n",
        "    yaxis=\"loss\", label=\"per batch\")\n",
        "n = length(loader)\n",
        "plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
        "    label=\"epoch mean\", dpi=200)"
      ],
      "id": "1952d7ad",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.10",
      "language": "julia",
      "display_name": "Julia 1.10.2",
      "path": "/Users/jihong/Library/Jupyter/kernels/julia-1.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}