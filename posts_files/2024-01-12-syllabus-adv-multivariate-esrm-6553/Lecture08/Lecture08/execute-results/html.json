{
  "hash": "d2141a8297a3ef6ad954fcc9bc810d1d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 08\"\nsubtitle: \"Generalized Measurement Models: Modeling Observed Data II\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\nexecute: \n  echo: false\n  eval: false\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: false  # choose \"false \"if want to show all together\n    transition: slide\n    background-transition: fade\n    theme: [simple, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    scrollable: true\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-annotations: below\n    code-copy: true\n    code-summary: ''\n    highlight-style: arrow\n    view: 'scroll' # Activate the scroll view\n    scrollProgress: true # Force the scrollbar to remain visible\n    mermaid:\n      theme: neutral\n\n#bibliography: references.bib\n---\n\n\n\n\n# Review Previous Class\n\n## In Previous Class...\n\n1.  We introduced a self-reported survey data, called `Conspiracy Theories`\n\n-   The scale has 10 items with 5-point Likert scale response\n-   Items have varied item difficult and Positive skewed item response distribution\n\n2.  We talked about using if factor analysis is a proper method when normality assumption is violated\n3.  We checked the item characteristics curves\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture08_files/figure-revealjs/unnamed-chunk-1-1.png){width=1728}\n:::\n:::\n\n\n\n\n## Today's Lecture Objectives\n\n1.  Dive deep into factor scoring\n2.  Show how different initial values affect Bayesian model estimation\n3.  Show how parameterization differs for standardized latent variables vs. marker item scale identification\n\n## Posterior Distribution for Item Parameters\n\nBefore moving onto the latent variable, let's note the posterior distribution of item parameters (for a single item):\n\n$$\nf(\\mu_i,\\lambda_i,\\psi_i\\mid\\boldsymbol{Y})\\propto f(\\boldsymbol{Y}\\mid\\mu_i,\\lambda_i,\\psi_i)f(\\mu_i, \\lambda_i,\\psi_i)\n$$\n\nwhere $f(\\boldsymbol{Y}\\mid\\mu_i,\\lambda_i,\\psi_i)$ is the (joint) posterior distribution of the parameters for item $i$ conditional on the adta\n\n$f(\\boldsymbol{Y}\\mid\\mu_i,\\lambda_i,\\psi_i)$ is the distribution we defined for our observed data:\n\n$$\nf(\\boldsymbol{Y}\\mid\\mu_i,\\lambda_i,\\psi_i)\\sim N(\\mu_i+\\lambda_i\\theta_p, \\psi_i)\n$$\n\n$f(\\mu_i, \\lambda_i, \\psi_i)$ is the (joint) prior distribution for each of the parameters, which, are independent:\n\n$$\nf(\\mu_i, \\lambda_i, \\psi_i) = f(\\mu_i)f(\\lambda_i)f(\\psi_i)\n$$\n\n## Investigating the Latent Variables\n\nThe estimated latent variables are then:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 177 Ã— 10\n   variable     mean  median    sd   mad     q5     q95  rhat ess_bulk ess_tail\n   <chr>       <dbl>   <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n 1 theta[1]   0.0221  0.0222 0.246 0.250 -0.386  0.419   1.00    5122.    5185.\n 2 theta[2]   1.53    1.53   0.257 0.254  1.11   1.96    1.00    3754.    4698.\n 3 theta[3]   1.71    1.71   0.264 0.263  1.27   2.14    1.00    3809.    4720.\n 4 theta[4]  -0.927  -0.924  0.250 0.246 -1.34  -0.517   1.00    4936.    4752.\n 5 theta[5]   0.0443  0.0485 0.248 0.244 -0.368  0.449   1.00    5126.    4699.\n 6 theta[6]  -0.976  -0.972  0.252 0.249 -1.40  -0.566   1.00    3786.    4650.\n 7 theta[7]  -0.336  -0.338  0.254 0.249 -0.754  0.0846  1.00    4421.    4882.\n 8 theta[8]  -0.0520 -0.0548 0.248 0.247 -0.460  0.349   1.00    5037.    5503.\n 9 theta[9]  -0.785  -0.784  0.244 0.248 -1.19  -0.388   1.00    4633.    5479.\n10 theta[10]  0.0477  0.0460 0.248 0.250 -0.357  0.455   1.00    5000.    4896.\n# â„¹ 167 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## EAP Estimates of Latent Variables\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_Estimates.png)\n\n## Density of EAP Estimates\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_Estimates_density.png)\n\n## Density of 500 Posterior draws of Î¸\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/Theta_Draws_density.png)\n\n## Comparing Posterior Distribution for Individuals\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/Theta_density_ThreeIndividuals.png)\n\n## Comparing EAP Estimate with Posterior SD\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_SD.png)\n\n## Comparing EAP Estimate with Sum Score\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_SumScore.png)\n\n## Comparing EAP Estimate with Factor Score by `lavaan`\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_FactorScore.png)\n\n## Posterior Distribution for Person Parameter Î¸\n\nThe posterior distribution of the person parameters (the latent variable for a single person):\n\n$$\nf(\\theta_p\\mid\\boldsymbol{Y}) \\propto f(\\boldsymbol{Y} \\mid \\theta_p)f(\\theta_p)\n$$\n\nHere:\n\n-   $f(\\theta_p\\mid\\boldsymbol{Y})$ is the posterior distribution of the latent variable conditional on the observed data;\n\n-   $f(\\boldsymbol{Y} \\mid \\theta_p)$ is the model (data) likelihood:\n\n    $$\n    f(\\boldsymbol{Y} \\mid \\theta_p) = \\prod_{i=1}^{I} f(Y_i\\mid\\theta_p)\n    $$\n\n    -   $f(Y_i \\mid \\theta_p)$ is one individual item's data likelihood: $f(Y_i \\mid \\theta_p) \\sim N(\\mu_i+\\lambda_i\\theta_p, \\psi_i)$;\n\n-   $f(\\theta_p) \\sim N(0,1)$ is the prior distribution for the latent variable â€“ $\\theta_p$\n\n# Measurement Model Estimation Fails\n\n## Recall: Stan's `parameters {}` Block\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nparameters {\n  vector[nObs] theta;                // the latent variables (one for each person)\n  vector[nItems] mu;                 // the item intercepts (one for each item)\n  vector[nItems] lambda;             // the factor loadings/item discriminations (one for each item)\n  vector<lower=0>[nItems] psi;       // the unique standard deviations (one for each item)   \n}\n```\n:::\n\n\n\n\nHere, the parameterization of $\\lambda$ (factor loadings / discrimination parameters) can lead to problems in estimation\n\n-   The issue is $\\lambda_i\\theta_p=(-\\lambda_i)(-\\theta_p)$\n\n    -   Depending on the random starting values of each of these parameters (per Markov chain), a given chain may converge to different region with others\n\n-   To demonstrate, we will start with different random number seend\n\n    -   Currently use 09102022: works fine\n\n    -   Change to 25102022: big problem\n\n## New Samples Syntax\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|3\"}\nmodelCFA_samplesFail = modelCFA_stan$sample(\n  data = modelCFA_data,\n  seed = 25102022,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 2000\n)\n```\n:::\n\n\n\n\nConvergence fail with maximum of $\\hat R$ as:\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.73562\n```\n\n\n:::\n:::\n\n\n\n\n## Why Convergence Failed\n\n-   The issue of exchangeable likelihood: $\\lambda_i\\theta_p=(-\\lambda_i)(-\\theta_p)$\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 10\n   variable     mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail\n   <chr>       <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>\n 1 lambda[1]  -0.001  0.01  0.743 1.09  -0.843 0.841  1.73     6.08     106.\n 2 lambda[2]  -0.002 -0.004 0.873 1.29  -0.972 0.969  1.73     6.10     121.\n 3 lambda[3]  -0.002  0.007 0.805 1.19  -0.901 0.899  1.73     6.08     113.\n 4 lambda[4]  -0.001 -0.012 0.846 1.25  -0.945 0.941  1.73     6.09     113.\n 5 lambda[5]  -0.001 -0.002 0.999 1.47  -1.09  1.09   1.73     6.13     122.\n 6 lambda[6]  -0.001  0.008 0.9   1.33  -0.983 0.98   1.73     6.14     103.\n 7 lambda[7]  -0.002 -0.002 0.767 1.13  -0.855 0.85   1.73     6.08     118.\n 8 lambda[8]  -0.001 -0.002 0.855 1.26  -0.931 0.932  1.73     6.09     108.\n 9 lambda[9]  -0.002  0.029 0.863 1.27  -0.963 0.962  1.73     6.12     101.\n10 lambda[10] -0.001 -0.003 0.677 0.993 -0.775 0.774  1.73     6.08     105.\n```\n\n\n:::\n:::\n\n\n\n\n## Posterior Trace Plots of Î»\n\nUnfortunately, we are unable to extract initial values that were generated automatically by `cmdstanr`. See [here](https://mc-stan.org/cmdstanr/reference/fit-method-init.html) for more details.\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/Posterior_lambda.png){fig-align=\"center\"}\n\n## Posterior Density Plots of Î»\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/Posterior_lambda_density.png){fig-align=\"center\"}\n\n## Examining Latent Variable\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 177 Ã— 10\n    variable     mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail\n    <chr>       <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>\n  1 theta[1]   -0.002  0.001 0.247 0.245 -0.414 0.405  1.00  3167.     4819. \n  2 theta[2]   -0.001  0.004 1.56  2.27  -1.87  1.87   1.73     6.08    104. \n  3 theta[3]    0      0.029 1.73  2.53  -2.06  2.05   1.73     6.07    105. \n  4 theta[4]   -0.008  0.018 0.962 1.37  -1.26  1.24   1.73     6.07    130. \n  5 theta[5]    0.001  0     0.247 0.249 -0.403 0.410  1.02   177.     4690. \n  6 theta[6]   -0.004  0.05  1.01  1.45  -1.30  1.30   1.73     6.07    106. \n  7 theta[7]   -0.005 -0.002 0.422 0.503 -0.672 0.656  1.60     6.65    104. \n  8 theta[8]   -0.003 -0.001 0.254 0.248 -0.427 0.417  1.03   112.     3799. \n  9 theta[9]   -0.009  0.015 0.825 1.16  -1.11  1.09   1.73     6.06    113. \n 10 theta[10]  -0.004 -0.004 0.252 0.251 -0.416 0.418  1.02   168.     3352. \n 11 theta[11]  -0.006 -0.055 1.01  1.45  -1.30  1.29   1.73     6.07     99.4\n 12 theta[12]  -0.004 -0.006 0.356 0.395 -0.571 0.578  1.46     7.62    105. \n 13 theta[13]  -0.007 -0.025 0.787 1.11  -1.07  1.06   1.73     6.06    106. \n 14 theta[14]  -0.006 -0.101 1.01  1.45  -1.32  1.29   1.73     6.06     99.0\n 15 theta[15]  -0.005  0.037 0.875 1.24  -1.16  1.14   1.73     6.06     97.9\n 16 theta[16]  -0.004 -0.005 0.259 0.259 -0.434 0.424  1.03    82.2    3490. \n 17 theta[17]  -0.004 -0.006 0.263 0.263 -0.438 0.430  1.03    88.1    4232. \n 18 theta[18]  -0.006 -0.017 0.710 0.985 -0.984 0.984  1.73     6.08    101. \n 19 theta[19]   0.001 -0.005 1.89  2.77  -2.22  2.23   1.73     6.07    116. \n 20 theta[20]  -0.007  0.072 0.962 1.37  -1.26  1.25   1.74     6.05    110. \n 21 theta[21]  -0.008  0     0.350 0.390 -0.571 0.548  1.45     7.81    104. \n 22 theta[22]  -0.005 -0.051 1.01  1.45  -1.30  1.30   1.73     6.08    109. \n 23 theta[23]  -0.003  0.032 1.01  1.45  -1.30  1.29   1.73     6.06    114. \n 24 theta[24]  -0.008 -0.027 1.01  1.43  -1.30  1.30   1.73     6.06    108. \n 25 theta[25]  -0.005 -0.008 0.602 0.807 -0.883 0.865  1.73     6.07    106. \n 26 theta[26]  -0.003 -0.009 0.779 1.09  -1.07  1.06   1.73     6.07    107. \n 27 theta[27]  -0.002 -0.007 0.276 0.275 -0.452 0.451  1.10    24.5     375. \n 28 theta[28]  -0.002  0.091 1.24  1.80  -1.55  1.55   1.73     6.07     99.7\n 29 theta[29]  -0.004 -0.032 0.928 1.32  -1.23  1.21   1.73     6.06    107. \n 30 theta[30]  -0.001 -0.057 1.70  2.49  -2.03  2.04   1.73     6.07    112. \n 31 theta[31]  -0.003 -0.004 0.602 0.803 -0.877 0.871  1.73     6.09    111. \n 32 theta[32]  -0.003 -0.005 0.487 0.619 -0.743 0.736  1.69     6.24    106. \n 33 theta[33]  -0.003 -0.003 0.412 0.486 -0.654 0.649  1.58     6.79    107. \n 34 theta[34]  -0.007 -0.068 1.01  1.45  -1.30  1.28   1.73     6.07    107. \n 35 theta[35]  -0.007 -0.034 0.710 0.981 -0.991 0.977  1.73     6.08    107. \n 36 theta[36]  -0.006  0.035 0.834 1.17  -1.13  1.10   1.73     6.07    103. \n 37 theta[37]  -0.005  0.031 1.01  1.45  -1.29  1.30   1.73     6.07    109. \n 38 theta[38]  -0.005 -0.002 0.324 0.348 -0.533 0.517  1.36     8.87    128. \n 39 theta[39]  -0.007 -0.072 1.01  1.44  -1.32  1.27   1.73     6.06    102. \n 40 theta[40]  -0.002 -0.003 0.249 0.250 -0.408 0.409  1.02   169.     3659. \n 41 theta[41]  -0.008  0.044 1.01  1.45  -1.30  1.28   1.73     6.08    115. \n 42 theta[42]  -0.005 -0.007 0.265 0.262 -0.437 0.427  1.06    39.6    1019. \n 43 theta[43]  -0.006 -0.014 0.686 0.951 -0.962 0.950  1.73     6.08    108. \n 44 theta[44]  -0.004 -0.003 0.258 0.254 -0.425 0.420  1.04    72.9    3059. \n 45 theta[45]  -0.003  0.001 0.455 0.566 -0.700 0.692  1.67     6.33    110. \n 46 theta[46]  -0.003 -0.007 0.361 0.406 -0.574 0.580  1.49     7.39    107. \n 47 theta[47]  -0.005 -0.027 0.564 0.749 -0.821 0.827  1.73     6.09    107. \n 48 theta[48]  -0.005 -0.021 0.869 1.23  -1.17  1.15   1.73     6.08    111. \n 49 theta[49]  -0.009 -0.033 1.01  1.44  -1.29  1.30   1.73     6.06    101. \n 50 theta[50]  -0.006 -0.008 0.304 0.308 -0.505 0.497  1.21    12.8     142. \n 51 theta[51]  -0.005  0.058 1.01  1.44  -1.31  1.29   1.73     6.07    124. \n 52 theta[52]  -0.006 -0.003 0.408 0.483 -0.654 0.640  1.58     6.75    115. \n 53 theta[53]  -0.008 -0.031 1.01  1.46  -1.31  1.29   1.73     6.06    109. \n 54 theta[54]  -0.001  0.02  1.24  1.80  -1.54  1.53   1.73     6.07    109. \n 55 theta[55]  -0.006 -0.001 1.01  1.45  -1.31  1.30   1.73     6.07    109. \n 56 theta[56]  -0.005  0.06  1.01  1.45  -1.30  1.29   1.73     6.06    121. \n 57 theta[57]  -0.006 -0.001 0.771 1.07  -1.05  1.04   1.73     6.07    108. \n 58 theta[58]  -0.006  0.027 1.01  1.45  -1.31  1.28   1.73     6.06    100. \n 59 theta[59]  -0.003 -0.004 0.263 0.265 -0.434 0.433  1.05    56.3     974. \n 60 theta[60]  -0.005 -0.008 0.686 0.949 -0.958 0.955  1.73     6.08    109. \n 61 theta[61]  -0.002 -0.006 1.41  2.05  -1.71  1.71   1.73     6.07     99.2\n 62 theta[62]  -0.004 -0.012 0.407 0.482 -0.633 0.642  1.60     6.67    111. \n 63 theta[63]  -0.006  0.012 1.01  1.45  -1.32  1.29   1.73     6.07    112. \n 64 theta[64]  -0.004  0.039 1.40  2.04  -1.71  1.70   1.73     6.07    118. \n 65 theta[65]   0     -0.025 1.24  1.80  -1.53  1.54   1.73     6.07    122. \n 66 theta[66]  -0.006 -0.011 0.788 1.10  -1.08  1.06   1.73     6.06    109. \n 67 theta[67]  -0.005 -0.011 0.604 0.805 -0.881 0.873  1.73     6.09    108. \n 68 theta[68]  -0.005  0.01  0.896 1.26  -1.19  1.18   1.73     6.07    114. \n 69 theta[69]  -0.004 -0.009 0.588 0.792 -0.867 0.846  1.73     6.10    101. \n 70 theta[70]  -0.004 -0.041 0.923 1.31  -1.21  1.21   1.73     6.08    114. \n 71 theta[71]  -0.009 -0.008 0.252 0.250 -0.431 0.404  1.04    69.1    2621. \n 72 theta[72]  -0.004 -0.055 1.08  1.55  -1.39  1.40   1.73     6.07    114. \n 73 theta[73]  -0.004  0     0.247 0.250 -0.411 0.401  1.00  3708.     4860. \n 74 theta[74]  -0.005  0.044 1.01  1.45  -1.31  1.29   1.73     6.07    105. \n 75 theta[75]  -0.006 -0.099 1.01  1.44  -1.31  1.30   1.73     6.06    109. \n 76 theta[76]   0.003 -0.065 3.27  4.82  -3.64  3.64   1.73     6.08    112. \n 77 theta[77]  -0.002 -0.032 0.826 1.16  -1.11  1.10   1.73     6.07    108. \n 78 theta[78]  -0.001 -0.065 1.18  1.70  -1.48  1.48   1.73     6.07     97.5\n 79 theta[79]  -0.004  0.059 1.89  2.77  -2.23  2.21   1.73     6.08    105. \n 80 theta[80]  -0.003 -0.016 0.666 0.909 -0.944 0.943  1.73     6.08    114. \n 81 theta[81]  -0.006 -0.005 0.253 0.254 -0.426 0.404  1.02   146.     3265. \n 82 theta[82]  -0.004 -0.001 0.249 0.250 -0.411 0.402  1.01  2604.     4613. \n 83 theta[83]  -0.004  0.085 1.01  1.45  -1.31  1.29   1.73     6.06    103. \n 84 theta[84]  -0.001 -0.062 1.60  2.33  -1.92  1.92   1.73     6.07     98.9\n 85 theta[85]  -0.006 -0.007 0.327 0.344 -0.538 0.517  1.33     9.36    125. \n 86 theta[86]  -0.008 -0.021 1.01  1.44  -1.30  1.29   1.74     6.06    107. \n 87 theta[87]  -0.005 -0.006 0.308 0.319 -0.508 0.503  1.23    12.4     164. \n 88 theta[88]  -0.004  0     0.405 0.475 -0.637 0.645  1.57     6.80    103. \n 89 theta[89]  -0.009 -0.054 1.01  1.45  -1.32  1.29   1.73     6.07    115. \n 90 theta[90]  -0.005  0.045 0.880 1.24  -1.18  1.16   1.73     6.07    116. \n 91 theta[91]  -0.004  0.034 0.872 1.24  -1.16  1.14   1.73     6.06    106. \n 92 theta[92]  -0.002 -0.04  1.09  1.56  -1.39  1.39   1.73     6.05    106. \n 93 theta[93]  -0.007 -0.012 0.247 0.249 -0.412 0.399  1.00  3345.     4727. \n 94 theta[94]  -0.003  0.073 2.61  3.84  -2.96  2.96   1.73     6.09    115. \n 95 theta[95]  -0.003 -0.231 1.68  2.46  -2.00  2.00   1.73     6.09    112. \n 96 theta[96]  -0.003 -0.005 0.408 0.484 -0.639 0.636  1.60     6.67    121. \n 97 theta[97]  -0.001 -0.007 0.364 0.407 -0.581 0.592  1.48     7.44    100. \n 98 theta[98]  -0.007  0.089 1.01  1.44  -1.30  1.30   1.73     6.06    124. \n 99 theta[99]  -0.001  0.08  1.24  1.79  -1.54  1.55   1.73     6.07    110. \n100 theta[100] -0.003  0.129 0.958 1.37  -1.25  1.24   1.73     6.07    109. \n101 theta[101] -0.006 -0.011 0.751 1.05  -1.03  1.02   1.73     6.06    104. \n102 theta[102]  0.001  0.033 2.83  4.17  -3.18  3.19   1.73     6.08     96.6\n103 theta[103] -0.005  0.01  0.709 0.981 -0.988 0.968  1.73     6.09    101. \n104 theta[104] -0.004 -0.037 0.900 1.27  -1.20  1.20   1.73     6.07    110. \n105 theta[105] -0.006 -0.007 0.266 0.261 -0.436 0.434  1.06    40.5    2806. \n106 theta[106] -0.003  0.035 1.16  1.67  -1.46  1.46   1.73     6.07    108. \n107 theta[107] -0.005  0.021 1.60  2.34  -1.91  1.91   1.73     6.07    106. \n108 theta[108] -0.001 -0.016 0.879 1.24  -1.17  1.17   1.73     6.09    106. \n109 theta[109] -0.003 -0.197 1.07  1.54  -1.36  1.36   1.73     6.06    112. \n110 theta[110] -0.006 -0.004 0.255 0.255 -0.426 0.408  1.02   179.     3672. \n111 theta[111] -0.004 -0.008 0.276 0.276 -0.460 0.449  1.12    20.8     196. \n112 theta[112] -0.006 -0.028 0.685 0.945 -0.962 0.951  1.73     6.07     96.1\n113 theta[113] -0.002 -0.011 0.449 0.557 -0.682 0.693  1.66     6.39    104. \n114 theta[114] -0.004 -0.001 0.403 0.476 -0.645 0.630  1.57     6.80    114. \n115 theta[115] -0.004  0.013 0.903 1.27  -1.21  1.20   1.73     6.09    110. \n116 theta[116] -0.002  0.001 0.297 0.306 -0.494 0.480  1.24    12.0     115. \n117 theta[117] -0.002 -0.006 0.254 0.255 -0.418 0.416  1.04    71.7    3080. \n118 theta[118] -0.006 -0.007 0.409 0.482 -0.651 0.637  1.60     6.67    106. \n119 theta[119] -0.007 -0.038 0.873 1.23  -1.17  1.16   1.74     6.07    117. \n120 theta[120] -0.006 -0.059 1.01  1.45  -1.31  1.28   1.73     6.07    113. \n121 theta[121] -0.005 -0.002 0.323 0.333 -0.536 0.521  1.31     9.80    120. \n122 theta[122] -0.003 -0.005 0.683 0.941 -0.952 0.952  1.73     6.06    108. \n123 theta[123] -0.002  0.031 1.24  1.80  -1.54  1.53   1.73     6.08    120. \n124 theta[124] -0.002  0.003 1.07  1.53  -1.36  1.35   1.73     6.06    108. \n125 theta[125] -0.006 -0.009 1.01  1.44  -1.30  1.29   1.73     6.06    103. \n126 theta[126] -0.009  0.004 0.687 0.945 -0.966 0.949  1.73     6.08    101. \n127 theta[127] -0.003 -0.031 1.16  1.67  -1.46  1.46   1.73     6.08    114. \n128 theta[128] -0.008  0.037 0.883 1.25  -1.17  1.16   1.74     6.06    103. \n129 theta[129] -0.009  0.003 0.706 0.981 -0.986 0.964  1.73     6.06    107. \n130 theta[130]  0      0.033 1.88  2.75  -2.22  2.21   1.73     6.07    107. \n131 theta[131] -0.004  0.002 0.419 0.507 -0.660 0.651  1.62     6.58    113. \n132 theta[132]  0     -0.095 1.68  2.46  -2.01  1.99   1.73     6.08    110. \n133 theta[133] -0.008 -0.135 1.01  1.45  -1.32  1.30   1.73     6.06    103. \n134 theta[134]  0.001 -0.084 2.61  3.84  -2.95  2.96   1.73     6.08    118. \n135 theta[135] -0.005  0.009 0.826 1.17  -1.11  1.11   1.73     6.07    113. \n136 theta[136] -0.007  0.037 1.01  1.45  -1.30  1.30   1.73     6.06    109. \n137 theta[137] -0.002  0.012 0.816 1.15  -1.10  1.10   1.73     6.06    108. \n138 theta[138] -0.003  0.038 0.826 1.17  -1.11  1.11   1.74     6.05    108. \n139 theta[139] -0.004 -0.004 0.256 0.253 -0.425 0.411  1.02   166.     4446. \n140 theta[140] -0.004 -0.002 0.249 0.246 -0.416 0.402  1.03   111.     3771. \n141 theta[141] -0.004  0.004 0.872 1.24  -1.17  1.15   1.73     6.07    104. \n142 theta[142] -0.007 -0.107 1.01  1.45  -1.31  1.29   1.73     6.06    103. \n143 theta[143] -0.005  0.023 1.01  1.44  -1.29  1.31   1.73     6.07    113. \n144 theta[144] -0.001  0.035 0.855 1.21  -1.14  1.13   1.73     6.06    104. \n145 theta[145] -0.008  0.009 1.01  1.44  -1.31  1.29   1.73     6.07    109. \n146 theta[146] -0.004  0.007 0.663 0.910 -0.932 0.930  1.73     6.07    120. \n147 theta[147]  0.001 -0.001 0.894 1.27  -1.19  1.18   1.73     6.07    103. \n148 theta[148] -0.002 -0.005 0.279 0.280 -0.455 0.460  1.13    19.9     279. \n149 theta[149] -0.006 -0.05  1.01  1.44  -1.30  1.31   1.73     6.07    108. \n150 theta[150] -0.009 -0.135 1.01  1.46  -1.30  1.28   1.73     6.07    102. \n151 theta[151] -0.006 -0.029 0.689 0.947 -0.969 0.957  1.73     6.07    105. \n152 theta[152] -0.005 -0.024 1.01  1.44  -1.31  1.30   1.73     6.07    100. \n153 theta[153] -0.001  0.011 1.55  2.26  -1.87  1.87   1.73     6.07    116. \n154 theta[154] -0.007 -0.019 0.960 1.37  -1.25  1.24   1.73     6.07    114. \n155 theta[155] -0.008  0.114 1.01  1.45  -1.29  1.29   1.73     6.07    100. \n156 theta[156] -0.006 -0.071 0.870 1.24  -1.16  1.15   1.73     6.07    102. \n157 theta[157] -0.008 -0.011 0.869 1.24  -1.16  1.14   1.73     6.07    114. \n158 theta[158] -0.005 -0.004 0.248 0.248 -0.412 0.403  1.00  4189.     4432. \n159 theta[159] -0.004  0     0.664 0.916 -0.940 0.934  1.73     6.07    101. \n160 theta[160] -0.008 -0.003 1.01  1.45  -1.30  1.29   1.73     6.08    104. \n161 theta[161] -0.002 -0.087 1.33  1.92  -1.63  1.62   1.73     6.07    118. \n162 theta[162] -0.003 -0.005 0.307 0.314 -0.500 0.509  1.24    11.8     133. \n163 theta[163] -0.007 -0.068 1.01  1.45  -1.30  1.29   1.73     6.08    105. \n164 theta[164] -0.005 -0.01  0.595 0.798 -0.864 0.869  1.73     6.08    102. \n165 theta[165] -0.004  0     0.308 0.316 -0.517 0.491  1.21    12.9     142. \n166 theta[166] -0.007 -0.005 1.01  1.45  -1.32  1.29   1.73     6.07    120. \n167 theta[167] -0.004 -0.005 0.410 0.484 -0.651 0.639  1.58     6.77    105. \n168 theta[168] -0.001 -0.069 1.40  2.04  -1.71  1.73   1.73     6.07    112. \n169 theta[169] -0.005 -0.008 0.428 0.515 -0.674 0.655  1.64     6.48    100. \n170 theta[170] -0.006 -0.005 0.273 0.271 -0.459 0.445  1.13    20.3     244. \n171 theta[171] -0.007 -0.005 0.789 1.11  -1.09  1.06   1.73     6.09    102. \n172 theta[172] -0.003 -0.007 0.318 0.333 -0.516 0.525  1.32     9.58    128. \n173 theta[173] -0.003  0.016 0.663 0.911 -0.941 0.930  1.73     6.07    112. \n174 theta[174] -0.004  0.027 0.747 1.04  -1.04  1.02   1.73     6.05     98.2\n175 theta[175] -0.004 -0.009 0.707 0.982 -0.980 0.967  1.73     6.09     99.9\n176 theta[176] -0.007 -0.012 0.827 1.17  -1.11  1.11   1.73     6.07    104. \n177 theta[177] -0.005 -0.006 0.501 0.642 -0.760 0.753  1.70     6.17    106. \n```\n\n\n:::\n:::\n\n\n\n\n## Posterior Trace Plots of Î¸\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/Posterior_theta_traceplot.png){fig-align=\"center\"}\n\n## Posterior Density Plots of Î¸\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/Posterior_theta_density.png)\n\n## Fixing Convergence\n\n`Stan` allows starting values to be set via `cmdstanr`\n\n-   Documentation is very lacking, I can show you one method but it may change in the future.\n\nAlternatively:\n\n-   In `Stan` file, restrict $\\lambda$ to be positive: `vector<lower=0>[nItems] lambda;`\n\n-   Can also choose prior that has strictly positive range (like log-normal distribution)\n\n-   Note: the restriction on the space of $\\lambda$ will not permit truely negative values\n\n    -   Not ideals as negative $\\lambda$ values are informative as a problem with data\n\n\n\n\n    ::: {.cell output.var='display'}\n    \n    ```{.stan .cell-code}\n    parameters {\n      vector[nObs] theta;                // the latent variables (one for each person)\n      vector[nItems] mu;                 // the item intercepts (one for each item)\n      vector<lower=0>[nItems] lambda;    // the factor loadings/item discriminations (one for each item)\n      vector<lower=0>[nItems] psi;       // the unique standard deviations (one for each item)   \n    }\n    ```\n    :::\n\n\n\n\n## Setting Starting (Inital) Values in `Stan`\n\nStarting values (initial values) are the first values used when an MCMC chain starts\n\n-   In `Stan`, by default, parameters are randomly started between -2 and 2\n\n    -   Bounded parameters are transformed so they are unbounded in the algorithm\n\n-   What we need:\n\n    -   Randomly start all $\\lambda$ parameters so that they converge to the $\\lambda_i\\theta_p$ mode\n\n    -   As opposed to the (-$\\lambda_i$)(-$\\theta_p$) mode\n\n## `cmdstanr` Syntax for Initial Values\n\nAdd the init option to the `$sample()` function of the `cmdstanr` object:\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\n# set starting values for some of the parameters\nmodelCFA_samples2fixed = modelCFA_stan$sample(\n  data = modelCFA_data,\n  seed = 25102022,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 2000,\n  iter_sampling = 2000, \n  init = function() list(lambda=rnorm(nItems, mean=10, sd=2))\n)\n```\n:::\n\n\n\n\nThe `init` option can be specified as a function, here, randomly starting each $\\lambda$ following a normal distribution\n\n## Initialization Process\n\nSee the lecture R syntax for information on how to confirm starting values are set.\n\nYou should find the initial values using `$init()` function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelCFA_samplesFix$init()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$lambda\n [1]  7.505537  8.652382 13.870335 11.813123 11.473054 10.293251 10.566242\n [8] 11.178553 10.431699 10.310106\n\n\n[[2]]\n[[2]]$lambda\n [1] 11.907653  9.668295 10.510007  8.205055 13.167940 12.785517  6.375484\n [8]  9.567361 11.099712 10.965472\n\n\n[[3]]\n[[3]]$lambda\n [1] 11.521697  9.108684  7.589333 10.602933  6.921710 11.270741 11.405904\n [8]  6.188234 11.877843  9.551016\n\n\n[[4]]\n[[4]]$lambda\n [1]  8.652366 10.891575 12.561234 13.130260  7.598908  9.124610 10.292763\n [8] 10.132038  9.109928  5.315775\n```\n\n\n:::\n:::\n\n\n\n\n## Final Results: Parameters\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check convergence\nmax(modelCFA_samplesFix$summary()$'rhat')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.006357\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(modelCFA_samplesFix$summary(c(\"mu\", \"lambda\", \"psi\"),.cores = 4) |> mutate(across(c(mean, median), \\(x) round(x, 3))), n = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 Ã— 10\n   variable    mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n 1 mu[1]      2.37   2.37  0.0866 0.0851 2.22  2.51   1.00    1317.    3146.\n 2 mu[2]      1.95   1.95  0.0857 0.0846 1.81  2.09   1.01     908.    2329.\n 3 mu[3]      1.87   1.87  0.0839 0.0848 1.74  2.01   1.00    1111.    3011.\n 4 mu[4]      2.01   2.01  0.0847 0.0844 1.87  2.15   1.00    1040.    2582.\n 5 mu[5]      1.98   1.98  0.0859 0.0868 1.84  2.12   1.01     790.    2080.\n 6 mu[6]      1.89   1.89  0.0766 0.0768 1.77  2.02   1.01     750.    2168.\n 7 mu[7]      1.72   1.72  0.0767 0.0740 1.59  1.85   1.01    1000.    2708.\n 8 mu[8]      1.84   1.84  0.0723 0.0719 1.72  1.96   1.01     775.    2012.\n 9 mu[9]      1.80   1.81  0.0863 0.0854 1.66  1.95   1.00    1010.    2552.\n10 mu[10]     1.52   1.52  0.0807 0.0805 1.39  1.65   1.00    1428.    3074.\n11 lambda[1]  0.738  0.737 0.0823 0.0817 0.606 0.877  1.00    3138.    5274.\n12 lambda[2]  0.87   0.867 0.0743 0.0743 0.752 0.993  1.00    1903.    3675.\n13 lambda[3]  0.802  0.8   0.0777 0.0765 0.678 0.934  1.00    2452.    4289.\n14 lambda[4]  0.843  0.839 0.0765 0.0750 0.723 0.971  1.00    2267.    4160.\n15 lambda[5]  0.997  0.995 0.0700 0.0687 0.886 1.12   1.00    1393.    3095.\n16 lambda[6]  0.899  0.896 0.0637 0.0646 0.800 1.01   1.00    1439.    3092.\n17 lambda[7]  0.765  0.764 0.0689 0.0701 0.654 0.879  1.00    2213.    4109.\n18 lambda[8]  0.853  0.851 0.0611 0.0612 0.755 0.957  1.00    1443.    2821.\n19 lambda[9]  0.861  0.858 0.0784 0.0785 0.736 0.996  1.00    2267.    4043.\n20 lambda[10] 0.673  0.672 0.0773 0.0764 0.549 0.800  1.00    3313.    4230.\n21 psi[1]     0.891  0.889 0.0494 0.0491 0.814 0.976  1.00   15504.    6185.\n22 psi[2]     0.735  0.733 0.0433 0.0436 0.667 0.809  1.00   11583.    5925.\n23 psi[3]     0.782  0.78  0.0443 0.0442 0.712 0.858  1.00   13289.    6547.\n24 psi[4]     0.757  0.755 0.0445 0.0443 0.687 0.833  1.00   12802.    6653.\n25 psi[5]     0.545  0.544 0.0366 0.0369 0.486 0.607  1.00    7541.    6505.\n26 psi[6]     0.505  0.503 0.0336 0.0334 0.452 0.563  1.00    9362.    6500.\n27 psi[7]     0.686  0.685 0.0404 0.0401 0.623 0.755  1.00   14624.    6491.\n28 psi[8]     0.48   0.479 0.0318 0.0315 0.430 0.534  1.00    9793.    7087.\n29 psi[9]     0.781  0.779 0.0458 0.0455 0.708 0.858  1.00   11809.    7030.\n30 psi[10]    0.839  0.837 0.0467 0.0450 0.766 0.920  1.00   15303.    5988.\n```\n\n\n:::\n:::\n\n\n\n\n## Comparing Results with different inits\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_mu_lambda_psi_theta.png){fig-align=\"center\"}\n\nCorrelation of all parameters across both algorithm runs:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(modelCFA_samplesFix$summary(variables = c(\"mu\", \"lambda\", \"psi\", \"theta\"), .cores = 4)$mean,\n    modelCFA_samples$summary(variables = c(\"mu\", \"lambda\", \"psi\", \"theta\"), .cores = 4)$mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9999963\n```\n\n\n:::\n:::\n\n\n\n\n## CFA model with `blavaan`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblavaan.model <- ' theta  =~ item1 + item2 + item3 + item4 + item5 + item6 + item7 + item8 + item9 + item10 '\nmodel01_blv <- bcfa(blavaan.model, data=conspiracyItems,\n            n.chains = 4, burnin = 1000, sample = 2000,\n            target = 'stan', seed = 09102022,\n            save.lvs = TRUE, # save sampled latent variable\n            std.lv = TRUE,\n            bcontrol = list(cores = 4),\n            mcmcfile=TRUE # save Stan file\n            ) # standardized latent variable\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### `blavaan` Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model01_blv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nblavaan 0.5.3 ended normally after 2000 iterations\n\n  Estimator                                      BAYES\n  Optimization method                             MCMC\n  Number of model parameters                        20\n\n  Number of observations                           177\n\n  Statistic                                 MargLogLik         PPP\n  Value                                      -2153.697       0.000\n\nParameter Estimates:\n\n\nLatent Variables:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n  theta =~                                                                     \n    item1             0.738    0.082    0.581    0.902    1.000    normal(0,10)\n    item2             0.869    0.077    0.726    1.027    1.001    normal(0,10)\n    item3             0.802    0.076    0.659    0.955    1.001    normal(0,10)\n    item4             0.842    0.078    0.696    1.002    1.001    normal(0,10)\n    item5             0.997    0.071    0.867    1.144    1.001    normal(0,10)\n    item6             0.899    0.064    0.780    1.033    1.001    normal(0,10)\n    item7             0.764    0.069    0.634    0.911    1.001    normal(0,10)\n    item8             0.852    0.061    0.739    0.976    1.001    normal(0,10)\n    item9             0.859    0.078    0.709    1.018    1.001    normal(0,10)\n    item10            0.673    0.076    0.528    0.826    1.000    normal(0,10)\n\nVariances:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n   .item1             0.791    0.088    0.636    0.976    1.000 gamma(1,.5)[sd]\n   .item2             0.538    0.064    0.426    0.677    1.000 gamma(1,.5)[sd]\n   .item3             0.608    0.071    0.481    0.760    1.000 gamma(1,.5)[sd]\n   .item4             0.571    0.068    0.454    0.716    1.000 gamma(1,.5)[sd]\n   .item5             0.296    0.040    0.224    0.385    1.000 gamma(1,.5)[sd]\n   .item6             0.254    0.034    0.193    0.327    1.000 gamma(1,.5)[sd]\n   .item7             0.469    0.055    0.374    0.587    1.000 gamma(1,.5)[sd]\n   .item8             0.230    0.030    0.177    0.293    1.000 gamma(1,.5)[sd]\n   .item9             0.609    0.073    0.484    0.767    1.000 gamma(1,.5)[sd]\n   .item10            0.701    0.079    0.562    0.871    1.000 gamma(1,.5)[sd]\n    theta             1.000                                                    \n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Comparing factors score\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_FactorScore_Stan_blavaan.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Comparing Parameters\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code/EAP_psi_lambda_Stan_blavaan.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Limitation of blavaan ðŸ§›ðŸ¼\n\nYou can find more details in the [Edgar Merkle's paper](https://www.jstatsoft.org/article/view/v100i06).\n\n-   Very unreadable Stan file (1891 lines)\n\n-   limits in setting up prior distributions (cannot change distribution family for lambda and psi)\n\n-   Use `rstan` rather than `cmdstanr`\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\n/* This file is based on LERSIL.stan by Ben Goodrich.\n   https://github.com/bgoodri/LERSIL */\nfunctions { // you can use these in R following `rstan::expose_stan_functions(\"foo.stan\")`\n  // mimics lav_mvnorm_cluster_implied22l():\n  matrix calc_W_tilde(matrix sigma_w, vector mu_w, array[] int var1_idx, int p_tilde) {\n    matrix[p_tilde, p_tilde + 1] out = rep_matrix(0, p_tilde, p_tilde + 1); // first column is mean vector\n    vector[p_tilde] mu1 = rep_vector(0, p_tilde);\n    matrix[p_tilde, p_tilde] sig1 = rep_matrix(0, p_tilde, p_tilde);\n    mu1[var1_idx] = mu_w;\n    sig1[var1_idx, var1_idx] = sigma_w;\n    out = append_col(mu1, sig1);\n    return out;\n  }\n  matrix calc_B_tilde(matrix sigma_b, vector mu_b, array[] int var2_idx, int p_tilde) {\n    matrix[p_tilde, p_tilde + 1] out = rep_matrix(0, p_tilde, p_tilde + 1);\n    vector[p_tilde] mu2 = rep_vector(0, p_tilde);\n    matrix[p_tilde, p_tilde] sig2 = rep_matrix(0, p_tilde, p_tilde);\n    mu2[var2_idx] = mu_b;\n    sig2[var2_idx, var2_idx] = sigma_b;\n    out = append_col(mu2, sig2);\n    return out;\n  }\n  vector twolevel_logdens(array[] vector mean_d, array[] matrix cov_d, matrix S_PW, array[] vector YX, array[] int nclus, array[] int clus_size, array[] int clus_sizes, int nclus_sizes, array[] int clus_size_ns, vector impl_Muw, matrix impl_Sigmaw, vector impl_Mub, matrix impl_Sigmab, array[] int ov_idx1, array[] int ov_idx2, array[] int within_idx, array[] int between_idx, array[] int both_idx, int p_tilde, int N_within, int N_between, int N_both){\n    matrix[p_tilde, p_tilde + 1] W_tilde;\n    matrix[p_tilde, p_tilde] W_tilde_cov;\n    matrix[p_tilde, p_tilde + 1] B_tilde;\n    matrix[p_tilde, p_tilde] B_tilde_cov;\n    vector[p_tilde] Mu_WB_tilde;\n    vector[N_between] Mu_z;\n    int N_wo_b = p_tilde - N_between;\n    vector[N_wo_b] Mu_y;\n    vector[N_wo_b] Mu_w;\n    vector[N_wo_b] Mu_b;\n    vector[N_between + N_wo_b] Mu_full;\n    matrix[N_between, N_between] Sigma_zz;\n    matrix[N_between, N_between] Sigma_zz_inv;\n    real Sigma_zz_ld;\n    matrix[N_wo_b, N_between] Sigma_yz;\n    matrix[N_wo_b, N_between] Sigma_yz_zi;\n    matrix[N_wo_b, N_wo_b] Sigma_b;\n    matrix[N_wo_b, N_wo_b] Sigma_b_z;\n    matrix[N_wo_b, N_wo_b] Sigma_w;\n    matrix[N_wo_b, N_wo_b] Sigma_w_inv;\n    real Sigma_w_ld;\n    matrix[N_wo_b, N_wo_b] Sigma_j;\n    matrix[N_wo_b, N_wo_b] Sigma_j_inv;\n    matrix[N_wo_b, N_between] Sigma_ji_yz_zi;\n    matrix[N_between, N_between] Vinv_11;\n    real Sigma_j_ld;\n    vector[nclus_sizes] L;\n    vector[nclus_sizes] B;\n    array[N_between] int bidx;\n    array[p_tilde - N_between] int notbidx;\n    real q_zz;\n    real q_yz;\n    real q_yyc;\n    vector[nclus_sizes] P;\n    vector[nclus_sizes] q_W;\n    vector[nclus_sizes] L_W;\n    vector[nclus_sizes] loglik;\n    vector[nclus_sizes] nperclus = to_vector(clus_sizes) .* to_vector(clus_size_ns);\n    int cluswise = 0;\n    int r1 = 1; // running index of rows of YX corresponding to each cluster\n    // 1. compute necessary vectors/matrices, like lav_mvnorm_cluster_implied22l() of lav_mvnorm_cluster.R\n    if (nclus[2] == nclus_sizes) cluswise = 1;\n    W_tilde = calc_W_tilde(impl_Sigmaw, impl_Muw, ov_idx1, p_tilde);\n    W_tilde_cov = block(W_tilde, 1, 2, p_tilde, p_tilde);\n    B_tilde = calc_B_tilde(impl_Sigmab, impl_Mub, ov_idx2, p_tilde);\n    B_tilde_cov = block(B_tilde, 1, 2, p_tilde, p_tilde);\n    Mu_WB_tilde = rep_vector(0, p_tilde);\n    if (N_within > 0) {\n      for (i in 1:N_within) {\n Mu_WB_tilde[within_idx[i]] = W_tilde[within_idx[i], 1];\n B_tilde[within_idx[i], 1] = 0;\n      }\n    }\n    if (N_both > 0) {\n      for (i in 1:N_both) {\n Mu_WB_tilde[both_idx[i]] = B_tilde[both_idx[i], 1] + W_tilde[both_idx[i], 1];\n      }\n    }\n    // around line 71 of lav_mvnorm_cluster.R\n    if (N_between > 0) {\n      bidx = between_idx[1:N_between];\n      notbidx = between_idx[(N_between + 1):p_tilde];\n      Mu_z = to_vector(B_tilde[bidx, 1]);\n      Mu_y = Mu_WB_tilde[notbidx];\n      Mu_w = to_vector(W_tilde[notbidx, 1]);\n      Mu_b = to_vector(B_tilde[notbidx, 1]);\n      Sigma_zz = B_tilde_cov[bidx, bidx];\n      Sigma_yz = B_tilde_cov[notbidx, bidx];\n      Sigma_b = B_tilde_cov[notbidx, notbidx];\n      Sigma_w = W_tilde_cov[notbidx, notbidx];\n    } else {\n      Mu_y = Mu_WB_tilde;\n      Mu_w = W_tilde[,1];\n      Mu_b = B_tilde[,1];\n      Sigma_b = B_tilde_cov;\n      Sigma_w = W_tilde_cov;\n    }\n    // 2. compute lpdf, around line 203 of lav_mvnorm_cluster\n    Sigma_w_inv = inverse_spd(Sigma_w);\n    Sigma_w_ld = log_determinant(Sigma_w);\n    if (N_between > 0) {\n      Sigma_zz_inv = inverse_spd(Sigma_zz);\n      Sigma_zz_ld = log_determinant(Sigma_zz);\n      Sigma_yz_zi = Sigma_yz * Sigma_zz_inv;\n      Sigma_b_z = Sigma_b - Sigma_yz * Sigma_yz_zi';\n    } else {\n      Sigma_zz_ld = 0;\n      Sigma_b_z = Sigma_b;\n    }\n    Mu_full = append_row(Mu_z, Mu_y);\n    for (clz in 1:nclus_sizes) {\n      int nj = clus_sizes[clz];\n      matrix[N_between + N_wo_b, N_between + N_wo_b] Y2Yc = crossprod(to_matrix(mean_d[clz] - Mu_full)');\n      matrix[N_between, N_between] Y2Yc_zz;\n      matrix[N_wo_b, N_between] Y2Yc_yz;\n      matrix[N_wo_b, N_wo_b] Y2Yc_yy;\n      array[N_between] int uord_bidx;\n      array[N_wo_b] int uord_notbidx;\n      if (!cluswise) Y2Yc += cov_d[clz]; // variability between clusters of same size, will always equal 0 for clusterwise\n      if (N_between > 0) {\n for (k in 1:N_between) {\n   uord_bidx[k] = k;\n }\n if (N_wo_b > 0) {\n   for (k in 1:N_wo_b) {\n     uord_notbidx[k] = N_between + k;\n   }\n }\n Y2Yc_zz = Y2Yc[uord_bidx, uord_bidx];\n Y2Yc_yz = Y2Yc[uord_notbidx, uord_bidx];\n Y2Yc_yy = Y2Yc[uord_notbidx, uord_notbidx];\n      } else {\n Y2Yc_yy = Y2Yc;\n      }\n      Sigma_j = (nj * Sigma_b_z) + Sigma_w;\n      Sigma_j_inv = inverse_spd(Sigma_j); // FIXME npd exceptions for within-only\n      Sigma_j_ld = log_determinant(Sigma_j);\n      L[clz] = Sigma_zz_ld + Sigma_j_ld;\n      if (N_between > 0) {\n Sigma_ji_yz_zi = Sigma_j_inv * Sigma_yz_zi;\n Vinv_11 = Sigma_zz_inv + nj * (Sigma_yz_zi' * Sigma_ji_yz_zi);\n q_zz = sum(Vinv_11 .* Y2Yc_zz);\n q_yz = -nj * sum(Sigma_ji_yz_zi .* Y2Yc_yz);\n      } else {\n q_zz = 0;\n q_yz = 0;\n      }\n      q_yyc = -nj * sum(Sigma_j_inv .* Y2Yc_yy);\n      B[clz] = q_zz + 2 * q_yz - q_yyc;\n      if (cluswise) {\n matrix[N_wo_b, nj] Y_j;\n if (N_between > 0) {\n   for (i in 1:nj) {\n     Y_j[, i] = YX[r1 - 1 + i, notbidx] - mean_d[clz, uord_notbidx]; // would be nice to have to_matrix() here\n   }\n } else {\n   for (i in 1:nj) {\n     Y_j[, i] = YX[r1 - 1 + i] - mean_d[clz]; // would be nice to have to_matrix() here\n   }\n }\n r1 += nj; // for next iteration through loop\n q_W[clz] = sum(Sigma_w_inv .* tcrossprod(Y_j));\n      }\n    }\n    if (!cluswise) {\n      q_W = (nperclus - to_vector(clus_size_ns)) * sum(Sigma_w_inv .* S_PW);\n    }\n    L_W = (nperclus - to_vector(clus_size_ns)) * Sigma_w_ld;\n    loglik = -.5 * ((L .* to_vector(clus_size_ns)) + (B .* to_vector(clus_size_ns)) + q_W + L_W);\n    // add constant, line 300 lav_mvnorm_cluster\n    P = nperclus * (N_within + N_both) + to_vector(clus_size_ns) * N_between;\n    loglik += -.5 * (P * log(2 * pi()));\n    return loglik;\n  }\n  /*\n    Fills in the elements of a coefficient matrix containing some mix of\n    totally free, free subject to a sign constraint, and fixed elements\n    @param free_elements vector of unconstrained elements\n    @param skeleton matrix of the same dimensions as the output whose elements are\n      positive_infinity(): if output element is totally free\n      other: if output element is fixed to that number\n    @return matrix of coefficients\n  */\n  matrix fill_matrix(vector free_elements, matrix skeleton, array[,] int eq_skeleton, int pos_start, int spos_start) {\n    int R = rows(skeleton);\n    int C = cols(skeleton);\n    matrix[R, C] out;\n    int pos = spos_start; // position of eq_skeleton\n    int freepos = pos_start; // position of free_elements\n    int eqelem = 0;\n    for (c in 1:C) for (r in 1:R) {\n      real rc = skeleton[r, c];\n      if (is_inf(rc)) { // free\n int eq = eq_skeleton[pos, 1];\n int wig = eq_skeleton[pos, 3];\n if (eq == 0 || wig == 1) {\n   out[r,c] = free_elements[freepos];\n   freepos += 1;\n } else {\n   eqelem = eq_skeleton[pos, 2];\n   out[r,c] = free_elements[eqelem];\n }\n pos += 1;\n      } else out[r,c] = skeleton[r, c]; // fixed, so do not bump pos\n    }\n    return out;\n  }\n  vector fill_prior(vector free_elements, array[] real pri_mean, array[,] int eq_skeleton) {\n    int R = dims(eq_skeleton)[1];\n    int eqelem = 0;\n    int pos = 1;\n    vector[num_elements(pri_mean)] out;\n    for (r in 1:R) {\n      if (pos <= num_elements(pri_mean)) {\n int eq = eq_skeleton[r, 1];\n int wig = eq_skeleton[r, 3];\n if (eq == 0) {\n   out[pos] = pri_mean[pos];\n   pos += 1;\n } else if (wig == 1) {\n   eqelem = eq_skeleton[r, 2];\n   out[pos] = free_elements[eqelem];\n   pos += 1;\n }\n      }\n    }\n    return out;\n  }\n  /*\n   * This is a bug-free version of csr_to_dense_matrix and has the same arguments\n   */\n  matrix to_dense_matrix(int m, int n, vector w, array[] int v, array[] int u) {\n    matrix[m, n] out = rep_matrix(0, m, n);\n    int pos = 1;\n    for (i in 1:m) {\n      int start = u[i];\n      int nnz = u[i + 1] - start;\n      for (j in 1:nnz) {\n        out[i, v[pos]] = w[pos];\n        pos += 1;\n      }\n    }\n    return out;\n  }\n  // sign function\n  int sign(real x) {\n    if (x > 0)\n      return 1;\n    else\n      return -1;\n  }\n  // sign-constrain a vector of loadings\n  vector sign_constrain_load(vector free_elements, int npar, array[,] int sign_mat) {\n    vector[npar] out;\n    for (i in 1:npar) {\n      if (sign_mat[i,1]) {\n        int lookupval = sign_mat[i,2];\n        if (free_elements[lookupval] < 0) {\n   out[i] = -free_elements[i];\n } else {\n   out[i] = free_elements[i];\n }\n      } else {\n        out[i] = free_elements[i];\n      }\n    }\n    return out;\n  }\n  // sign-constrain a vector of regressions or covariances\n  vector sign_constrain_reg(vector free_elements, int npar, array[,] int sign_mat, vector load_par1, vector load_par2) {\n    vector[npar] out;\n    for (i in 1:npar) {\n      if (sign_mat[i,1]) {\n        int lookupval1 = sign_mat[i,2];\n int lookupval2 = sign_mat[i,3];\n        if (sign(load_par1[lookupval1]) * sign(load_par2[lookupval2]) < 0) {\n   out[i] = -free_elements[i];\n } else {\n   out[i] = free_elements[i];\n }\n      } else {\n        out[i] = free_elements[i];\n      }\n    }\n    return out;\n  }\n  // obtain covariance parameter vector for correlation/sd matrices\n  vector cor2cov(array[] matrix cormat, array[] matrix sdmat, int num_free_elements, array[] matrix matskel, array[,] int wskel, int ngrp) {\n    vector[num_free_elements] out;\n    int R = rows(to_matrix(cormat[1]));\n    int pos = 1; // position of eq_skeleton\n    int freepos = 1; // position of free_elements\n    for (g in 1:ngrp) {\n      for (c in 1:(R-1)) for (r in (c+1):R) {\n        if (is_inf(matskel[g,r,c])) {\n   if (wskel[pos,1] == 0) {\n     out[freepos] = sdmat[g,r,r] * sdmat[g,c,c] * cormat[g,r,c];\n     freepos += 1;\n   }\n   pos += 1;\n }\n      }\n    }\n    return out;\n  }\n  // E step of EM algorithm on latent continuous space\n  array[] matrix estep(array[] vector YXstar, array[] vector Mu, array[] matrix Sigma, array[] int Nobs, array[,] int Obsvar, array[] int startrow, array[] int endrow, array[] int grpnum, int Np, int Ng) {\n    int p = dims(YXstar)[2];\n    array[Ng] matrix[p, p + 1] out; //mean vec + cov mat\n    matrix[dims(YXstar)[1], p] YXfull; // columns consistenly ordered\n    matrix[p, p] T2pat;\n    array[p] int obsidx;\n    int r1;\n    int r2;\n    int grpidx;\n    int Nmis;\n    for (g in 1:Ng) {\n      out[g] = rep_matrix(0, p, p + 1);\n    }\n    for (mm in 1:Np) {\n      obsidx = Obsvar[mm,];\n      r1 = startrow[mm];\n      r2 = endrow[mm];\n      grpidx = grpnum[mm];\n      Nmis = p - Nobs[mm];\n      if (Nobs[mm] < p) {\n matrix[Nobs[mm], Nobs[mm]] Sig22 = Sigma[grpidx, obsidx[1:Nobs[mm]], obsidx[1:Nobs[mm]]];\n matrix[Nmis, Nmis] Sig11 = Sigma[grpidx, obsidx[(Nobs[mm] + 1):p], obsidx[(Nobs[mm] + 1):p]];\n matrix[Nmis, Nobs[mm]] Sig12 = Sigma[grpidx, obsidx[(Nobs[mm] + 1):p], obsidx[1:Nobs[mm]]];\n matrix[Nobs[mm], Nobs[mm]] S22inv = inverse_spd(Sig22);\n matrix[Nmis, Nmis] T2p11 = Sig11 - (Sig12 * S22inv * Sig12');\n        // partition into observed/missing, compute Sigmas, add to out\n for (jj in r1:r2) {\n   vector[Nmis] ymis;\n   ymis = Mu[grpidx, obsidx[(Nobs[mm] + 1):p]] + (Sig12 * S22inv * (YXstar[jj, 1:Nobs[mm]] - Mu[grpidx, obsidx[1:Nobs[mm]]]));\n   for (kk in 1:Nobs[mm]) {\n     YXfull[jj, obsidx[kk]] = YXstar[jj, kk];\n   }\n   for (kk in (Nobs[mm] + 1):p) {\n     YXfull[jj, obsidx[kk]] = ymis[kk - Nobs[mm]];\n   }\n }\n T2pat = crossprod(YXfull[r1:r2,]);\n // correction for missing cells/conditional covariances\n for (jj in 1:Nmis) {\n   for (kk in jj:Nmis) {\n     T2pat[obsidx[Nobs[mm] + jj], obsidx[Nobs[mm] + kk]] = T2pat[obsidx[Nobs[mm] + jj], obsidx[Nobs[mm] + kk]] + (r2 - r1 + 1) * T2p11[jj, kk];\n     if (kk > jj) {\n       T2pat[obsidx[Nobs[mm] + kk], obsidx[Nobs[mm] + jj]] = T2pat[obsidx[Nobs[mm] + jj], obsidx[Nobs[mm] + kk]];\n     }\n   }\n }\n      } else {\n // complete data\n for (jj in r1:r2) {\n   for (kk in 1:Nobs[mm]) {\n     YXfull[jj, obsidx[kk]] = YXstar[jj, kk];\n   }\n }\n T2pat = crossprod(YXfull[r1:r2,]);\n      }\n      for (i in 1:p) {\n out[grpidx,i,1] += sum(YXfull[r1:r2,i]);\n      }\n      out[grpidx,,2:(p+1)] += T2pat;\n    }\n    return out;\n  }\n  matrix sig_inv_update(matrix Sigmainv, array[] int obsidx, int Nobs, int np, real logdet) {\n    matrix[Nobs + 1, Nobs + 1] out = rep_matrix(0, Nobs + 1, Nobs + 1);\n    int nrm = np - Nobs;\n    matrix[nrm, nrm] H;\n    matrix[nrm, Nobs] A;\n    if (nrm == 0) {\n      out[1:Nobs, 1:Nobs] = Sigmainv;\n      out[Nobs + 1, Nobs + 1] = logdet;\n    } else {\n      H = Sigmainv[obsidx[(Nobs + 1):np], obsidx[(Nobs + 1):np]];\n      A = Sigmainv[obsidx[(Nobs + 1):np], obsidx[1:Nobs]];\n      out[1:Nobs, 1:Nobs] = Sigmainv[obsidx[1:Nobs], obsidx[1:Nobs]] - A' * mdivide_left_spd(H, A);\n      out[Nobs + 1, Nobs + 1] = logdet + log_determinant(H);\n    }\n    return out;\n  }\n  real multi_normal_suff(vector xbar, matrix S, vector Mu, matrix Supdate, int N) {\n    int Nobs = dims(S)[1];\n    real out;\n    // using elementwise multiplication + sum here for efficiency\n    out = -.5 * N * ( sum(Supdate[1:Nobs, 1:Nobs] .* (S + (xbar - Mu) * (xbar - Mu)')) + Supdate[Nobs + 1, Nobs + 1] + Nobs * log(2 * pi()) );\n    if(is_nan(out) || out == positive_infinity()) out = negative_infinity();\n    return out;\n  }\n  // compute mean vectors and cov matrices for a single group (two-level models)\n  array[] vector calc_mean_vecs(array[] vector YXstar, array[] vector mean_d, array[] int nclus, array[] int Xvar, array[] int Xbetvar, int Nx, int Nx_between, int p_tilde) {\n    vector[Nx] ov_mean = rep_vector(0, Nx);\n    vector[Nx_between] ov_mean_d = rep_vector(0, Nx_between);\n    int nr = dims(YXstar)[1];\n    array[2] vector[p_tilde] out;\n    for (i in 1:2) out[i] = rep_vector(0, p_tilde);\n    if (Nx > 0) {\n      for (i in 1:nr) {\n ov_mean += YXstar[i, Xvar[1:Nx]];\n      }\n      ov_mean *= pow(nclus[1], -1);\n      out[1, 1:Nx] = ov_mean;\n    }\n    if (Nx_between > 0) {\n      for (cc in 1:nclus[2]) {\n ov_mean_d += mean_d[cc, 1:Nx_between];\n      }\n      ov_mean_d *= pow(nclus[2], -1);\n      out[2, 1:Nx_between] = ov_mean_d;\n    }\n    return out;\n  }\n  array[] matrix calc_cov_mats(array[] vector YXstar, array[] vector mean_d, array[] vector mean_vecs, array[] int nclus, array[] int Xvar, array[] int Xbetvar, int Nx, int Nx_between, int p_tilde) {\n    matrix[Nx_between, Nx_between] cov_mean_d = rep_matrix(0, Nx_between, Nx_between);\n    matrix[Nx, Nx] cov_w = rep_matrix(0, Nx, Nx);\n    matrix[Nx, Nx] cov_w_inv;\n    int nr = dims(YXstar)[1];\n    array[3] matrix[p_tilde, p_tilde] out;\n    for (i in 1:3) out[i] = rep_matrix(0, p_tilde, p_tilde);\n    if (Nx > 0) {\n      for (i in 1:nr) {\n cov_w += tcrossprod(to_matrix(YXstar[i, Xvar[1:Nx]] - mean_vecs[1, 1:Nx]));\n      }\n      cov_w *= pow(nclus[1], -1);\n      cov_w_inv[1:Nx, 1:Nx] = inverse_spd(cov_w);\n      out[2, 1:Nx, 1:Nx] = cov_w;\n      out[3, 1:Nx, 1:Nx] = cov_w_inv;\n      out[3, Nx + 1, Nx + 1] = log_determinant(cov_w); // need log_determinant for multi_normal_suff\n    }\n    if (Nx_between > 0) {\n      for (cc in 1:nclus[2]) {\n cov_mean_d += tcrossprod(to_matrix(mean_d[cc, 1:Nx_between] - mean_vecs[2, 1:Nx_between]));\n      }\n      cov_mean_d *= pow(nclus[2], -1);\n      out[1, 1:Nx_between, 1:Nx_between] = cov_mean_d;\n    }\n    return out;\n  }\n  // compute log_lik of fixed.x variables for a single group (two-level models)\n  vector calc_log_lik_x(array[] vector mean_d, vector ov_mean_d, matrix cov_mean_d, matrix cov_w, matrix cov_w_inv, array[] int nclus, array[] int cluster_size, array[] int Xvar, array[] int Xbetvar, int Nx, int Nx_between) {\n    vector[nclus[2]] out = rep_vector(0, nclus[2]);\n    for (cc in 1:nclus[2]) {\n      if (Nx > 0) {\n out[cc] += multi_normal_suff(mean_d[cc, Xvar[1:Nx]], cov_w[1:Nx, 1:Nx], mean_d[cc, Xvar[1:Nx]], cov_w_inv[1:(Nx + 1), 1:(Nx + 1)], cluster_size[cc]);\n      }\n      if (Nx_between > 0) {\n out[cc] += multi_normal_lpdf(mean_d[cc, 1:Nx_between] | ov_mean_d[1:Nx_between], cov_mean_d[1:Nx_between, 1:Nx_between]);\n      }\n    }\n    return out;\n  }\n  // fill covariance matrix with blocks\n  array[] matrix fill_cov(array[] matrix covmat, array[,] int blkse, array[] int nblk,\n     array[] matrix mat_1, array[] matrix mat_2, array[] matrix mat_3,\n     array[] matrix mat_4, array[] matrix mat_5) {\n    array[dims(covmat)[1]] matrix[dims(covmat)[2], dims(covmat)[3]] out = covmat;\n    for (k in 1:sum(nblk)) {\n      int blkidx = blkse[k, 6];\n      int arrayidx = blkse[k, 5];\n      int blkgrp = blkse[k, 4];\n      int srow = blkse[k, 1];\n      int erow = blkse[k, 2];\n      if (arrayidx == 1) {\n out[blkgrp, srow:erow, srow:erow] = mat_1[blkidx];\n      } else if (arrayidx == 2) {\n out[blkgrp, srow:erow, srow:erow] = mat_2[blkidx];\n      } else if (arrayidx == 3) {\n out[blkgrp, srow:erow, srow:erow] = mat_3[blkidx];\n      } else if (arrayidx == 4) {\n out[blkgrp, srow:erow, srow:erow] = mat_4[blkidx];\n      } else {\n out[blkgrp, srow:erow, srow:erow] = mat_5[blkidx];\n      }\n    }\n    return out;\n  }\n}\ndata {\n  // see https://books.google.com/books?id=9AC-s50RjacC&lpg=PP1&dq=LISREL&pg=PA2#v=onepage&q=LISREL&f=false\n  int<lower=0> p; // number of manifest response variables\n  int<lower=0> p_c; // number of manifest level 2 variables\n  int<lower=0> q; // number of manifest predictors\n  int<lower=0> m; // number of latent endogenous variables\n  int<lower=0> m_c; // number of latent level 2 variables\n  int<lower=0> n; // number of latent exogenous variables\n  int<lower=1> Ng; // number of groups\n  int<lower=0, upper=1> missing; // are there missing values?\n  int<lower=0, upper=1> save_lvs; // should we save lvs?\n  int<lower=1> Np; // number of group-by-missing patterns combos\n  array[Ng] int<lower=1> N; // number of observations per group\n  array[Np] int<lower=1> Nobs; // number of observed variables in each missing pattern\n  array[Np] int<lower=0> Nordobs; // number of ordinal observed variables in each missing pattern\n  array[Np, p + q] int<lower=0> Obsvar; // indexing of observed variables\n  int<lower=1> Ntot; // number of observations across all groups\n  array[Np] int<lower=1> startrow; // starting row for each missing pattern\n  array[Np] int<lower=1,upper=Ntot> endrow; // ending row for each missing pattern\n  array[Np] int<lower=1,upper=Ng> grpnum; // group number for each row of data\n  int<lower=0,upper=1> wigind; // do any parameters have approx equality constraint ('wiggle')?\n  int<lower=0, upper=1> has_data; // are the raw data on y and x available?\n  int<lower=0, upper=1> ord; // are there any ordinal variables?\n  int<lower=0, upper=1> multilev; // is this a multilevel dataset?\n  int<lower=0> Nord; // how many ordinal variables?\n  array[Nord] int<lower=0> ordidx; // indexing of ordinal variables\n  array[Np, Nord] int<lower=0> OrdObsvar; // indexing of observed ordinal variables in YXo\n  int<lower=0> Noent; // how many observed entries of ordinal variables (for data augmentation)\n  array[p + q - Nord] int<lower=0> contidx; // indexing of continuous variables\n  array[Nord] int<lower=1> nlevs; // how many levels does each ordinal variable have\n  array[Ng, 2] int<lower=1> nclus; // number of level 1 + level 2 observations\n  int<lower=0> p_tilde; // total number of variables\n  array[Ntot] vector[multilev ? p_tilde : p + q - Nord] YX; // continuous data\n  array[Ntot, Nord] int YXo; // ordinal data\n  array[Np] int<lower=0> Nx; // number of fixed.x variables (within)\n  array[Np] int<lower=0> Nx_between; // number of fixed.x variables (between)\n  int<lower=0, upper=1> use_cov;\n  int<lower=0, upper=1> pri_only;\n  int<lower=0> emiter; // number of em iterations for saturated model in ppp (missing data only)\n  int<lower=0, upper=1> use_suff; // should we compute likelihood via mvn sufficient stats?\n  int<lower=0, upper=1> do_test; // should we do everything in generated quantities?\n  array[Np] vector[multilev ? p_tilde : p + q - Nord] YXbar; // sample means of continuous manifest variables\n  array[Np] matrix[multilev ? (p_tilde + 1) : (p + q - Nord + 1), multilev ? (p_tilde + 1) : (p + q - Nord + 1)] S; // sample covariance matrix among all continuous manifest variables NB!! multiply by (N-1) to use wishart lpdf!!\n  array[sum(nclus[,2])] int<lower=1> cluster_size; // number of obs per cluster\n  array[Ng] int<lower=1> ncluster_sizes; // number of unique cluster sizes\n  array[sum(ncluster_sizes)] int<lower=1> cluster_sizes; // unique cluster sizes\n  array[sum(ncluster_sizes)] int<lower=1> cluster_size_ns; // number of clusters of each size\n  array[Np, multilev ? p_tilde : p + q] int<lower=0> Xvar; // indexing of fixed.x variables (within)\n  array[Np, multilev ? p_tilde : p + q] int<lower=0> Xdatvar; // indexing of fixed.x in data (differs from Xvar when missing)\n  array[Np, multilev ? p_tilde : p + q] int<lower=0> Xbetvar; // indexing of fixed.x variables (between)\n  array[sum(ncluster_sizes)] vector[p_tilde] mean_d; // sample means by unique cluster size\n  array[sum(ncluster_sizes)] matrix[p_tilde, p_tilde] cov_d; // sample covariances by unique cluster size\n  array[Ng] matrix[p_tilde, p_tilde] cov_w; // observed \"within\" covariance matrix\n  array[sum(nclus[,2])] vector[p_tilde] mean_d_full; // sample means/covs by cluster, for clusterwise log-densities\n  array[sum(nclus[,2])] matrix[p_tilde, p_tilde] cov_d_full;\n  array[Ng] vector[p_tilde] xbar_w; // data estimates of within/between means/covs (for saturated logl)\n  array[Ng] vector[p_tilde] xbar_b;\n  array[Ng] matrix[p_tilde, p_tilde] cov_b;\n  array[Ng] real gs; // group size constant, for computation of saturated logl\n  int N_within; // number of within variables\n  int N_between; // number of between variables\n  int N_both; // number of variables at both levels\n  array[2] int N_lev; // number of observed variables at each level\n  array[N_within] int within_idx;\n  array[p_tilde] int between_idx; // between indexing, followed by within/both\n  array[N_lev[1]] int ov_idx1;\n  array[N_lev[2]] int ov_idx2;\n  array[N_both] int both_idx;\n  vector[multilev ? sum(ncluster_sizes) : Ng] log_lik_x; // ll of fixed x variables by unique cluster size\n  vector[multilev ? sum(nclus[,2]) : Ng] log_lik_x_full; // ll of fixed x variables by cluster\n  /* sparse matrix representations of skeletons of coefficient matrices,\n     which is not that interesting but necessary because you cannot pass\n     missing values into the data block of a Stan program from R */\n  int<lower=0> len_w1; // max number of free elements in Lambda_y per grp\n  array[Ng] int<lower=0> wg1; // number of free elements in Lambda_y per grp\n  array[Ng] vector[len_w1] w1; // values of free elements in Lambda_y\n  array[Ng, len_w1] int<lower=1> v1; // index  of free elements in Lambda_y\n  array[Ng, p + 1] int<lower=1> u1; // index  of free elements in Lambda_y\n  array[sum(wg1), 3] int<lower=0> w1skel;\n  array[sum(wg1), 2] int<lower=0> lam_y_sign;\n  int<lower=0> len_lam_y; // number of free elements minus equality constraints\n  array[len_lam_y] real lambda_y_mn; // prior\n  array[len_lam_y] real<lower=0> lambda_y_sd;\n  // same things but for B\n  int<lower=0> len_w4;\n  array[Ng] int<lower=0> wg4;\n  array[Ng] vector[len_w4] w4;\n  array[Ng, len_w4] int<lower=1> v4;\n  array[Ng, m + 1] int<lower=1> u4;\n  array[sum(wg4), 3] int<lower=0> w4skel;\n  array[sum(wg4), 3] int<lower=0> b_sign;\n  int<lower=0> len_b;\n  array[len_b] real b_mn;\n  array[len_b] real<lower=0> b_sd;\n  // same things but for diag(Theta)\n  int<lower=0> len_w5;\n  array[Ng] int<lower=0> wg5;\n  array[Ng] vector[len_w5] w5;\n  array[Ng, len_w5] int<lower=1> v5;\n  array[Ng, p + 1] int<lower=1> u5;\n  array[sum(wg5), 3] int<lower=0> w5skel;\n  int<lower=0> len_thet_sd;\n  array[len_thet_sd] real<lower=0> theta_sd_shape;\n  array[len_thet_sd] real<lower=0> theta_sd_rate;\n  int<lower=-2, upper=2> theta_pow;\n  // same things but for Theta_r\n  int<lower=0> len_w7;\n  array[Ng] int<lower=0> wg7;\n  array[Ng] vector[len_w7] w7;\n  array[Ng, len_w7] int<lower=1> v7;\n  array[Ng, p + 1] int<lower=1> u7;\n  array[sum(wg7), 3] int<lower=0> w7skel;\n  int<lower=0> len_thet_r;\n  array[len_thet_r] real<lower=0> theta_r_alpha;\n  array[len_thet_r] real<lower=0> theta_r_beta;\n  // same things but for Psi\n  int<lower=0> len_w9;\n  array[Ng] int<lower=0> wg9;\n  array[Ng] vector[len_w9] w9;\n  array[Ng, len_w9] int<lower=1> v9;\n  array[Ng, m + 1] int<lower=1> u9;\n  array[sum(wg9), 3] int<lower=0> w9skel;\n  int<lower=0> len_psi_sd;\n  array[len_psi_sd] real<lower=0> psi_sd_shape;\n  array[len_psi_sd] real<lower=0> psi_sd_rate;\n  int<lower=-2,upper=2> psi_pow;\n  // same things but for Psi_r\n  int<lower=0> len_w10;\n  array[Ng] int<lower=0> wg10;\n  array[Ng] vector[len_w10] w10;\n  array[Ng, len_w10] int<lower=1> v10;\n  array[Ng, m + 1] int<lower=1> u10;\n  array[sum(wg10), 3] int<lower=0> w10skel;\n  array[sum(wg10), 3] int<lower=0> psi_r_sign;\n  int<lower=0> len_psi_r;\n  array[len_psi_r] real<lower=0> psi_r_alpha;\n  array[len_psi_r] real<lower=0> psi_r_beta;\n  // for blocks within Psi_r that receive lkj\n  array[5] int<lower=0> nblk;\n  array[5] int<lower=3> psidims;\n  array[sum(nblk), 7] int<lower=0> blkse;\n  int<lower=0> len_w11;\n  array[Ng] int<lower=0> wg11;\n  array[Ng] vector[len_w11] w11;\n  array[Ng, len_w11] int<lower=1> v11;\n  array[Ng, m + 1] int<lower=1> u11;\n  array[sum(wg11), 3] int<lower=0> w11skel;\n  // same things but for Nu\n  int<lower=0> len_w13;\n  array[Ng] int<lower=0> wg13;\n  array[Ng] vector[len_w13] w13;\n  array[Ng, len_w13] int<lower=1> v13;\n  array[Ng, use_cov ? 1 : p + q + 1] int<lower=1> u13;\n  array[sum(wg13), 3] int<lower=0> w13skel;\n  int<lower=0> len_nu;\n  array[len_nu] real nu_mn;\n  array[len_nu] real<lower=0> nu_sd;\n  // same things but for Alpha\n  int<lower=0> len_w14;\n  array[Ng] int<lower=0> wg14;\n  array[Ng] vector[len_w14] w14;\n  array[Ng, len_w14] int<lower=0> v14;\n  array[Ng, use_cov ? 1 : m + n + 1] int<lower=1> u14;\n  array[sum(wg14), 3] int<lower=0> w14skel;\n  int<lower=0> len_alph;\n  array[len_alph] real alpha_mn;\n  array[len_alph] real<lower=0> alpha_sd;\n  // same things but for Tau\n  int<lower=0> len_w15;\n  array[Ng] int<lower=0> wg15;\n  array[Ng] vector[len_w15] w15;\n  array[Ng, len_w15] int<lower=0> v15;\n  array[Ng, sum(nlevs) - Nord + 1] int<lower=1> u15;\n  array[sum(wg15), 3] int<lower=0> w15skel;\n  int<lower=0> len_tau;\n  array[len_tau] real tau_mn;\n  array[len_tau] real<lower=0> tau_sd;\n  // Level 2 matrices start here!!\n  // Lambda\n  int<lower=0> len_w1_c;\n  array[Ng] int<lower=0> wg1_c;\n  array[Ng] vector[len_w1_c] w1_c;\n  array[Ng, len_w1_c] int<lower=1> v1_c;\n  array[Ng, p_c + 1] int<lower=1> u1_c;\n  array[sum(wg1_c), 3] int<lower=0> w1skel_c;\n  array[sum(wg1_c), 2] int<lower=0> lam_y_sign_c;\n  int<lower=0> len_lam_y_c;\n  array[len_lam_y_c] real lambda_y_mn_c;\n  array[len_lam_y_c] real<lower=0> lambda_y_sd_c;\n  // same things but for B\n  int<lower=0> len_w4_c;\n  array[Ng] int<lower=0> wg4_c;\n  array[Ng] vector[len_w4_c] w4_c;\n  array[Ng, len_w4_c] int<lower=1> v4_c;\n  array[Ng, m_c + 1] int<lower=1> u4_c;\n  array[sum(wg4_c), 3] int<lower=0> w4skel_c;\n  array[sum(wg4_c), 3] int<lower=0> b_sign_c;\n  int<lower=0> len_b_c;\n  array[len_b_c] real b_mn_c;\n  array[len_b_c] real<lower=0> b_sd_c;\n  // same things but for diag(Theta)\n  int<lower=0> len_w5_c;\n  array[Ng] int<lower=0> wg5_c;\n  array[Ng] vector[len_w5_c] w5_c;\n  array[Ng, len_w5_c] int<lower=1> v5_c;\n  array[Ng, p_c + 1] int<lower=1> u5_c;\n  array[sum(wg5_c), 3] int<lower=0> w5skel_c;\n  int<lower=0> len_thet_sd_c;\n  array[len_thet_sd_c] real<lower=0> theta_sd_shape_c;\n  array[len_thet_sd_c] real<lower=0> theta_sd_rate_c;\n  int<lower=-2, upper=2> theta_pow_c;\n  // same things but for Theta_r\n  int<lower=0> len_w7_c;\n  array[Ng] int<lower=0> wg7_c;\n  array[Ng] vector[len_w7_c] w7_c;\n  array[Ng, len_w7_c] int<lower=1> v7_c;\n  array[Ng, p_c + 1] int<lower=1> u7_c;\n  array[sum(wg7_c), 3] int<lower=0> w7skel_c;\n  int<lower=0> len_thet_r_c;\n  array[len_thet_r_c] real<lower=0> theta_r_alpha_c;\n  array[len_thet_r_c] real<lower=0> theta_r_beta_c;\n  // same things but for Psi\n  int<lower=0> len_w9_c;\n  array[Ng] int<lower=0> wg9_c;\n  array[Ng] vector[len_w9_c] w9_c;\n  array[Ng, len_w9_c] int<lower=1> v9_c;\n  array[Ng, m_c + 1] int<lower=1> u9_c;\n  array[sum(wg9_c), 3] int<lower=0> w9skel_c;\n  int<lower=0> len_psi_sd_c;\n  array[len_psi_sd_c] real<lower=0> psi_sd_shape_c;\n  array[len_psi_sd_c] real<lower=0> psi_sd_rate_c;\n  int<lower=-2,upper=2> psi_pow_c;\n  // same things but for Psi_r\n  int<lower=0> len_w10_c;\n  array[Ng] int<lower=0> wg10_c;\n  array[Ng] vector[len_w10_c] w10_c;\n  array[Ng, len_w10_c] int<lower=1> v10_c;\n  array[Ng, m_c + 1] int<lower=1> u10_c;\n  array[sum(wg10_c), 3] int<lower=0> w10skel_c;\n  array[sum(wg10_c), 3] int<lower=0> psi_r_sign_c;\n  int<lower=0> len_psi_r_c;\n  array[len_psi_r_c] real<lower=0> psi_r_alpha_c;\n  array[len_psi_r_c] real<lower=0> psi_r_beta_c;\n  // for blocks within Psi_r that receive lkj\n  array[5] int<lower=0> nblk_c;\n  array[5] int<lower=3> psidims_c;\n  array[sum(nblk_c), 7] int<lower=0> blkse_c;\n  int<lower=0> len_w11_c;\n  array[Ng] int<lower=0> wg11_c;\n  array[Ng] vector[len_w11_c] w11_c;\n  array[Ng, len_w11_c] int<lower=1> v11_c;\n  array[Ng, m_c + 1] int<lower=1> u11_c;\n  array[sum(wg11_c), 3] int<lower=0> w11skel_c;\n  // same things but for Nu\n  int<lower=0> len_w13_c;\n  array[Ng] int<lower=0> wg13_c;\n  array[Ng] vector[len_w13_c] w13_c;\n  array[Ng, len_w13_c] int<lower=1> v13_c;\n  array[Ng, p_c + 1] int<lower=1> u13_c;\n  array[sum(wg13_c), 3] int<lower=0> w13skel_c;\n  int<lower=0> len_nu_c;\n  array[len_nu_c] real nu_mn_c;\n  array[len_nu_c] real<lower=0> nu_sd_c;\n  // same things but for Alpha\n  int<lower=0> len_w14_c;\n  array[Ng] int<lower=0> wg14_c;\n  array[Ng] vector[len_w14_c] w14_c;\n  array[Ng, len_w14_c] int<lower=0> v14_c;\n  array[Ng, m_c + 1] int<lower=1> u14_c;\n  array[sum(wg14_c), 3] int<lower=0> w14skel_c;\n  int<lower=0> len_alph_c;\n  array[len_alph_c] real alpha_mn_c;\n  array[len_alph_c] real<lower=0> alpha_sd_c;\n}\ntransformed data { // (re)construct skeleton matrices in Stan (not that interesting)\n  array[Ng] matrix[p, m] Lambda_y_skeleton;\n  array[Ng] matrix[m, m] B_skeleton;\n  array[Ng] matrix[p, p] Theta_skeleton;\n  array[Ng] matrix[p, p] Theta_r_skeleton;\n  array[Ng] matrix[m, m] Psi_skeleton;\n  array[Ng] matrix[m, m] Psi_r_skeleton;\n  array[Ng] matrix[m, m] Psi_r_skeleton_f;\n  array[Ng] matrix[p, 1] Nu_skeleton;\n  array[Ng] matrix[m, 1] Alpha_skeleton;\n  array[Ng] matrix[sum(nlevs) - Nord, 1] Tau_skeleton;\n  array[Np] vector[ord ? 0 : (p + q)] YXbarstar;\n  array[Np] matrix[ord ? 0 : (p + q), ord ? 0 : (p + q)] Sstar;\n  array[Ng] matrix[p_c, m_c] Lambda_y_skeleton_c;\n  array[Ng] matrix[m_c, m_c] B_skeleton_c;\n  array[Ng] matrix[p_c, p_c] Theta_skeleton_c;\n  array[Ng] matrix[p_c, p_c] Theta_r_skeleton_c;\n  array[Ng] matrix[m_c, m_c] Psi_skeleton_c;\n  array[Ng] matrix[m_c, m_c] Psi_r_skeleton_c;\n  array[Ng] matrix[m_c, m_c] Psi_r_skeleton_f_c;\n  array[Ng] matrix[p_c, 1] Nu_skeleton_c;\n  array[Ng] matrix[m_c, 1] Alpha_skeleton_c;\n  matrix[m, m] I = diag_matrix(rep_vector(1, m));\n  matrix[m_c, m_c] I_c = diag_matrix(rep_vector(1, m_c));\n  int Ncont = p + q - Nord;\n  array[max(nclus[,2]) > 1 ? max(nclus[,2]) : 0] int<lower = 0> intone;\n  array[Ng,2] int g_start1;\n  array[Ng,2] int g_start4;\n  array[Ng,2] int g_start5;\n  array[Ng,2] int g_start7;\n  array[Ng,2] int g_start9;\n  array[Ng,2] int g_start10;\n  array[Ng,2] int g_start13;\n  array[Ng,2] int g_start14;\n  array[Ng,2] int g_start15;\n  array[Ng,2] int g_start1_c;\n  array[Ng,2] int g_start4_c;\n  array[Ng,2] int g_start5_c;\n  array[Ng,2] int g_start7_c;\n  array[Ng,2] int g_start9_c;\n  array[Ng,2] int g_start10_c;\n  array[Ng,2] int g_start13_c;\n  array[Ng,2] int g_start14_c;\n  array[15] int len_free;\n  array[15] int pos;\n  array[15] int len_free_c;\n  array[15] int pos_c;\n  for (i in 1:15) {\n    len_free[i] = 0;\n    pos[i] = 1;\n    len_free_c[i] = 0;\n    pos_c[i] = 1;\n  }\n  for (g in 1:Ng) {\n    Lambda_y_skeleton[g] = to_dense_matrix(p, m, w1[g], v1[g,], u1[g,]);\n    B_skeleton[g] = to_dense_matrix(m, m, w4[g], v4[g,], u4[g,]);\n    Theta_skeleton[g] = to_dense_matrix(p, p, w5[g], v5[g,], u5[g,]);\n    Theta_r_skeleton[g] = to_dense_matrix(p, p, w7[g], v7[g,], u7[g,]);\n    Psi_skeleton[g] = to_dense_matrix(m, m, w9[g], v9[g,], u9[g,]);\n    Psi_r_skeleton[g] = to_dense_matrix(m, m, w10[g], v10[g,], u10[g,]);\n    Psi_r_skeleton_f[g] = to_dense_matrix(m, m, w11[g], v11[g,], u11[g,]);\n    if (!use_cov) {\n      Nu_skeleton[g] = to_dense_matrix((p + q), 1, w13[g], v13[g,], u13[g,]);\n      Alpha_skeleton[g] = to_dense_matrix((m + n), 1, w14[g], v14[g,], u14[g,]);\n    }\n    Tau_skeleton[g] = to_dense_matrix(sum(nlevs) - Nord, 1, w15[g], v15[g,], u15[g,]);\n    Lambda_y_skeleton_c[g] = to_dense_matrix(p_c, m_c, w1_c[g], v1_c[g,], u1_c[g,]);\n    B_skeleton_c[g] = to_dense_matrix(m_c, m_c, w4_c[g], v4_c[g,], u4_c[g,]);\n    Theta_skeleton_c[g] = to_dense_matrix(p_c, p_c, w5_c[g], v5_c[g,], u5_c[g,]);\n    Theta_r_skeleton_c[g] = to_dense_matrix(p_c, p_c, w7_c[g], v7_c[g,], u7_c[g,]);\n    Psi_skeleton_c[g] = to_dense_matrix(m_c, m_c, w9_c[g], v9_c[g,], u9_c[g,]);\n    Psi_r_skeleton_c[g] = to_dense_matrix(m_c, m_c, w10_c[g], v10_c[g,], u10_c[g,]);\n    Psi_r_skeleton_f_c[g] = to_dense_matrix(m_c, m_c, w11_c[g], v11_c[g,], u11_c[g,]);\n    Nu_skeleton_c[g] = to_dense_matrix(p_c, 1, w13_c[g], v13_c[g,], u13_c[g,]);\n    Alpha_skeleton_c[g] = to_dense_matrix(m_c, 1, w14_c[g], v14_c[g,], u14_c[g,]);\n    // count free elements in Lambda_y_skeleton\n    g_start1[g,1] = len_free[1] + 1;\n    g_start1[g,2] = pos[1];\n    for (i in 1:p) {\n      for (j in 1:m) {\n        if (is_inf(Lambda_y_skeleton[g,i,j])) {\n   if (w1skel[pos[1],2] == 0 || w1skel[pos[1],3] == 1) len_free[1] += 1;\n   pos[1] += 1;\n        }\n      }\n    }\n    // same thing but for B_skeleton\n    g_start4[g,1] = len_free[4] + 1;\n    g_start4[g,2] = pos[4];\n    for (i in 1:m) {\n      for (j in 1:m) {\n if (is_inf(B_skeleton[g,i,j])) {\n   if (w4skel[pos[4],2] == 0 || w4skel[pos[4],3] == 1) len_free[4] += 1;\n   pos[4] += 1;\n }\n      }\n    }\n    // same thing but for Theta_skeleton\n    g_start5[g,1] = len_free[5] + 1;\n    g_start5[g,2] = pos[5];\n    for (i in 1:p) {\n      if (is_inf(Theta_skeleton[g,i,i])) {\n if (w5skel[pos[5],2] == 0 || w5skel[pos[5],3] == 1) len_free[5] += 1;\n pos[5] += 1;\n      }\n    }\n    // same thing but for Theta_r_skeleton\n    g_start7[g,1] = len_free[7] + 1;\n    g_start7[g,2] = pos[7];\n    for (i in 1:(p-1)) {\n      for (j in (i+1):p) {\n if (is_inf(Theta_r_skeleton[g,j,i])) {\n   if (w7skel[pos[7],2] == 0 || w7skel[pos[7],3] == 1) len_free[7] += 1;\n   pos[7] += 1;\n }\n      }\n    }\n    // same thing but for Psi_skeleton\n    g_start9[g,1] = len_free[9] + 1;\n    g_start9[g,2] = pos[9];\n    for (i in 1:m) {\n      if (is_inf(Psi_skeleton[g,i,i])) {\n if (w9skel[pos[9],2] == 0 || w9skel[pos[9],3] == 1) len_free[9] += 1;\n pos[9] += 1;\n      }\n    }\n    // same thing but for Psi_r_skeleton\n    g_start10[g,1] = len_free[10] + 1;\n    g_start10[g,2] = pos[10];\n    for (i in 1:(m-1)) {\n      for (j in (i+1):m) {\n if (is_inf(Psi_r_skeleton[g,j,i])) {\n   if (w10skel[pos[10],2] == 0 || w10skel[pos[10],3] == 1) len_free[10] += 1;\n   pos[10] += 1;\n }\n if (is_inf(Psi_r_skeleton_f[g,j,i])) {\n   if (w11skel[pos[11],2] == 0 || w11skel[pos[11],3] == 1) len_free[11] += 1;\n   pos[11] += 1;\n }\n      }\n    }\n    if (!use_cov) {\n      // same thing but for Nu_skeleton\n      // pos = len_free13 + 1;\n      g_start13[g,1] = len_free[13] + 1;\n      g_start13[g,2] = pos[13];\n      for (i in 1:(p+q)) {\n if (is_inf(Nu_skeleton[g,i,1])) {\n   if (w13skel[pos[13],2] == 0 || w13skel[pos[13],3] == 1) len_free[13] += 1;\n   pos[13] += 1;\n }\n      }\n      // same thing but for Alpha_skeleton\n      g_start14[g,1] = len_free[14] + 1;\n      g_start14[g,2] = pos[14];\n      for (i in 1:(m+n)) {\n if (is_inf(Alpha_skeleton[g,i,1])) {\n   if (w14skel[pos[14],2] == 0 || w14skel[pos[14],3] == 1) len_free[14] += 1;\n   pos[14] += 1;\n }\n      }\n    }\n    // same thing but for Tau_skeleton\n    g_start15[g,1] = len_free[15] + 1;\n    g_start15[g,2] = pos[15];\n    for (i in 1:(sum(nlevs) - Nord)) {\n      if (is_inf(Tau_skeleton[g,i,1])) {\n if (w15skel[pos[15],2] == 0 || w15skel[pos[15],3] == 1) len_free[15] += 1;\n pos[15] += 1;\n      }\n    }\n    // now level 2\n    // count free elements in Lambda_y_skeleton\n    g_start1_c[g,1] = len_free_c[1] + 1;\n    g_start1_c[g,2] = pos_c[1];\n    for (i in 1:p_c) {\n      for (j in 1:m_c) {\n        if (is_inf(Lambda_y_skeleton_c[g,i,j])) {\n   if (w1skel_c[pos_c[1],2] == 0 || w1skel_c[pos_c[1],3] == 1) len_free_c[1] += 1;\n   pos_c[1] += 1;\n        }\n      }\n    }\n    // same thing but for B_skeleton\n    g_start4_c[g,1] = len_free_c[4] + 1;\n    g_start4_c[g,2] = pos_c[4];\n    for (i in 1:m_c) {\n      for (j in 1:m_c) {\n if (is_inf(B_skeleton_c[g,i,j])) {\n   if (w4skel_c[pos_c[4],2] == 0 || w4skel_c[pos_c[4],3] == 1) len_free_c[4] += 1;\n   pos_c[4] += 1;\n }\n      }\n    }\n    // same thing but for Theta_skeleton\n    g_start5_c[g,1] = len_free_c[5] + 1;\n    g_start5_c[g,2] = pos_c[5];\n    for (i in 1:p_c) {\n      if (is_inf(Theta_skeleton_c[g,i,i])) {\n if (w5skel_c[pos_c[5],2] == 0 || w5skel_c[pos_c[5],3] == 1) len_free_c[5] += 1;\n pos_c[5] += 1;\n      }\n    }\n    // same thing but for Theta_r_skeleton\n    g_start7_c[g,1] = len_free_c[7] + 1;\n    g_start7_c[g,2] = pos_c[7];\n    for (i in 1:(p_c-1)) {\n      for (j in (i+1):p_c) {\n if (is_inf(Theta_r_skeleton_c[g,j,i])) {\n   if (w7skel_c[pos_c[7],2] == 0 || w7skel_c[pos_c[7],3] == 1) len_free_c[7] += 1;\n   pos_c[7] += 1;\n }\n      }\n    }\n    // same thing but for Psi_skeleton\n    g_start9_c[g,1] = len_free_c[9] + 1;\n    g_start9_c[g,2] = pos_c[9];\n    for (i in 1:m_c) {\n      if (is_inf(Psi_skeleton_c[g,i,i])) {\n if (w9skel_c[pos_c[9],2] == 0 || w9skel_c[pos_c[9],3] == 1) len_free_c[9] += 1;\n pos_c[9] += 1;\n      }\n    }\n    // same thing but for Psi_r_skeleton\n    g_start10_c[g,1] = len_free_c[10] + 1;\n    g_start10_c[g,2] = pos_c[10];\n    for (i in 1:(m_c-1)) {\n      for (j in (i+1):m_c) {\n if (is_inf(Psi_r_skeleton_c[g,j,i])) {\n   if (w10skel_c[pos_c[10],2] == 0 || w10skel_c[pos_c[10],3] == 1) len_free_c[10] += 1;\n   pos_c[10] += 1;\n }\n if (is_inf(Psi_r_skeleton_f_c[g,j,i])) {\n   if (w11skel_c[pos_c[11],2] == 0 || w11skel_c[pos_c[11],3] == 1) len_free_c[11] += 1;\n   pos_c[11] += 1;\n }\n      }\n    }\n    // same thing but for Nu_skeleton\n    // pos = len_free13 + 1;\n    g_start13_c[g,1] = len_free_c[13] + 1;\n    g_start13_c[g,2] = pos_c[13];\n    for (i in 1:p_c) {\n      if (is_inf(Nu_skeleton_c[g,i,1])) {\n if (w13skel_c[pos_c[13],2] == 0 || w13skel_c[pos_c[13],3] == 1) len_free_c[13] += 1;\n pos_c[13] += 1;\n      }\n    }\n    // same thing but for Alpha_skeleton\n    g_start14_c[g,1] = len_free_c[14] + 1;\n    g_start14_c[g,2] = pos_c[14];\n    for (i in 1:m_c) {\n      if (is_inf(Alpha_skeleton_c[g,i,1])) {\n if (w14skel_c[pos_c[14],2] == 0 || w14skel_c[pos_c[14],3] == 1) len_free_c[14] += 1;\n pos_c[14] += 1;\n      }\n    }\n  }\n  // for clusterwise loglik computations\n  if (max(nclus[,2]) > 1) for (i in 1:max(nclus[,2])) intone[i] = 1;\n  if (!ord && (use_suff || use_cov)) {\n    // sufficient stat matrices by pattern, moved to left for missing\n    for (patt in 1:Np) {\n      Sstar[patt] = rep_matrix(0, p + q, p + q);\n      Sstar[patt, 1:Nobs[patt], 1:Nobs[patt]] = S[patt, Obsvar[patt, 1:Nobs[patt]], Obsvar[patt, 1:Nobs[patt]]];\n      for (j in 1:Nobs[patt]) {\n YXbarstar[patt,j] = YXbar[patt, Obsvar[patt,j]];\n      }\n    }\n  }\n}\nparameters {\n  // free elements (possibly with inequality constraints) for coefficient matrices\n  vector[len_free[1]] Lambda_y_free;\n  vector[len_free[4]] B_free;\n  vector<lower=0>[len_free[5]] Theta_sd_free;\n  vector<lower=-1,upper=1>[len_free[7]] Theta_r_free; // to use beta prior\n  vector<lower=0>[len_free[9]] Psi_sd_free;\n  array[nblk[1]] corr_matrix[psidims[1]] Psi_r_mat_1;\n  array[nblk[2]] corr_matrix[psidims[2]] Psi_r_mat_2;\n  array[nblk[3]] corr_matrix[psidims[3]] Psi_r_mat_3;\n  array[nblk[4]] corr_matrix[psidims[4]] Psi_r_mat_4;\n  array[nblk[5]] corr_matrix[psidims[5]] Psi_r_mat_5;\n  vector<lower=-1,upper=1>[len_free[10]] Psi_r_free;\n  vector[len_free[13]] Nu_free;\n  vector[len_free[14]] Alpha_free;\n  vector[len_free[15]] Tau_ufree;\n  vector<lower=0,upper=1>[Noent] z_aug; //augmented ordinal data\n  vector[len_free_c[1]] Lambda_y_free_c;\n  vector[len_free_c[4]] B_free_c;\n  vector<lower=0>[len_free_c[5]] Theta_sd_free_c;\n  vector<lower=-1,upper=1>[len_free_c[7]] Theta_r_free_c; // to use beta prior\n  vector<lower=0>[len_free_c[9]] Psi_sd_free_c;\n  array[nblk_c[1]] corr_matrix[psidims_c[1]] Psi_r_mat_1_c;\n  array[nblk_c[2]] corr_matrix[psidims_c[2]] Psi_r_mat_2_c;\n  array[nblk_c[3]] corr_matrix[psidims_c[3]] Psi_r_mat_3_c;\n  array[nblk_c[4]] corr_matrix[psidims_c[4]] Psi_r_mat_4_c;\n  array[nblk_c[5]] corr_matrix[psidims_c[5]] Psi_r_mat_5_c;\n  vector<lower=-1,upper=1>[len_free_c[10]] Psi_r_free_c;\n  vector[len_free_c[13]] Nu_free_c;\n  vector[len_free_c[14]] Alpha_free_c;\n}\ntransformed parameters {\n  array[Ng] matrix[p, m] Lambda_y;\n  array[Ng] matrix[m, m] B;\n  array[Ng] matrix[p, p] Theta_sd;\n  array[Ng] matrix[p, p] T_r_lower;\n  array[Ng] matrix[p, p] Theta_r;\n  array[Ng] matrix[p + q, 1] Nu;\n  array[Ng] matrix[m + n, 1] Alpha;\n  array[Ng] matrix[p_c, m_c] Lambda_y_c;\n  array[Ng] matrix[m_c, m_c] B_c;\n  array[Ng] matrix[p_c, p_c] Theta_sd_c;\n  array[Ng] matrix[p_c, p_c] T_r_lower_c;\n  array[Ng] matrix[p_c, p_c] Theta_r_c;\n  array[Ng] matrix[p_c, 1] Nu_c;\n  array[Ng] matrix[m_c, 1] Alpha_c;\n  array[Ng] matrix[sum(nlevs) - Nord, 1] Tau_un;\n  array[Ng] matrix[sum(nlevs) - Nord, 1] Tau;\n  vector[len_free[15]] Tau_free;\n  real tau_jacobian;\n  array[Ng] matrix[m, m] Psi;\n  array[Ng] matrix[m, m] Psi_sd;\n  array[Ng] matrix[m, m] Psi_r_lower;\n  array[Ng] matrix[m, m] Psi_r;\n  array[Ng] matrix[m_c, m_c] Psi_c;\n  array[Ng] matrix[m_c, m_c] Psi_sd_c;\n  array[Ng] matrix[m_c, m_c] Psi_r_lower_c;\n  array[Ng] matrix[m_c, m_c] Psi_r_c;\n  vector[len_free[1]] lambda_y_primn;\n  vector[len_free[4]] b_primn;\n  vector[len_free[13]] nu_primn;\n  vector[len_free[14]] alpha_primn;\n  vector[len_free[15]] tau_primn;\n  vector[len_free_c[1]] lambda_y_primn_c;\n  vector[len_free_c[4]] b_primn_c;\n  vector[len_free_c[13]] nu_primn_c;\n  vector[len_free_c[14]] alpha_primn_c;\n  array[Ng] matrix[p, m] Lambda_y_A; // = Lambda_y * (I - B)^{-1}\n  array[Ng] matrix[p_c, m_c] Lambda_y_A_c;\n  array[Ng] vector[p + q] Mu;\n  array[Ng] matrix[p + q, p + q] Sigma; // model covariance matrix\n  array[Ng] matrix[p + q, p + q] Sigmainv_grp; // model covariance matrix\n  array[Ng] real logdetSigma_grp;\n  array[Np] matrix[p + q + 1, p + q + 1] Sigmainv; // for updating S^-1 by missing data pattern\n  array[Ng] vector[p_c] Mu_c;\n  array[Ng] matrix[p_c, p_c] Sigma_c; // level 2 model covariance matrix\n  array[Ng] matrix[N_both + N_within, N_both + N_within] S_PW;\n  array[Ntot] vector[p + q] YXstar;\n  array[Ntot] vector[Nord] YXostar; // ordinal data\n  for (g in 1:Ng) {\n    // model matrices\n    Lambda_y[g] = fill_matrix(Lambda_y_free, Lambda_y_skeleton[g], w1skel, g_start1[g,1], g_start1[g,2]);\n    B[g] = fill_matrix(B_free, B_skeleton[g], w4skel, g_start4[g,1], g_start4[g,2]);\n    Theta_sd[g] = fill_matrix(Theta_sd_free, Theta_skeleton[g], w5skel, g_start5[g,1], g_start5[g,2]);\n    T_r_lower[g] = fill_matrix(Theta_r_free, Theta_r_skeleton[g], w7skel, g_start7[g,1], g_start7[g,2]);\n    Theta_r[g] = T_r_lower[g] + transpose(T_r_lower[g]) - diag_matrix(rep_vector(1, p));\n    if (!use_cov) {\n      Nu[g] = fill_matrix(Nu_free, Nu_skeleton[g], w13skel, g_start13[g,1], g_start13[g,2]);\n      Alpha[g] = fill_matrix(Alpha_free, Alpha_skeleton[g], w14skel, g_start14[g,1], g_start14[g,2]);\n    }\n    Psi[g] = diag_matrix(rep_vector(0, m));\n    if (m > 0) {\n      Psi_sd[g] = fill_matrix(Psi_sd_free, Psi_skeleton[g], w9skel, g_start9[g,1], g_start9[g,2]);\n      Psi_r_lower[g] = fill_matrix(Psi_r_free, Psi_r_skeleton[g], w10skel, g_start10[g,1], g_start10[g,2]);\n      Psi_r[g] = Psi_r_lower[g] + transpose(Psi_r_lower[g]) - diag_matrix(rep_vector(1, m));\n    }\n    // level 2 matrices\n    Lambda_y_c[g] = fill_matrix(Lambda_y_free_c, Lambda_y_skeleton_c[g], w1skel_c, g_start1_c[g,1], g_start1_c[g,2]);\n    B_c[g] = fill_matrix(B_free_c, B_skeleton_c[g], w4skel_c, g_start4_c[g,1], g_start4_c[g,2]);\n    Theta_sd_c[g] = fill_matrix(Theta_sd_free_c, Theta_skeleton_c[g], w5skel_c, g_start5_c[g,1], g_start5_c[g,2]);\n    T_r_lower_c[g] = fill_matrix(Theta_r_free_c, Theta_r_skeleton_c[g], w7skel_c, g_start7_c[g,1], g_start7_c[g,2]);\n    Theta_r_c[g] = T_r_lower_c[g] + transpose(T_r_lower_c[g]) - diag_matrix(rep_vector(1, p_c));\n    Nu_c[g] = fill_matrix(Nu_free_c, Nu_skeleton_c[g], w13skel_c, g_start13_c[g,1], g_start13_c[g,2]);\n    Alpha_c[g] = fill_matrix(Alpha_free_c, Alpha_skeleton_c[g], w14skel_c, g_start14_c[g,1], g_start14_c[g,2]);\n    Psi_c[g] = diag_matrix(rep_vector(0, m_c));\n    if (m_c > 0) {\n      Psi_sd_c[g] = fill_matrix(Psi_sd_free_c, Psi_skeleton_c[g], w9skel_c, g_start9_c[g,1], g_start9_c[g,2]);\n      Psi_r_lower_c[g] = fill_matrix(Psi_r_free_c, Psi_r_skeleton_c[g], w10skel_c, g_start10_c[g,1], g_start10_c[g,2]);\n      Psi_r_c[g] = Psi_r_lower_c[g] + transpose(Psi_r_lower_c[g]) - diag_matrix(rep_vector(1, m_c));\n    }\n  }\n  if (sum(nblk) > 0) {\n    // we need to define a separate parameter for each dimension of correlation matrix,\n    // so we need all these Psi_r_mats\n    Psi_r = fill_cov(Psi_r, blkse, nblk, Psi_r_mat_1, Psi_r_mat_2, Psi_r_mat_3, Psi_r_mat_4, Psi_r_mat_5);\n  }\n  if (sum(nblk_c) > 0) {\n    Psi_r_c = fill_cov(Psi_r_c, blkse_c, nblk_c, Psi_r_mat_1_c, Psi_r_mat_2_c, Psi_r_mat_3_c, Psi_r_mat_4_c, Psi_r_mat_5_c);\n  }\n  // see https://books.google.com/books?id=9AC-s50RjacC&lpg=PP1&dq=LISREL&pg=PA3#v=onepage&q=LISREL&f=false\n  for (g in 1:Ng) {\n    if (m > 0) {\n      Lambda_y_A[g] = mdivide_right(Lambda_y[g], I - B[g]); // = Lambda_y * (I - B)^{-1}\n      Psi[g] = quad_form_sym(Psi_r[g], Psi_sd[g]);\n    }\n    if (!use_cov) {\n      Mu[g] = to_vector(Nu[g]);\n    } else if(has_data) {\n      Mu[g] = YXbar[g]; // doesn't enter in likelihood, just for lppd + loo\n    }\n    if (p > 0) {\n      Sigma[g, 1:p, 1:p] = quad_form_sym(Theta_r[g], Theta_sd[g]);\n      if (m > 0) {\n        Sigma[g, 1:p, 1:p] += quad_form_sym(Psi[g], transpose(Lambda_y_A[g]));\n if (!use_cov) Mu[g, 1:p] += to_vector(Lambda_y_A[g] * Alpha[g, 1:m, 1]);\n      }\n    }\n    if (m_c > 0) {\n      Lambda_y_A_c[g] = mdivide_right(Lambda_y_c[g], I_c - B_c[g]);\n      Psi_c[g] = quad_form_sym(Psi_r_c[g], Psi_sd_c[g]);\n    }\n    Mu_c[g] = to_vector(Nu_c[g]);\n    if (p_c > 0) {\n      Sigma_c[g, 1:p_c, 1:p_c] = quad_form_sym(Theta_r_c[g], Theta_sd_c[g]);\n      if (m_c > 0) {\n        Sigma_c[g, 1:p_c, 1:p_c] += quad_form_sym(Psi_c[g], transpose(Lambda_y_A_c[g]));\n Mu_c[g, 1:p_c] += to_vector(Lambda_y_A_c[g] * Alpha_c[g, 1:m_c, 1]);\n      }\n    }\n    if (nclus[g,2] > 1) {\n      // remove between variables, for likelihood computations\n      S_PW[g] = cov_w[g, between_idx[(N_between + 1):p_tilde], between_idx[(N_between + 1):p_tilde]];\n    }\n  }\n  // obtain ordered thresholds; NB untouched for two-level models\n  if (ord) {\n    int opos = 1;\n    int ofreepos = 1;\n    tau_jacobian = 0;\n    for (g in 1:Ng) {\n      int vecpos = 1;\n      Tau_un[g] = fill_matrix(Tau_ufree, Tau_skeleton[g], w15skel, g_start15[g,1], g_start15[g,2]);\n      for (i in 1:Nord) {\n for (j in 1:(nlevs[i] - 1)) {\n   real rc = Tau_skeleton[g, vecpos, 1];\n   int eq = w15skel[opos, 1];\n   int wig = w15skel[opos, 3];\n   if (is_inf(rc)) {\n     if (eq == 0 || wig == 1) {\n       if (j == 1) {\n  Tau[g, vecpos, 1] = Tau_un[g, vecpos, 1];\n       } else {\n  Tau[g, vecpos, 1] = Tau[g, (vecpos - 1), 1] + exp(Tau_un[g, vecpos, 1]);\n       }\n       Tau_free[ofreepos] = Tau[g, vecpos, 1];\n       // this is used if a prior goes on Tau_free, instead of Tau_ufree:\n       //if (j > 1) {\n       //  tau_jacobian += Tau_un[g, vecpos, 1]; // see https://mc-stan.org/docs/2_24/reference-manual/ordered-vector.html\n       // }\n       ofreepos += 1;\n     } else if (eq == 1) {\n       int eqent = w15skel[opos, 2];\n       Tau[g, vecpos, 1] = Tau_free[eqent];\n     }\n     opos += 1;\n   } else {\n     // fixed value\n     Tau[g, vecpos, 1] = Tau_un[g, vecpos, 1];\n   }\n   vecpos += 1;\n }\n      }\n    }\n  }\n  // prior vectors\n  if (wigind) {\n    lambda_y_primn = fill_prior(Lambda_y_free, lambda_y_mn, w1skel);\n    b_primn = fill_prior(B_free, b_mn, w4skel);\n    nu_primn = fill_prior(Nu_free, nu_mn, w13skel);\n    alpha_primn = fill_prior(Alpha_free, alpha_mn, w14skel);\n    tau_primn = fill_prior(Tau_ufree, tau_mn, w15skel);\n    lambda_y_primn_c = fill_prior(Lambda_y_free_c, lambda_y_mn_c, w1skel_c);\n    b_primn_c = fill_prior(B_free_c, b_mn_c, w4skel_c);\n    nu_primn_c = fill_prior(Nu_free_c, nu_mn_c, w13skel_c);\n    alpha_primn_c = fill_prior(Alpha_free_c, alpha_mn_c, w14skel_c);\n  } else {\n    lambda_y_primn = to_vector(lambda_y_mn);\n    b_primn = to_vector(b_mn);\n    nu_primn = to_vector(nu_mn);\n    alpha_primn = to_vector(alpha_mn);\n    tau_primn = to_vector(tau_mn);\n    lambda_y_primn_c = to_vector(lambda_y_mn_c);\n    b_primn_c = to_vector(b_mn_c);\n    nu_primn_c = to_vector(nu_mn_c);\n    alpha_primn_c = to_vector(alpha_mn_c);\n  }\n  // NB nothing below this will be used for two level, because we need other tricks to\n  //    compute the likelihood\n  // continuous responses underlying ordinal data\n  if (ord) {\n    int idxvec = 0;\n    for (patt in 1:Np) {\n      for (i in startrow[patt]:endrow[patt]) {\n for (j in 1:Nordobs[patt]) {\n   int obspos = OrdObsvar[patt,j];\n   int vecpos = YXo[i,obspos] - 1;\n   idxvec += 1;\n   if (obspos > 1) vecpos += sum(nlevs[1:(obspos - 1)]) - (obspos - 1);\n   if (YXo[i,obspos] == 1) {\n     YXostar[i,obspos] = -10 + (Tau[grpnum[patt], (vecpos + 1), 1] + 10) .* z_aug[idxvec];\n     tau_jacobian += log(abs(Tau[grpnum[patt], (vecpos + 1), 1] + 10)); // must add log(U) to tau_jacobian\n   } else if (YXo[i,obspos] == nlevs[obspos]) {\n     YXostar[i,obspos] = Tau[grpnum[patt], vecpos, 1] + (10 - Tau[grpnum[patt], vecpos, 1]) .* z_aug[idxvec];\n     tau_jacobian += log(abs(10 - Tau[grpnum[patt], vecpos, 1]));\n   } else {\n     YXostar[i,obspos] = Tau[grpnum[patt], vecpos, 1] + (Tau[grpnum[patt], (vecpos + 1), 1] - Tau[grpnum[patt], vecpos, 1]) .* z_aug[idxvec];\n     tau_jacobian += Tau_un[grpnum[patt], (vecpos + 1), 1]; // jacobian is log(exp(Tau_un))\n   }\n   YXstar[i, ordidx[obspos]] = YXostar[i, obspos];\n }\n      }\n    }\n  }\n  if (Ncont > 0) {\n    for (patt in 1:Np) {\n      for (i in startrow[patt]:endrow[patt]) {\n for (j in 1:Ncont) {\n   YXstar[i, contidx[j]] = YX[i,j];\n }\n      }\n    }\n  }\n  // move observations to the left\n  if (missing) {\n    for (patt in 1:Np) {\n      for (i in startrow[patt]:endrow[patt]) {\n for (j in 1:Nobs[patt]) {\n   YXstar[i,j] = YXstar[i, Obsvar[patt,j]];\n }\n      }\n    }\n  }\n  // for computing mvn with sufficient stats\n  if (!multilev) {\n    for (g in 1:Ng) {\n      Sigmainv_grp[g] = inverse_spd(Sigma[g]);\n      logdetSigma_grp[g] = log_determinant(Sigma[g]);\n    }\n    for (patt in 1:Np) {\n      Sigmainv[patt, 1:(Nobs[patt] + 1), 1:(Nobs[patt] + 1)] = sig_inv_update(Sigmainv_grp[grpnum[patt]], Obsvar[patt,], Nobs[patt], p + q, logdetSigma_grp[grpnum[patt]]);\n    }\n  }\n}\nmodel { // N.B.: things declared in the model block do not get saved in the output, which is okay here\n  /* transformed sd parameters for priors */\n  vector[len_free[5]] Theta_pri;\n  vector[len_free[9]] Psi_pri;\n  vector[len_free_c[5]] Theta_pri_c;\n  vector[len_free_c[9]] Psi_pri_c;\n  /* log-likelihood */\n  if (multilev && has_data) {\n    int grpidx;\n    int r1 = 1; // index clusters per group\n    int r2 = 0;\n    int rr1 = 1; // index units per group\n    int rr2 = 0;\n    int r3 = 1; // index unique cluster sizes per group\n    int r4 = 0;\n    for (mm in 1:Np) {\n      grpidx = grpnum[mm];\n      if (grpidx > 1) {\n r1 += nclus[(grpidx - 1), 2];\n rr1 += nclus[(grpidx - 1), 1];\n r3 += ncluster_sizes[(grpidx - 1)];\n      }\n      r2 += nclus[grpidx, 2];\n      rr2 += nclus[grpidx, 1];\n      r4 += ncluster_sizes[grpidx];\n      target += twolevel_logdens(mean_d[r3:r4], cov_d[r3:r4], S_PW[grpidx], YX[rr1:rr2],\n     nclus[grpidx,], cluster_size[r1:r2], cluster_sizes[r3:r4],\n     ncluster_sizes[grpidx], cluster_size_ns[r3:r4], Mu[grpidx],\n     Sigma[grpidx], Mu_c[grpidx], Sigma_c[grpidx],\n     ov_idx1, ov_idx2, within_idx, between_idx, both_idx,\n     p_tilde, N_within, N_between, N_both);\n      if (Nx[grpidx] + Nx_between[grpidx] > 0) target += -log_lik_x;\n    }\n  } else if (use_cov && !pri_only) {\n    for (g in 1:Ng) {\n      target += wishart_lpdf((N[g] - 1) * Sstar[g] | N[g] - 1, Sigma[g]);\n      if (Nx[g] > 0) {\n array[Nx[g]] int xvars = Xdatvar[g, 1:Nx[g]];\n target += -wishart_lpdf((N[g] - 1) * Sstar[g, xvars, xvars] | N[g] - 1, Sigma[g, xvars, xvars]);\n      }\n    }\n  } else if (has_data && !pri_only) {\n    array[p + q] int obsidx;\n    array[p + q] int xidx;\n    array[p + q] int xdatidx;\n    int grpidx;\n    int r1;\n    int r2;\n    for (mm in 1:Np) {\n      obsidx = Obsvar[mm,];\n      xidx = Xvar[mm,];\n      xdatidx = Xdatvar[mm,];\n      grpidx = grpnum[mm];\n      r1 = startrow[mm];\n      r2 = endrow[mm];\n      if (!use_suff) {\n target += multi_normal_lpdf(YXstar[r1:r2,1:Nobs[mm]] | Mu[grpidx, obsidx[1:Nobs[mm]]], Sigma[grpidx, obsidx[1:Nobs[mm]], obsidx[1:Nobs[mm]]]);\n if (Nx[mm] > 0) {\n   target += -multi_normal_lpdf(YXstar[r1:r2,xdatidx[1:Nx[mm]]] | Mu[grpidx, xidx[1:Nx[mm]]], Sigma[grpidx, xidx[1:Nx[mm]], xidx[1:Nx[mm]]]);\n }\n      } else {\n // sufficient stats\n target += multi_normal_suff(YXbarstar[mm, 1:Nobs[mm]], Sstar[mm, 1:Nobs[mm], 1:Nobs[mm]], Mu[grpidx, obsidx[1:Nobs[mm]]], Sigmainv[mm, 1:(Nobs[mm] + 1), 1:(Nobs[mm] + 1)], r2 - r1 + 1);\n if (Nx[mm] > 0) {\n   target += -multi_normal_suff(YXbarstar[mm, xdatidx[1:Nx[mm]]], Sstar[mm, xdatidx[1:Nx[mm]], xdatidx[1:Nx[mm]]], Mu[grpidx, xidx[1:Nx[mm]]], sig_inv_update(Sigmainv[grpidx], xidx, Nx[mm], p + q, logdetSigma_grp[grpidx]), r2 - r1 + 1);\n }\n      }\n    }\n    if (ord) {\n      target += tau_jacobian;\n    }\n  }\n  /* prior densities in log-units */\n  target += normal_lpdf(Lambda_y_free | lambda_y_primn, lambda_y_sd);\n  target += normal_lpdf(B_free | b_primn, b_sd);\n  target += normal_lpdf(Nu_free | nu_primn, nu_sd);\n  target += normal_lpdf(Alpha_free | alpha_primn, alpha_sd);\n  target += normal_lpdf(Tau_ufree | tau_primn, tau_sd);\n  target += normal_lpdf(Lambda_y_free_c | lambda_y_primn_c, lambda_y_sd_c);\n  target += normal_lpdf(B_free_c | b_primn_c, b_sd_c);\n  target += normal_lpdf(Nu_free_c | nu_primn_c, nu_sd_c);\n  target += normal_lpdf(Alpha_free_c | alpha_primn_c, alpha_sd_c);\n  /* transform sd parameters to var or prec, depending on\n     what the user wants. */\n  Theta_pri = Theta_sd_free;\n  if (len_free[5] > 0 && theta_pow != 1) {\n    for (i in 1:len_free[5]) {\n      Theta_pri[i] = Theta_sd_free[i]^(theta_pow);\n      target += log(abs(theta_pow)) + (theta_pow - 1)*log(Theta_sd_free[i]);\n    }\n  }\n  Psi_pri = Psi_sd_free;\n  if (len_free[9] > 0 && psi_pow != 1) {\n    for (i in 1:len_free[9]) {\n      Psi_pri[i] = Psi_sd_free[i]^(psi_pow);\n      target += log(abs(psi_pow)) + (psi_pow - 1)*log(Psi_sd_free[i]);\n    }\n  }\n  target += gamma_lpdf(Theta_pri | theta_sd_shape, theta_sd_rate);\n  target += gamma_lpdf(Psi_pri | psi_sd_shape, psi_sd_rate);\n  target += beta_lpdf(.5 * (1 + Theta_r_free) | theta_r_alpha, theta_r_beta) + log(.5) * len_free[7]; // the latter term is the jacobian moving from (-1,1) to (0,1), because beta_lpdf is defined on (0,1)\n  if (sum(nblk) > 0) {\n    for (k in 1:sum(nblk)) {\n      int blkidx = blkse[k, 6];\n      int arrayidx = blkse[k, 5];\n      if (arrayidx == 1) {\n target += lkj_corr_lpdf(Psi_r_mat_1[blkidx] | blkse[k,7]);\n      } else if (arrayidx == 2) {\n target += lkj_corr_lpdf(Psi_r_mat_2[blkidx] | blkse[k,7]);\n      } else if (arrayidx == 3) {\n target += lkj_corr_lpdf(Psi_r_mat_3[blkidx] | blkse[k,7]);\n      } else if (arrayidx == 4) {\n target += lkj_corr_lpdf(Psi_r_mat_4[blkidx] | blkse[k,7]);\n      } else {\n target += lkj_corr_lpdf(Psi_r_mat_5[blkidx] | blkse[k,7]);\n      }\n    }\n  }\n  if (len_free[10] > 0) {\n    target += beta_lpdf(.5 * (1 + Psi_r_free) | psi_r_alpha, psi_r_beta) + log(.5) * len_free[10];\n  }\n  // and the same for level 2\n  Theta_pri_c = Theta_sd_free_c;\n  if (len_free_c[5] > 0 && theta_pow_c != 1) {\n    for (i in 1:len_free_c[5]) {\n      Theta_pri_c[i] = Theta_sd_free_c[i]^(theta_pow_c);\n      target += log(abs(theta_pow_c)) + (theta_pow_c - 1)*log(Theta_sd_free_c[i]);\n    }\n  }\n  Psi_pri_c = Psi_sd_free_c;\n  if (len_free_c[9] > 0 && psi_pow_c != 1) {\n    for (i in 1:len_free_c[9]) {\n      Psi_pri_c[i] = Psi_sd_free_c[i]^(psi_pow_c);\n      target += log(abs(psi_pow_c)) + (psi_pow_c - 1)*log(Psi_sd_free_c[i]);\n    }\n  }\n  target += gamma_lpdf(Theta_pri_c | theta_sd_shape_c, theta_sd_rate_c);\n  target += gamma_lpdf(Psi_pri_c | psi_sd_shape_c, psi_sd_rate_c);\n  target += beta_lpdf(.5 * (1 + Theta_r_free_c) | theta_r_alpha_c, theta_r_beta_c) + log(.5) * len_free_c[7];\n  if (sum(nblk_c) > 0) {\n    for (k in 1:sum(nblk_c)) {\n      int blkidx = blkse_c[k, 6];\n      int arrayidx = blkse_c[k, 5];\n      if (arrayidx == 1) {\n target += lkj_corr_lpdf(Psi_r_mat_1_c[blkidx] | blkse_c[k,7]);\n      } else if (arrayidx == 2) {\n target += lkj_corr_lpdf(Psi_r_mat_2_c[blkidx] | blkse_c[k,7]);\n      } else if (arrayidx == 3) {\n target += lkj_corr_lpdf(Psi_r_mat_3_c[blkidx] | blkse_c[k,7]);\n      } else if (arrayidx == 4) {\n target += lkj_corr_lpdf(Psi_r_mat_4_c[blkidx] | blkse_c[k,7]);\n      } else {\n target += lkj_corr_lpdf(Psi_r_mat_5_c[blkidx] | blkse_c[k,7]);\n      }\n    }\n  } else if (len_free_c[10] > 0) {\n    target += beta_lpdf(.5 * (1 + Psi_r_free_c) | psi_r_alpha_c, psi_r_beta_c) + log(.5) * len_free_c[10];\n  }\n}\ngenerated quantities { // these matrices are saved in the output but do not figure into the likelihood\n  // see https://books.google.com/books?id=9AC-s50RjacC&lpg=PP1&dq=LISREL&pg=PA34#v=onepage&q=LISREL&f=false\n  // sign constraints and correlations\n  vector[len_free[1]] ly_sign;\n  vector[len_free[4]] bet_sign;\n  array[Ng] matrix[m, m] PSmat;\n  array[Ng] matrix[m, m] PS;\n  vector[len_free[7]] Theta_cov;\n  vector[len_free[5]] Theta_var;\n  vector[len_free[10]] P_r;\n  vector[len_free[11]] Psi_cov;\n  vector[len_free[9]] Psi_var;\n  // level 2\n  vector[len_free_c[1]] ly_sign_c;\n  vector[len_free_c[4]] bet_sign_c;\n  array[Ng] matrix[m_c, m_c] PSmat_c;\n  array[Ng] matrix[m_c, m_c] PS_c;\n  vector[len_free_c[7]] Theta_cov_c;\n  vector[len_free_c[5]] Theta_var_c;\n  vector[len_free_c[10]] P_r_c;\n  vector[len_free_c[11]] Psi_cov_c;\n  vector[len_free_c[9]] Psi_var_c;\n  // loglik + ppp\n  vector[multilev ? sum(nclus[,2]) : (use_cov ? Ng : Ntot)] log_lik; // for loo, etc\n  vector[multilev ? sum(nclus[,2]) : (use_cov ? Ng : Ntot)] log_lik_sat; // for ppp\n  array[Ntot] vector[multilev ? p_tilde : p + q] YXstar_rep; // artificial data\n  vector[multilev ? sum(nclus[,2]) : (use_cov ? Ng : Ntot)] log_lik_rep; // for loo, etc\n  vector[multilev ? sum(nclus[,2]) : (use_cov ? Ng : Ntot)] log_lik_rep_sat; // for ppp\n  array[Ng] matrix[p + q, p + q + 1] satout;\n  array[Ng] matrix[p + q, p + q + 1] satrep_out;\n  array[Ng] vector[p + q] Mu_sat;\n  array[Ng] matrix[p + q, p + q] Sigma_sat;\n  array[Ng] matrix[p + q, p + q] Sigma_sat_inv_grp;\n  array[Ng] real logdetS_sat_grp;\n  array[Np] matrix[p + q + 1, p + q + 1] Sigma_sat_inv;\n  array[Ng] vector[p + q] Mu_rep_sat;\n  array[Ng] matrix[p + q, p + q] Sigma_rep_sat;\n  array[Ng] matrix[p + q, p + q] Sigma_rep_sat_inv_grp;\n  array[Np] matrix[p + q + 1, p + q + 1] Sigma_rep_sat_inv;\n  array[Ng] real logdetS_rep_sat_grp;\n  matrix[p + q, p + q] zmat;\n  array[sum(nclus[,2])] vector[p_tilde] mean_d_rep;\n  vector[multilev ? sum(nclus[,2]) : Ng] log_lik_x_rep;\n  array[Ng] matrix[N_both + N_within, N_both + N_within] S_PW_rep;\n  array[Ng] matrix[p_tilde, p_tilde] S_PW_rep_full;\n  array[Ng] vector[p_tilde] ov_mean_rep;\n  array[Ng] vector[p_tilde] xbar_b_rep;\n  array[Ng] matrix[N_between, N_between] S2_rep;\n  array[Ng] matrix[p_tilde, p_tilde] S_B_rep;\n  array[Ng] matrix[p_tilde, p_tilde] cov_b_rep;\n  real<lower=0, upper=1> ppp;\n  // first deal with sign constraints:\n  ly_sign = sign_constrain_load(Lambda_y_free, len_free[1], lam_y_sign);\n  bet_sign = sign_constrain_reg(B_free, len_free[4], b_sign, Lambda_y_free, Lambda_y_free);\n  if (len_free[10] > 0) {\n    P_r = sign_constrain_reg(Psi_r_free, len_free[10], psi_r_sign, Lambda_y_free, Lambda_y_free);\n  }\n  ly_sign_c = sign_constrain_load(Lambda_y_free_c, len_free_c[1], lam_y_sign_c);\n  bet_sign_c = sign_constrain_reg(B_free_c, len_free_c[4], b_sign_c, Lambda_y_free_c, Lambda_y_free_c);\n  if (len_free_c[10] > 0) {\n    P_r_c = sign_constrain_reg(Psi_r_free_c, len_free_c[10], psi_r_sign_c, Lambda_y_free_c, Lambda_y_free_c);\n  }\n  for (g in 1:Ng) {\n    if (m > 0) {\n      PSmat[g] = fill_matrix(P_r, Psi_r_skeleton[g], w10skel, g_start10[g,1], g_start10[g,2]) + transpose(fill_matrix(P_r, Psi_r_skeleton[g], w10skel, g_start10[g,1], g_start10[g,2])) - diag_matrix(rep_vector(1, m));\n    }\n    if (m_c > 0) {\n      PSmat_c[g] = fill_matrix(P_r_c, Psi_r_skeleton_c[g], w10skel_c, g_start10_c[g,1], g_start10_c[g,2]) + transpose(fill_matrix(P_r_c, Psi_r_skeleton_c[g], w10skel_c, g_start10_c[g,1], g_start10_c[g,2])) - diag_matrix(rep_vector(1, m_c));\n    }\n  }\n  if (sum(nblk) > 0) {\n    PSmat = fill_cov(PSmat, blkse, nblk, Psi_r_mat_1, Psi_r_mat_2, Psi_r_mat_3, Psi_r_mat_4, Psi_r_mat_5);\n  }\n  if (sum(nblk_c) > 0) {\n    PSmat_c = fill_cov(PSmat_c, blkse_c, nblk_c, Psi_r_mat_1_c, Psi_r_mat_2_c, Psi_r_mat_3_c, Psi_r_mat_4_c, Psi_r_mat_5_c);\n  }\n  for (g in 1:Ng) {\n    PS[g] = quad_form_sym(PSmat[g], Psi_sd[g]);\n    PS_c[g] = quad_form_sym(PSmat_c[g], Psi_sd_c[g]);\n  }\n  // off-diagonal covariance parameter vectors, from cor/sd matrices:\n  Theta_cov = cor2cov(Theta_r, Theta_sd, num_elements(Theta_r_free), Theta_r_skeleton, w7skel, Ng);\n  Theta_var = Theta_sd_free .* Theta_sd_free;\n  if (m > 0 && len_free[11] > 0) {\n    /* iden is created so that we can re-use cor2cov, even though\n       we don't need to multiply to get covariances */\n    array[Ng] matrix[m, m] iden;\n    for (g in 1:Ng) {\n      iden[g] = diag_matrix(rep_vector(1, m));\n    }\n    Psi_cov = cor2cov(PS, iden, len_free[11], Psi_r_skeleton_f, w11skel, Ng);\n  } else {\n    Psi_cov = P_r;\n  }\n  Psi_var = Psi_sd_free .* Psi_sd_free;\n  // and for level 2\n  Theta_cov_c = cor2cov(Theta_r_c, Theta_sd_c, num_elements(Theta_r_free_c), Theta_r_skeleton_c, w7skel_c, Ng);\n  Theta_var_c = Theta_sd_free_c .* Theta_sd_free_c;\n  if (m_c > 0 && len_free_c[11] > 0) {\n    array[Ng] matrix[m_c, m_c] iden_c;\n    for (g in 1:Ng) {\n      iden_c[g] = diag_matrix(rep_vector(1, m_c));\n    }\n    Psi_cov_c = cor2cov(PS_c, iden_c, len_free_c[11], Psi_r_skeleton_f_c, w11skel_c, Ng);\n  } else {\n    Psi_cov_c = P_r_c;\n  }\n  Psi_var_c = Psi_sd_free_c .* Psi_sd_free_c;\n  { // log-likelihood\n    array[p + q] int obsidx;\n    array[p + q] int xidx;\n    array[p + q] int xdatidx;\n    int r1;\n    int r2;\n    int r3;\n    int r4;\n    int rr1;\n    int rr2;\n    int grpidx;\n    int clusidx;\n    if (do_test && use_cov) {\n      for (g in 1:Ng) {\n Sigma_rep_sat[g] = wishart_rng(N[g] - 1, Sigma[g]);\n      }\n    } else if (do_test && has_data) {\n      // generate level 2 data, then level 1\n      if (multilev) {\n array[p_tilde - N_between] int notbidx;\n notbidx = between_idx[(N_between + 1):p_tilde];\n r1 = 1;\n rr1 = 1;\n clusidx = 1;\n r2 = 1;\n for (gg in 1:Ng) {\n   matrix[p_c, p_c] Sigma_c_chol = cholesky_decompose(Sigma_c[gg]);\n   matrix[p + q, p + q] Sigma_chol = cholesky_decompose(Sigma[gg]);\n   S_PW_rep[gg] = rep_matrix(0, N_both + N_within, N_both + N_within);\n   S_PW_rep_full[gg] = rep_matrix(0, p_tilde, p_tilde);\n   S_B_rep[gg] = rep_matrix(0, p_tilde, p_tilde);\n   ov_mean_rep[gg] = rep_vector(0, p_tilde);\n   for (cc in 1:nclus[gg, 2]) {\n     vector[p_c] YXstar_rep_c;\n     vector[p_tilde] YXstar_rep_tilde;\n     YXstar_rep_c = multi_normal_cholesky_rng(Mu_c[gg], Sigma_c_chol);\n     YXstar_rep_tilde = calc_B_tilde(Sigma_c[gg], YXstar_rep_c, ov_idx2, p_tilde)[,1];\n     for (ii in r1:(r1 + cluster_size[clusidx] - 1)) {\n       vector[N_within + N_both] Ywb_rep;\n       Ywb_rep = multi_normal_cholesky_rng(Mu[gg], Sigma_chol);\n       YXstar_rep[ii] = YXstar_rep_tilde;\n       for (ww in 1:(p_tilde - N_between)) {\n  YXstar_rep[ii, notbidx[ww]] += Ywb_rep[ww];\n       }\n       ov_mean_rep[gg] += YXstar_rep[ii];\n     }\n     for (jj in 1:p_tilde) {\n       mean_d_rep[clusidx, jj] = mean(YXstar_rep[r1:(r1 + cluster_size[clusidx] - 1), jj]);\n            }\n     r1 += cluster_size[clusidx];\n     clusidx += 1;\n   } // cc\n   ov_mean_rep[gg] *= pow(nclus[gg, 1], -1);\n   xbar_b_rep[gg] = ov_mean_rep[gg];\n   r1 -= nclus[gg, 1]; // reset for S_PW\n   clusidx -= nclus[gg, 2];\n   if (N_between > 0) {\n     S2_rep[gg] = rep_matrix(0, N_between, N_between);\n     for (ii in 1:N_between) {\n       xbar_b_rep[gg, between_idx[ii]] = mean(mean_d_rep[clusidx:(clusidx + nclus[gg, 2] - 1), between_idx[ii]]);\n     }\n   }\n   for (cc in 1:nclus[gg, 2]) {\n     for (ii in r1:(r1 + cluster_size[clusidx] - 1)) {\n       S_PW_rep_full[gg] += tcrossprod(to_matrix(YXstar_rep[ii] - mean_d_rep[clusidx]));\n     }\n     S_B_rep[gg] += cluster_size[clusidx] * tcrossprod(to_matrix(mean_d_rep[clusidx] - ov_mean_rep[gg]));\n     if (N_between > 0) {\n       S2_rep[gg] += tcrossprod(to_matrix(mean_d_rep[clusidx, between_idx[1:N_between]] - xbar_b_rep[gg, between_idx[1:N_between]]));\n     }\n     r1 += cluster_size[clusidx];\n     clusidx += 1;\n   }\n   S_PW_rep_full[gg] *= pow(nclus[gg, 1] - nclus[gg, 2], -1);\n   S_B_rep[gg] *= pow(nclus[gg, 2] - 1, -1);\n   S2_rep[gg] *= pow(nclus[gg, 2], -1);\n   // mods to between-only variables:\n   if (N_between > 0) {\n     array[N_between] int betonly = between_idx[1:N_between];\n     S_PW_rep_full[gg, betonly, betonly] = rep_matrix(0, N_between, N_between);\n     // Y2: mean_d_rep; Y2c: mean_d_rep - ov_mean_rep\n     for (ii in 1:N_between) {\n       for (jj in 1:(N_both + N_within)) {\n  S_B_rep[gg, between_idx[ii], between_idx[(N_between + jj)]] *= (gs[gg] * nclus[gg, 2] * pow(nclus[gg, 1], -1));\n  S_B_rep[gg, between_idx[(N_between + jj)], between_idx[ii]] = S_B_rep[gg, between_idx[ii], between_idx[(N_between + jj)]];\n       }\n     }\n     S_B_rep[gg, betonly, betonly] = rep_matrix(0, N_between, N_between);\n     for (cc in 1:nclus[gg, 2]) {\n       S_B_rep[gg, betonly, betonly] += tcrossprod(to_matrix(mean_d_rep[cc, betonly] - ov_mean_rep[gg, betonly]));\n     }\n     S_B_rep[gg, betonly, betonly] *= gs[gg] * pow(nclus[gg, 2], -1);\n   }\n   cov_b_rep[gg] = pow(gs[gg], -1) * (S_B_rep[gg] - S_PW_rep_full[gg]);\n   if (N_between > 0) {\n     cov_b_rep[gg, between_idx[1:N_between], between_idx[1:N_between]] = S2_rep[gg];\n   }\n   rr1 = r1 - nclus[gg, 1];\n   r2 = clusidx - nclus[gg, 2];\n   Mu_rep_sat[gg] = rep_vector(0, N_within + N_both);\n   if (N_within > 0) {\n     for (j in 1:N_within) {\n       xbar_b_rep[gg, within_idx[j]] = 0;\n       Mu_rep_sat[gg, within_idx[j]] = ov_mean_rep[gg, within_idx[j]];\n     }\n   }\n   S_PW_rep[gg] = S_PW_rep_full[gg, notbidx, notbidx];\n   if (Nx[gg] > 0 || Nx_between[gg] > 0) {\n     array[2] vector[p_tilde] mnvecs;\n     array[3] matrix[p_tilde, p_tilde] covmats;\n     mnvecs = calc_mean_vecs(YXstar_rep[rr1:(r1 - 1)], mean_d_rep[r2:(clusidx - 1)], nclus[gg], Xvar[gg], Xbetvar[gg], Nx[gg], Nx_between[gg], p_tilde);\n     covmats = calc_cov_mats(YXstar_rep[rr1:(r1 - 1)], mean_d_rep[r2:(clusidx - 1)], mnvecs, nclus[gg], Xvar[gg], Xbetvar[gg], Nx[gg], Nx_between[gg], p_tilde);\n     log_lik_x_rep[r2:(clusidx - 1)] = calc_log_lik_x(mean_d_rep[r2:(clusidx - 1)],\n            mnvecs[2], covmats[1],\n            covmats[2], covmats[3],\n            nclus[gg], cluster_size[r2:(clusidx - 1)],\n            Xvar[gg], Xbetvar[gg], Nx[gg], Nx_between[gg]);\n   } // Nx[gg] > 0\n } // gg\n      } else {\n for (mm in 1:Np) {\n   matrix[Nobs[mm], Nobs[mm]] Sigma_chol;\n   obsidx = Obsvar[mm,];\n   xidx = Xvar[mm,];\n   xdatidx = Xdatvar[mm,];\n   grpidx = grpnum[mm];\n   r1 = startrow[mm];\n   r2 = endrow[mm];\n   Sigma_chol = cholesky_decompose(Sigma[ grpidx, obsidx[1:Nobs[mm]], obsidx[1:Nobs[mm]] ]);\n   for (jj in r1:r2) {\n     YXstar_rep[jj, 1:Nobs[mm]] = multi_normal_cholesky_rng(Mu[grpidx, obsidx[1:Nobs[mm]]], Sigma_chol);\n   }\n }\n if (missing) {\n   // start values for Mu and Sigma\n   for (g in 1:Ng) {\n     Mu_sat[g] = rep_vector(0, p + q);\n     Mu_rep_sat[g] = Mu_sat[g];\n     Sigma_sat[g] = diag_matrix(rep_vector(1, p + q));\n     Sigma_rep_sat[g] = Sigma_sat[g];\n   }\n   for (jj in 1:emiter) {\n     satout = estep(YXstar, Mu_sat, Sigma_sat, Nobs, Obsvar, startrow, endrow, grpnum, Np, Ng);\n     satrep_out = estep(YXstar_rep, Mu_rep_sat, Sigma_rep_sat, Nobs, Obsvar, startrow, endrow, grpnum, Np, Ng);\n     // M step\n     for (g in 1:Ng) {\n       Mu_sat[g] = satout[g,,1]/N[g];\n       Sigma_sat[g] = satout[g,,2:(p + q + 1)]/N[g] - Mu_sat[g] * Mu_sat[g]';\n       Mu_rep_sat[g] = satrep_out[g,,1]/N[g];\n       Sigma_rep_sat[g] = satrep_out[g,,2:(p + q + 1)]/N[g] - Mu_rep_sat[g] * Mu_rep_sat[g]';\n     }\n   }\n } else {\n   // complete data; Np patterns must only correspond to groups\n   for (mm in 1:Np) {\n     array[3] int arr_dims = dims(YXstar);\n     matrix[endrow[mm] - startrow[mm] + 1, arr_dims[2]] YXsmat; // crossprod needs matrix\n     matrix[endrow[mm] - startrow[mm] + 1, arr_dims[2]] YXsrepmat;\n     r1 = startrow[mm];\n     r2 = endrow[mm];\n     grpidx = grpnum[mm];\n     for (jj in 1:(p + q)) {\n       Mu_sat[grpidx,jj] = mean(YXstar[r1:r2,jj]);\n       Mu_rep_sat[grpidx,jj] = mean(YXstar_rep[r1:r2,jj]);\n     }\n     for (jj in r1:r2) {\n       YXsmat[jj - r1 + 1] = (YXstar[jj] - Mu_sat[grpidx])';\n       YXsrepmat[jj - r1 + 1] = (YXstar_rep[jj] - Mu_rep_sat[grpidx])';\n     }\n     Sigma_sat[grpidx] = crossprod(YXsmat)/N[grpidx];\n     Sigma_rep_sat[grpidx] = crossprod(YXsrepmat)/N[grpidx];\n     // FIXME? Sigma_sat[grpidx] = tcrossprod(YXsmat); does not throw an error??\n   }\n }\n for (g in 1:Ng) {\n   Sigma_sat_inv_grp[g] = inverse_spd(Sigma_sat[g]);\n   logdetS_sat_grp[g] = log_determinant(Sigma_sat[g]);\n   Sigma_rep_sat_inv_grp[g] = inverse_spd(Sigma_rep_sat[g]);\n   logdetS_rep_sat_grp[g] = log_determinant(Sigma_rep_sat[g]);\n }\n for (mm in 1:Np) {\n   Sigma_sat_inv[mm, 1:(Nobs[mm] + 1), 1:(Nobs[mm] + 1)] = sig_inv_update(Sigma_sat_inv_grp[grpnum[mm]], Obsvar[mm,], Nobs[mm], p + q, logdetS_sat_grp[grpnum[mm]]);\n   Sigma_rep_sat_inv[mm, 1:(Nobs[mm] + 1), 1:(Nobs[mm] + 1)] = sig_inv_update(Sigma_rep_sat_inv_grp[grpnum[mm]], Obsvar[mm,], Nobs[mm], p + q, logdetS_rep_sat_grp[grpnum[mm]]);\n }\n      }\n    }\n    // compute log-likelihoods\n    if (multilev) { // multilevel\n      r1 = 1;\n      r3 = 1;\n      r2 = 0;\n      r4 = 0;\n      for (mm in 1:Np) {\n grpidx = grpnum[mm];\n if (grpidx > 1) {\n   r1 += nclus[(grpidx - 1), 2];\n   r3 += nclus[(grpidx - 1), 1];\n }\n r2 += nclus[grpidx, 2];\n r4 += nclus[grpidx, 1];\n log_lik[r1:r2] = twolevel_logdens(mean_d_full[r1:r2], cov_d_full[r1:r2], S_PW[grpidx], YX[r3:r4],\n       nclus[grpidx,], cluster_size[r1:r2], cluster_size[r1:r2],\n       nclus[grpidx,2], intone[1:nclus[grpidx,2]], Mu[grpidx],\n       Sigma[grpidx], Mu_c[grpidx], Sigma_c[grpidx],\n       ov_idx1, ov_idx2, within_idx, between_idx, both_idx,\n       p_tilde, N_within, N_between, N_both);\n if (Nx[grpidx] + Nx_between[grpidx] > 0) log_lik[r1:r2] -= log_lik_x_full[r1:r2];\n      }\n    }\n    zmat = rep_matrix(0, p + q, p + q);\n    // reset for 2-level loglik:\n    rr1 = 1;\n    r3 = 1;\n    rr2 = 0;\n    r4 = 0;\n    for (mm in 1:Np) {\n      obsidx = Obsvar[mm,];\n      xidx = Xvar[mm, 1:(p + q)];\n      xdatidx = Xdatvar[mm, 1:(p + q)];\n      grpidx = grpnum[mm];\n      r1 = startrow[mm];\n      r2 = endrow[mm];\n      if (use_cov) {\n log_lik[mm] = wishart_lpdf((N[mm] - 1) * Sstar[mm] | N[mm] - 1, Sigma[mm]);\n if (do_test) {\n   log_lik_sat[mm] = -log_lik[mm] + wishart_lpdf((N[mm] - 1) * Sstar[mm] | N[mm] - 1, Sstar[mm]);\n   log_lik_rep[mm] = wishart_lpdf(Sigma_rep_sat[mm] | N[mm] - 1, Sigma[mm]);\n   log_lik_rep_sat[mm] = wishart_lpdf(Sigma_rep_sat[mm] | N[mm] - 1, pow(N[mm] - 1, -1) * Sigma_rep_sat[mm]);\n }\n if (Nx[mm] > 0) {\n   array[Nx[mm]] int xvars = xdatidx[1:Nx[mm]];\n   log_lik[mm] += -wishart_lpdf((N[mm] - 1) * Sstar[mm, xvars, xvars] | N[mm] - 1, Sigma[mm, xvars, xvars]);\n   if (do_test) {\n     log_lik_sat[mm] += wishart_lpdf((N[mm] - 1) * Sstar[mm, xvars, xvars] | N[mm] - 1, Sigma[mm, xvars, xvars]);\n     log_lik_sat[mm] += -wishart_lpdf((N[mm] - 1) * Sstar[mm, xvars, xvars] | N[mm] - 1, Sstar[mm, xvars, xvars]);\n     log_lik_rep[mm] += -wishart_lpdf(Sigma_rep_sat[mm, xvars, xvars] | N[mm] - 1, Sigma[mm, xvars, xvars]);\n     log_lik_rep_sat[mm] += -wishart_lpdf(Sigma_rep_sat[mm, xvars, xvars] | N[mm] - 1, pow(N[mm] - 1, -1) * Sigma_rep_sat[mm, xvars, xvars]);\n   }\n }\n      } else if (has_data && !multilev) {\n for (jj in r1:r2) {\n   log_lik[jj] = multi_normal_suff(YXstar[jj, 1:Nobs[mm]], zmat[1:Nobs[mm], 1:Nobs[mm]], Mu[grpidx, obsidx[1:Nobs[mm]]], Sigmainv[mm], 1);\n   if (Nx[mm] > 0) {\n     log_lik[jj] += -multi_normal_suff(YXstar[jj, xdatidx[1:Nx[mm]]], zmat[1:Nx[mm], 1:Nx[mm]], Mu[grpidx, xidx[1:Nx[mm]]], sig_inv_update(Sigmainv[grpidx], xidx, Nx[mm], p + q, logdetSigma_grp[grpidx]), 1);\n   }\n }\n      }\n      // saturated and y_rep likelihoods for ppp\n      if (do_test) {\n if (multilev) {\n   // compute clusterwise log_lik_rep for grpidx\n   if (grpidx > 1) {\n     rr1 += nclus[(grpidx - 1), 2];\n     r3 += nclus[(grpidx - 1), 1];\n   }\n   rr2 += nclus[grpidx, 2];\n   r4 += nclus[grpidx, 1];\n   // NB: cov_d is 0 when we go cluster by cluster.\n   // otherwise it is covariance of cluster means by each unique cluster size\n   // because we go cluster by cluster here, we can reuse cov_d_full everywhere\n   log_lik_rep[rr1:rr2] = twolevel_logdens(mean_d_rep[rr1:rr2], cov_d_full[rr1:rr2],\n        S_PW_rep[grpidx], YXstar_rep[r3:r4],\n        nclus[grpidx,], cluster_size[rr1:rr2],\n        cluster_size[rr1:rr2], nclus[grpidx,2],\n        intone[1:nclus[grpidx,2]], Mu[grpidx],\n        Sigma[grpidx], Mu_c[grpidx], Sigma_c[grpidx],\n        ov_idx1, ov_idx2, within_idx, between_idx,\n        both_idx, p_tilde, N_within, N_between, N_both);\n   log_lik_sat[rr1:rr2] = twolevel_logdens(mean_d_full[rr1:rr2], cov_d_full[rr1:rr2],\n        S_PW[grpidx], YX[r3:r4],\n        nclus[grpidx,], cluster_size[rr1:rr2],\n        cluster_size[rr1:rr2], nclus[grpidx,2],\n        intone[1:nclus[grpidx,2]], xbar_w[grpidx, ov_idx1],\n        S_PW[grpidx], xbar_b[grpidx, ov_idx2], cov_b[grpidx, ov_idx2, ov_idx2],\n        ov_idx1, ov_idx2, within_idx, between_idx,\n        both_idx, p_tilde, N_within, N_between, N_both);\n   log_lik_rep_sat[rr1:rr2] = twolevel_logdens(mean_d_rep[rr1:rr2], cov_d_full[rr1:rr2],\n            S_PW_rep[grpidx], YXstar_rep[r3:r4],\n            nclus[grpidx,], cluster_size[rr1:rr2],\n            cluster_size[rr1:rr2], nclus[grpidx,2],\n            intone[1:nclus[grpidx,2]], Mu_rep_sat[grpidx],\n            S_PW_rep[grpidx], xbar_b_rep[grpidx, ov_idx2],\n            cov_b_rep[grpidx, ov_idx2, ov_idx2],\n            ov_idx1, ov_idx2,\n            within_idx, between_idx, both_idx, p_tilde,\n            N_within, N_between, N_both);\n   if (Nx[grpidx] + Nx_between[grpidx] > 0) {\n     log_lik_rep[rr1:rr2] -= log_lik_x_rep[rr1:rr2];\n     log_lik_sat[rr1:rr2] -= log_lik_x_full[rr1:rr2];\n     log_lik_rep_sat[rr1:rr2] -= log_lik_x_rep[rr1:rr2];\n   }\n   // we subtract log_lik here so that _sat always varies and does not lead to\n   // problems with rhat and neff computations\n   log_lik_sat[rr1:rr2] -= log_lik[rr1:rr2];\n } else if (!use_cov) {\n   r1 = startrow[mm];\n   r2 = endrow[mm];\n   for (jj in r1:r2) {\n     log_lik_rep[jj] = multi_normal_suff(YXstar_rep[jj, 1:Nobs[mm]], zmat[1:Nobs[mm], 1:Nobs[mm]], Mu[grpidx, obsidx[1:Nobs[mm]]], Sigmainv[mm], 1);\n     log_lik_sat[jj] = multi_normal_suff(YXstar[jj, 1:Nobs[mm]], zmat[1:Nobs[mm], 1:Nobs[mm]], Mu_sat[grpidx, obsidx[1:Nobs[mm]]], Sigma_sat_inv[mm], 1);\n     log_lik_rep_sat[jj] = multi_normal_suff(YXstar_rep[jj, 1:Nobs[mm]], zmat[1:Nobs[mm], 1:Nobs[mm]], Mu_rep_sat[grpidx, obsidx[1:Nobs[mm]]], Sigma_rep_sat_inv[mm], 1);\n     // log_lik_sat, log_lik_sat_rep\n     if (Nx[mm] > 0) {\n       log_lik_rep[jj] += -multi_normal_suff(YXstar_rep[jj, xdatidx[1:Nx[mm]]], zmat[1:Nx[mm], 1:Nx[mm]], Mu[grpidx, xidx[1:Nx[mm]]], sig_inv_update(Sigmainv[grpidx], xidx, Nx[mm], p + q, logdetSigma_grp[grpidx]), 1);\n       log_lik_sat[jj] += -multi_normal_suff(YXstar[jj, xdatidx[1:Nx[mm]]], zmat[1:Nx[mm], 1:Nx[mm]], Mu_sat[grpidx, xidx[1:Nx[mm]]], sig_inv_update(Sigma_sat_inv[grpidx], xidx, Nx[mm], p + q, logdetS_sat_grp[grpidx]), 1);\n       log_lik_rep_sat[jj] += -multi_normal_suff(YXstar_rep[jj, xdatidx[1:Nx[mm]]], zmat[1:Nx[mm], 1:Nx[mm]], Mu_rep_sat[grpidx, xidx[1:Nx[mm]]], sig_inv_update(Sigma_rep_sat_inv[grpidx], xidx, Nx[mm], p + q, logdetS_rep_sat_grp[grpidx]), 1);\n     }\n   }\n   // we subtract log_lik here so that _sat always varies and does not lead to\n   // problems with rhat and neff computations\n   log_lik_sat[r1:r2] -= log_lik[r1:r2];\n }\n      }\n    }\n    if (do_test) {\n      ppp = step((-sum(log_lik_rep) + sum(log_lik_rep_sat)) - (sum(log_lik_sat)));\n    } else {\n      ppp = 0;\n    }\n  }\n} // end with a completely blank line (not even whitespace)\n```\n:::\n\n\n\n\n## Wrapping up\n\nToday, we showed how to model observed data using a normal distribution\n\n-   Assumptions of Confirmatory Factor Analysis\n\n    -   Not appropriate for our data\n\n    -   May not be appropriate for many data sets\n\n-   We will have to keep our loading/discrimination parameters positive to ensure converges to the same posterior mode\n\n    -   This will continue through the next types of data\n\n## Next Class\n\n-   Next up, categorical distributions for observed data\n\n    -   More appropriate for these data as they are discrete categorical responses\n\n## Resources\n\n-   [Dr. Templin's slide](https://jonathantemplin.github.io/Bayesian-Psychometric-Modeling-Course-Fall2022/lectures/lecture04b/04b_Modeling_Observed_Data#/title-slide)\n",
    "supporting": [
      "Lecture08_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}