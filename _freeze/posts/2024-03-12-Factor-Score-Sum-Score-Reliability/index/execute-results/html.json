{
  "hash": "efd9577d0d28843c826c9b3eeef45bb8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Sum Score, Factor Score, and Reliability'\nauthor: 'Jihong Zhang'\ndate: 'Mar 12 2024'\ncategories:\n  - Scoring\n  - Reliability\n  - CTT\n  - Factor Analysis\nexecute: \n  warning: false\n  message: false\nformat: \n  html:\n    code-fold: false\n    code-line-numbers: false\nbibliography: references.bib\n---\n\n\n\nOverall, the purpose of this post is to illustrate the definition, assumption, psychometric properties, usage, and interpretation of factor scores in Structural Equation Models (SEM), and how those features of factor scores compare to sum scores in Classical Test Theory (CTT).\n\nThis post is draft by Dr. Jihong Zhang and was inspired by Dr. Templin's [2022 presentation](https://jonathantemplin.com/wp-content/uploads/2022/10/sem15pre906_lecture11.pdf).\n\n## Definition of Test Score\n\nIn CTT, the test score is unit of analysis of the whole test, which can be statistically expressed as:\n\n$$\nY_{total} = T+e\n$$\n\nWhere $Y_{total}$ denotes test scores, $T$ denotes true score, and $e$ denotes error. There are some assumptions:\n\n1.  Items are assumed exchangeable;\n2.  Expected value of $e$ is 0;\n3.  Error $e$ is expected to be uncorrelated with true score $T$\n\n## Reliability of Test Score\n\n***Reliability*** is computed as the proportion of variance in the sum score that is due to variation in the latent trait or true score.\n\n### CTT-based reliability\n\nWe can derive the reliability as following:\n\n$$\n\\text{Var}(Y_{total}) = \\text{Var}(T+e) = \\text{Var}(T)+\\text{Var}(e)+2\\text{Cov}(T,e)\n$$\n\nBut, since T and e are assumed independent $\\text{Cov}(T, e) = 0$, so,\n\n$$\n\\text{Var}(Y_{total}) = \\text{Var}(T)+\\text{Var}(e)\n$$\n\nThen, reliability can be computed as:\n\n$$\n\\rho = \\frac{\\text{Var}(T)}{\\text{Var}(Y)} = \\frac{\\text{Var}(T)}{\\text{Var}(T)+\\text{Var}(e)} \n$$\n\nWhere,\n\n-   $\\text{Var}(T)$ is variance of true score and could be interpreted as variability in the latent trait in the context of factor analysis.\n\n-   $\\text{Var}(e)$ is variance of error, and could be interpreted as measurement error\n\n#### Mini example\n\nUsing a 10-item example with each having 1-5 scale.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(kableExtra)\nlibrary(psych)\ndat_path <- 'posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Lecture07/Code'\nconspiracy <- read.csv(here(dat_path, 'conspiracies.csv'))\nitemResp <- conspiracy |> select(starts_with('PolConsp'))\nconspiracy |> \n  mutate(ID = 1:177) |> \n  pivot_longer(starts_with('PolConsp'), names_to = 'Item', values_to = 'Resp') |> \n  mutate(Item = factor(Item, paste0('PolConsp', 1:10))) |> \n  group_by(Item) |> \n  summarise(\n    Mean = mean(Resp),\n    SD = sd(Resp),\n    Min = min(Resp),\n    Max = max(Resp),\n    Skew = psych::skew(Resp)\n  ) |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Item       |  Mean|    SD| Min| Max|  Skew|\n|:----------|-----:|-----:|---:|---:|-----:|\n|PolConsp1  | 2.367| 1.136|   1|   5| 0.432|\n|PolConsp2  | 1.955| 1.112|   1|   5| 1.050|\n|PolConsp3  | 1.876| 1.096|   1|   5| 1.019|\n|PolConsp4  | 2.011| 1.108|   1|   5| 0.925|\n|PolConsp5  | 1.983| 1.105|   1|   5| 0.962|\n|PolConsp6  | 1.893| 1.003|   1|   5| 0.887|\n|PolConsp7  | 1.723| 1.004|   1|   5| 1.340|\n|PolConsp8  | 1.842| 0.952|   1|   5| 0.788|\n|PolConsp9  | 1.808| 1.137|   1|   5| 1.324|\n|PolConsp10 | 1.520| 1.056|   1|   5| 2.224|\n\n\n:::\n:::\n\n\n\nOne estimate of the internal consistency reliability of a test is <mark>Cronbach's $\\alpha$</mark>, which summarizes the average item-test correlation. \n\nThe standard Cronbach's $\\alpha$ is .93. The average Iter-Item Correlation is .564. Ideally, the average inter-item correlation for a set of items should be between .20 and .40, suggesting that while the items are reasonably homogenous, they do contain sufficiently unique variance so as to not be isomorphic with each other [@piedmont2014].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(alpha(itemResp)$total, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|   | raw_alpha| std.alpha| G6(smc)| average_r|    S/N|   ase|  mean|    sd| median_r|\n|:--|---------:|---------:|-------:|---------:|------:|-----:|-----:|-----:|--------:|\n|   |     0.927|     0.928|   0.935|     0.564| 12.937| 0.008| 1.898| 0.833|    0.582|\n\n\n:::\n:::\n\n\n\nUsing factor analysis and `lavaan`, we can reproduce average inter-item correlations assuming items are tau-equavalent:\n\n-   Item responses are standardized (mean as 0, variance as 1)\n\n-   Factor loadings are constrained to be equal as 1\n\n-   Residual variances of items are constrained to be equal\n\nThen, iter-item correlation and Cronbach's alpha can be computed as:\n\n$$\n\\rho = \\frac{\\text{Var}(\\theta)}{\\text{Var}(\\theta)+\\text{Var}(\\psi)}\n$$\n\n$$\n\\alpha = \\frac{N\\rho}{\\sigma^2 +(N-1)\\rho}\n$$\n\nwhere N is sample size, $\\rho$ is average iter-item correlation, and $\\sigma^2$ are average item variances and equal to 1 if items are standardized.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lavaan)\nitemResp_std <- itemResp |> \n  mutate(across(everything(), scale))\nunifac_model <- '\nF1 =~ 1*PolConsp1+1*PolConsp2+1*PolConsp3+1*PolConsp4+1*PolConsp5+1*PolConsp6+1*PolConsp7+1*PolConsp8+1*PolConsp9+1*PolConsp10\nPolConsp1 ~~equal(\"e1\")*PolConsp1\nPolConsp2 ~~equal(\"e1\")*PolConsp2\nPolConsp3 ~~equal(\"e1\")*PolConsp3\nPolConsp4 ~~equal(\"e1\")*PolConsp4\nPolConsp5 ~~equal(\"e1\")*PolConsp5\nPolConsp6 ~~equal(\"e1\")*PolConsp6\nPolConsp7 ~~equal(\"e1\")*PolConsp7\nPolConsp8 ~~equal(\"e1\")*PolConsp8\nPolConsp9 ~~equal(\"e1\")*PolConsp9\nPolConsp10~~equal(\"e1\")*PolConsp10\n'\nfit = cfa(model = unifac_model, data = itemResp_std, std.lv = FALSE)\n# summary(fit)\nVar_F1 = as.numeric(coef(fit)['F1~~F1'])\nVar_errors= as.numeric(coef(fit)[1])\nrho = Var_F1 / (Var_F1 + Var_errors) # reliability \nrho # average iter-item correlation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5640165\n```\n\n\n:::\n\n```{.r .cell-code}\nCron_alpha = 10*rho / (1 + (10-1)*rho )\nCron_alpha # Cronbach's alpha\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9282467\n```\n\n\n:::\n:::\n\n\n\nCronbach's alpha is related to number of items:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(greekLetters)  \ntibble(nItems = 2:20, alpha = nItems*rho / (1 + (nItems-1)*rho )) |> \n  ggplot() +\n  aes(x = nItems, y = alpha) +\n  geom_point(size = 3, shape = 1) +\n  geom_path(group = 1) +\n  scale_x_continuous(breaks = 2:20) +\n  scale_y_continuous(breaks = seq(0.7, 1, .025), limits = c(0.7, 1)) +\n  labs(x = 'Retrospective Number of items', y = paste0(\"Cronbach's \", greeks('alpha')),\n       title = 'Relationship between number of items with alpha')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}