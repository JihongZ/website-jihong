{
  "hash": "9e46030c40e6bf0467a7fc27586cd8b2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'How to use lavaan package to perform Confirmatory Factor Analysis'\nauthor: 'Jihong Zhang'\ndate: '2017-11-08'\nexecute: \n  warning: false\ncategories:\n  - Manual\n  - R\n  - lavaan\n---\n\n\n\n\n> This is one of my homework in Structural Equation Modeling in Fall 2017. Dr. Templin provided a excelent example showing how to perform Confirmatory Factor Analysis (CFA) using `Lavaan` Package. I elaborated each step as following.\n\n<!--more-->\n\n-   First, load packages needed: If you don't have the packages installed below, please use `install.packages()` to install them.\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lavaan)\n#library(semPlot)\nlibrary(psych)\nlibrary(knitr)\nlibrary(kableExtra)\n```\n:::\n\n\n\n\n## Background\n\n### CFA on Attitude towards Inclusive Education Survey (N = 507)\n\nThe affective dimension of attitudes subscale includes 6 items on a 6-point likert scale (1 = Strongly Agree, 6 = Strongly Disagree), measuring teachers' feelings and emotions associated with inclusive education:\n\n1.  I get frustrated when I have difficulty communicating with students with a disability.\n2.  I get upset when students with a disability cannot keep up with the day-to-day curriculum in my classroom.\n3.  I get irritated when I am unable to understand students with a disability.\n4.  I am uncomfortable including students with a disability in a regular classroom with other students without a disability.\n5.  I am disconcerted that students with a disability are included in the regular classroom, regardless of the severity of the disability.\n6.  I get frustrated when I have to adapt the curriculum to meet the individual needs of all students.\n\nThe sample size (**N**) is 507, which includes 6 males and 501 females. I used one-factor model as first step. All items are loaded on one general factor - affective attitude towards inclusive education. Higher response score means more positive attitude towards inclusive education.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read.csv(\"AttitudeForInclusiveEducation.csv\")\n# head(dat)\ndat2 <- dat %>% select(X,Aff.1:Aff.6)\ncolnames(dat2) <- c(\"PersonID\", paste0(\"Aff\",1:6))\n```\n:::\n\n\n\n\n::: callout-tip\n## Tip\n\nNote that for such unbalanced data (# of females \\>\\> \\# of males), the interpretation should be cautious. In the Limitation, authors mentioned that the results can not be overgeneralized to the population.\n:::\n\n## Descriptive Statistics\n\nThe descriptive statistics for all items are provided below. It appears that item 4 is the least difficult item as it has the highest mean ($\\mu = 4.189, sd = 1.317$); item 5 is the most difficult item as it has lowest mean score ($\\mu = 3.604, sd = 1.423$). All responses for each item range from 1 to 6 (1 = Strongly agree, 6 = Strongly disagree). Thus, all categories are responded. In term of item discrimination, as item 3 has the largest standard deviation ($sd = 1.364$) and item 6 has the smallest, item 3 has highest discrimination whearas item 6 has lowest in CTT.\n\n\n\n\n\n|         | vars|   n|    mean|      sd| median| trimmed|     mad| min| max| range|   skew| kurtosis|    se|\n|:--------|----:|---:|-------:|-------:|------:|-------:|-------:|---:|---:|-----:|------:|--------:|-----:|\n|PersonID |    1| 507| 254.000| 146.503|    254| 254.000| 188.290|   1| 507|   506|  0.000|   -1.207| 6.506|\n|Aff1     |    2| 507|   3.765|   1.337|      4|   3.779|   1.483|   1|   6|     5| -0.131|   -0.927| 0.059|\n|Aff2     |    3| 507|   3.635|   1.335|      4|   3.636|   1.483|   1|   6|     5| -0.026|   -0.963| 0.059|\n|Aff3     |    4| 507|   3.493|   1.364|      3|   3.472|   1.483|   1|   6|     5|  0.124|   -0.969| 0.061|\n|Aff4     |    5| 507|   4.189|   1.317|      4|   4.287|   1.483|   1|   6|     5| -0.589|   -0.327| 0.058|\n|Aff5     |    6| 507|   3.604|   1.423|      4|   3.590|   1.483|   1|   6|     5|  0.000|   -0.939| 0.063|\n|Aff6     |    7| 507|   4.018|   1.313|      4|   4.061|   1.483|   1|   6|     5| -0.356|   -0.733| 0.058|\n\n\n\n\nItem-total correlation table was provided below. All item-total correlation coefficients are higher than 0.7, which suggests good internal consistence. Item 1 has lowest item-total correlation ($r = 0.733, sd = 1.337$).\n\n\n\n\n\nTable: Item-total Correlation Table\n\n|     |   n| raw.r| std.r| r.cor| r.drop|  mean|    sd|\n|:----|---:|-----:|-----:|-----:|------:|-----:|-----:|\n|Aff1 | 507| 0.733| 0.735| 0.652|  0.611| 3.765| 1.337|\n|Aff2 | 507| 0.835| 0.836| 0.806|  0.753| 3.635| 1.335|\n|Aff3 | 507| 0.813| 0.812| 0.771|  0.718| 3.493| 1.364|\n|Aff4 | 507| 0.789| 0.790| 0.742|  0.689| 4.189| 1.317|\n|Aff5 | 507| 0.774| 0.769| 0.702|  0.658| 3.604| 1.423|\n|Aff6 | 507| 0.836| 0.838| 0.805|  0.755| 4.018| 1.313|\n\n\n\n\n### Sample Correlation Matrix\n\nAccording to Pearson Correlation Matrix below, we can see all items have fairly high pearson correlation coefficients ranging from 0.44 to 0.72. This provides the evidence of dimensionality. Item 2 and item 3 has highest correlation coefficient($r_{23} = 0.717$). The lowest correlations lies between item 1 and item 4 as well as item 1 and item 5.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(dat2[2:7]) %>% round(3) %>% kable(caption = \"Pearson Correlation Matrix\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Pearson Correlation Matrix\n\n|     |  Aff1|  Aff2|  Aff3|  Aff4|  Aff5|  Aff6|\n|:----|-----:|-----:|-----:|-----:|-----:|-----:|\n|Aff1 | 1.000| 0.590| 0.525| 0.448| 0.411| 0.538|\n|Aff2 | 0.590| 1.000| 0.717| 0.534| 0.553| 0.602|\n|Aff3 | 0.525| 0.717| 1.000| 0.505| 0.527| 0.608|\n|Aff4 | 0.448| 0.534| 0.505| 1.000| 0.609| 0.682|\n|Aff5 | 0.411| 0.553| 0.527| 0.609| 1.000| 0.577|\n|Aff6 | 0.538| 0.602| 0.608| 0.682| 0.577| 1.000|\n\n\n:::\n:::\n\n\n\n\n### Sample Mean and Variance\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- dat2[,2:7] %>% \n  summarise_all(funs(mean)) %>% round(3) %>% t() %>% as.data.frame()\nsds <- dat2[,2:7] %>% \n  summarise_all(funs(sd)) %>% round(3) %>% t() %>% as.data.frame()\ntable1 <- cbind(means,sds)\n\ncolnames(table1) <- c(\"Mean\", \"SD\")\ntable1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Mean    SD\nAff1 3.765 1.337\nAff2 3.635 1.335\nAff3 3.493 1.364\nAff4 4.189 1.317\nAff5 3.604 1.423\nAff6 4.018 1.313\n```\n\n\n:::\n:::\n\n\n\n\n### Sample Item Response Distributions\n\nThose items did not exactly match normal distribution but acceptable.\n\n\n\n\n\n```{.r .cell-code}\n# stack data\ndat2_melted <- dat2 %>% gather(key, value,Aff1:Aff6) %>% arrange(PersonID)\n\n# plot by variable\nggplot(dat2_melted, aes(value)) + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\", binwidth = 1) +\n  geom_density(alpha=.2, fill=\"#FF6666\") +\n  scale_x_continuous(breaks = 1:6) +\n  facet_wrap(~ key)\n```\n\n![](index_files/figure-html/dist-1.png){width=672}\n\n\n\n\n## Estimation with CFA\n\n### One-factor Model\n\nOne-factor model was conducted as first step. The model has one latent facor - affective attitude and six indicators. In general, one-factor model does not provide great model fit except SRMR. The test statistics for chi-square is 75.835 ($p < 0.05$). CFI is 0.929, which larger than 0.95 suggests good model fit. RMSEA is 0.121, which lower than 0.05 suggest good model fit. SRMR is 0.04, which lower than 0.08. The standardized factor loadings range from 0.66 to 0.8. All factor loadings are significant at the level of alpha equals 0.05.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1.syntax <- '\n  AA =~ Aff1 + Aff2 + Aff3 + Aff4 + Aff5 + Aff6\n'\nmodel1 <- cfa(model1.syntax, data = dat2,std.lv = TRUE, mimic = \"mplus\", estimator = \"MLR\")\nsummary(model1, fit.measures = TRUE, standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6.17 ended normally after 14 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                           507\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               115.410      75.834\n  Degrees of freedom                                 9           9\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.522\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1573.730     960.267\n  Degrees of freedom                                15          15\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.639\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.932       0.929\n  Tucker-Lewis Index (TLI)                       0.886       0.882\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.934\n  Robust Tucker-Lewis Index (TLI)                            0.891\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4492.146   -4492.146\n  Scaling correction factor                                  1.138\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4434.440   -4434.440\n  Scaling correction factor                                  1.266\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                9020.291    9020.291\n  Bayesian (BIC)                              9096.404    9096.404\n  Sample-size adjusted Bayesian (SABIC)       9039.270    9039.270\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.153       0.121\n  90 Percent confidence interval - lower         0.129       0.101\n  90 Percent confidence interval - upper         0.178       0.142\n  P-value H_0: RMSEA <= 0.050                    0.000       0.000\n  P-value H_0: RMSEA >= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.149\n  90 Percent confidence interval - lower                     0.120\n  90 Percent confidence interval - upper                     0.181\n  P-value H_0: Robust RMSEA <= 0.050                         0.000\n  P-value H_0: Robust RMSEA >= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  AA =~                                                                 \n    Aff1              0.886    0.055   16.183    0.000    0.886    0.663\n    Aff2              1.078    0.046   23.284    0.000    1.078    0.808\n    Aff3              1.066    0.055   19.499    0.000    1.066    0.783\n    Aff4              0.968    0.061   15.934    0.000    0.968    0.736\n    Aff5              1.004    0.055   18.291    0.000    1.004    0.706\n    Aff6              1.057    0.049   21.644    0.000    1.057    0.805\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .Aff1              3.765    0.059   63.455    0.000    3.765    2.818\n   .Aff2              3.635    0.059   61.382    0.000    3.635    2.726\n   .Aff3              3.493    0.060   57.741    0.000    3.493    2.564\n   .Aff4              4.189    0.058   71.689    0.000    4.189    3.184\n   .Aff5              3.604    0.063   57.057    0.000    3.604    2.534\n   .Aff6              4.018    0.058   68.948    0.000    4.018    3.062\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .Aff1              1.000    0.081   12.285    0.000    1.000    0.560\n   .Aff2              0.617    0.066    9.283    0.000    0.617    0.347\n   .Aff3              0.719    0.089    8.089    0.000    0.719    0.387\n   .Aff4              0.794    0.095    8.390    0.000    0.794    0.459\n   .Aff5              1.014    0.090   11.226    0.000    1.014    0.501\n   .Aff6              0.605    0.068    8.898    0.000    0.605    0.351\n    AA                1.000                               1.000    1.000\n```\n\n\n:::\n:::\n\n\n\n\n### Local Misfit for One-factor Model\n\nBy looking into local misfit with residual variance-covariance matrix we can get the clues to improve the model. According to the model residuals, item 4 has relatively high positive residual covariance with item 5 and item 6. It suggests that the one-factor model underestimates the correlations among item 4, item 5 and item 6. In other words, another latent factor may be needed to explain the strong relations among item 4, 5, 6 which cannot be explained by a general Affective attitude factor.\n\nMoreover, modification indices below also suggest that adding the error covariances among item 4, 5 and 6 will improve chi-square much better.\n\nThus, I decided to add one more factor - AAE. AAE was labeled as affective attitude towards educational environment which indicated by item 4, 5, 6. The other latent factor - AAC which was indicated by item 1, 2, 3 was labeled as Affective Attitude towards communication.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid(model1)$cov %>% kable(caption = \"Normalized Residual Variance-Covariance Matrix\",digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Normalized Residual Variance-Covariance Matrix\n\n|     |   Aff1|   Aff2|   Aff3|   Aff4|   Aff5|   Aff6|\n|:----|------:|------:|------:|------:|------:|------:|\n|Aff1 |  0.000|  0.095|  0.011| -0.070| -0.109|  0.006|\n|Aff2 |  0.095|  0.000|  0.153| -0.106| -0.034| -0.085|\n|Aff3 |  0.011|  0.153|  0.000| -0.128| -0.051| -0.041|\n|Aff4 | -0.070| -0.106| -0.128|  0.000|  0.168|  0.155|\n|Aff5 | -0.109| -0.034| -0.051|  0.168|  0.000|  0.015|\n|Aff6 |  0.006| -0.085| -0.041|  0.155|  0.015|  0.000|\n\n\n:::\n\n```{.r .cell-code}\nmodificationindices(model1, standardized = TRUE,sort. = TRUE) %>% slice(1:10) %>% kable(caption = \"Modification Indices\", digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Modification Indices\n\n|lhs  |op |rhs  |     mi|    epc| sepc.lv| sepc.all| sepc.nox|\n|:----|:--|:----|------:|------:|-------:|--------:|--------:|\n|Aff2 |~~ |Aff3 | 55.906|  0.319|   0.319|    0.479|    0.479|\n|Aff4 |~~ |Aff6 | 45.674|  0.279|   0.279|    0.403|    0.403|\n|Aff4 |~~ |Aff5 | 25.673|  0.243|   0.243|    0.270|    0.270|\n|Aff3 |~~ |Aff4 | 24.228| -0.214|  -0.214|   -0.283|   -0.283|\n|Aff2 |~~ |Aff6 | 22.494| -0.194|  -0.194|   -0.318|   -0.318|\n|Aff2 |~~ |Aff4 | 21.303| -0.193|  -0.193|   -0.276|   -0.276|\n|Aff1 |~~ |Aff2 | 11.974|  0.153|   0.153|    0.195|    0.195|\n|Aff1 |~~ |Aff5 |  7.918| -0.145|  -0.145|   -0.144|   -0.144|\n|Aff1 |~~ |Aff4 |  4.309| -0.096|  -0.096|   -0.108|   -0.108|\n|Aff3 |~~ |Aff6 |  4.018| -0.084|  -0.084|   -0.128|   -0.128|\n\n\n:::\n:::\n\n\n\n\n### Two-factor Model\n\nThe neccessity of adding another factor was tested by specifying a two-factor model.\n\nIn term of model fit indices, it appears that the global model fit indices are great with two-factor model (CFI = 0.986; RMSEA = 0.058; SRMR = 0.022). Ideally, two latent factors could be labeled as moderately correlated aspects of attitudes towards inclusive education. Thus, the first factor (AAC) could be labeled as how teachers feel about communicating with students with disability. The second (AAE) could be labeled as how teachers feel about evironment of inclusive education. All standardized factor loadings are statistically significant ranging from 0.676 to 0.865. The factor correlation between 2 factors is high ($r = 0.838,p = 0.00$).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2.syntax <- '\n  AAC =~ Aff1 + Aff2 + Aff3\n  AAE =~ Aff4 + Aff5 + Aff6\n'\nmodel2 <- cfa(model2.syntax, data = dat2, std.lv = TRUE, mimic = \"mplus\", estimator = \"MLR\")\nsummary(model2, fit.measures = TRUE, standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6.17 ended normally after 19 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                           507\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                32.332      21.512\n  Degrees of freedom                                 8           8\n  P-value (Chi-square)                           0.000       0.006\n  Scaling correction factor                                  1.503\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1573.730     960.267\n  Degrees of freedom                                15          15\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.639\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.984       0.986\n  Tucker-Lewis Index (TLI)                       0.971       0.973\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.987\n  Robust Tucker-Lewis Index (TLI)                            0.975\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4450.606   -4450.606\n  Scaling correction factor                                  1.166\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4434.440   -4434.440\n  Scaling correction factor                                  1.266\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                8939.212    8939.212\n  Bayesian (BIC)                              9019.554    9019.554\n  Sample-size adjusted Bayesian (SABIC)       8959.246    8959.246\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.077       0.058\n  90 Percent confidence interval - lower         0.051       0.034\n  90 Percent confidence interval - upper         0.106       0.082\n  P-value H_0: RMSEA <= 0.050                    0.046       0.268\n  P-value H_0: RMSEA >= 0.080                    0.475       0.068\n                                                                  \n  Robust RMSEA                                               0.071\n  90 Percent confidence interval - lower                     0.036\n  90 Percent confidence interval - upper                     0.108\n  P-value H_0: Robust RMSEA <= 0.050                         0.148\n  P-value H_0: Robust RMSEA >= 0.080                         0.375\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.022       0.022\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  AAC =~                                                                \n    Aff1              0.903    0.055   16.357    0.000    0.903    0.676\n    Aff2              1.153    0.042   27.771    0.000    1.153    0.865\n    Aff3              1.117    0.051   22.079    0.000    1.117    0.820\n  AAE =~                                                                \n    Aff4              1.047    0.053   19.604    0.000    1.047    0.796\n    Aff5              1.036    0.053   19.366    0.000    1.036    0.728\n    Aff6              1.107    0.044   25.112    0.000    1.107    0.844\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  AAC ~~                                                                \n    AAE               0.838    0.028   29.829    0.000    0.838    0.838\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .Aff1              3.765    0.059   63.455    0.000    3.765    2.818\n   .Aff2              3.635    0.059   61.382    0.000    3.635    2.726\n   .Aff3              3.493    0.060   57.741    0.000    3.493    2.564\n   .Aff4              4.189    0.058   71.689    0.000    4.189    3.184\n   .Aff5              3.604    0.063   57.057    0.000    3.604    2.534\n   .Aff6              4.018    0.058   68.948    0.000    4.018    3.062\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .Aff1              0.970    0.083   11.753    0.000    0.970    0.543\n   .Aff2              0.449    0.057    7.860    0.000    0.449    0.253\n   .Aff3              0.607    0.084    7.244    0.000    0.607    0.327\n   .Aff4              0.635    0.085    7.504    0.000    0.635    0.367\n   .Aff5              0.950    0.090   10.539    0.000    0.950    0.470\n   .Aff6              0.497    0.061    8.191    0.000    0.497    0.288\n    AAC               1.000                               1.000    1.000\n    AAE               1.000                               1.000    1.000\n```\n\n\n:::\n:::\n\n\n\n\n### Local Misfit for two-factor model\n\nThe local misfit indices for two-factor model also suggest that the model fits data well. The largest normalized residuals is 1.215. Modification indices suggest that add covariance between item 5 and item 6. These local misfit is not theoretically defensible. Thus, the final model is two-factor model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid(model2)$cov %>% kable(caption = \"Normalized Residual Variance-Covariance Matrix\",digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Normalized Residual Variance-Covariance Matrix\n\n|     |   Aff1|   Aff2|   Aff3|   Aff4|   Aff5|   Aff6|\n|:----|------:|------:|------:|------:|------:|------:|\n|Aff1 |  0.000|  0.010| -0.053| -0.005| -0.003|  0.105|\n|Aff2 |  0.010|  0.000|  0.014| -0.075|  0.048| -0.016|\n|Aff3 | -0.053|  0.014|  0.000| -0.076|  0.050|  0.049|\n|Aff4 | -0.005| -0.075| -0.076|  0.000|  0.056|  0.019|\n|Aff5 | -0.003|  0.048|  0.050|  0.056|  0.000| -0.070|\n|Aff6 |  0.105| -0.016|  0.049|  0.019| -0.070|  0.000|\n\n\n:::\n\n```{.r .cell-code}\nmodificationindices(model2, standardized = TRUE,sort. = TRUE) %>% slice(1:10) %>% kable(caption = \"Modification Indices\", digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Modification Indices\n\n|lhs  |op |rhs  |     mi|    epc| sepc.lv| sepc.all| sepc.nox|\n|:----|:--|:----|------:|------:|-------:|--------:|--------:|\n|Aff5 |~~ |Aff6 | 16.894| -0.224|  -0.224|   -0.326|   -0.326|\n|AAC  |=~ |Aff4 | 16.893| -0.578|  -0.578|   -0.439|   -0.439|\n|Aff1 |~~ |Aff6 |  7.064|  0.108|   0.108|    0.155|    0.155|\n|Aff4 |~~ |Aff5 |  5.977|  0.128|   0.128|    0.164|    0.164|\n|AAC  |=~ |Aff6 |  5.976|  0.368|   0.368|    0.280|    0.280|\n|Aff1 |~~ |Aff3 |  5.093| -0.112|  -0.112|   -0.146|   -0.146|\n|AAE  |=~ |Aff2 |  5.093| -0.362|  -0.362|   -0.272|   -0.272|\n|Aff3 |~~ |Aff4 |  4.045| -0.077|  -0.077|   -0.124|   -0.124|\n|Aff2 |~~ |Aff3 |  3.160|  0.119|   0.119|    0.228|    0.228|\n|AAE  |=~ |Aff1 |  3.160|  0.236|   0.236|    0.176|    0.176|\n\n\n:::\n:::\n\n\n\n\n## Path Diagram\n\n\n\n\n\n```{.r .cell-code}\n#semPlot::semPaths(model2, what = \"est\")\n```\n\n\n\n\n## Reliability in Factor Analysis\n\nTo get the estimates of reliabilities, Omega coefficients were calculated for each factor($\\Omega_{AAC} = 0.832, p < 0.01; \\Omega_{AAE} = 0.830, p < 0.01$).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel03SyntaxOmega = \"\n  # AAC loadings (all estimated)\n  AAC =~ L1*Aff1 + L2*Aff2 + L3*Aff3\n  \n  # AAE loadings (all estimated)\n  AAE =~ L4*Aff4 + L5*Aff5 + L6*Aff6\n  \n  # Unique Variances:\n  Aff1 ~~ E1*Aff1; Aff2 ~~ E2*Aff2; Aff3 ~~ E3*Aff3; Aff4 ~~ E4*Aff4; Aff5 ~~ E5*Aff5; Aff6 ~~ E6*Aff6; \n  \n  \n  # Calculate Omega Reliability for Sum Scores:\n  OmegaAAC := ((L1 + L2 + L3)^2) / ( ((L1 + L2 + L3)^2) + E1 + E2 + E3)\n  OmegaAAE := ((L4 + L5 + L6)^2) / ( ((L4 + L5 + L6)^2) + E4 + E5 + E6)\n\"\n\nmodel03EstimatesOmega = sem(model = model03SyntaxOmega, data = dat2, estimator = \"MLR\", mimic = \"mplus\", std.lv = TRUE)\nsummary(model03EstimatesOmega, fit.measures = FALSE, rsquare = FALSE, standardized = FALSE, header = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  AAC =~                                              \n    Aff1      (L1)    0.903    0.055   16.357    0.000\n    Aff2      (L2)    1.153    0.042   27.771    0.000\n    Aff3      (L3)    1.117    0.051   22.079    0.000\n  AAE =~                                              \n    Aff4      (L4)    1.047    0.053   19.604    0.000\n    Aff5      (L5)    1.036    0.053   19.366    0.000\n    Aff6      (L6)    1.107    0.044   25.112    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  AAC ~~                                              \n    AAE               0.838    0.028   29.829    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .Aff1              3.765    0.059   63.455    0.000\n   .Aff2              3.635    0.059   61.382    0.000\n   .Aff3              3.493    0.060   57.741    0.000\n   .Aff4              4.189    0.058   71.689    0.000\n   .Aff5              3.604    0.063   57.057    0.000\n   .Aff6              4.018    0.058   68.948    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .Aff1      (E1)    0.970    0.083   11.753    0.000\n   .Aff2      (E2)    0.449    0.057    7.860    0.000\n   .Aff3      (E3)    0.607    0.084    7.244    0.000\n   .Aff4      (E4)    0.635    0.085    7.504    0.000\n   .Aff5      (E5)    0.950    0.090   10.539    0.000\n   .Aff6      (E6)    0.497    0.061    8.191    0.000\n    AAC               1.000                           \n    AAE               1.000                           \n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    OmegaAAC          0.832    0.016   51.512    0.000\n    OmegaAAE          0.830    0.017   49.000    0.000\n```\n\n\n:::\n:::\n\n\n\n\n## Factor Scores and Distributions\n\nThe AAC factor scores have an estimated mean of 0 with a variance of 0.88 due to the effect of the prior distribution. The SE for each person's AAC factor score is 0.347; 95% confidence interval for AAC factor score is $Score \\pm 2*0.347$ = $Score \\pm 0.694$. The AAE factor scores have an estimated mean of 0 with a variance of 0.881 due to the effect of the prior distribution. The SE for each person's AAC factor score is 0.357; 95% confidence interval for AAC factor score is $Score \\pm 2*0.357$ = $Score \\pm 0.714$.\n\nFactor Realiability for AAC is 0.892 and factor realibility for AAE is 0.887. Both factor reliability are larger than omega.\n\nThe resulting distribution of the EAP estimates of factor score as shown in Figure 1. Figure 2 shows the predicted response for each item as a linear function of the latent factor based on the estimated model parameters. As shown, for AAE factor, the predicted item response goes above the highest response option just before a latent factor score of 2 (i.e., 2 SDs above the mean), resulting in a ceiling effect for AAE factor, as also shown in Figure 1.\n\nThe extent to which the items within each factor could be seen as exchangeable was then examined via an additional set of nested model comparisons, as reported in Table 1 (for fit) and Table 2 (for comparisons of fit). Two-factor has better model fit than one-facor model. Moreover, according to chi-square difference test, two-factor is significantly better than one-factor in model fit.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      AAC       AAE \n0.8925361 0.8867031 \n```\n\n\n:::\n:::\n\n\n\n\n## Figures\n\n### Figure 1 : Factor Score Distribution\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n\n\n### Figure 2 : Expected Item Response Plots\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/item-response-1.png){width=672}\n:::\n:::\n\n\n\n\n## Tables\n\n### Table 1: Model Fit Statistics Using MLR\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> # Items </th>\n   <th style=\"text-align:right;\"> # Parameters </th>\n   <th style=\"text-align:right;\"> Scaled Chi-Square </th>\n   <th style=\"text-align:right;\"> Chi-Square Scale Factor </th>\n   <th style=\"text-align:right;\"> DF </th>\n   <th style=\"text-align:right;\"> p-value </th>\n   <th style=\"text-align:right;\"> CFI </th>\n   <th style=\"text-align:right;\"> RMSEA </th>\n   <th style=\"text-align:right;\"> RMSEA Lower </th>\n   <th style=\"text-align:right;\"> RMSEA Upper </th>\n   <th style=\"text-align:right;\"> RMSEA p-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> One-Factor </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;\"> 75.834 </td>\n   <td style=\"text-align:right;\"> 1.522 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> 0.927 </td>\n   <td style=\"text-align:right;\"> 0.9 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> 0.05 </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Two-Factor </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 21.512 </td>\n   <td style=\"text-align:right;\"> 1.503 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 0.006 </td>\n   <td style=\"text-align:right;\"> 0.979 </td>\n   <td style=\"text-align:right;\"> 0.9 </td>\n   <td style=\"text-align:right;\"> 0.046 </td>\n   <td style=\"text-align:right;\"> 0.05 </td>\n   <td style=\"text-align:right;\"> 0.475 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n### Table 2: Model Comparisons\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Df </th>\n   <th style=\"text-align:right;\"> Chisq diff </th>\n   <th style=\"text-align:right;\"> Df diff </th>\n   <th style=\"text-align:right;\"> Pr(&gt;Chisq) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> One-Factor vs. Two-Factor </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 49.652 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n### Table 3: Model Estimates\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Unstandardized</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"1\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Standardized</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> SE </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n  </tr>\n </thead>\n<tbody>\n  <tr grouplength=\"3\"><td colspan=\"4\" style=\"border-bottom: 1px solid;\"><strong>Forgiveness Factor Loadings</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 1 </td>\n   <td style=\"text-align:right;\"> 0.903 </td>\n   <td style=\"text-align:right;\"> 0.055 </td>\n   <td style=\"text-align:right;\"> 0.676 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 2 </td>\n   <td style=\"text-align:right;\"> 1.153 </td>\n   <td style=\"text-align:right;\"> 0.042 </td>\n   <td style=\"text-align:right;\"> 0.865 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 3 </td>\n   <td style=\"text-align:right;\"> 1.117 </td>\n   <td style=\"text-align:right;\"> 0.051 </td>\n   <td style=\"text-align:right;\"> 0.820 </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"4\" style=\"border-bottom: 1px solid;\"><strong>Not Unforgiveness Factor Loadings</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 4 </td>\n   <td style=\"text-align:right;\"> 1.047 </td>\n   <td style=\"text-align:right;\"> 0.053 </td>\n   <td style=\"text-align:right;\"> 0.796 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 5 </td>\n   <td style=\"text-align:right;\"> 1.036 </td>\n   <td style=\"text-align:right;\"> 0.053 </td>\n   <td style=\"text-align:right;\"> 0.728 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 6 </td>\n   <td style=\"text-align:right;\"> 1.107 </td>\n   <td style=\"text-align:right;\"> 0.044 </td>\n   <td style=\"text-align:right;\"> 0.844 </td>\n  </tr>\n  <tr grouplength=\"1\"><td colspan=\"4\" style=\"border-bottom: 1px solid;\"><strong>Factor Covariance</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Factor Covariance </td>\n   <td style=\"text-align:right;\"> 0.838 </td>\n   <td style=\"text-align:right;\"> 0.028 </td>\n   <td style=\"text-align:right;\"> 0.838 </td>\n  </tr>\n  <tr grouplength=\"6\"><td colspan=\"4\" style=\"border-bottom: 1px solid;\"><strong>Item Intercepts</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 1 </td>\n   <td style=\"text-align:right;\"> 3.765 </td>\n   <td style=\"text-align:right;\"> 0.059 </td>\n   <td style=\"text-align:right;\"> 2.818 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 2 </td>\n   <td style=\"text-align:right;\"> 3.635 </td>\n   <td style=\"text-align:right;\"> 0.059 </td>\n   <td style=\"text-align:right;\"> 2.726 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 3 </td>\n   <td style=\"text-align:right;\"> 3.493 </td>\n   <td style=\"text-align:right;\"> 0.060 </td>\n   <td style=\"text-align:right;\"> 2.564 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 4 </td>\n   <td style=\"text-align:right;\"> 4.189 </td>\n   <td style=\"text-align:right;\"> 0.058 </td>\n   <td style=\"text-align:right;\"> 3.184 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 5 </td>\n   <td style=\"text-align:right;\"> 3.604 </td>\n   <td style=\"text-align:right;\"> 0.063 </td>\n   <td style=\"text-align:right;\"> 2.534 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 6 </td>\n   <td style=\"text-align:right;\"> 4.018 </td>\n   <td style=\"text-align:right;\"> 0.058 </td>\n   <td style=\"text-align:right;\"> 3.062 </td>\n  </tr>\n  <tr grouplength=\"6\"><td colspan=\"4\" style=\"border-bottom: 1px solid;\"><strong>Item Unique Variances</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 1 </td>\n   <td style=\"text-align:right;\"> 0.970 </td>\n   <td style=\"text-align:right;\"> 0.083 </td>\n   <td style=\"text-align:right;\"> 0.543 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 2 </td>\n   <td style=\"text-align:right;\"> 0.449 </td>\n   <td style=\"text-align:right;\"> 0.057 </td>\n   <td style=\"text-align:right;\"> 0.253 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 3 </td>\n   <td style=\"text-align:right;\"> 0.607 </td>\n   <td style=\"text-align:right;\"> 0.084 </td>\n   <td style=\"text-align:right;\"> 0.327 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 4 </td>\n   <td style=\"text-align:right;\"> 0.635 </td>\n   <td style=\"text-align:right;\"> 0.085 </td>\n   <td style=\"text-align:right;\"> 0.367 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 5 </td>\n   <td style=\"text-align:right;\"> 0.950 </td>\n   <td style=\"text-align:right;\"> 0.090 </td>\n   <td style=\"text-align:right;\"> 0.470 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;padding-left: 2em;\" indentlevel=\"1\"> Item 6 </td>\n   <td style=\"text-align:right;\"> 0.497 </td>\n   <td style=\"text-align:right;\"> 0.061 </td>\n   <td style=\"text-align:right;\"> 0.288 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}