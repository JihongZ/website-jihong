{
  "hash": "005c8d0cb3559a26b97a1c304d8c3ac9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 02\"\nsubtitle: \"Introduction to Bayesian Concepts\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\n  data-background-opacity: \"0.9\"\nexecute: \n  echo: true\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: true  # choose \"false \"if want to show all together\n    theme: [serif, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    transition: slide\n    background-transition: fade\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    code-line-numbers: true\n    code-annotations: hover\n---\n\n\n\n\n## Today's Lecture Objectives\n\n1.  Bayes' Theorem\n\n2.  Likelihood function, Posterior distribution\n\n3.  How to report posterior distribution of parameters\n\n4.  Bayesian update\n\nbut, before we begin... [Music: Good Bayesian](https://www.youtube.com/watch?v=qV6Wc_f1Cgo)\n\n[Music: Good Bayesian](https://www.youtube.com/watch?v=qV6Wc_f1Cgo)\n\n[Anil Seth](https://www.anilseth.com/)\n\n## Quiz: What is Bayesian?\n\n1.  What are key components of Bayesian models?\n\n    1.  Likelihood function - **data**\n    2.  Prior distribution - belief / previous evidences of **parameters**\n    3.  Posterior distribution - updated information of parameters given our data and model\n    4.  Posterior predictive distribution - future / predicted data\n\n2.  What are the differences between Bayesian with Frequentist analysis?\n\n    1.  prior distribution: Bayesian\n    2.  hypothesis of fixed parameters: frequentist\n    3.  estimation process: MCMC vs. MLE\n    4.  posterior distribution vs. point estimates of parameters\n    5.  credible interval (plausibility of the parameters having those values) vs. confidence interval (the proportion of infinite samples having the fixed parameters)\n\n## Bayes' Theorem: How Bayesian Statistics Work\n\nBayesian methods rely on Bayes' Theorem\n\n$$\nP(\\theta | Data) = \\frac{P(Data|\\theta)P(\\theta)}{P(Data)} \\propto P(Data|\\theta) P(\\theta)\n$$\n\nWhere:\n\n1.  P: probability distribution function (PDF)\n2.  $P(\\theta|Data)$ : the [posterior distribution]{.underline} of parameter $\\theta$, given the observed data\n3.  $P(Data|\\theta)$: the likelihood function (conditional distributin) of the observed data, given the parameters\n4.  $P(\\theta)$: the [prior distribution]{.underline} of parameter $\\theta$\n5.  $P(Data)$: the marginal distribution of the observed data\n\n------------------------------------------------------------------------\n\n**A Live Bayesian Example**\n\n-   Suppose we want to assess the probability of rolling a \"1\" on a six-sided die:\n\n    $$\n    \\theta \\equiv p_{1} = P(D = 1)\n    $$\n\n-   Suppose we collect a sample of data (N = 5):\\\n    $$Data \\equiv X = \\{0, 1, 0, 1, 1\\}$$\n\n-   The [prior distribution]{.underline} of parameters is denoted as $P(p_1)$ ;\n\n-   The [likelihood function]{.underline}is denoted as $P(X | p_1)$;\n\n-   The [posterior distribution]{.underline} is denoted as $P(p_1|X)$\n\n-   Then we have:\n\n    $$\n    P(p_1|X) = \\frac{P(X|p_1)P(p_1)}{P(X)} \\propto P(X|p_1) P(p_1)\n    $$\n\n------------------------------------------------------------------------\n\n## Step 1: Choose the Likelihood function (Model)\n\n-   The Likelihood function $P(X|p_1)$ follows Binomial distribution of 3 succuess out of 5 samples:\\\n    $$P(X|p_1) = \\prod_{i =1}^{N=5} p_1^{X_i}(1-p_1)^{X_i} \n    \\\\= (1-p_i) \\cdot p_i \\cdot (1-p_i) \\cdot p_i \\cdot p_i$$\n\n-   Question here: WHY use Bernoulli (Binomial) Distribution (feat. *Jacob Bernoulli, 1654-1705*)?\n\n-   My answer: Bernoulli dist. has nice statistical probability. \"Nice\" means making totally sense in normal life-- a common belief. For example, the $p_1$ value that maximizes the Bernoulli-based likelihood function is $Mean(X)$, and the $p_1$ values that minimizes the Bernoulli-based likelihood function is 0 or 1\n\n------------------------------------------------------------------------\n\n### Log-likelihood Function along \"Parameter\"\n\n$$\nLL(p_1|\\{0, 1, 0, 1, 1\\})\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\np1 = seq(0, 1, length.out = 21)\nLikelihood = (p1)^3 * (1-p1)^2\nplot(x = p1, y = log(Likelihood))\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n\n```{.r .cell-code  code-line-numbers=\"2\"}\np1[which.max(Likelihood)] # p1 = 0.6 = 3 / 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Log-likelihood Function along \"Data\"\n\n$$\nLL(Data|p_1 \\in \\{0, 0.2,0.4, 0.6, 0.8,1\\})\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-line-numbers=\"5\"}\nlibrary(tidyverse)\np1 = c(0, 2, 4, 6, 8, 10) / 10\nnTrails = 5\nnSuccess = 0:nTrails\nLikelihood = sapply(p1, \\(x) choose(nTrails,nSuccess)*(x)^nSuccess*(1-x)^(nTrails - nSuccess))\nLikelihood_forPlot <- as.data.frame(Likelihood)\ncolnames(Likelihood_forPlot) <- p1\nLikelihood_forPlot$Sample = factor(paste0(nSuccess, \" out of \", nTrails), levels = paste0(nSuccess, \" out of \", nTrails))\n# plot\nLikelihood_forPlot %>% \n  pivot_longer(-Sample, names_to = \"p1\") %>% \n  mutate(`log(Likelihood)` = log(value)) %>% \n  ggplot(aes(x = Sample, y = `log(Likelihood)`, color = p1)) +\n  geom_point(size = 2) +\n  geom_path(aes(group=p1), size = 1.3)\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Step 2: Choose the Prior Distribution for $p_1$\n\nWe must now pick the prior distribution of $p_1$:\n\n$$\nP(p_1)\n$$\n\n-   Compared to likelihood function, we have much more choices. Many distributions to choose from\n\n-   To choose [prior distribution]{.underline}, think about what we know about a \"fair\" die.\n\n    -   the probability of rolling a \"1\" is about $\\frac{1}{6}$\n\n    -   the probabilities much higher/lower than $\\frac{1}{6}$ are very unlikely\n\n-   Let's consider a Beta distribution:\n\n    $$\n    p_1 \\sim Beta(\\alpha, \\beta)\n    $$\n\n------------------------------------------------------------------------\n\n### The Beta Distribution\n\nFor parameters that range between 0 and 1, the beta distribution makes a good choice for prior distribution:\n\n$$\nP(p_1) = \\frac{(p_1)^{a-1} (1-p_1)^{\\beta-1}}{B(\\alpha,\\beta)}\n$$\n\nWhere denominator $B$ is:\n\n$$\nB(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n$$\n\nand (fun fact: derived by *Daniel Bernoulli (1700-1782)*),\n\n$$\n\\Gamma(\\alpha) = \\int_0^{\\infty}t^{\\alpha-1}e^{-t}dt\n$$\n\n------------------------------------------------------------------------\n\n### More Beta Distribution\n\n-   The Beta distribution has a mean of $\\frac{\\alpha}{\\alpha+\\beta}$ and a mode of $\\frac{\\alpha -1}{\\alpha + \\beta -2}$ for $\\alpha > 1$ & $\\beta > 1$; (fun fact: when $\\alpha\\ \\&\\ \\beta< 1$, pdf is U-shape, what that mean?)\n\n-   $\\alpha$ and $\\beta$ are called [hyperparameters]{.underline} of the parameter $p_1$;\n\n-   [Hyperparameters]{.underline} are parameters of prior parameters ;\n\n-   When $\\alpha = \\beta = 1$, the distribution is uniform distribution;\n\n-   To make sure $P(p_1 = \\frac{1}{6})$ is the largest, we can:\n\n    -   Many choices: $\\alpha =2$ and $\\beta = 6$ has the same mode as $\\alpha = 100$ and $\\beta = 496$; (hint: $\\beta = 5\\alpha - 4$)\n\n    -   The differences between choices is how strongly we feel in our beliefs\n\n------------------------------------------------------------------------\n\n### How strongly we believe in the prior\n\n::: columns\n::: {.column width=\"40%\"}\n-   The Beta distribution has a variance of $\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 (\\alpha + \\beta + 1)}$\n-   The smaller prior variance means the prior is more [informative]{.underline}\n    -   [Informative]{.underline} priors are those that have relatively small variances\n    -   [Uninformative]{.underline} priors are those that have relatively large variances\n-   Suppose we have four sets of hyperparameters choices: (2,6);(20,96);(100,496);(200,996)\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Function for variance of Beta distribution\nvarianceFun = function(alpha_beta){\n  alpha = alpha_beta[1]\n  beta = alpha_beta[2]\n  return((alpha * beta)/((alpha + beta)^2*(alpha + beta + 1)))\n}\n\n# Multiple prior choices from uninformative to informative\nalpha_beta_choices = list(\n  I_dont_trust = c(2, 6), \n  not_strong =   c(20, 96), \n  litte_strong = c(100, 496), \n  very_strong =  c(200, 996))\n\n## Transform to data.frame for plot\nalpha_beta_variance_plot <- t(sapply(alpha_beta_choices, varianceFun)) %>% \n  as.data.frame() %>% \n  pivot_longer(everything(), names_to = \"Degree\", values_to = \"Variance\") %>% \n  mutate(Degree = factor(Degree, levels = unique(Degree))) %>% \n  mutate(Alpha_Beta = c(\n    \"(2, 6)\",\n    \"(20, 96)\",\n    \"(100, 496)\",\n    \"(200, 996)\"\n  ))\n\nalpha_beta_variance_plot %>% \n  ggplot(aes(x = Degree, y = Variance)) +\n  geom_col(aes(fill = Degree)) +\n  geom_text(aes(label = Alpha_Beta), size = 6) +\n  labs(title = \"Variances along difference choices of alpha and beta\") +\n  theme(text = element_text(size = 21)) \n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Visualize prior distribution $P(p_1)$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndbeta <- function(p1, alpha, beta) {\n  # probability density function\n  PDF = ((p1)^(alpha-1)*(1-p1)^(beta-1)) / beta(alpha, beta)\n  return(PDF)\n}\n\ncondition <- data.frame(\n  alpha = c(2, 20, 100, 200),\n  beta = c(6, 96, 496, 996)\n)\npdf_bycond <- condition %>% \n  nest_by(alpha, beta) %>% \n  mutate(data = list(\n    dbeta(p1 = (1:99)/100, alpha = alpha, beta = beta)\n  ))\n\n## prepare data for plotting pdf by conditions\npdf_forplot <- Reduce(cbind, pdf_bycond$data) %>% \n  t() %>% \n  as.data.frame() ## merge conditions together\ncolnames(pdf_forplot) <- (1:99)/100 # add p1 values as x-axis\npdf_forplot <- pdf_forplot %>% \n  mutate(Alpha_Beta = c( # add alpha_beta conditions as colors\n    \"(2, 6)\",\n    \"(20, 96)\",\n    \"(100, 496)\",\n    \"(200, 996)\"\n  )) %>% \n  pivot_longer(-Alpha_Beta, names_to = 'p1', values_to = 'pdf') %>% \n  mutate(p1 = as.numeric(p1),\n         Alpha_Beta = factor(Alpha_Beta,levels = unique(Alpha_Beta)))\n\npdf_forplot %>% \n  ggplot() +\n  geom_vline(xintercept = 0.17, col = \"black\") +\n  geom_point(aes(x = p1, y = pdf, col = Alpha_Beta)) +\n  geom_path(aes(x = p1, y = pdf, col = Alpha_Beta, group = Alpha_Beta)) + \n  scale_color_discrete(name = \"Prior choices\")\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n\n**Question: WHY NOT USE NORMAL DISTRIBUTION?**\n\n------------------------------------------------------------------------\n\n## Step 3: The Posterior Distribution\n\nChoose a Beta distribution as the prior distribution of $p_1$ is very convenient:\n\n-   When combined with Bernoulli (Binomial) data likelihood, the [posterior distribution]{.underline} ($P(p_1|Data)$) can be derived analytically\n\n-   The posterior distribution is also a Beta distribution\n\n    -   $\\alpha' = \\alpha + \\sum_{i=1}^{N}X_i$ ($\\alpha'$ is parameter of the posterior distribution)\n\n    -   $\\beta' = \\beta + (N - \\sum_{i=1}^{N}X_i)$ ($\\beta'$ is parameter of the posterior distribution)\n\n-   The Beta distribution is said to be a [conjugate prior]{.underline} in Bayesian analysis: A prior distribution that leads to posterior distribution of the same family\n\n    -   Prior and Posterior distribution are all Beta distribution\n\n------------------------------------------------------------------------\n\n### Visualize the posterior distribution $P(p_1|Data)$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndbeta_posterior <- function(p1, alpha, beta, data) {\n  alpha_new = alpha + sum(data) \n  beta_new = beta + (length(data) - sum(data) )\n  # probability density function\n  PDF = ((p1)^(alpha_new-1)*(1-p1)^(beta_new-1)) / beta(alpha_new, beta_new)\n  return(PDF)\n}\n# Observed data\ndat = c(0, 1, 0, 1, 1)\n\ncondition <- data.frame(\n  alpha = c(2, 20, 100, 200),\n  beta = c(6, 96, 496, 996)\n)\n\npdf_posterior_bycond <- condition %>% \n  nest_by(alpha, beta) %>% \n  mutate(data = list(\n    dbeta_posterior(p1 = (1:99)/100, alpha = alpha, beta = beta,\n                    data = dat)\n  ))\n\n## prepare data for plotting pdf by conditions\npdf_posterior_forplot <- Reduce(cbind, pdf_posterior_bycond$data) %>% \n  t() %>% \n  as.data.frame() ## merge conditions together\ncolnames(pdf_posterior_forplot) <- (1:99)/100 # add p1 values as x-axis\npdf_posterior_forplot <- pdf_posterior_forplot %>% \n  mutate(Alpha_Beta = c( # add alpha_beta conditions as colors\n    \"(2, 6)\",\n    \"(20, 96)\",\n    \"(100, 496)\",\n    \"(200, 996)\"\n  )) %>% \n  pivot_longer(-Alpha_Beta, names_to = 'p1', values_to = 'pdf') %>% \n  mutate(p1 = as.numeric(p1),\n         Alpha_Beta = factor(Alpha_Beta,levels = unique(Alpha_Beta)))\n\npdf_posterior_forplot %>% \n  ggplot() +\n  geom_vline(xintercept = 0.17, col = \"black\") +\n  geom_point(aes(x = p1, y = pdf, col = Alpha_Beta)) +\n  geom_path(aes(x = p1, y = pdf, col = Alpha_Beta, group = Alpha_Beta)) + \n  scale_color_discrete(name = \"Prior choices\") +\n  labs( y = '')\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Manually calculate summaries of the posterior distribution\n\nTo determine the estimate of $p_1$, we need to summarize the posterior distribution:\n\n-   With prior hyperparameters $\\alpha = 2$ and $\\beta = 6$:\n\n    -   $\\hat p_1 = \\frac{2+3}{2+3+6+2}=\\frac{5}{13} = .3846$\n\n    -   SD = 0.1300237\n\n-   With prior hyperparameters $\\alpha = 100$ and $\\beta = 496$:\n\n    -   $\\hat p_1 = \\frac{100+3}{100+3+496+2}=\\frac{103}{601} = .1714$\n\n    -   SD = 0.0153589\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuninfo <- pdf_posterior_forplot %>% filter(Alpha_Beta == \"(2, 6)\")\npaste0(\"Sample Posterior Mean: \", mean(uninfo$p1 * uninfo$pdf)) # .3885\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sample Posterior Mean: 0.388500388484552\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuninfo2 <- pdf_posterior_forplot %>% filter(Alpha_Beta == \"(100, 496)\")\npaste0(\"Sample Posterior Mean: \", mean(uninfo2$p1 * uninfo2$pdf)) # 0.1731\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sample Posterior Mean: 0.173112153145432\"\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Stan Code for the example model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\nregister_knitr_engine(override = FALSE)\n```\n:::\n\n::: {.cell output.var='dice_model'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> N; // sample size\n  array[N] int<lower=0, upper=1> y; // observed data\n  real<lower=1> alpha; // hyperparameter alpha\n  real<lower=1> beta; // hyperparameter beta\n}\nparameters {\n  real<lower=0,upper=1> p1; // parameters\n}\nmodel {\n  p1 ~ beta(alpha, beta); // prior distribution\n  for(n in 1:N){\n    y[n] ~ bernoulli(p1); // model\n  }\n}\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# mod <- cmdstan_model('dice_model.stan')\ndata_list1 <- list(\n  y = c(0, 1, 0, 1, 1),\n  N = 5,\n  alpha = 2,\n  beta = 6\n)\ndata_list2 <- list(\n  y = c(0, 1, 0, 1, 1),\n  N = 5,\n  alpha = 100,\n  beta = 496\n)\nfit1 <- dice_model$sample(\n  data = data_list1,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nfit2 <- dice_model$sample(\n  data = data_list2,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n#### Uninformative prior distribution (2, 6) and posterior distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbayesplot::mcmc_dens(fit1$draws('p1')) +\n  geom_path(aes(x = p1, y = pdf), data = pdf_posterior_forplot %>% filter(Alpha_Beta == '(2, 6)'), col = \"red\") +\n  geom_vline(xintercept = 1/6, col = \"green\") +\n  geom_vline(xintercept = 3/5, col = \"purple\") +\n  labs(title = \"Posterior distribution using uninformative prior vs. the conjugate prior (The Gibbs sampler) method\", caption = 'Green: mode of prior distribution; Purple: expected value from observed data; Red: Gibbs sampling method')\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nfit1$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 10\n  variable   mean median    sd   mad      q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -9.18  -8.90  0.735 0.325 -10.6   -8.66   1.00    1736.    2219.\n2 p1        0.381  0.377 0.130 0.135   0.177  0.603  1.00    1499.    1691.\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n#### Informative prior distribution (100, 496) and posterior distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbayesplot::mcmc_dens(fit2$draws('p1')) +\n  geom_path(aes(x = p1, y = pdf), data = pdf_posterior_forplot %>% filter(Alpha_Beta == '(100, 496)'), col = \"red\") +\n  geom_vline(xintercept = 1/6, col = \"green\") +\n  geom_vline(xintercept = 3/5, col = \"purple\") +\n  labs(title = \"Posterior distribution using informative prior vs. the conjugate prior (The Gibbs sampler) method\", caption = 'Green: mode of prior distribution; Purple: expected value from observed data; Red: Gibbs sampling method')\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nfit2$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -276.    -276.    0.764  0.310  -277.    -275.     1.00    1770.\n2 p1          0.171    0.170 0.0155 0.0153    0.146    0.196  1.00    1527.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Install CmdStan\n\n[cmdstanr Installation](https://mc-stan.org/cmdstanr/articles/cmdstanr.html#installing-cmdstan) [cmdstanr Users' Guide](https://mc-stan.org/docs/cmdstan-guide/cmdstan-installation.html#troubleshooting-the-installation)\n\n## Next Class\n\nWe will talk more about how Bayesian model works. Thank you.\n\n## Bonus: Conjugacy of Beta Distribution\n\nWhen the binomial likelihood is multiplied by the beta prior, the result is proportional to another beta distribution:\n\n-   $\\text{Posterior} \\propto \\theta^x (1 - \\theta)^{n-x} \\times \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}$\n-   Simplifies to: $\\theta^{x + \\alpha - 1} (1 - \\theta)^{n - x + \\beta - 1}$\n\nThis is the kernel of a beta distribution with updated parameters $\\alpha' = x + \\alpha$ and $\\beta' = n - x + \\beta$. The fact that the posterior is still a beta distribution is what makes the beta distribution a conjugate prior for the binomial likelihood.\n\n> Human langauge: Both beta (prior) and binomial (likelihood) are so-called \"exponetial family\". The muliplication of them is still a \"exponential family\" distribution.\n\n------------------------------------------------------------------------\n\n## Bayesian updating\n\nWe can use the posterior distribution as a prior!\n\n1\\. data {0, 1, 0, 1, 1} with the prior hyperparameter {2, 6} -\\> posterior parameter {5, 9}\n\n2\\. new data {1, 1, 1, 1, 1} with the prior hyperparameter {5, 9} -\\> posterior parameter {10, 9}\n\n3\\. one more new data {0, 0, 0, 0, 0} with the prior hyperparameter {10, 9} -\\> posterior parameter {10, 14}\n\n## Wrapping up\n\nToday is a quick introduction to Bayesian Concept\n\n-   Bayes' Theorem: Fundamental theorem of Bayesian Inference\n-   Prior distribution: What we know about the parameter before seeing the data\n    -   hyperparameter: parameter of the prior distribution\n    -   Uninformative prior: Prior distribution that does not convey any information about the parameter\n    -   Informative prior: Prior distribution that conveys information about the parameter\n    -   Conjugate prior: Prior distribution that makes the posterior distribution the same family as the prior distribution\n-   Likelihood: What the data tell us about the parameter\n    -   Likelihood function: Probability of the data given the parameter\n    -   Likelihood principle: All the information about the data is contained in the likelihood function\n    -   Likelihood is a sort of \"scientific judgment\" on the generation process of the data\n-   Posterior distribution: What we know about the parameter after seeing the data\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}