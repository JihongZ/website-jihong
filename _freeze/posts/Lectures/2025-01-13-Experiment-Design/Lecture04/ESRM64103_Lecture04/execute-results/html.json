{
  "hash": "b4cac5da39a897100c80c343d9054079",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04: ANOVA Assumptions\"\nsubtitle: \"Experimental Design in Education\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2025-08-18\"\nsidebar: false\nexecute: \n  eval: true\n  echo: true\nformat: \n  #html: \n  #  page-layout: full\n  #  toc: true\n  #  toc-depth: 2\n  #  lightbox: true\n  uark-revealjs:\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: true\n    number-depth: 1\n    footer: \"ESRM 64503: Lecture 04\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    scrollable: true\n#jupyter: python3\n---\n\n\n\n## Class Outline\n\n-   Go through three assumptions of ANOVA and their checking statistics\n-   Omnibus test for more group comparisons.\n-   Example: Intervention and Verbal Acquisition\n-   Exercise: Effect of Sleep Duration on Cognitive Performance\n\n## ANOVA Procedure\n\n1.  **Set hypotheses**:\n    -   **Null hypothesis** ($H_0$): All group means are equal.\n    -   **Alternative hypothesis** ($H_A$): At least one group mean differs.\n2.  **Determine statistical parameters**:\n    -   Significance level $\\alpha$\n    -   Degrees of freedom for between-group ($df_b$) and within-group ($df_w$)\n    -   Find the critical F-value.\n3.  **Compute test statistic**:\n    -   Calculate **F-ratio** based on between-group and within-group variance.\n4.  **Compare results**:\n    -   Either compare $F_{\\text{obs}}$ with $F_{\\text{crit}}$ or $p$-value with $\\alpha$.\n    -   If $p < \\alpha$, reject $H_0$.\n\n## ANOVA Type I Error\n\n1.  The way to compare the means of multiple groups (i.e., more than two groups)\n\n-   Compared to multiple t-tests, the ANOVA does not inflate Type I error rates.\n    -   For F-test in ANOVA, Type I error rate is not inflated\n    -   Type I error rate is inflated when we do multiple comparison\n-   Family-wise error rate: probability of at least one Type I error occur = $1 −  (1 − \\alpha)^c$ where c =number of tests\n\n::: callout-note\n## Family-Wise Error Rate\n\n1.  The **Family-Wise Error Rate** (FWER) is the probability of making at least one **Type I error** (false positive) across a set of multiple comparisons.\n2.  In **Analysis of Variance (ANOVA)**, multiple comparisons are often necessary when analyzing the differences between group means. FWER control is crucial to ensure the validity of statistical conclusions.\n:::\n\n# ANOVA Assumptions\n\n## Overview\n\n-   Like all statistical tests, ANOVA requires certain assumptions to be met for valid conclusions:\n    -   **Independence**: Observations are independent of each other.\n    -   **Normality**: The residuals (errors) follow a normal distribution.\n    -   **Homogeneity of variance (HOV)**: The variance within each group is approximately equal.\n\n## Importance of Assumptions\n\n-   **If assumptions are violated**, the results of ANOVA may not be reliable.\n-   **We call it Robustness** as to what degree ANOVAs are not influenced by the violation of assumption.\n-   Typically:\n    -   ANOVA is **robust** to minor violations of normality, especially for large sample sizes (Central Limit Theorem).\n    -   **Not robust** to violations of independence—if independence is violated, ANOVA is inappropriate.\n    -   **Moderately robust** to HOV violations if sample sizes are equal.\n\n## Assumption 1: Independence\n\n-   **Definition**: Each observation should be independent of others.\n-   **Violations**:\n    -   Clustering of data (e.g., repeated measures).\n    -   Participants influencing each other (e.g., classroom discussions).\n-   **Check (Optional)**: Use the [Durbin-Watson test]{.redcolor}.\n-   **Solution**: Repeated Measured ANOVA, Mixed ANOVA, Multilevel Model\n-   **Consequences**: If independence is violated, [ANOVA results are not valid]{.red}.\n\n------------------------------------------------------------------------\n\n[Durbin-Watson (DW) Test]{.bluecolor .bigger}\n\n::: callout-note\n-   The Durbin-Watson test is primarily used for detecting **autocorrelation** in time-series data.\n\n-   In the context of ANOVA with independent groups, residuals are generally assumed to be independent.\n\n    -   However, it's still good practice to check this assumption, especially if there's a reason to suspect potential autocorrelation.\n:::\n\n::: callout-important\n-   Properties of DW Statistic:\n    -   $H_0$: [Independence of residual is satisfied.]{.redcolor}\n    -   Ranges from 0 to 4.\n        -   A value around 2 suggests no autocorrelation.\n        -   Values approaching 0 indicate positive autocorrelation.\n        -   Values toward 4 suggest negative autocorrelation.\n    -   P-value\n        -   P-Value: A small p-value (typically \\< 0.05) indicates violation of independency\n:::\n\n------------------------------------------------------------------------\n\n[`lmtest::dwtest()`]{.bluecolor .bigger}\n\n-   Performs the Durbin-Watson test for autocorrelation of disturbances.\n\n-   The Durbin-Watson test has the null hypothesis that the autocorrelation of the disturbances is 0.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerr1 <- rnorm(100)\n\n## generate regressor and dependent variable\nx <- rep(c(-1,1), 50)\ny1 <- 1 + x + err1 \n## perform Durbin-Watson test\ndwtest(y1 ~ x)\n```\n:::\n\n\n\nThis results suggest there are no autocorrelation at the alpha level of .05.\n\n## Assumption 2: Normality\n\n-   The **dependent variable (DV)** should be normally distributed within each group.\n-   **Assessments**:\n    -   Graphical methods: Histograms, Q-Q plots.\n    -   Statistical tests:\n        -   Shapiro-Wilk test (common)\n        -   Kolmogorov-Smirnov (KS) test (for large samples)\n        -   Anderson-Darling test (detects kurtosis issues).\n-   **Robustness**:\n    -   ANOVA is robust to normality violations for large samples.\n    -   If normality is violated, consider transformations or non-parametric tests.\n\n------------------------------------------------------------------------\n\n[Normality within Each Group]{.bluecolor .bigger}\n\n-   Assume that the DV (Y) is distributed normally within each group for ANOVA\n\n-   ANOVA is **robust** to minor violations of normality\n\n    -   So generally start with homogeneity, then assess normality\n\n![](images/clipboard-3483482689.png)\n\n---\n\n[`shapiro.test()`]{.bigger}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # For reproducibility\nsample_normal_data <- rnorm(100, mean = 50, sd = 10)  # Generate normal data\nsample_nonnormal_data <- runif(100, min = 2, max = 4)  # Generate non-normal data\n\nshapiro.test(sample_normal_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sample_normal_data\nW = 0.99388, p-value = 0.9349\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(sample_nonnormal_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sample_nonnormal_data\nW = 0.9454, p-value = 0.0004182\n```\n\n\n:::\n:::\n\n\n\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform Kolmogorov-Smirnov test against a normal distribution\nks.test(scale(sample_normal_data), \"pnorm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  scale(sample_normal_data)\nD = 0.058097, p-value = 0.8884\nalternative hypothesis: two-sided\n```\n\n\n:::\n\n```{.r .cell-code}\nks.test(scale(sample_nonnormal_data), \"pnorm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  scale(sample_nonnormal_data)\nD = 0.081096, p-value = 0.5264\nalternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\n\n\n## Assumption 3: Homogeneity of Variance (HOV)\n\n-   Variance across groups should be equal.\n-   **Assessments**:\n    -   **Levene’s test**: Tests equality of variances.\n    -   **Brown-Forsythe test**: More robust to non-normality.\n    -   **Boxplots**: Visual inspection.\n-   **What if violated?**\n    -   Use **Welch’s ANOVA**, which corrects for variance differences.\n\n## ANOVA Robustness\n\n-   **Robust to**:\n\n    -   Minor normality violations (for large samples).\n    -   Small HOV violations if group sizes are **equal**.\n\n-   **Not robust to**:\n\n    -   **Independence violations**—ANOVA is invalid if data points are dependent.\n    -   **Severe HOV violations**—Type I error rates become unreliable.\n\n-   The robustness of assumptions is something you should be carefull before/when you perform data collection. They are not something you can do after data collection has been finished.\n\n------------------------------------------------------------------------\n\n[Robustness to Violations of Normality Assumption]{.redcolor}\n\n-   ANOVA assumes that the residuals (errors) are normally distributed within each group.\n\n-   However, ANOVA is generally robust to violations of normality, particularly when **the sample size is large**.\n\n-   **Theoretical Justification**: This robustness is primarily due to the Central Limit Theorem (CLT), which states that, for sufficiently large sample sizes (typically $n≥30$ per group), the sampling distribution of the mean approaches normality, even if the underlying population distribution is non-normal.\n\n-   This means that, unless the data are heavily skewed or have extreme outliers, ANOVA results remain valid and Type I error rates are not severely inflated.\n\n------------------------------------------------------------------------\n\n### Robustness to Violations of Homogeneity of Variance\n\n-   The homogeneity of variance (homoscedasticity) assumption states that all groups should have equal variances. ANOVA can tolerate moderate violations of this assumption, particularly when:\n\n    -   **Sample sizes are equal (or nearly equal) across groups** – When groups have equal sample sizes, the F-test remains robust to variance heterogeneity because the pooled variance estimate remains balanced.\n\n    -   **The degree of variance heterogeneity is not extreme** – If the largest group variance is no more than about four times the smallest variance, ANOVA results tend to remain accurate.\n\n------------------------------------------------------------------------\n\n### ANOVA: Lack of Robustness to Violations of Independence of Errors\n\n-   The assumption of independence of errors means that observations within and between groups must be uncorrelated. Violations of this assumption severely compromise ANOVA’s validity because:\n\n    -   **Inflated Type I error rates** – If errors are correlated (e.g., due to clustering or repeated measures), standard errors are underestimated, leading to an increased likelihood of falsely rejecting the null hypothesis.\n    -   **Biased parameter estimates** – When observations are not independent, the variance estimates do not accurately reflect the true variability in the data, distorting F-statistics and p-values.\n    -   **Common sources of dependency** – Examples include nested data (e.g., students within schools), repeated measurements on the same subjects, or time-series data. In such cases, alternatives like mixed-effects models or generalized estimating equations (GEE) should be considered.\n\n## Homogeneity of Variance Violation Checking\n\n-   If HOV is **violated**, options include:\n    -   **Welch’s ANOVA** (adjusted for variance differences).\n    -   **Transforming** the dependent variable.\n    -   **Using non-parametric tests** (e.g., Kruskal-Wallis).\n\n# Omnibus ANOVA Test\n\n## Overview\n\n-   **What does it test?**\n    -   Whether there is **at least one** significant difference among means.\n-   **Limitation**:\n    -   Does **not** tell **which** groups are different.\n-   **Solution**:\n    -   Conduct **post-hoc tests**.\n\n## Individual Comparisons of Means\n\n-   If ANOVA is **significant**, follow-up tests identify **where** differences occur.\n-   **Types**:\n    -   **Planned comparisons**: Defined **before** data collection.\n    -   **Unplanned (post-hoc) comparisons**: Conducted **after** ANOVA.\n\n## Planned vs. Unplanned Comparisons\n\n-   **Planned**:\n    -   Based on **theory**.\n    -   Can be done **even if ANOVA is not significant**.\n-   **Unplanned (post-hoc)**:\n    -   **Data-driven**.\n    -   Only performed **if ANOVA is significant**.\n\n## Types of Unplanned Comparisons\n\n-   **Common post-hoc tests**:\n    1.  **Fisher’s LSD**\n    2.  **Bonferroni correction**\n    3.  **Sidák correction**\n    4.  **Tukey’s HSD**\n\n## Fisher’s LSD\n\n-   **Least Significant Difference test**.\n-   **Problem**: Does not control for **multiple comparisons** (inflated Type I error).\n\n## Bonferroni Correction\n\n-   Adjusts **alpha** to reduce **Type I error**.\n-   New alpha: $\\alpha / c$ (where $c$ is the number of comparisons).\n-   **Conservative**: Less power, avoids false positives.\n\n## Sidák Correction\n\n-   Similar to Bonferroni but slightly more powerful.\n-   New alpha: $1 - (1 - \\alpha)^{1/c}$.\n\n## Tukey’s HSD\n\n-   Controls for **Type I error** across multiple comparisons.\n-   Uses a **q-statistic** from a Tukey table.\n-   Preferred when **all pairs** need comparison.\n\n## ANOVA Example: Intervention and Verbal Acquisition\n\n### Background\n\n-   Research Question: Does an intensive intervention improve students’ verbal acquisition scores?\n-   Study Design:\n    -   4 groups: Control, G1, G2, G3 (treatment levels).\n    -   Outcome variable: Verbal acquisition score (average of three assessments).\n-   Hypotheses:\n    -   $H_0$: No difference in verbal acquisition scores across groups.\n    -   $H_A$: At least one group has a significantly different mean.\n\n## Step 1: Generate Simulated Data in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic data for 4 groups\ndata <- tibble(\n  group = rep(c(\"Control\", \"G1\", \"G2\", \"G3\"), each = 30),\n  verbal_score = c(\n    rnorm(30, mean = 70, sd = 10),  # Control group\n    rnorm(30, mean = 75, sd = 12),  # G1\n    rnorm(30, mean = 80, sd = 10),  # G2\n    rnorm(30, mean = 85, sd = 8)    # G3\n  )\n)\n\n# View first few rows\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  group   verbal_score\n  <chr>          <dbl>\n1 Control         64.4\n2 Control         67.7\n3 Control         85.6\n4 Control         70.7\n5 Control         71.3\n6 Control         87.2\n```\n\n\n:::\n:::\n\n\n\n## Step 2: Summary Statistics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics by group\ndata %>%\n  group_by(group) %>%\n  summarise(\n    mean_score = mean(verbal_score),\n    sd_score = sd(verbal_score),\n    n = n()\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  group   mean_score sd_score     n\n  <chr>        <dbl>    <dbl> <int>\n1 Control       69.5     9.81    30\n2 G1            77.1    10.0     30\n3 G2            80.2     8.70    30\n4 G3            84.2     7.25    30\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplot visualization\nggplot(data, aes(x = group, y = verbal_score, fill = group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Verbal Acquisition Scores Across Groups\", y = \"Score\", x = \"Group\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n## Step 3: Check ANOVA Assumptions\n\n### Assumption Check 1: Independence of residuals Check\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the ANOVA model\nanova_model <- lm(verbal_score ~ group, data = data)\n\n# Install lmtest package if not already installed\n# install.packages(\"lmtest\")\n\n# Load the lmtest package\nlibrary(lmtest)\n\n# Perform the Durbin-Watson test\ndw_test_result <- dwtest(anova_model)\n\n# View the test results\nprint(dw_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tDurbin-Watson test\n\ndata:  anova_model\nDW = 2.0519, p-value = 0.5042\nalternative hypothesis: true autocorrelation is greater than 0\n```\n\n\n:::\n:::\n\n\n\n-   Interpretation:\n    -   In this example, the `DW` value is close to 2, and the p-value is greater than 0.05, indicating no significant autocorrelation in the residuals.\n\n------------------------------------------------------------------------\n\n### Assumption Check 2: Normality Check\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk normality test for each group\ndata %>%\n  group_by(group) %>%\n  summarise(\n    shapiro_p = shapiro.test(verbal_score)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  group   shapiro_p\n  <chr>       <dbl>\n1 Control     0.797\n2 G1          0.961\n3 G2          0.848\n4 G3          0.568\n```\n\n\n:::\n:::\n\n\n\n-   Interpretation:\n    -   If $p>0.05$, normality assumption is not violated.\n    -   If $p<0.05$, data deviates from normal distribution.\n\n------------------------------------------------------------------------\n\n-   Alternative Check: Q-Q Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(sample = verbal_score)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~group) +\n  theme_minimal() +\n  labs(title = \"Q-Q Plot for Normality Check\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### Assumption Check 3: Homogeneity of Variance (HOV) Check\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Levene's Test for homogeneity of variance\nlibrary(car)\nleveneTest(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   3  1.1718 0.3236\n      116               \n```\n\n\n:::\n\n```{.r .cell-code}\nbartlett.test(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  verbal_score by group\nBartlett's K-squared = 3.5385, df = 3, p-value = 0.3158\n```\n\n\n:::\n:::\n\n\n\n-   Interpretation:\n    -   If $p>0.05$, variance is homogeneous (ANOVA assumption met).\n    -   If $p<0.05$, variance differs across groups (consider Welch’s ANOVA).\n\n## Step 4: Perform One-Way ANOVA\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_model <- aov(verbal_score ~ group, data = data)\nsummary(anova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ngroup         3   3492  1164.1   14.33 5.28e-08 ***\nResiduals   116   9424    81.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n-   Interpretation:\n    -   If $p<0.05$, at least one group mean is significantly different.\n    -   If $p>0.05$, fail to reject $H0$ (no significant differences).\n\n## Step 5: Post-Hoc Tests (Tukey’s HSD)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tukey HSD post-hoc test\ntukey_results <- TukeyHSD(anova_model)\nround(tukey_results$group, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             diff    lwr    upr p adj\nG1-Control  7.611  1.545 13.677 0.008\nG2-Control 10.715  4.649 16.782 0.000\nG3-Control 14.720  8.654 20.786 0.000\nG2-G1       3.104 -2.962  9.170 0.543\nG3-G1       7.109  1.042 13.175 0.015\nG3-G2       4.005 -2.062 10.071 0.318\n```\n\n\n:::\n:::\n\n\n\n-   Interpretation:\n    -   Identifies which groups differ.\n    -   If $p<0.05$, the groups significantly differ.\n        -   G1-Control\n        -   G2-Control\n        -   G3-Control\n        -   G3-G1\n\n------------------------------------------------------------------------\n\n### multcomp: Choose which method for adjustment of P-Values\n\nOther multicomparison method allow you to choose which method for adjust p-values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(multcomp)\n# install.packages(\"multcomp\")\n### set up multiple comparisons object for all-pair comparisons\n# head(model.matrix(anova_model))\ncomprs <- rbind(\n  \"G1 - Ctrl\" = c(0, 1, 0,  0),\n  \"G2 - Ctrl\" = c(0, 0, 1,  0),\n  \"G3 - Ctrl\" = c(0, 0, 0,  1),\n  \"G2 - G1\" = c(0, -1, 1,  0),\n  \"G3 - G1\" = c(0, -1, 0,  1),\n  \"G3 - G2\" = c(0, 0, -1,  1)\n)\ncht <- glht(anova_model, linfct = comprs)\nsummary(cht, test = adjusted(\"fdr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = verbal_score ~ group, data = data)\n\nLinear Hypotheses:\n               Estimate Std. Error t value Pr(>|t|)    \nG1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** \nG2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***\nG3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***\nG2 - G1 == 0      3.104      2.327   1.334  0.18487    \nG3 - G1 == 0      7.109      2.327   3.055  0.00419 ** \nG3 - G2 == 0      4.005      2.327   1.721  0.10555    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- fdr method)\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### Side-by-Side comparison of two methods\n\n::::: columns\n::: {.column width=\"50%\"}\n#### TukeyHSD method\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"TukeyHSD method\" code-line-numbers=\"false\"}\n             diff    lwr    upr p adj\nG1-Control  7.611  1.545 13.677 0.008\nG2-Control 10.715  4.649 16.782 0.000\nG3-Control 14.720  8.654 20.786 0.000\nG2-G1       3.104 -2.962  9.170 0.543\nG3-G1       7.109  1.042 13.175 0.015\nG3-G2       4.005 -2.062 10.071 0.318\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n#### FDR method\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"multcomp package\" code-line-numbers=\"false\"}\n               Estimate Std. Error t value Pr(>|t|)    \nG1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** \nG2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***\nG3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***\nG2 - G1 == 0      3.104      2.327   1.334  0.18487    \nG3 - G1 == 0      7.109      2.327   3.055  0.00419 ** \nG3 - G2 == 0      4.005      2.327   1.721  0.10555 \n```\n:::\n\n\n:::\n\n-   The differences in p-value adjustment between the `TukeyHSD` method and the `multcomp` package stem from how each approach calculates and applies the multiple comparisons correction. Below is a detailed explanation of these differences\n:::::\n\n### Comparison of Differences\n\n| Feature | `TukeyHSD()` (Base R) | `multcomp::glht()` |\n|-------------------|-----------------------------|-------------------------|\n| **Distribution Used** | Studentized Range (q-distribution) | t-distribution |\n| **Error Rate Control** | Strong FWER control | Flexible error control |\n| **Simultaneous Confidence Intervals** | Yes | Typically not (depends on method used) |\n| **Adjustment Method** | Tukey-Kramer adjustment | Single-step, Westfall, Holm, Bonferroni, etc. |\n| **P-value Differences** | More conservative (larger p-values) | Slightly different due to t-distribution |\n\n## Step 6: Reporting ANOVA Results\n\nA one-way ANOVA was conducted to examine the effect of an intensive intervention on verbal acquisition scores. There was a statistically significant difference between groups, $F(3,116)=14.33$, $p<.001$. Tukey's post-hoc comparisons revealed that the G3 intervention group (M=84.2, SD=7.25) had significantly higher scores than the Control (M=69.5,SD=9.81) and G1 (M=77.1,SD=10.0) groups (all p\\<.05). However, no significant difference was found between G2 and G3 (p=.31). These findings suggest that higher intervention intensity improves verbal acquisition performance.\n\n# Exercise: Effect of Sleep Duration on Cognitive Performance\n\n## Background\n\n-   Research Question:\n\n    -   Does the amount of sleep affect cognitive performance on a standardized test?\n\n-   Study Design\n\n    -   Independent variable: Sleep duration (3 groups: Short (≤5 hrs), Moderate (6-7 hrs), Long (≥8 hrs)).\n    -   Dependent variable: Cognitive performance scores (measured as test scores out of 100).\n\n## Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate synthetic data for sleep study\nsleep_data <- tibble(\n  sleep_group = rep(c(\"Short\", \"Moderate\", \"Long\"), each = 30),\n  cognitive_score = c(\n    rnorm(30, mean = 65, sd = 10),  # Short sleep group (≤5 hrs)\n    rnorm(30, mean = 75, sd = 12),  # Moderate sleep group (6-7 hrs)\n    rnorm(30, mean = 80, sd = 8)    # Long sleep group (≥8 hrs)\n  )\n)\n\n# View first few rows\nhead(sleep_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  sleep_group cognitive_score\n  <chr>                 <dbl>\n1 Short                  78.7\n2 Short                  59.4\n3 Short                  68.6\n4 Short                  71.3\n5 Short                  69.0\n6 Short                  63.9\n```\n\n\n:::\n:::\n\n\n\nGo through all six steps.\n\n## Answer:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 5: Perform One-Way ANOVA\nanova_sleep <- aov(cognitive_score ~ sleep_group, data = sleep_data)\nsummary(anova_sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nsleep_group  2   3764  1881.9   15.88 1.32e-06 ***\nResiduals   87  10311   118.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: heimu\n-   A one-way ANOVA was conducted to examine the effect of sleep duration on cognitive performance.\n\n-   There was a statistically significant difference in cognitive test scores across sleep groups, $F(2,87)=15.88$,$p<.001$.\n\n-   Tukey's post-hoc test revealed that participants in the Long sleep group (M=81.52,SD=6.27) performed significantly better than those in the Short sleep group (M=65.68,SD=12.55), p\\<.01.\n\n-   These results suggest that inadequate sleep is associated with lower cognitive performance.\n:::\n",
    "supporting": [
      "ESRM64103_Lecture04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}