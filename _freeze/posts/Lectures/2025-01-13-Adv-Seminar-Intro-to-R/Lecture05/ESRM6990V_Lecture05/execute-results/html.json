{
  "hash": "12b7db6d4a383a442c4f343be4f88279",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 05: Text Data Analysis and Data Summary\"\nsubtitle: \"R Function\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2025-02-05\"\nsidebar: false\nexecute: \n  eval: true\n  echo: true\n  warning: false\noutput-location: default\ncode-annotations: below\nhighlight-style: \"nord\"\nformat: \n  html:\n    code-tools: true\n    code-line-numbers: false\n    code-fold: false\n    number-offset: 0\n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"ESRM 64503 - Lecture 03: Object/Function/Package\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\n---\n\n\n\n\n## Class Outline\n\n1.  String and Text Data\n2.  Basic Data Summary Using `dplyr`\n3.  Case Study of Trump Tweets\n\n# String and Text Data\n\n## Special characters\n\n-   Quotation marks can be used in strings. Note that a single quote can only be used within a double quote or the other way around.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"I'm a student\"\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"I'm a student\"\n```\n\n\n:::\n\n```{.r .cell-code}\n'He says \"it is ok!\"'\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"He says \\\"it is ok!\\\"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n\"I'm a student. He says \\\"it is ok\\\"!\"\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"I'm a student. He says \\\"it is ok\\\"!\"\n```\n\n\n:::\n:::\n\n\n\n\n-   It is generally a good idea to “escape” the quotation marks using the backslash `\\`.\n    -   If it is needed in a string, one needs to use two of them `\\\\`. For example,\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"To show \\\\ , we need to use two of them.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTo show \\ , we need to use two of them.\n```\n\n\n:::\n:::\n\n\n\n\n::: callout-tip\n`cat()`: Print the objects.\n:::\n\n------------------------------------------------------------------------\n\n### Other special characters\n\n-   [`\\t` and `\\n`]{.redcolor}: Special characters for changing lines and tab.\n\n-   `\\u` for escaping special characters in Unicode other than alphabetical letters. https://unicode-table.com/en/\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- \"This is the first line. \\nThis the \\t second line with a tab.\"\ncat(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is the first line. \nThis the \t second line with a tab.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\u03BC \\u03A3 \\u03B1 \\u03B2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nμ Σ α β\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"❤ ♫\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n❤ ♫\n```\n\n\n:::\n:::\n\n\n\n\n## Basic string operations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n# or \nlibrary(stringr) # install.packages(\"stringr\")\n```\n:::\n\n\n\n\n-   A vector of strings\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet1 <- \"MAKE AMERICA GREAT AGAIN!\" \ntweet2 <- \"Congratulations @ClemsonFB! https://t.co/w8viax0OWY\"\n\n(tweet <- c(tweet1, tweet2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MAKE AMERICA GREAT AGAIN!\"                          \n[2] \"Congratulations @ClemsonFB! https://t.co/w8viax0OWY\"\n```\n\n\n:::\n:::\n\n\n\n\n-   Change the lower/upper case of strings\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntolower(tweet)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"make america great again!\"                          \n[2] \"congratulations @clemsonfb! https://t.co/w8viax0owy\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntoupper(tweet)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MAKE AMERICA GREAT AGAIN!\"                          \n[2] \"CONGRATULATIONS @CLEMSONFB! HTTPS://T.CO/W8VIAX0OWY\"\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n[Basic string operations (Cont.)]{.bluecolor .bigger}\n\n-   Calculate length of a string\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnchar(tweet1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25\n```\n\n\n:::\n\n```{.r .cell-code}\nstr_length(tweet) # function of package stringr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25 51\n```\n\n\n:::\n:::\n\n\n\n\n-   Split strings based on a [pattern]{.redcolor}.\n    1.  pattern tells how to split a string.\n    2.  the output is a list.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_split(tweet, pattern = \" \")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"MAKE\"    \"AMERICA\" \"GREAT\"   \"AGAIN!\" \n\n[[2]]\n[1] \"Congratulations\"         \"@ClemsonFB!\"            \n[3] \"https://t.co/w8viax0OWY\"\n```\n\n\n:::\n\n```{.r .cell-code}\n## Use str_split_1 to return one vector rather than list\nstr_split_1(tweet2, pattern = \" https://\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Congratulations @ClemsonFB!\" \"t.co/w8viax0OWY\"            \n```\n\n\n:::\n:::\n\n\n\n\n-   Check regular expresssion for more pattern detection\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvignette(\"regular-expressions\")\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n[Basic string operations (Cont.)]{.bluecolor .bigger}\n\n-   Combine string vectors into one\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(tweet.words <- unlist(str_split(tweet, pattern = \" \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MAKE\"                    \"AMERICA\"                \n[3] \"GREAT\"                   \"AGAIN!\"                 \n[5] \"Congratulations\"         \"@ClemsonFB!\"            \n[7] \"https://t.co/w8viax0OWY\"\n```\n\n\n:::\n\n```{.r .cell-code}\nstr_c(tweet.words, collapse=\" \")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MAKE AMERICA GREAT AGAIN! Congratulations @ClemsonFB! https://t.co/w8viax0OWY\"\n```\n\n\n:::\n:::\n\n\n\n\n# AI + Text Analysis\n\n## Motivating Example: Extract Structural Information from Text\n\n-   Make use of language models to extract key information more easily\n\n-   This is not a step-by-step guide of using LLMs, but a motivating example so that you may want to explore more usage of LLMs in data analysis.\n\n    -   The details of techniques can be found in this [link](https://ellmer.tidyverse.org/articles/structured-data.html).\n\n-   Note that the following code cannot be successfully executed in your local device without ChatGPT account and access to API keys.\n\n    -   Here is a video [tutorial](https://www.youtube.com/watch?v=B_Fbd_vxZyE) for using ChatGPT in R.\n\n## Extract Structured data\n\n-   To programming LLMs for text analysis, I suggested using `ellmer` package (the [manual](https://ellmer.tidyverse.org/)) which was developed by the same person who developed `tidyverse` package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nchat <- chat_openai(echo = FALSE, model = \"gpt-4o-mini\")\n\nchat$extract_data(\n  tweet2,\n  type = type_object(\n    URL = type_string(\"URL address starting with 'http'\")\n  )  \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$URL\n[1] \"https://t.co/w8viax0OWY\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$extract_data(\n  \"My name is Susan and I'm 13 years old. I like traveling and hiking.\",\n  type = type_object(\n    age = type_number(\"Age, in years.\"), # extract the numeric information as \"age\" from the provided text\n    name = type_string(\"Name, Uppercase\"), # extract the character information as \"name\" from the provided text\n    hobbies = type_array(\n      description = \"List of hobbies.\",\n      items = type_string()\n    )\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$age\n[1] 13\n\n$name\n[1] \"SUSAN\"\n\n$hobbies\n[1] \"traveling\" \"hiking\"   \n```\n\n\n:::\n:::\n\n\n\n\n-   Here, `type_()`\n\n## Open Sourced LLM - LLAMA\n\n-   You can freely download the open source LLM - Llama developed by Meta on this [link](https://ollama.com/).\n\n-   Teaching how to set up the LLMs is out of scope of this class. There are a lot of tutorials that you can use. For example, this [medium post](https://medium.com/@arunpatidar26/run-llm-locally-ollama-8ea296747505).\n\n-   Just showcase how you can extract key information. Llama needs more guide information to extract certain key words than ChatGPT.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat <- chat_ollama(model = \"llama3.2\")\n\nchat$extract_data(\n  \"My name is Jihong and I'm an assistant professor. I like reading and hiking.\",\n  type = type_object(\n    job = type_string(\"Job\"), # extract the numeric information as \"age\" from the provided text\n    name = type_string(\"Name of the person, uppercase\"), # extract the character information as \"name\" from the provided text\n    hobbies = type_array(\n      description = \"List of hobbies. transform to Uppercase\",\n      items = type_string()\n    )\n  )\n)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$job\n[1] \"assistant\"\n\n$name\n[1] \"Jihong\"\n\n$hobbies\n[1] \"reading\" \"hiking\" \n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Deepseek distilled model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat <- chat_ollama(model = \"deepseek-r1:8b\")\n\nText_to_summarize <- \"Researchers have devised an ecological momentary assessment study following 80 students (mean age = 20.38 years, standard deviation = 3.68, range = 18–48 years; n = 60 female, n = 19 male, n = 1 other) from Leiden University for 2 weeks in their daily lives.\"\n\nchat$extract_data(\n  Text_to_summarize,\n  type = type_object(\n    N = type_number(\"total sample size\"),\n    Age = type_number(\"Average age in years\"),\n    Method = type_string(\"Assessment for data collection\"),\n    Participants = type_string(\"source of data collection\"),\n    Days = type_string(\"Duration of data collection\")\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$N\n[1] 6\n\n$Age\n[1] 20.38\n\n$Method\n[1] \"Momentary Assessment\"\n\n$Participants\n[1] \"Our participants are a diverse group of students.\"\n\n$Days\n[1] \"Two weeks.\"\n```\n\n\n:::\n:::\n\n\n\n\n# Data Transformation\n\n## Overview\n\n-   Visualization is a key tool for generating insights.\n-   Data often needs transformation to fit the desired analysis or visualization.\n\n## Prerequisites\n\n-   Learn to use the `dplyr` package for data transformation.\n-   Explore the `nycflights13` dataset.\n\n### Required Libraries\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nycflights13) # `nycflights13` for the dataset flights.\nlibrary(tidyverse)\nglimpse(flights)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n```\n\n\n:::\n:::\n\n\n\n\n-   `glimpse()` for the quick screening of the data.\n\n## `dplyr` Basics\n\n------------------------------------------------------------------------\n\n### Core Functions\n\n-   `filter()`: Subset rows based on conditions.\n-   `arrange()`: Reorder rows.\n-   `select()`: Choose columns by name.\n-   `mutate()`: Create new columns.\n-   `summarize()`: Aggregate data.\n\n------------------------------------------------------------------------\n\n### `filter()`: select cases based on condtions\n\n-   Use `|>` (Preferred) or `|>` to chain multiple operations.\n-   Select flights on January 1st:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njan1 <- flights |> filter(month == 1, day == 1)\njan1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 842 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 832 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n```\n\n\n:::\n:::\n\n\n\n\n-   Comparison operators: `==`, `!=`, `<`, `<=`, `>`, `>=`.\n-   Logical operators: `&`, `|`, `!`.\n-   Include: `%in%`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights |> filter(month != 1) # Months other than January\nflights |> filter(month %in% 1:10) # Months other than Nov. and Dec.\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### `arrange()`: Arranging Rows\n\n-   Sort flights by departure delay:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights[, c(\"year\", \"month\", \"day\", \"dep_delay\")] |> arrange(dep_delay)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 4\n    year month   day dep_delay\n   <int> <int> <int>     <dbl>\n 1  2013    12     7       -43\n 2  2013     2     3       -33\n 3  2013    11    10       -32\n 4  2013     1    11       -30\n 5  2013     1    29       -27\n 6  2013     8     9       -26\n 7  2013    10    23       -25\n 8  2013     3    30       -25\n 9  2013     3     2       -24\n10  2013     5     5       -24\n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n\n\n-   Descending order:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights[, c(\"year\", \"month\", \"day\", \"dep_delay\")] |> arrange(desc(dep_delay))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 4\n    year month   day dep_delay\n   <int> <int> <int>     <dbl>\n 1  2013     1     9      1301\n 2  2013     6    15      1137\n 3  2013     1    10      1126\n 4  2013     9    20      1014\n 5  2013     7    22      1005\n 6  2013     4    10       960\n 7  2013     3    17       911\n 8  2013     6    27       899\n 9  2013     7    22       898\n10  2013    12     5       896\n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### `select()`: Selecting Columns\n\n-   Choose specific columns:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights |> select(year, month, day)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n## is equivalent to \n# flights[, c(\"year\", \"month\", \"day\")]\n```\n:::\n\n\n\n\n-   Helper functions for selecting the variables: `starts_with()`, `ends_with()`, `contains()`, `matches()`, `num_range()`.\n\n------------------------------------------------------------------------\n\n### `mutate()`: Adding New Variables\n\n-   Create new columns:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_sml <- flights |> select(\n  year:day,\n  ends_with(\"delay\"),\n  distance,\n  air_time\n)\n\nflights_sml |> mutate(\n  gain = dep_delay - arr_delay,\n  speed = distance / air_time * 60\n) |> \n  select(year:day, gain, speed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 5\n    year month   day  gain speed\n   <int> <int> <int> <dbl> <dbl>\n 1  2013     1     1    -9  370.\n 2  2013     1     1   -16  374.\n 3  2013     1     1   -31  408.\n 4  2013     1     1    17  517.\n 5  2013     1     1    19  394.\n 6  2013     1     1   -16  288.\n 7  2013     1     1   -24  404.\n 8  2013     1     1    11  259.\n 9  2013     1     1     5  405.\n10  2013     1     1   -10  319.\n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n\n\n-   Use `transmute()` to keep only the new variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_sml |> transmute(\n  gain = dep_delay - arr_delay,\n  speed = distance / air_time * 60\n) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 2\n    gain speed\n   <dbl> <dbl>\n 1    -9  370.\n 2   -16  374.\n 3   -31  408.\n 4    17  517.\n 5    19  394.\n 6   -16  288.\n 7   -24  404.\n 8    11  259.\n 9     5  405.\n10   -10  319.\n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n#### `mutate()` and `case_when`: Create new categories\n\n-   `case_when`: The left hand side (LHS) determines which values match this case. The right hand side (RHS) provides the replacement value.\n    -   The LHS inputs must evaluate to logical vectors.\n    -   The RHS inputs will be coerced to their common type. In following case, it is character type\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1:10, NA)\ncategorized_x <- case_when(\n  x %in% 1:3 ~ \"low\",\n  x %in% 4:7 ~ \"med\",\n  x %in% 7:10 ~ \"high\",\n  is.na(x) ~ \"Missing\"\n)\ncategorized_x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"low\"     \"low\"     \"low\"     \"med\"     \"med\"     \"med\"     \"med\"    \n [8] \"high\"    \"high\"    \"high\"    \"Missing\"\n```\n\n\n:::\n:::\n\n\n\n\n-   Combine `mutate()` and `case_when()` to create a new categorical variable\n    -   `na.rm = TRUE` to ignore the NA values when calculating the mean\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights |> \n  mutate(\n    Half_year = case_when(\n      month %in% 1:6 ~ 1,\n      month %in% 6:12 ~ 2,\n      is.na(month) ~ 999\n    )\n  ) |> \n  group_by(year, Half_year) |> \n  summarise(\n    Mean_dep_delay = mean(dep_delay, na.rm = TRUE)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n# Groups:   year [1]\n   year Half_year Mean_dep_delay\n  <int>     <dbl>          <dbl>\n1  2013         1           13.7\n2  2013         2           11.6\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### `summarize()` with `group_by()`: Summarizing Data\n\n-   Calculate average delay by destination:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nby_dest <- flights |> group_by(dest)\ndelay <- summarize(by_dest,\n  count = n(),\n  dist = mean(distance, na.rm = TRUE),\n  delay = mean(arr_delay, na.rm = TRUE)\n)\n```\n:::\n\n\n\n\n-   Visualize the results:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = delay, mapping = aes(x = dist, y = delay)) +\n  geom_point(aes(size = count), alpha = 1/3) +\n  geom_smooth(se = FALSE)\n```\n\n::: {.cell-output-display}\n![](ESRM6990V_Lecture05_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### `group_by()` and `ungroup()`: Grouping and Ungrouping\n\n-   Group data by multiple variables:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nby_day <- flights |> group_by(year, month, day)\nby_day |> \n  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day avg_dep_delay\n   <int> <int> <int>         <dbl>\n 1  2013     1     1         11.5 \n 2  2013     1     2         13.9 \n 3  2013     1     3         11.0 \n 4  2013     1     4          8.95\n 5  2013     1     5          5.73\n 6  2013     1     6          7.15\n 7  2013     1     7          5.42\n 8  2013     1     8          2.55\n 9  2013     1     9          2.28\n10  2013     1    10          2.84\n# ℹ 355 more rows\n```\n\n\n:::\n:::\n\n\n\n\n-   Ungroup data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nby_day <- ungroup(by_day)\n```\n:::\n\n\n\n\n# Case Study: Analysis of Trump Tweets\n\n## Download of Trump Tweets\n\n-   For demonstration, we will analyze the tweets from President Donald Trump from 2009 to 2017.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dslabs) # install.packages(\"dslabs\")\nlibrary(tidyverse) \nglimpse(dslabs::trump_tweets)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 20,761\nColumns: 8\n$ source                  <chr> \"Twitter Web Client\", \"Twitter Web Client\", \"T…\n$ id_str                  <chr> \"6971079756\", \"6312794445\", \"6090839867\", \"577…\n$ text                    <chr> \"From Donald Trump: Wishing everyone a wonderf…\n$ created_at              <dttm> 2009-12-23 12:38:18, 2009-12-03 14:39:09, 200…\n$ retweet_count           <int> 28, 33, 13, 5, 7, 4, 2, 4, 1, 22, 7, 5, 1, 1, …\n$ in_reply_to_user_id_str <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ favorite_count          <int> 12, 6, 11, 3, 6, 5, 2, 10, 4, 30, 6, 3, 4, 3, …\n$ is_retweet              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n```\n\n\n:::\n:::\n\n\n\n\n::: callout-note\n-   `source`. Device or service used to compose tweet.\n-   `id_str`. Tweet ID.\n-   `text`. Tweet.\n-   `created_at`. Data and time tweet was tweeted.\n-   `retweet_count`. How many times tweet had been retweeted at time dataset was created.\n-   `in_reply_to_user_id_str`. If a reply, the user id of person being replied to.\n-   `favorite_count`. Number of times tweet had been favored at time dataset was created.\n-   `is_retweet`. A logical telling us if it is a retweet or not.\n:::\n\n## Basic summary of Trump tweets\n\n-   Where the tweets were sent from\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntrump_tweets |> \n  group_by(\n    source\n  ) |> \n  summarise(N = n()) |> \n  arrange(desc(N))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 19 × 2\n   source                       N\n   <chr>                    <int>\n 1 Twitter Web Client       10718\n 2 Twitter for Android       4652\n 3 Twitter for iPhone        3962\n 4 TweetDeck                  468\n 5 TwitLonger Beta            288\n 6 Instagram                  133\n 7 Media Studio               114\n 8 Facebook                   104\n 9 Twitter Ads                 96\n10 Twitter for BlackBerry      78\n11 Mobile Web (M5)             54\n12 Twitter for iPad            39\n13 Twitlonger                  22\n14 Twitter QandA               10\n15 Vine - Make a Scene         10\n16 Periscope                    7\n17 Neatly For BlackBerry 10     4\n18 Twitter Mirror for iPad      1\n19 Twitter for Websites         1\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Histogram of tweet sources\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn_source_tbl <- trump_tweets |> \n  group_by(source) |> \n  summarise(\n    N = n()\n  )\nggplot(data = n_source_tbl) +\n  geom_col(aes(x = fct_reorder(source, N), y = N)) +\n  geom_label(aes(x = fct_reorder(source, N), y = N, label = N), nudge_y = 500) +\n  labs(x = \"\", y = \"\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](ESRM6990V_Lecture05_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### The length of each tweet\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(str_length(trump_tweets$text))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0    81.0   119.0   106.4   137.0   320.0 \n```\n\n\n:::\n:::\n\n\n\n\n-   Most tweets have the length from 100 to 150 characters.\n\n-   Filter the tweet less than 20 characters\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrump_short_tweets <- trump_tweets |> \n  mutate(\n    N_characters = str_length(text)\n  ) |> \n  filter(N_characters <= 20)\n```\n:::\n\n\n\n\n## Extract Frequent Words from the Short Tweets\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrump_short_clean_tweets <- trump_short_tweets |> \n  mutate(\n    clean_text = str_remove(text, \"@\\\\S+ \")\n  ) |> \n  mutate(\n    clean_text2 = str_remove_all(clean_text, \"[[:punct:]]\") # Remove punctuation\n  ) |> \n  select(text, clean_text, clean_text2)\ntrump_short_clean_tweets\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    text           clean_text        clean_text2\n1           @MjTis  Yes!                 Yes!                Yes\n2       @mayorgas  Good.                Good.               Good\n3    @ReighnSeed Thanks!              Thanks!             Thanks\n4        @CRLindke True!                True!               True\n5    @chidrole   Hi John              Hi John            Hi John\n6         @toddulu  Fun!                 Fun!                Fun\n7   @MandiDeren  I will!              I will!             I will\n8   @JNorr11  March 3rd.           March 3rd.          March 3rd\n9   @jean_penny  Hi Jean              Hi Jean            Hi Jean\n10  @martinwatt382 True!                True!               True\n11   @achieverdan Thanks               Thanks             Thanks\n12  @damonpostal  Thanks               Thanks             Thanks\n13  @Arnoldmuks  Thanks.              Thanks.             Thanks\n14    @parisitony  True!                True!               True\n15        @mjb7  Thanks!              Thanks!             Thanks\n16    @AmauryD3  Thanks!              Thanks!             Thanks\n17  @RetroJohnny  Thanks               Thanks             Thanks\n18      @utxgent  Thanks               Thanks             Thanks\n19   @shanedent  Agreed.              Agreed.             Agreed\n20      @kc5yvv  Thanks!              Thanks!             Thanks\n21      @Jixllr  Thanks!              Thanks!             Thanks\n22  @Corene7  Thank you!           Thank you!          Thank you\n23     @EMC_xoxo Thanks!              Thanks!             Thanks\n24  @BriannaTurano Good!                Good!               Good\n25  @Relmark Thanks Mark          Thanks Mark        Thanks Mark\n26   @ojmart  Thank you!           Thank you!          Thank you\n27       @acman84  True!                True!               True\n28        @eheinlen Yes.                 Yes.                Yes\n29      @stass13 Thanks!              Thanks!             Thanks\n30    @JakfromBk Thanks!              Thanks!             Thanks\n31   @darbizman  Thanks!              Thanks!             Thanks\n32   @usa67us  Thank you            Thank you          Thank you\n33   @Ajb073  Thank you.           Thank you.          Thank you\n34       @baeck I agree!             I agree!            I agree\n35    @OwenKelly Thanks!              Thanks!             Thanks\n36   @trumthomp  Thanks.              Thanks.             Thanks\n37   @jeffreypham Great!               Great!              Great\n38   @wakimotho  Thanks.              Thanks.             Thanks\n39  @Brown97M Thank you.           Thank you.          Thank you\n40  @StefThomason Great!               Great!              Great\n41   @clary77 Thank you.           Thank you.          Thank you\n42     @nolanitenyc Sad!                 Sad!                Sad\n43    @WantonSoop  True!                True!               True\n44  @MeghanFaheyy Great!               Great!              Great\n45    @MikeDemkiw  True!                True!               True\n46  @rrpsllc  Thank you.           Thank you.          Thank you\n47       @JShue20 Great!               Great!              Great\n48     @wsaleen  Thanks.              Thanks.             Thanks\n49   @SonnyGirard  True.                True.               True\n50  @CasaMadison Thanks.              Thanks.             Thanks\n51      @RyDev22  Great!               Great!              Great\n52     @bgholms  Thanks.              Thanks.             Thanks\n53  @DJCastiello Thanks.              Thanks.             Thanks\n54        @905K9 Thanks.              Thanks.             Thanks\n55  @jeffanie Thank you.           Thank you.          Thank you\n56   @BillageT1 I agree.             I agree.            I agree\n57        @dalasner Yes.                 Yes.                Yes\n58  @ewilli1204  Thanks!              Thanks!             Thanks\n59           Here we go!          Here we go!         Here we go\n60   @mathewaiken Great!               Great!              Great\n61     @dhtherex Thanks.              Thanks.             Thanks\n62  @tkeller316  Thanks!              Thanks!             Thanks\n63  @akmackey   So true!             So true!            So true\n64   @MarlenaWells True.                True.               True\n65   @Jungledad  Thanks!              Thanks!             Thanks\n66   @TheLanes08 Thanks.              Thanks.             Thanks\n67  @AaronPha_q  Thanks.              Thanks.             Thanks\n68       Fox and Friends      Fox and Friends    Fox and Friends\n69  @nikkio   Thank you.           Thank you.          Thank you\n70  @Marilynn555 Thanks!              Thanks!             Thanks\n71  @J_TJOHNSON   Thanks               Thanks             Thanks\n72     @cloverc8 Thanks!              Thanks!             Thanks\n73  @Chuckdhere So true!             So true!            So true\n74   @merryman34 Thanks!              Thanks!             Thanks\n75     @samtrisi Thanks!              Thanks!             Thanks\n76     @Timc1021 Thanks!              Thanks!             Thanks\n77   @garrendj   Thanks!              Thanks!             Thanks\n78  @rogergreig  Thanks.              Thanks.             Thanks\n79    @aa_newey  Thanks.              Thanks.             Thanks\n80       @mjobrey27   66                   66                 66\n81     @tvmario  Thanks!              Thanks!             Thanks\n82      @RachelYohee HI!                  HI!                 HI\n83       @KelBug99   HI!                  HI!                 HI\n84   @DanielHR27 Thanks!              Thanks!             Thanks\n85      @cloverc8 Great!               Great!              Great\n86   @noall   Thank you!           Thank you!          Thank you\n87     @lcamp34  Thanks.              Thanks.             Thanks\n88  @eboy12318   Thanks!              Thanks!             Thanks\n89       @An3T   Thanks.              Thanks.             Thanks\n90      @Oloch3  Thanks.              Thanks.             Thanks\n91     @PIETIEV  Thanks!              Thanks!             Thanks\n92       @MKasun Thanks.              Thanks.             Thanks\n93     @Dawson304  True!                True!               True\n94    @leaheary  Thanks.              Thanks.             Thanks\n95  @DanScavino   Great!               Great!              Great\n96   @JMHPolitics  True!                True!               True\n97    @lucasnahn Thanks.              Thanks.             Thanks\n98       @Page08   True!                True!               True\n99  @coolioneal  Thanks.              Thanks.             Thanks\n100   @cloverc8  Thanks.              Thanks.             Thanks\n101  @vinceritch Thanks.              Thanks.             Thanks\n102     @CMCoe Have fun!            Have fun!           Have fun\n103      @JMonaski1 Sad!                 Sad!                Sad\n104   @Polatseck Thanks!              Thanks!             Thanks\n105   @ajshaw1003 Great!               Great!              Great\n106 @Sandy070707 Thanks.              Thanks.             Thanks\n107   @rockster62   Wow!                 Wow!                Wow\n108 @anitanewell   True!                True!               True\n109  @rickywatt1 Thanks.              Thanks.             Thanks\n110      @Skooogs Great!               Great!              Great\n111      @kjclt1 Thanks!              Thanks!             Thanks\n112      @NebSeb Thanks.              Thanks.             Thanks\n113   @kylejkrantz True!                True!               True\n114    @L100RAN  Thanks!              Thanks!             Thanks\n115     @theoclass True.                True.               True\n116  @Lee_Renn1  Thanks.              Thanks.             Thanks\n117     @ptr2mm  Thanks.              Thanks.             Thanks\n118 @drunkrhino   Great!               Great!              Great\n119 @farm_mom   So true!             So true!            So true\n120 @Dzeroseven   Wrong!               Wrong!              Wrong\n121 @dwangpow   So true!             So true!            So true\n122 Fox and.Friends now! Fox and.Friends now! Fox andFriends now\n123 @kenzig   Thank you.           Thank you.          Thank you\n124 @danshaw2012   True!                True!               True\n125   @Fordette12 Great!               Great!              Great\n126 @pbanura or Ireland?          or Ireland?         or Ireland\n127  @KJLLeonard Thanks!              Thanks!             Thanks\n128 @tubby511 Thank you!           Thank you!          Thank you\n129  @Eurphrosye  Great!               Great!              Great\n130  @nicla_b Thank you.           Thank you.          Thank you\n131     @Judare So true!             So true!            So true\n132   @Cgroll33 Totally!             Totally!            Totally\n133   @Wicz_2003 Thanks.              Thanks.             Thanks\n134    @cbmcq Thank you.           Thank you.          Thank you\n135    @amandaphantm No.                  No.                 No\n136 @NYCTiggy Hi Joanna.           Hi Joanna.          Hi Joanna\n137     @onedaddy26 Yes.                 Yes.                Yes\n138 @lisakaminsky   Yes.                 Yes.                Yes\n139     @TheRC3  Thanks.              Thanks.             Thanks\n140  @BeccaPiano Thanks.              Thanks.             Thanks\n141    @TomPuiseux  Yes!                 Yes!                Yes\n142 @Travianno   Thanks!              Thanks!             Thanks\n143    @gingram66 Wrong!               Wrong!              Wrong\n144  @TishaLewis Thanks.              Thanks.             Thanks\n145 @resplatt123 Thanks.              Thanks.             Thanks\n146     @SunnyJL52  True                 True               True\n147     @cous13  Thanks.              Thanks.             Thanks\n148  @DylMyr  Go for it.           Go for it.          Go for it\n149 @Corte74  It helped!           It helped!          It helped\n150 @tbott22  Very true!           Very true!          Very true\n151 @Roddy_Brown Thanks!              Thanks!             Thanks\n152  @abaldwin31 Thanks.              Thanks.             Thanks\n153       @nikkio  True!                True!               True\n154  @Colvio117  Thanks!              Thanks!             Thanks\n155  @SiegristA  Thanks!              Thanks!             Thanks\n156    @Core15   Thanks.              Thanks.             Thanks\n157       @piano807 Wow!                 Wow!                Wow\n158   @HRdiva_NY Thanks.              Thanks.             Thanks\n159   @Nautilus916 True!                True!               True\n160  @afolasoji So true.             So true.            So true\n161     @RamboFo  Great!               Great!              Great\n162 @DoroMundo   Thanks!              Thanks!             Thanks\n163   @jamezypell   Yes!                 Yes!                Yes\n164        Me, by a lot!        Me, by a lot!        Me by a lot\n165    @cjdew67  Thanks!              Thanks!             Thanks\n166    @camarrone Great!               Great!              Great\n167  @GauravK725  Great!               Great!              Great\n168 @KathyLooper Thanks.              Thanks.             Thanks\n169 @ccade937 Thank you.           Thank you.          Thank you\n170 @adamjackson73  Yes.                 Yes.                Yes\n171    @Bogart_Ron True!                True!               True\n172  @AlxG21  Thank you!           Thank you!          Thank you\n173  @BrosHoban  Thanks!              Thanks!             Thanks\n174           @riggs_deb           @riggs_deb           riggsdeb\n175         @Brian_Legit         @Brian_Legit         BrianLegit\n176     @StephenHCornell     @StephenHCornell    StephenHCornell\n177           @thefarfar           @thefarfar          thefarfar\n178   @thejbrain Thanks!              Thanks!             Thanks\n179    @PhilipMyrer Wow!                 Wow!                Wow\n180      @higginsane Hi.                  Hi.                 Hi\n181 @BigBobScott I will.              I will.             I will\n182 @SamRyzen Thank you!           Thank you!          Thank you\n183 @CforColby Hi Colby.            Hi Colby.           Hi Colby\n184    @jrkirk22 Thanks.              Thanks.             Thanks\n185        @__Jibz True!                True!               True\n186 @dennizenx   Thanks!              Thanks!             Thanks\n187 @Doyle5sMom  Thanks!              Thanks!             Thanks\n188  @BillDaley1   True!                True!               True\n189 @borland_jim  Great!               Great!              Great\n190  @kalevans    Great!               Great!              Great\n191 @Ziggi92  Good luck.           Good luck.          Good luck\n192     @melpoluk. Great                Great              Great\n193  @SMK4ye  Good goal.           Good goal.          Good goal\n194    @sadatDM. Thanks.              Thanks.             Thanks\n195   @Dattan78. Thanks!              Thanks!             Thanks\n196    @lmward7. Thanks.              Thanks.             Thanks\n197  @dgibson120. GREAT!               GREAT!              GREAT\n198           @jostephan           @jostephan          jostephan\n199            @saphoros            @saphoros           saphoros\n200      Thanks Matthew!      Thanks Matthew!     Thanks Matthew\n201 Thank you - so nice. Thank you - so nice. Thank you  so nice\n202        @NYMag  True!                True!               True\n203  @JTBone97 7 months.            7 months.           7 months\n204   @stacihogan  True!                True!               True\n205   @nal53199  I will.              I will.             I will\n206  @kat3500 Fantastic.           Fantastic.          Fantastic\n207    @lynk339  Thanks.              Thanks.             Thanks\n208         @KarsynSharp         @KarsynSharp        KarsynSharp\n209        @JaylonSeales        @JaylonSeales       JaylonSeales\n210 @NewYorkATM. Thanks.              Thanks.             Thanks\n211  @DeasWorld. Thanks.              Thanks.             Thanks\n212   @reidtruuu. Great!               Great!              Great\n213       @mike121z. No!                  No!                 No\n214 @MJGrilliot. Thanks.              Thanks.             Thanks\n215   @turrizal. Thanks.              Thanks.             Thanks\n216      Very tacky set!      Very tacky set!     Very tacky set\n217     @MKasun. Thanks.              Thanks.             Thanks\n218     @farm_mom. True.                True.               True\n219   @piano807  Thanks!              Thanks!             Thanks\n220    @Ukfan71  Thanks.              Thanks.             Thanks\n221      @lp084. Thanks.              Thanks.             Thanks\n222      @ff3476. Great.               Great.              Great\n223 @Katt8808. Thanks K.            Thanks K.           Thanks K\n224 @RobertSuppa. Thanks               Thanks             Thanks\n225    @Yolie4MS. Loser!               Loser!              Loser\n226  @sufiology. Thanks.              Thanks.             Thanks\n227   @jschwab2. Thanks.              Thanks.             Thanks\n228    @zachcb1. Thanks.              Thanks.             Thanks\n229  @sebspikes. Thanks.              Thanks.             Thanks\n230    @CATHCO9. Thanks.              Thanks.             Thanks\n231 @walexjoe9   Thanks!              Thanks!             Thanks\n232 @BobLo_Lady  Thanks.              Thanks.             Thanks\n233    @Ralphige Thanks!              Thanks!             Thanks\n234 @tylerponton I will.              I will.             I will\n235 @louriz  Thanks Lou.          Thanks Lou.         Thanks Lou\n236   @KayWiggs  Thanks!              Thanks!             Thanks\n237 @TJKnox8.    Thanks.              Thanks.             Thanks\n238 @SucioGato.  Thanks.              Thanks.             Thanks\n239         Great going.         Great going.        Great going\n240      @RuRu_89 thanks               thanks             thanks\n241   @rosscooker thanks               thanks             thanks\n242  @mandielaurin Good!                Good!               Good\n243 @uptownfrills  True!                True!               True\n244 @ElishaNEWS  Thanks.              Thanks.             Thanks\n245  @chinnis_28  Today.               Today.              Today\n246 @ECNew_York  Thanks!              Thanks!             Thanks\n247 @yadapup I will try!          I will try!         I will try\n248     @Emeka89 Thanks!              Thanks!             Thanks\n249        @NMcCay  Yes.                 Yes.                Yes\n250   @ZKoppe Work hard!           Work hard!          Work hard\n251 @nydave77 Good luck!           Good luck!          Good luck\n252 @ixfor  Okay, great!         Okay, great!         Okay great\n253 @parrguy Thanks Tom.          Thanks Tom.         Thanks Tom\n254    @madisonjar  Yes.                 Yes.                Yes\n255    @UTCRAGER Thanks.              Thanks.             Thanks\n256    @mayorgas Thanks!              Thanks!             Thanks\n257   @MaryGraceDH Good!                Good!               Good\n258   @ericboucher True!                True!               True\n259  @Lurdkberry Thanks.              Thanks.             Thanks\n260    @scmaness Thanks!              Thanks!             Thanks\n261 @BarkyJason  Thanks.              Thanks.             Thanks\n262   @AGNateLee  Great!               Great!              Great\n263 @DeedsJames  Thanks.              Thanks.             Thanks\n264  @moogoo3544 Thanks.              Thanks.             Thanks\n265  @gordondhue Thanks.              Thanks.             Thanks\n266 @JohnMark88  Thanks.              Thanks.             Thanks\n267     @1967porky  Yes.                 Yes.                Yes\n268 @groovyhank  Thanks.              Thanks.             Thanks\n269    @loisleyn Thanks.              Thanks.             Thanks\n270    @maggiedubh  Yes.                 Yes.                Yes\n271        @omz213  Yes!                 Yes!                Yes\n272 @SteelMagn New York.            New York.           New York\n273 @RFussell Good idea!           Good idea!          Good idea\n274  @Orangepop911 I do!                I do!               I do\n275   @Rapone91 I agree!             I agree!            I agree\n276  @Frankb550  Thanks.              Thanks.             Thanks\n277      @mena_le  True!                True!               True\n278    @NLaSpada7  True!                True!               True\n279        @cGjeezy Tea.                 Tea.                Tea\n280 @SandyInu We'll see.           We'll see.           Well see\n281  @DanSchreibs  Dope!                Dope!               Dope\n282    @InTheHere  True!                True!               True\n283 @GuinnessCode  True!                True!               True\n284  @freeillinois  Yes.                 Yes.                Yes\n285  @MrJPL It is Trump!         It is Trump!        It is Trump\n286   @mamaberg63  True!                True!               True\n287   @unique4x  Thanks.              Thanks.             Thanks\n288   @trentdav  Thanks.              Thanks.             Thanks\n289 @CrewSawyer Hope so!             Hope so!            Hope so\n290  @northsydgirl  Yes.                 Yes.                Yes\n291  @Brunhooooo  Wrong!               Wrong!              Wrong\n292   @cmedenise Thanks.              Thanks.             Thanks\n293   @efswagner Thanks.              Thanks.             Thanks\n294   @colinhaney Great!               Great!              Great\n295         @kidsalty No                   No                 No\n296         @8zz8 Wrong!               Wrong!              Wrong\n297 @HoboShane  I agree.             I agree.            I agree\n298   @Otgdy27  I won't.             I won't.             I wont\n299  @JamieHumble  True!                True!               True\n300 @jackiediop  Thanks.              Thanks.             Thanks\n301   @tazracet  Thanks.              Thanks.             Thanks\n302 @RayNealRay  Thanks.              Thanks.             Thanks\n303    @Seame_huntin EST                  EST                EST\n304  @ScottVH1  So am I.             So am I.            So am I\n305  @marioraez1 Thanks.              Thanks.             Thanks\n306    @nolanitenyc  No.                  No.                 No\n307    @Melpace  Thanks!              Thanks!             Thanks\n308   @C_A_Pearson True!                True!               True\n309 @381days  Thank you.           Thank you.          Thank you\n310  @ethwhite  Hi Ethan             Hi Ethan           Hi Ethan\n311      @vibow54  True!                True!               True\n312 @MegAlove04  Thanks!              Thanks!             Thanks\n313  @jkaburu Work hard!           Work hard!          Work hard\n314   @V3CEO  Thank you!           Thank you!          Thank you\n315     @_msccc  Thanks.              Thanks.             Thanks\n316    @A123Meoli  True.                True.               True\n317    @pastorcab  True.                True.               True\n318    @jmendowns  True!                True!               True\n319   @GrantGill  Thanks               Thanks             Thanks\n320    @WesDunn  Wilson!              Wilson!             Wilson\n321 @vikinggroup1  True!                True!               True\n322   .@GMA at 7:00 A.M.        .at 7:00 A.M.          at 700 AM\n323      .@FoxNewsSunday      .@FoxNewsSunday      FoxNewsSunday\n324  I am on @greta now!         I am on now!        I am on now\n325  Here we go - Enjoy!  Here we go - Enjoy!  Here we go  Enjoy\n326       AMERICA FIRST!       AMERICA FIRST!      AMERICA FIRST\n327   #StandForOurAnthem   #StandForOurAnthem  StandForOurAnthem\n328  Big week coming up!  Big week coming up! Big week coming up\n329                   We                   We                 We\n330           THANK YOU!           THANK YOU!          THANK YOU\n331       Going to CPAC!       Going to CPAC!      Going to CPAC\n```\n\n\n:::\n:::\n\n\n\n\nExplanation of the Regular Expression:\n\n-   `@` matches the \\@ symbol.\n-   `\\\\S+` matches one or more non-whitespace characters (i.e., the username).\n-   `str_remove()` removes the matched pattern.\n\n------------------------------------------------------------------------\n\n-   Top 20 most frequently used words\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrump.split <- unlist(str_split(trump_short_clean_tweets$clean_text2, \n                                pattern = \" \"))\n\nword.freq <- as.data.frame(sort(table(word = tolower(trump.split)), decreasing = T))\n\nword_freq_tbl <- word.freq |> \n  mutate(word = trimws(word)) |> \n  filter(\n    word != \"\",\n    !(word %in% stopwords::stopwords(\"en\")),\n    !(word %in% c(\"I\", \"&amp;\", \"The\", \"-\", \"just\"))\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = word_freq_tbl[1:20, ]) +\n  geom_col(aes(x = fct_reorder(word, Freq), y = Freq)) +\n  geom_label(aes(x = fct_reorder(word, Freq), y = Freq, label = Freq)) +\n  labs(x = \"\", y = \"\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](ESRM6990V_Lecture05_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\n## Reference\n\n1.  [Tidyverse Skills for Data Science](https://jhudatascience.org/tidyversecourse/get-data.html#images)\n2.  [Practical Data Processing for Social and Behavioral Research Using R](https://books.psychstat.org/rdata/image-data.html)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}