{
  "hash": "4a4994b67bd787c2a795b8fc5da9e891",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 10: Mixed Models for Multivariate Regression\"\nsubtitle: \"\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-10-09\"\ndate-modified: \"2024-10-11\"\nsidebar: false\nexecute: \n  echo: true\n  warning: false\noutput-location: default\ncode-annotations: below\nhighlight-style: \"nord\"\nformat: \n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"ESRM 64503 - Lecture 10: Introduction to mixed model\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n    fig-align: center\nfilters:\n  - quarto\n  - line-highlight\n---\n\n\n\n\n## \n\n\n\n\n```{=html}\n<div class=\"card shadow\">\n    <div class=\"ml-3 mt-2\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"54\" height=\"14\" viewBox=\"0 0 54 14\">\n            <g fill=\"none\" fill-rule=\"evenodd\" transform=\"translate(1 1)\">\n                <circle cx=\"6\" cy=\"6\" r=\"6\" fill=\"#FF5F56\" stroke=\"#E0443E\" stroke-width=\".5\"></circle>\n                <circle cx=\"26\" cy=\"6\" r=\"6\" fill=\"#FFBD2E\" stroke=\"#DEA123\" stroke-width=\".5\"></circle>\n                <circle cx=\"46\" cy=\"6\" r=\"6\" fill=\"#27C93F\" stroke=\"#1AAB29\" stroke-width=\".5\"></circle>\n            </g>\n        </svg>\n    </div>\n    <div class=\"card-body\">\n        <h4 class=\"card-title\"><b>Today's Class</b></h4>\n        <ul>\n          <li>Multivaraite regression via <b>mixed models</b></li>\n          <li>Comparing and contrasting path analysis with mixed models</li>\n          <ul>\n            <li>Differences in model fit measures </li>\n            <li>Differences in software estimation methods </li>\n            <li>Model comparisons via multivariate Wald tests (instead of LRTs) </li>\n            <li>How to compute R-square </li>\n          </ul>\n        </ul>\n    </div>\n</div>\n```\n\n\n\n## R Setup\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nlibrary(ESRM64503)\nlibrary(kableExtra)\nlibrary(tidyverse)\nlibrary(DescTools) # Desc() allows you to quick screen data\nlibrary(lavaan) # Desc() allows you to quick screen data\nhead(dataMath)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id hsl cc use msc mas mse perf female\n1  1  NA  9  44  55  39  NA   14      1\n2  2   3  2  77  70  42  71   12      0\n3  3  NA 12  64  52  31  NA   NA      1\n4  4   6 20  71  65  39  84   19      0\n5  5   2 15  48  19   2  60   12      0\n6  6  NA 15  61  62  42  87   18      0\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(dataMath)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 350   9\n```\n\n\n:::\n:::\n\n\n\n\n## Correction about `fixed.x` argument in previous lecture\n\n-   If TRUE, the exogenous `x` covariates are considered fixed variables and the means, variances and covariances of these variables are fixed to their sample values.\n\n-   If FALSE, they are considered random, and the means, variances and covariances are free parameters. Typically, called **latent variable**\n\n-   If \"default\", the value is set depending on the mimic option.\n\n[Thus, we considered the distributions of exogenous variables as known parameters.]{.underline}\n\n## What is Mixed Models\n\n-   A **mixed model**, **mixed-effects model** or **Linear mixed models** (LMMs) is a [statistical model](https://en.wikipedia.org/wiki/Statistical_model \"Statistical model\") containing both [fixed effects](https://en.wikipedia.org/wiki/Fixed_effect \"Fixed effect\") and [random effects](https://en.wikipedia.org/wiki/Random_effect \"Random effect\"). These models are useful in a wide variety of disciplines in the physical, biological and social sciences.\n\n-   Mixed model can answer similar research questions as Path Analysis (or structural equation model):\n\n    -   Relationships among multiple endogenous variables\n\n## Fixed effects vs. Random effects\n\n1.  Definition: Fixed effects are constant across individuals, and random effects vary.\n\nAssume a person is measured t times (repeated measure design or longitudinal design), thus we have t points of x and y for each individuals\n\n$$\ny_{it} = \\beta_{0i} + \\beta_1 x_{it}\n$$\n\nHere, $\\beta_{0i}$ is random intercept that varies across individuals. $\\beta_{1}$ is the fixed slope that shared acorss individuals.\n\n2.  Alternative definition: Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population.\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i +e_i\n$$ Thus, $\\sigma^e$ is random effect, $\\beta_0$ and $\\beta_1$ are fixed effects.\n\n## Properties of Mixed Models\n\n1.  Mixed models are used for many types of analyses:\n    -   Analogous to **MANOVA** and M-Regression (so repeated measures analyses)\n    -   Multilevel models for *clustered*, *longitudinal*, and *crossed-effects* data\n2.  The biggest difference between mixed models and path analysis software is the [assumed distribution of the exogenous variables]{.underline}：\n    -   Mixed models: **no distribution assumed**\n    -   Path analysis: most software assumes **multivariate normal distribution**\n    -   This affects how missing data are managed – mixed models cannot have any missing IVs\n3.  Mixed models also do not allow endogenous variables to predict other endogenous variables\n    -   No indirect effects are possible from a single analysis (multiple analyses needed)\n4.  Mixed models software also often needs variables to be stored in so-called “stacked” or long-format data (one row per DV)\n    -   We used **wide-format** data for `lavaan` (one row per person)\n\n## Wide to Long Data Transformation\n\n-   Original wide-format data (all DVs for a person on one row)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- dataMath\ndat$cc10 <- dat$cc - 10\ndat_wide <- dat |> select(id, perf, use, female, cc10)\nhead(dat_wide) # show first 6 lines\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id perf use female cc10\n1  1   14  44      1   -1\n2  2   12  77      0   -8\n3  3   NA  64      1    2\n4  4   19  71      0   10\n5  5   12  48      0    5\n6  6   18  61      0    5\n```\n\n\n:::\n:::\n\n\n\n\n-   Reshape with `pivot_longer()` function and Resulting data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_long <- dat_wide |> \n  pivot_longer(cols = c(perf, use), #<1>\n               names_to = \"DV\", #<2>\n               values_to = \"score\") |> #<3>\n  mutate(dPerf = ifelse(DV == 'perf', 0, 1)) # convert DVs into indicator variable - dperf\nhead(dat_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n     id female  cc10 DV    score dPerf\n  <int>  <int> <dbl> <chr> <int> <dbl>\n1     1      1    -1 perf     14     0\n2     1      1    -1 use      44     1\n3     2      0    -8 perf     12     0\n4     2      0    -8 use      77     1\n5     3      1     2 perf     NA     0\n6     3      1     2 use      64     1\n```\n\n\n:::\n:::\n\n\n\n\n1.  `cols`: Columns to pivot into longer format. Put multiple column names (no quote) into `c()`\n2.  `names_to`: A character vector specifying the new column to create from the information stored in the column names\n3.  `values_to`: A string specifying the name of the column to create from the data stored in cell values.\n\n## Execise 1: Wide to Long Transform\n\n-   Turn msc, mas, mse into long-form\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_wide <- dat |> select(id, msc, mas, mse)\n## You turn\n```\n:::\n\n\n\n\n| id  | DV  | Score |\n|-----|-----|-------|\n| 1   | msc | 55    |\n| 1   | mas | 39    |\n| 1   | mse | NA    |\n| 2   | msc | 70    |\n\n## Statistical Form: Bridge Path Model w/t Mixed Model\n\n-   Before we dive into mixed models, we will begin with a multivariate regression model:\n    -   Predicting mathematics performance (PERF) with female (F), college math experience (CC), and the interaction between female and college math experience (FxCC)\n    -   Predicting perceived usefulness (USE) with female (F), college math experience (CC), and the interaction between female and college math experience (FxCC)\n\n$$\nPERF_i = \\beta_{0,PERF} + \\beta_{F,PERF} F_i + \\beta_{CC,PERF}CC_i + \\beta_{F*CC,PERF}F_i*CC_i + e_{i,PERF}\n$$ {#eq-pathmodel1} $$\nUSE_i = \\beta_{0,USE} + \\beta_{F,USE} F_i + \\beta_{CC,USE}CC_i + \\beta_{F*CC,USE}F_i*CC_i + e_{i,USE}\n$$ {#eq-pathmodel2}\n\n-   Mixed Model: Here I use the symbol $\\delta$ to represent each fixed effect in the multivariate model from the mixed model perspective. `dPERF` is the DV indicator: 1 - Perf and 0 - Use\n\n$$\nScore_i = (\\delta_{0,PERF} + \\delta_{0, dPERF}) + (\\delta_{F,PERF} + \\delta_{F, dPERF}) F_i + (\\delta_{CC,PERF} \\\\ + \\delta_{CC, dPERF}) CC_i + (\\delta_{F*CC,PERF} + \\delta_{F*CC,dPERF}) F_i*CC_i + e_{i,PERF} + e_{i,dPERF}\n$$ {#eq-mixedmodel1}\n\n$$\nScore_i = \\delta_{0,PERF} + \\delta_{0, dPERF}dPERF_i + \\delta_{F,PERF} F_i + \\delta_{F, dPERF}dPERF_i * F_i \\\\ \n+ \\delta_{CC,PERF} CC_i + \\delta_{CC, dPERF} dPERF_i * CC_i \\\\ \n+ \\delta_{F*CC,PERF} F_i*CC_i + \\delta_{F*CC,dPERF} dPERF_i * F_i*CC_i + e_{i,PERF} + e_{i,dPERF}\n$$ {#eq-mixedmodel2}\n\n# Build the Empty Model: Not So Empty\n\n## Statistical Form of empty mixed model\n\n-   For illustration, let's start from the empty model\n\n-   A multivariate model using mixed model software uses the dummy code for DV to make all effects conditional on the specific DV in the model\n\n    -   I will compare/contrast these with the symbols $\\beta$ from the fixed effects in path analysis\n\n-   For instance, our empty model is thus:\n\n    -   Where *Score* is condition on the value of *dPerf*:\n        -   When $dPerf = 0$ $\\rightarrow$ DV = \"Use\" $\\rightarrow$ $Score_i = Use_i = \\delta_0 + e_{i, Use}$\n        -   When $dPerf = 1$ $\\rightarrow$ DV = \"Perf\" $\\rightarrow$ $Score_i = Perf_i = \\delta_0 + \\delta_1 + e_{i, perf}$\n\n$$\nScore_{i, DV} = \\delta_0 + \\delta_1 dPerf_i + e_{i, DV}\n$$\n\n## Estimating the Empty Model\n\n-   From the `nlme` library, we will use the `gls()` function\n\n    -   Be sure the library is installed and loaded before trying this!\n\n\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nlibrary(nlme) # install.packages(\"nlme\")\n\ndat_long <- dat_long[complete.cases(dat_long), ]\n# create empty model using REML estimation to attempt to mirror initial analysis:\nmodel01_mixed = gls(model = score ~ 1 + dPerf, #<1>\n                    data = dat_long,\n                    method = \"REML\", #<2>\n                    correlation = corSymm(form = ~1|id), #<3>\n                    weights = varIdent(form = ~1|DV)) #<4>\nsummary(model01_mixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized least squares fit by REML\n  Model: score ~ 1 + dPerf \n  Data: dat_long \n       AIC      BIC    logLik\n  3774.313 3795.871 -1882.156\n\nCorrelation Structure: General\n Formula: ~1 | id \n Parameter estimate(s):\n Correlation: \n  1    \n2 0.136\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | DV \n Parameter estimates:\n    perf      use \n1.000000 5.337397 \n\nCoefficients:\n               Value Std.Error  t-value p-value\n(Intercept) 13.94085 0.1868631 74.60461       0\ndPerf       38.46901 0.9399708 40.92575       0\n\n Correlation: \n      (Intr)\ndPerf -0.079\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-3.24789265 -0.64196261  0.01956532  0.68109325  2.99644101 \n\nResidual standard error: 3.023304 \nDegrees of freedom: 553 total; 551 residual\n```\n\n\n:::\n:::\n\n\n\n\n1.  `score ~ 1 + dPerf`: $Score_{i, DV} = \\delta_0 + \\delta_1 dPerf_i + e_{i, DV}$\n2.  \"REML\": Residual Maximum Likelihood Estimation\n3.  `corSymm`: General Correlation Structure; Provides estimates of all unique correlations; Needs `id` variable name after \\| for program to know which data comes from which person; `~ 1`, which corresponds to using the order of the observations in the data as a covariate, and no groups.\n4.  `varIdent`: a constant variance function structure; Estimates a different (residual) variance for each DV; With correlation line ensures an unstructured model is estimated\n\n## Empty Model Results I: Covariance Matrix of DVs\n\n-   The covariance matrix of DVs comes from the `getVarCov()` function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetVarCov(model01_mixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMarginal variance covariance matrix\n       [,1]     [,2]\n[1,] 9.1404   6.6311\n[2,] 6.6311 260.3900\n  Standard Deviations: 3.0233 16.137 \n```\n\n\n:::\n:::\n\n\n\n\n-   Estimated variance-covariance matrix of PERF and USE scores.\n\n## Mapping Multivariate Mixed Models onto Path Models\n\n-   To compare this result with the path analyses we conducted previously, we’ll have to use this data set\n    -   Omit the same observations\n-   So, we’ll need to take our long-format data and reshape it into wide-format:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dat_wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id perf use female cc10\n1  1   14  44      1   -1\n2  2   12  77      0   -8\n3  3   NA  64      1    2\n4  4   19  71      0   10\n5  5   12  48      0    5\n6  6   18  61      0    5\n```\n\n\n:::\n:::\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nlibrary(lavaan)\nmodel01_mirror.syntax = \"\n# Means:\nperf ~ 1\nuse ~ 1\n\n# Variances:\nperf ~~ perf\nuse ~~ use\n\n# Covariance:\nperf ~~ use\n\"\n\nmodel01_path_noNA.fit = sem(model01_mirror.syntax, data = dat_wide,\n                            fixed.x = TRUE, \n                            mimic = \"MPLUS\", \n                            estimator = \"MLR\")\nsummary(model01_path_noNA.fit, \n        fit.measures = TRUE,\n        standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6.17 ended normally after 29 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n                                                  Used       Total\n  Number of observations                           348         350\n  Number of missing patterns                         3            \n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                                 6.064       5.573\n  Degrees of freedom                                 1           1\n  P-value                                        0.014       0.018\n  Scaling correction factor                                  1.088\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                         1.000\n  Robust Tucker-Lewis Index (TLI)                            1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2085.032   -2085.032\n  Loglikelihood unrestricted model (H1)      -2085.032   -2085.032\n                                                                  \n  Akaike (AIC)                                4180.064    4180.064\n  Bayesian (BIC)                              4199.325    4199.325\n  Sample-size adjusted Bayesian (SABIC)       4183.464    4183.464\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000          NA\n  90 Percent confidence interval - lower         0.000          NA\n  90 Percent confidence interval - upper         0.000          NA\n  P-value H_0: RMSEA <= 0.050                       NA          NA\n  P-value H_0: RMSEA >= 0.080                       NA          NA\n                                                                  \n  Robust RMSEA                                               0.000\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n  P-value H_0: Robust RMSEA <= 0.050                            NA\n  P-value H_0: Robust RMSEA >= 0.080                            NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  perf ~~                                                               \n    use               6.847    2.850    2.403    0.016    6.847    0.147\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    perf             13.959    0.174   80.442    0.000   13.959    4.721\n    use              52.440    0.872   60.140    0.000   52.440    3.322\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    perf              8.742    0.754   11.596    0.000    8.742    1.000\n    use             249.245   19.212   12.973    0.000  249.245    1.000\n```\n\n\n:::\n:::\n\n\n\n\n## Comparing and Contrasting Results: Intercept (fixed effect)\n\n-   $\\beta_{0,Perf}$ and $\\beta_{0,Use}$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameterestimates(model01_path_noNA.fit) |> filter(op == \"~1\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   lhs op rhs    est    se      z pvalue ci.lower ci.upper\n1 perf ~1     13.959 0.174 80.442      0   13.619   14.299\n2  use ~1     52.440 0.872 60.140      0   50.731   54.149\n```\n\n\n:::\n:::\n\n\n\n\n-   $\\delta_{0,DV}$ and $\\delta_{1,DV}$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model01_mixed)$tTable\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Value Std.Error  t-value       p-value\n(Intercept) 13.94085 0.1868631 74.60461 3.552065e-290\ndPerf       38.46901 0.9399708 40.92575 3.477058e-169\n```\n\n\n:::\n:::\n\n\n\n\n$\\delta_{0,DV} + \\delta_{1,DV}$: 52.40986 is close to $\\beta_{0,Use}$\n\n## Comparing and Contrasting Results: Residual variance coviarance\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameterestimates(model01_path_noNA.fit) |> \n  filter(op == \"~~\") |> \n  select(lhs, rhs, est) |> \n  pivot_wider(names_from = rhs, values_from = est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  lhs    perf    use\n  <chr> <dbl>  <dbl>\n1 perf   8.74   6.85\n2 use   NA    249.  \n```\n\n\n:::\n:::\n\n\n\n\nIn mixed model, we cannot get the z-value (significance testing)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetVarCov(model01_mixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMarginal variance covariance matrix\n       [,1]     [,2]\n[1,] 9.1404   6.6311\n[2,] 6.6311 260.3900\n  Standard Deviations: 3.0233 16.137 \n```\n\n\n:::\n:::\n\n\n\n\n## Comparing and Contrasting Results: Correlation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandardizedsolution(model01_path_noNA.fit) |> filter(op == \"~~\", lhs != rhs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   lhs op rhs est.std   se     z pvalue ci.lower ci.upper\n1 perf ~~ use   0.147 0.06 2.437  0.015    0.029    0.265\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel01_mixed$modelStruct$corStruct\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorrelation structure of class corSymm representing\n Correlation: \n  1    \n2 0.136\n```\n\n\n:::\n:::\n\n\n\n\n## REML: Residual Maximum Likelihood Estimation\n\n-   The ML estimator is nice, but the variance estimate is downward biased (too small)\n    -   Remember – it divides by N for the residual covariance matrix\n-   In small samples, this is likely to lead to biased estimates and incorrect p-values\n    -   The variance goes into the SE, which goes into the Wald test, which dictates the p-value for the beta\n-   Instead, another maximum likelihood technique has been developed: **Residual Maximum Likelihood** (REML)\n    -   Maximizes the likelihood of the residuals rather than the data\n    -   Has unbiased estimates of the residual covariance matrix\n    -   Is the default method of estimation for most mixed model estimation packages\n\n------------------------------------------------------------------------\n\n-   There is one catch to REML: you cannot use a LRT to compare nested models with differing fixed effects\n    -   Because the algorithm uses residuals not data likelihood, if the residuals change, the likelihood changes\n    -   Residuals come from the fixed effects $\\rightarrow$ if fixed effects are different, then residuals change, causing the likelihood to change\n    -   Can use multivariate Wald test for fixed effects\n-   Don't mix ML and REML for the same analysis\n\n# Adding Predictors To The Model\n\n## Adding more predictors: female and cc10\n\n-   Adding predictors to the model is similar to adding predictors in regular regression models\n\n-   By using REML we cannot compare models using likelihood ratio tests\n\n    -   REML LRTs must have same fixed effects\n    -   Adding predictors adds new fixed effects to the empty model\n\n-   We are predicting each DV with `female`, `cc10`, and `female*cc10`\n\n## Model with Predictors: Syntax\n\n\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Model 02: all predictors included\nmodel02_formula = as.formula(\"score ~ 1 + dPerf + female + dPerf*female + cc10 + dPerf*cc10 + female*cc10 + dPerf*female*cc10\")\nmodel02_mixed <- gls(model = model02_formula, method = \"REML\",\n                     data = dat_long, \n                     correlation = corSymm(form = ~1|id),\n                     weights = varIdent(form = ~1|DV))\nsummary(model02_mixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized least squares fit by REML\n  Model: model02_formula \n  Data: dat_long \n       AIC      BIC    logLik\n  3771.308 3818.617 -1874.654\n\nCorrelation Structure: General\n Formula: ~1 | id \n Parameter estimate(s):\n Correlation: \n  1    \n2 0.107\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | DV \n Parameter estimates:\n   perf     use \n1.00000 5.50965 \n\nCoefficients:\n                     Value Std.Error  t-value p-value\n(Intercept)       13.68949 0.2237452 61.18340  0.0000\ndPerf             38.10983 1.1751688 32.42924  0.0000\nfemale             0.65832 0.3837798  1.71536  0.0868\ncc10               0.09871 0.0354295  2.78617  0.0055\ndPerf:female       1.17738 2.0068176  0.58669  0.5577\ndPerf:cc10         0.09653 0.1979034  0.48778  0.6259\nfemale:cc10        0.09377 0.0671627  1.39609  0.1633\ndPerf:female:cc10  0.16641 0.3529074  0.47155  0.6374\n\n Correlation: \n                  (Intr) dPerf  female cc10   dPrf:f dPr:10 fml:10\ndPerf             -0.096                                          \nfemale            -0.583  0.056                                   \ncc10              -0.121  0.011  0.070                            \ndPerf:female       0.056 -0.586 -0.098 -0.006                     \ndPerf:cc10         0.010 -0.147 -0.006 -0.081  0.086              \nfemale:cc10        0.064 -0.006 -0.001 -0.528  0.003  0.043       \ndPerf:female:cc10 -0.006  0.083  0.004  0.046  0.043 -0.561 -0.098\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-3.17907470 -0.67331699 -0.01178306  0.70547907  2.97992869 \n\nResidual standard error: 2.923886 \nDegrees of freedom: 553 total; 545 residual\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngetVarCov(model02_mixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMarginal variance covariance matrix\n       [,1]     [,2]\n[1,] 8.5491   5.0582\n[2,] 5.0582 259.5200\n  Standard Deviations: 2.9239 16.11 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model02_mixed)$tTable |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Value Std.Error t-value p-value\n(Intercept)       13.689     0.224  61.183   0.000\ndPerf             38.110     1.175  32.429   0.000\nfemale             0.658     0.384   1.715   0.087\ncc10               0.099     0.035   2.786   0.006\ndPerf:female       1.177     2.007   0.587   0.558\ndPerf:cc10         0.097     0.198   0.488   0.626\nfemale:cc10        0.094     0.067   1.396   0.163\ndPerf:female:cc10  0.166     0.353   0.472   0.637\n```\n\n\n:::\n:::\n\n\n\n\n1.  $\\beta_0 = 13.689, p < .001$\n2.  $\\beta_{dPerf} = 38.110, p < .001$\n3.  $\\beta_{female} = 0.658, p = 0.087$\n4.  $\\beta_{cc10} = 0.099, p = 0.006$\n5.  $\\beta_{dPerf*female} = 1.177, p = 0.558$\n6.  $\\beta_{dPerf*cc10} = 0.097, p = 0.626$\n7.  $\\beta_{female*cc10} = 0.094, p = 0.163$\n8.  $\\beta_{dPerf*female*cc10} = 0.166, p = 0.637$\n\n## First Question: Which Model “Fits” Better?\n\n-   After adding the predictors (estimating their betas) to the model, we must first ask which model fits better\n\n-   A likelihood ratio test (LRT) cannot be performed as we are using REML\n\n-   Use multivariate wald test\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(multcomp)\nmodel2_model_matrix <- diag(rep(1, 8))\nrownames(model2_model_matrix) <- c(\n  \"Intercept\",\n  \"dPerf\",\n  \"female\",\n  \"cc10\",\n  \"dPerf:female\",\n  \"dPerf:cc10\",\n  \"female:cc10\",\n  \"dPerf:female:cc10\"\n)\neffects <- glht(model = model02_mixed, linfct = model2_model_matrix)\nsummary(effects)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: gls(model = model02_formula, data = dat_long, correlation = corSymm(form = ~1 | \n    id), weights = varIdent(form = ~1 | DV), method = \"REML\")\n\nLinear Hypotheses:\n                       Estimate Std. Error z value Pr(>|z|)    \nIntercept == 0         13.68949    0.22375  61.183   <1e-04 ***\ndPerf == 0             38.10983    1.17517  32.429   <1e-04 ***\nfemale == 0             0.65832    0.38378   1.715   0.4735    \ncc10 == 0               0.09871    0.03543   2.786   0.0397 *  \ndPerf:female == 0       1.17738    2.00682   0.587   0.9973    \ndPerf:cc10 == 0         0.09653    0.19790   0.488   0.9992    \nfemale:cc10 == 0        0.09377    0.06716   1.396   0.7130    \ndPerf:female:cc10 == 0  0.16641    0.35291   0.472   0.9994    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(effects, test = Ftest())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t General Linear Hypotheses\n\nLinear Hypotheses:\n                       Estimate\nIntercept == 0         13.68949\ndPerf == 0             38.10983\nfemale == 0             0.65832\ncc10 == 0               0.09871\ndPerf:female == 0       1.17738\ndPerf:cc10 == 0         0.09653\nfemale:cc10 == 0        0.09377\ndPerf:female:cc10 == 0  0.16641\n\nGlobal Test:\n  Chisq DF Pr(>Chisq)\n1  8328  8          0\n```\n\n\n:::\n:::\n\n\n\n\n-   Note: R’s `nlme` function doesn’t do a good job with df.residual and provides a Chi-square test\n\n-   Also note there are 6 degrees of freedom (one for each additional beta weight in the model)\n\n## Questions that can be answered\n\n-   What is the effect of college experience on usefulness for males?\n-   What is the effect of college experience on usefulness for females?\n-   What is the difference between males and females ratings of usefulness when college experience = 10?\n-   How did the difference between males and females ratings change for each additional hour of college experience?\n-   What is the effect of college experience on performance for males?\n-   What is the effect of college experience on performance for females?\n-   What is the difference between males and females performance when college experience = 10?\n-   How did the difference between males and females performance change for each additional hour of college experience?\n\n## Model R-squared\n\nTo determine the model R-squared, we have to compare the variance/covariance matrix from model01 and model02 and make the statistics ourselves:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nVmodel01 = getVarCov(model01_mixed)\nVmodel02 = getVarCov(model02_mixed)\n\n## Rsquare for Performance and Usefulness\n(diag(Vmodel01) - diag(Vmodel02)) / diag(Vmodel01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.064686451 0.003341706\n```\n\n\n:::\n:::\n\n\n\n\n-   6.47% variance of performance was explained by added predictors.\n-   0.33% variance of usefulness was explained by added predictors.\n\n## Exercise 2: Model mixed model with mse, mas and msc\n\n-   Model 1: A path analysis with mse, mas, and msc as outcomes\n\n-   Model 2: A empty mixed model with mse, mas, and msc are repeated measures nested in each individual\n\n-   Compare two models: intercepts and correlations\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Wrapping up\n\n-   Things we get directly from path models that we do not get directly in mixed models:\n\n    -   Tests for approximate model fit\n\n    -   Scaled Chi-square for some types of non-normal data\n\n    -   Standardized parameter coefficients\n\n    -   Tests for indirect effects\n\n    -   R-squared statistics\n\n-   Things we get directly in mixed models that we do not get in path models:\n\n    -   REML (unbiased estimates of variances/covariances)\n\n-   In this lecture we discussed the basics of mixed model analyses for multivariate models\n\n    -   Model specification/identification\n\n    -   Model estimation\n\n    -   Model modification and re-estimation\n\n    -   Final model parameter interpretation\n\n-   There is a lot to the analysis\n\n    -   but what is important to remember is the over-arching principal of multivariate analyses: covariance between variables is important\n\n    -   Mixed models imply very specific covariance structures\n\n    -   The validity of the results still hinge upon accurately finding an approximation to the covariance matrix\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}