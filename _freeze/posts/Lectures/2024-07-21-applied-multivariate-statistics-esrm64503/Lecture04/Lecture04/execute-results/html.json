{
  "hash": "d40ca7d22a596c6bc1c6616a21c90250",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04: Distribution and Estimation\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-09-16\"\nsidebar: false\nexecute: \n  echo: true\n  warning: false\noutput-location: column\nformat: \n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"ESRM 64503: Lecture 04: Distribution and Estimation\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\nfilters:\n  - shinylive\n---\n\n\n\n\n## Today's Class\n\n-   Review Homework 1\n\n-   [The building blocks]{style=\"color: tomato\"}: **The basics of mathematical statistics:**\n\n    -   Random variables: Definition & Types\n\n    -   Univariate distribution\n\n        -   General terminology (e.g., sufficient statistics)\n\n        -   Univariate normal (aka Gaussian)\n\n        -   Other widely used univariate distributions\n\n    -   Types of distributions: Marginal \\| Conditional \\| Joint\n\n    -   Expected values: means, variances, and the algebra of expectations\n\n    -   Linear combinations of random variables\n\n-   [The finished product]{style=\"color: violet\"}: **How the GLM fits within statistics**\n\n    -   The GLM with the normal distribution\n\n    -   The statistical assumptions of the GLM\n\n    -   How to assess these assumptions\n    \n# ESRM 64503: Homework 1\n\n## Question 2\n\n<details>\n\n- Copy and paste your R syntax and R output that calculates the group **Senior-Old's standard error of group mean** (use the data and model we've used in class). \n\n  - **Aim**: Test whether the group mean of senior-old significantly higher than the baseline (here, 0 score)\n\n$$\n\\mathbf{Test = \\beta_0 +\\beta_1Senior + \\beta_2New +\\beta_3Senior*New}\n$$\n\n- The group mean of Senior-Old is $\\beta_0 + \\beta_1$.\n\n- Based on algebra for variance, we know hat\n\n$$\nVar(\\beta_0 + \\beta_1) = Var(\\beta_0)+Var(\\beta_1) +2*Cov(\\beta_0, \\beta_1)\n$$\n\n- `vcor` function provides the variances and covaraicnes for $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\n# R syntax\nlibrary(ESRM64503)\nlibrary(tidyverse)\nmodel1 <- lm(Test ~ Senior + New + Senior * New, data = dataTestExperiment)\ndata(\"dataTestExperiment\")\nVar_beta_0 <- vcov(model1)[1,1]\nVar_beta_1 <- vcov(model1)[2,2]\nCov_beta_0_1 <- vcov(model1)[1,2]\nVar_Test_SeniorOld <- Var_beta_0 + Var_beta_1 + 2*Cov_beta_0_1\n(SE_Test_SeniorOld <- sqrt(Var_Test_SeniorOld)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5364078\n```\n\n\n:::\n:::\n\n\n\n\n</details>\n\n## Question 3\n\n<details>\n\n- Copy and paste your R syntax and R output that calculates the standard error of conditional main effect of New (New vs. Old) when Senior = 1.\n\n  - **Aim**: Test whether the conditional main effect of New when Senior significantly different from 0\n     - In other words, when individuals are senior, whether new or old instruction method has significantly differences in their test scores\n\n\n$$\n\\beta_{1|Senior =1} = \\beta_2 + \\beta_3 * 1\n$$\n\n\n- Based on algebra for variance, we know that\n\n$$\nVar(\\beta_2 + \\beta_3) = Var(\\beta_2)+Var(\\beta_3) +2*Cov(\\beta_2, \\beta_3)\n$$\n\n\n-   `vcor` function provides the variances and covaraicnes for $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ \n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\n# R syntax\nVar_beta_2 <- vcov(model1)[3,3]\nVar_beta_3 <- vcov(model1)[4,4]\nCov_beta_2_3 <- vcov(model1)[3,3]\nVar_betaofNew_Senior1 <- Var_beta_2 + Var_beta_3 + 2*Cov_beta_2_3\n(SE_betaofNew_Senior1 <- sqrt(Var_betaofNew_Senior1)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.69627\n```\n\n\n:::\n:::\n\n\n\n</details>\n\n# Unit 1: Random Variables & Statistical Distribution\n\n## Definition of Random Variables\n\n- [**Random:**]{.underline} situations in which the certainty of the outcome is unknown and is at least in part due to chance\n\n- [**Variable:**]{.underline} a value that may change give the scope of a given problem or set of operations\n\n- [**Random Variable**]{.underline}: a variable whose outcome depends on chance (possible values might represent the possible outcomes of a yet-to-be performed experiment)\n\nToday, we will denote a random variable with a lower-cased: ***x***\n\n**Question**: which one in the following options is random variable:\n\na. any company's revenue in 2024\n\nb. one specific company's monthly revenue in 2024\n\nc. companies whose revenue over than $30 billions\n\n**My answer**: <span class=\"mohu\">only (a)</span>\n\n\n## Types of Random Variables\n\n1.  **Continuous**\n    -   Examples of continuous random variables:\n        -   ***x*** represent the height of a person, draw at random\n        -   *Y* (the outcome/DV in a GLM)\n        -   Some variables like **exam score or motivation scores** are not \"true\" continuous variables, but it is convenient to consider them as \"continuous\"\n2.  **Discrete** (also called categorical, generally)\n    -   Example of discrete random variables:\n        -   ***x*** represents the gender of a person, drawn at random\n        -   *Y* (outcomes like yes/no; pass/not pass; master / not master a skill; survive / die)\n3.  **Mixture of Continuous and Discrete**:\n    -   Example of mixture: \\begin{equation}\n          x =\n        \\begin{cases}\n          RT & \\text{between 0 and 45 seconds} \\\\\n          0 & \\text{otherwise}\n        \\end{cases}       \n        \\end{equation}\n\n## Key Features of Random Variable\n\n1.  Random variables each are described by a **probability density / mass function (PDF) –** $f(x)$\n\n    -   PDF indicates relative frequency of occurrence\n\n    -   A PDF is a math function that gives rough picture of the distribution from which a random variable is draw\n\n2.  The type of random variable dictates the name and nature of these functions:\n\n    -   [Continuous random variables]{style=\"color: tomato\"}:\n\n        -   $f(x)$ is called a probability density function\n\n        -   Area under curve must equal to 1 (found by calculus integration)\n\n        -   Height of curve (the function value $f(x)$):\n\n            -   Can be any positive number\n\n            -   Reflects relative likelihood of an observation occurring\n\n    -   [Discrete random variables]{style=\"color: violet\"}:\n\n        -   $f(x)$ is called a probability mass function\n\n        -   Sum across all values must equal 1\n\n------------------------------------------------------------------------\n\n::: {.callout-note style=\"font-size: 1.4em;\"}\nBoth max and min values of temperature can be considered as continuous random variables.\n:::\n\n- Question 1: what are the probabilities of integrating all values of max and min temperatures ({1, 1} or {0.5, 0.5})\n\n- **Answer**: <span class=\"mohu\"> it is {1, 1} for max and min temperatures. Because they are two separated random variables.</span>\n\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\ntemp <- read.csv(\"data/temp_fayetteville.csv\")\ntemp$value_F <- (temp$value / 10 * 1.8) + 32\ntemp |> \n  ggplot() +\n  geom_density(aes(x = value_F, fill = datatype), col = \"white\", alpha = .8) +\n  labs(x = \"Max/Min Temperature (F) at Fayetteville, AR (Sep-2023)\", \n       caption = \"Source: National Oceanic and Atmospheric Administration (https://www.noaa.gov/)\") +\n  scale_x_continuous(breaks = seq(min(temp$value_F), max(temp$value_F), by = 5)) +\n  scale_fill_manual(values = c(\"tomato\", \"turquoise\")) +\n  theme_classic(base_size = 13) \n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Other key Terms\n\n-   The sample space is the set of all values that a random variable x can take:\n    -   **Example 1:** The sample space for a random variable x from a normal distribution $x \\sim N(\\mu_x, \\sigma^2_x)$ is $(-\\infty, +\\infty)$.\n    -   **Example 2:** The sample space for a random variable x representing the outcome of a coin flip is {H, T}\n    -   **Example 3:** The sample space for a random variable x representing the outcome of a roll of a die is {1, 2, 3, 4, 5, 6}\n-   When using generalized models, the trick is to pick a distribution with a sample space that matches the range of values obtainable by data\n    - Logistic regression - Match Bernoulli distribution <span class=\"mohu\">(Example 2)</span>\n    - Poisson regression - Match Poisson distribution <span class=\"mohu\">(Example 3)</span>\n\n## Uses of Distributions in Data Analysis\n\n-   Statistical models make distributional assumptions on various parameters and / or parts of data\n\n-   These assumptions govern:\n\n    -   How models are estimated\n\n    -   How inferences are made\n\n    -   How missing data may be imputed\n\n-   If data do not follow an assumed distribution, inferences may be inaccurate\n\n    -   Sometimes a very big problem, other times not so much\n\n-   Therefore, it can be helpful to check distributional assumptions prior to running statistical analysis\n\n## Continuous Univariate distributions\n\n-   To demonstrate how continuous distributions work and look, we will discuss three:\n\n    -   Uniform distribution\n\n    -   Normal distribution\n\n    -   Chi-square distribution\n\n-   Each are described a set of parameters, which we will later see are what give us our inferences when we analysis data\n\n-   What we then do is put constraints on those parameters based on hypothesized effects in data\n\n## Uniform distribution\n\n-   The uniform distribution is how to help set up how continuous distributions work\n    \n    - Typically, used for simulation studies that parameters are randomly generated\n\n-   For a continuous random variable x that ranges from (a, b), the uniform probability density function is:\n\n    $f(x) = \\frac{1}{b-a}$\n\n-   The uniform distribution has two parameters\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    x = seq(0, 3, .1)\n    y = dunif(x, min = 0, max = 3)\n    ggplot() +\n      geom_point(aes(x = x, y = y)) +\n      geom_path(aes(x = x, y = y)) +\n      theme_bw()\n    ```\n    \n    ::: {.cell-output-display}\n    ![](Lecture04_files/figure-html/unnamed-chunk-4-1.png){width=672}\n    :::\n    :::\n\n\n\n\n------------------------------------------------------------------------\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nset.seed(1234)\n# Define UI for application that draws a histogram\nui <- fluidPage(\n  \n  # Application title\n  titlePanel(\"Uniform distribution\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"a\",\n                  \"Lower Bound (a):\",\n                  min = 1,\n                  max = 20,\n                  value = 1,\n                  animate = animationOptions(interval = 5000, loop = TRUE)),\n      uiOutput(\"b_slider\")\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nx <- seq(0, 40, .02)\nserver <- function(input, output) {\n  observeEvent(input$a, {\n    output$b_slider <<- renderUI({\n      sliderInput(\"b\",\n                  \"Upper Bound (b):\",\n                  min = as.numeric(input$a),\n                  max = 40,\n                  value = as.numeric(input$a) + 1)\n    })\n    # browser()\n    y <<- reactive({dunif(x, min = as.numeric(input$a), max = as.numeric(input$b))})\n  })\n  # browser()\n  observe({\n    output$distPlot <- renderPlot({\n      # generate bins based on input$bins from ui.R\n      ggplot() +\n        aes(x = x, y = y())+\n        geom_point() +\n        geom_path() +\n        labs(x = \"x\", y = \"probability\") +\n        theme_bw() +\n        theme(text = element_text(size = 20))\n    })\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n```\n\n\n## More on the Uniform Distribution\n\n-   To demonstrate how PDFs work, we will try a few values:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditions <- tribble(\n   ~x, ~a, ~b,\n   .5,  0,  1,\n  .75,  0,  1,\n   15,  0, 20,\n   15, 10, 20\n) |> \n  mutate(y = dunif(x, min = a, max = b))\nconditions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n      x     a     b     y\n  <dbl> <dbl> <dbl> <dbl>\n1  0.5      0     1  1   \n2  0.75     0     1  1   \n3 15        0    20  0.05\n4 15       10    20  0.1 \n```\n\n\n:::\n:::\n\n\n\n\n-   The uniform PDF has the feature that all values of ***x*** are equally likely across the sample space of the distribution\n    -   Therefore, you do not see ***x*** in the PDF $f(x)$\n-   The mean of the uniform distribution is $\\frac{1}{2}(a+b)$\n-   The variance of the uniform distribution is $\\frac{1}{12}(b-a)^2$\n\n## Univariate Normal Distribution\n\n-   For a continuous random variable ***x*** (ranging from $-\\infty$ to $\\infty$), the univariate normal distribution function is:\n\n$$\nf(x) = \\frac1{\\sqrt{2\\pi\\sigma^2_x}}\\exp(-\\frac{(x-\\mu_x)^2}{2\\sigma^2_x})\n$$\n\n-   The shape of the distribution is governed by two parameters:\n\n    -   The mean $\\mu_x$\n\n    -   The variance $\\sigma^2_x$\n\n    -   These parameters are called **sufficient statistics** (they contain all the information about the distribution)\n\n-   The skewness (lean) and kurtosis (peakedness) are fixed\n\n-   Standard notation for normal distributions is $x\\sim N(\\mu_x, \\sigma^2_x)$\n\n    -   Read as: \"*x* follows a normal distribution with a mean $\\mu_x$ and a variance $\\sigma^2_x$\"\n\n-   Linear combinations of random variables following normal distributions result in a random variable that is normally distributed\n\n## Univariate Normal Distribution in R: pnorm\n\nDensity (`dnorm`), distribution function (`pnorm`), quantile function (`qnorm`) and random generation (`rnorm`) for the normal distribution with mean equal to `mean` and standard deviation equal to `sd`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nZ = seq(-5, 5, .1) # Z-score\nggplot() +\n  aes(x = Z, y = pnorm(q = Z, lower.tail = TRUE)) +\n  geom_point() +\n  geom_path() +\n  labs(x = \"Z-score\", y = \"Cumulative probability\",\n       title = \"`pnorm()` gives the cumulative probability function P(X < T)\")\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n## Univariate Normal Distribution in R: dnorm\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-5, 5, .1)\n\nparams <- list(\n  y_set1 = c(mu =  0, sigma2 = 0.2),\n  y_set2 = c(mu =  0, sigma2 = 1.0),\n  y_set3 = c(mu =  0, sigma2 = 5.0),\n  y_set4 = c(mu = -2, sigma2 = 0.5)\n)\n\ny <- sapply(params, function(param) dnorm(x, mean = param['mu'], sd = param['sigma2']))\n\ndt <- cbind(x, y) |> \n  as.data.frame() |> \n  pivot_longer(starts_with(\"y_\"))\n\nggplot(dt) +\n  geom_path(aes(x = x, y = value, color = name, group = name), linewidth = 1.3) +\n  scale_color_manual(values = 1:4,\n                     name = \"\",\n                     labels = c('y_set1' = expression(mu*\"=0, \"*sigma^2*\"=.02\"),\n                                'y_set2' = expression(mu*\"=0, \"*sigma^2*\"=1.0\"),\n                                'y_set3' = expression(mu*\"=0, \"*sigma^2*\"=5.0\"),\n                                'y_set4' = expression(mu*\"=-2, \"*sigma^2*\"=0.5\"))\n                     ) +\n  theme_bw() +\n  theme(legend.position = \"top\", text = element_text(size = 13))\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n## Chi-Square Distribution\n\n-   Another frequently used univariate distribution is the Chi-square distribution\n\n    -   Sampling distribution of the variance follows a chi-square distribution\n\n    -   Likelihood ratios follow a chi-square distribution\n\n-   For a continuous random variable *x* (ranging from 0 to $\\infty$), the chi-square distribution is given by:\n\n    $$\n    f(x) =\\frac1{2^{\\frac{\\upsilon}{2}} \\Gamma(\\frac{\\upsilon}{2})} x^{\\frac{\\upsilon}{2}-1} \\exp(-\\frac{x}2)\n    $$\n\n-   $\\Gamma(\\cdot)$ is called the gamma function\n\n-   The chi-square distribution is govern by one parameter: $\\upsilon$ (the degrees of freedom)\n\n    -   The mean is equal to $\\upsilon$; the variance is equal to 2$\\upsilon$\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0.01, 15, .01)\ndf <- c(1, 2, 3, 5, 10)\ndt2 <- as.data.frame(sapply(df, \\(df) dchisq(x, df = df)))\ndt2_with_x <- cbind(x = x, dt2)\ndt2_with_x |> \n  pivot_longer(starts_with(\"V\")) |> \n  ggplot() +\n  geom_path(aes(x = x, y = value, color = name), linewidth = 1.2) +\n  scale_y_continuous(limits = c(0, 1)) +\n  scale_color_discrete(name = \"\", labels = paste(\"df =\", df)) +\n  labs(y = \"f(x)\") +\n  theme_bw() +\n  theme(legend.position = \"top\", text = element_text(size = 13))\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nset.seed(1234)\n# Define UI for application that draws a histogram\nui <- fluidPage(\n\n    # Application title\n    titlePanel(\"Chi-square distribution\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"Df\",\n                        \"Degree of freedom:\",\n                        min = 1,\n                        max = 20,\n                        value = 1,\n                        animate = animationOptions(interval = 5000, loop = TRUE)),\n           verbatimTextOutput(outputId = \"Df_display\")\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n    x <- seq(0.01, 15, .01)\n    df <- reactive({input$Df}) \n    \n    output$Df_display <- renderText({\n        paste0(\"DF = \", df())\n    })\n    output$distPlot <- renderPlot({\n        # generate bins based on input$bins from ui.R\n        \n        \n        dt2 <- as.data.frame(sapply(df(), \\(df) dchisq(x, df = df)))\n        dt2_with_x <- cbind(x = x, dt2)\n        dt2_with_x |> \n            pivot_longer(starts_with(\"V\")) |> \n            ggplot() +\n            geom_path(aes(x = x, y = value), linewidth = 1.2) +\n            scale_y_continuous(limits = c(0, 1)) +\n            labs(y = \"f(x)\") +\n            theme_bw() +\n            theme(legend.position = \"top\", text = element_text(size = 13))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n```\n\n# Marginal, Joint, And Conditional Distribution\n\n## Moving from One to Multiple Random Variables\n\n-   When more than one random variable is present, there are several different types of statistical distributions:\n\n-   We will first consider two discrete random variables：\n\n    -   *x* is the outcome of the flip of a penny {$H_p$, $T_p$}\n\n        -   $f(x=H_p) = .5$; $f(x =T_p) = .5$\n\n    -   *z* is the outcome of the flip of a dime {$H_d$, $T_d$}\n\n        -   $f(z=H_p) = .5$; $f(z =T_p) = .5$\n\n-   We will consider the following distributions:\n\n    -   Marginal distribution\n\n        -   The distribution of one variable only (either $f(x)$ or $f(z)$)\n\n    -   Joint distribution\n\n        -   $f(x, z)$: the distribution of both variables (both *x* and *z*)\n\n    -   **Conditional distribution**\n\n        -   The distribution of one variable, conditional on values of the other:\n\n            -   $f(x|z)$: the distribution of *x* given *z*\n\n            -   $f(z|x)$: the distribution of *z* given x\n\n## Marginal Distribution\n\n-   Marginal distributions are what we have worked with exclusively up to this point: they represent the distribution by itself\n\n    -   Continuous univariate distributions\n\n    -   Categorical distributions\n\n        -   The flip of a penny\n\n        -   The flip of a dime\n\n## Joint Distribution\n\n-   Joint distributions describe the distribution of more than one variable, simultaneously\n\n    -   Representations of multiple variables collected\n\n-   Commonly, the joint distribution function is denoted with all random variables separated by commas\n\n    -   In our example, $f(x,z)$ is the joint distribution of the outcome of flipping both a penny and a dime\n\n        -   As both are discrete, the joint distribution has four possible values:\n\n            \\(1\\) $f(x = H_p,z=H_d)$ (2) $f(x = H_p,z=T_d)$ (3) $f(x = T_p,z=H_d)$ (4) $f(x = T_p,z=T_d)$\n\n-   Joint distributions are **multivariate distributions**\n\n-   We will use joint distributions to introduce two topics\n\n    -   Joint distributions of independent variables\n\n    -   Joint distributions – used in maximum likelihood estimation\n\n## Joint Distributions of Independent Random Variables\n\n-   Random variables are said to be independent if the occurrence of one event makes it neither more nor less probable of another event\n\n    -   For joint distributions, this means: $f(x,z)=f(x)f(z)$\n\n-   In our example, flipping a penny and flipping a dime are independent – so we can complete the following table of their joint distribution:\n\n    |                 | z = $H_d$                          | z = $T_d$                         | Marginal (Penny)              |\n    |------------------|-------------------|------------------|------------------|\n    | $x = H_p$       | $\\color{tomato}{f(x=H_p, z=H_d)}$  | $\\color{tomato}{f(x=H_p, z=T_d)}$ | $\\color{turquoise}{f(z=H_p)}$ |\n    | $x = T_p$       | $\\color{tomato}{f(x= T_p, z=H_d)}$ | $\\color{tomato}{f(x=T_p, z=T_d)}$ | $\\color{turquoise}{f(z=T_d)}$ |\n    | Marginal (Dime) | $\\color{turquoise}{f(z=H_d)}$      | $\\color{turquoise}{f(z=T_d)}$     |                               |\n\n## Joint Distributions of Independent Random Variables\n\n-   Because the coin flips are independent, this because:\n\n|                 | z = $H_d$                            | z = $T_d$                           | Marginal (Penny)              |\n|-----------------|-------------------|-------------------|-----------------|\n| $x = H_p$       | $\\color{tomato}{f(x=H_p)f( z=H_d)}$  | $\\color{tomato}{f(x=H_p)f( z=T_d)}$ | $\\color{turquoise}{f(z=H_p)}$ |\n| $x = T_p$       | $\\color{tomato}{f(x= T_p)f( z=H_d)}$ | $\\color{tomato}{f(x=T_p)f( z=T_d)}$ | $\\color{turquoise}{f(z=T_d)}$ |\n| Marginal (Dime) | $\\color{turquoise}{f(z=H_d)}$        | $\\color{turquoise}{f(z=T_d)}$       |                               |\n\n-   Then, with numbers:\n\n|                 | z = $H_d$               | z = $T_d$               | Marginal (Penny)        |\n|------------------|------------------|------------------|------------------|\n| $x = H_p$       | $\\color{tomato}{.25}$   | $\\color{tomato}{.25}$   | $\\color{turquoise}{.5}$ |\n| $x = T_p$       | $\\color{tomato}{.25}$   | $\\color{tomato}{.25}$   | $\\color{turquoise}{.5}$ |\n| Marginal (Dime) | $\\color{turquoise}{.5}$ | $\\color{turquoise}{.5}$ |                         |\n\n## Marginalizing Across a Joint Distribution\n\n-   If you had a joint distribution, $\\color{orchid}{f(x, z)}$, but wanted the marginal distribution of either variable ($f(x)$ or $f(z)$) you would have to **marginalize** across one dimension of the joint distribution.\n\n-   For [**categorical random variables**]{.underline}, marginalize = sum across every value of z\n\n$$\nf(x) = \\sum_zf(x, z)\n$$\n\n-   For example, $f(x = H_p) = f(x = H_p, z=H_d) +f(x = H_p, z=T_d)=.5$\n\n-   For [continuous random variables,]{.underline} marginalize = integrate across z\n\n    -   The integral:\n\n        $$\n        f(x) = \\int_zf(x,z)dz\n        $$\n\n## Conditional Distributions\n\n-   For two random variables x and z, a conditional distribution is written as: $f(z|x)$\n\n    -   The distribution of z given x\n\n-   The conditional distribution is equal to the joint distribution divided by the marginal distribution of the conditioning random variable\n\n    $$\n    f(z|x) = \\frac{f(z,x)}{f(x)}\n    $$\n\n-   Conditional distributions are found everywhere in statistics\n\n    -   The general linear model uses the conditional distribution variable\n\n        $$\n        Y \\sim N(\\beta_0+\\beta_1X, \\sigma^2_e)\n        $$\n\n## Example: Conditional Distribution\n\n-   For two discrete random variables with {0, 1} values, the conditional distribution can be shown in a contingency table:\n\n|                 | z = $H_d$               | z = $T_d$               | Marginal (Penny)        |\n|------------------|------------------|------------------|------------------|\n| $x = H_p$       | $\\color{tomato}{.25}$   | $\\color{tomato}{.25}$   | $\\color{turquoise}{.5}$ |\n| $x = T_p$       | $\\color{tomato}{.25}$   | $\\color{tomato}{.25}$   | $\\color{turquoise}{.5}$ |\n| Marginal (Dime) | $\\color{turquoise}{.5}$ | $\\color{turquoise}{.5}$ |                         |\n\nConditional: $f(z | x= H_p)$:\n\n$f(z=H_d|x =H_p) = \\frac{f(z=H_d, x=H_p}{f(x = H_p)} = \\frac{.25}{.5}=.5$\n\n$f(z = T_d | x = H_p)= \\frac{f(z=T_d, x=H_p}{f(x=H_p)} = \\frac{.25}{.5} = .5$\n\n# Expected Values and The Algebra of Expectation\n\n## Expected Values\n\n-   Expected values are statistics taken the sample space of a random variable: they are essentially weighted averages\n\n\n\n\n    ::: {.cell output-location='default'}\n    \n    ```{.r .cell-code}\n    set.seed(1234)\n    x = rnorm(100, mean = 0, sd = 1)\n    weights = dnorm(x, mean = 0, sd = 1)\n    mean(weights * x)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] -0.05567835\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n-   The weights used in computing this average correspond to the probabilities (for a discrete random variable) or to the densities (for a continuous random variable)\n\n::: {.callout-note font-size=\"1.2em\"}\nThe expected value is represented by $E(x)$\n\nThe actual statistic that is being weighted by the PDF is put into the parenthesis where x is now\n:::\n\n-   Expected values allow us to understand what a statistical model implies about data, for instance:\n\n    -   How a GLM specifies the (conditional) mean and variance of a DV\n\n## Expected Value Calculation\n\n-   For discrete random variables, the expected value is found by:\n\n    $$\n    E(x) = \\sum_x xP(X=x)\n    $$\n\n-   For example, the expected value of a roll of a die is:\n\n    $$\n    E(x) = (1)\\frac16+ (2)\\frac16+(3)\\frac16+(4)\\frac16+(5)\\frac16+6\\frac16\n    $$\n\n-   For continuous random variables, the expected value is found by\n\n    $$\n    E(x) = \\int_x xf(x)dx\n    $$\n\n-   We won't be calculating theoretical expected values with calculus... we use them only to see how models imply things about out data\n\n## Variance and Covariance... As Expected Values\n\n-   A distribution's theoretical variance can also be written as an expected value:\n\n    $$\n    V(x) = E(x-E(x))^2 = E(x -\\mu_x)^2\n    $$\n\n    -   This formula helps us understand predictions made by GLMs and how that corresponds to statistical parameters we interpret\n\n-   For a roll of a die, the theoretical variance is:\n\n    $$\n    V(x) = E(x - 3.5)^2 = \\frac16(1-3.5)^2 + \\frac16(2-3.5)^2 + \\frac16(3-3.5)^2 + \\frac16(4-3.5)^2 + \\frac16(5-3.5)^2 + \\frac16(6-3.5)^2 = 2.92\n    $$\n\n-   Likewise, for a pair of random variable *x* and *z*, the covariance can be found from their joint distributions:\n\n    $$\n    \\text{Cov}(x,z)=E(xz)-E(x)E(z) = E(xz)-\\mu_x\\mu_z\n    $$\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    set.seed(1234)\n    N = 100\n    x = rnorm(N, 2, 1)\n    z = rnorm(N, 3, 1)\n    xz = x*z\n    (Cov_xz = mean(xz)-mean(x)*mean(z)) / ((N-1)/N) # unbiased covariance\n    cov(x, z)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] -0.02631528\n    [1] -0.02631528\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n# Linear Combination of Random Variables\n\n## Linear Combinations of Random Variables\n\n-   A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and then adding the results $$\n    x = c + a_1z_1+a_2z_2+a_3z_3 +\\cdots+a_nz_n\n    $$\n\n-   More generally, linear combinations of random variables have specific implications for the mean, variance, and possibly covariance of the new random variable\n\n-   As such, there are predicable ways in which the means, variances, and covariances change\n\n## Algebra of Expectations\n\n-   Sums of Constants\n\n$$\nE(x+c) = E(x)+c \\\\\n\\text{Var}(x+c) = \\text{Var}(x) \\\\\n\\text{Cov}(x+c, z) = \\text{Cov}(x, z)\n$$\n\n### Example\n\nImagine for weight variable, each individual increases 3 lbs:\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nset.seed(1234)\nlibrary(ESRM64503)\nx = dataSexHeightWeight$weightLB\nc_ = 3\nz = dataSexHeightWeight$heightIN\nmean(x + c_); mean(x)+c_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186.4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186.4\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(x + c_); var(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3179.095\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3179.095\n```\n\n\n:::\n\n```{.r .cell-code}\ncov(x, z); cov(x+c_, z) # decimal place issue, near() accepts two values' diff less than .00001\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 334.8316\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 334.8316\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n-   Products of Constants:\n\n$$\nE(cx) = cE(x) \\\\\n\\text{Var}(cx) = c^2\\text{Var}(x) \\\\\n\\text{Cov}(cx, dz) = c*d*\\text{Cov}(x, z)\n$$\nImagine you wanted to convert weight from pounds to kilograms (where 1 pound = .453 kg) and convert height from inches to cm (where 1 inch = 2.54 cm)\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nc_ = .453\nmean(x*c_); mean(x)*c_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 83.0802\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 83.0802\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(x*c_); var(x)*c_^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 652.3789\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 652.3789\n```\n\n\n:::\n\n```{.r .cell-code}\nd_ = 2.54\ncov(c_*x, d_*z);c_*d_*cov(x, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 385.2639\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 385.2639\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n-   Sums of Multiple Random Variables:\n\n$$\nE(cx+dz) = cE(x) + dE(z) \\\\\n\\text{Var}(cx+dz) = c^2\\text{Var}(x) + d^2\\text{Var}(z) + 2c*d*\\text{Cov}(x,z)\\\\\n$$\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmean(x*c_+z*d_); mean(x)*c_+mean(z)*d_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 255.5462\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 255.5462\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(x*c_+z*d_); c_^2*var(x) + d_^2*var(z) + 2*c_*d_*cov(x, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1780.054\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1780.054\n```\n\n\n:::\n:::\n\n\n\n\n## Where We Use This Algebra\n\n- Remember how we calculated the standard error of conditional main effect from simple main effect and interaction effect\n\n$$\n\\mathbf{Test = 82.20 + 2.16*Senior + 7.76*New - 3.04*Senior*New}\n$$\n\n- Conditional Main Effects of Senior When New is 1 is (2.16 - 3.04) = -0.88\n\n    - We know that $Var(\\beta_{Senior})$ is 0.575,  $Var(\\beta_{Senior*New})$ is 1.151, and $Cov(\\beta_{Senior}, \\beta_{Senior*New})$ is -0.575.\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmodel1 <- lm(Test ~ Senior + New + Senior * New, data = dataTestExperiment)\nvcov(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept)     Senior        New Senior:New\n(Intercept)   0.2877333 -0.2877333 -0.2877333  0.2877333\nSenior       -0.2877333  0.5754667  0.2877333 -0.5754667\nNew          -0.2877333  0.2877333  0.5754667 -0.5754667\nSenior:New    0.2877333 -0.5754667 -0.5754667  1.1509333\n```\n\n\n:::\n:::\n\n\n\n\nThen, \n\n$$\n\\mathbf{Var(\\beta_{Senior}+\\beta_{Senior*New}) = Var(\\beta_{Senior}) + Var(\\beta_{Senior*New}) + 2Cov(\\beta_{Senior},\\beta_{Senior*New})\n\\\\ = 0.575 + 1.151 - 2* 0.575 = 0.576} \n$$\nThus, $SE(\\beta_{Senior}+\\beta_{Senior*New})=0.759$\n\n## Combining the GLM with Expections\n\n- Using the algebra of expectations predicting Y from X and Z:\n\n$$\n\\hat{Y}_p=E(Y_p)=E(\\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p+e_p) \\\\\n= \\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p+E(e_p) \\\\\n= \\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p\n$$\n\n- The variance of $f(Y_p|X_p, Z_p)$:\n\n$$\nV(Y_P)=V(\\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p+e_p)\\\\ = V(e_p)=\\sigma_e^2\n$$\n\n## Examining What This Means in the Context of Data\n\n- If you recall from the regression analysis of the height/weight data, the final model we decided to interpret: Model 5\n\n$$\nW_p = \\beta_0+\\beta_1 (H_p-\\bar{H}) +\\beta_2F_p+\\beta_3 (H_p-\\bar{H}) F_p + e_p\n$$\n\nwhere $e_p \\sim N(0, \\sigma_e^2)$\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\ndat <- dataSexHeightWeight\ndat$heightIN_MC <- dat$heightIN - mean(dat$heightIN)\ndat$female <- dat$sex == 'F'\nmod5 <- lm(weightLB ~ heightIN_MC + female + female*heightIN_MC, data = dat)\nsummary(mod5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weightLB ~ heightIN_MC + female + female * heightIN_MC, \n    data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8312 -1.7797  0.4958  1.3575  3.3585 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            222.1842     0.8381  265.11  < 2e-16 ***\nheightIN_MC              3.1897     0.1114   28.65 3.55e-15 ***\nfemaleTRUE             -82.2719     1.2111  -67.93  < 2e-16 ***\nheightIN_MC:femaleTRUE  -1.0939     0.1678   -6.52 7.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.175 on 16 degrees of freedom\nMultiple R-squared:  0.9987,\tAdjusted R-squared:  0.9985 \nF-statistic:  4250 on 3 and 16 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Picturing the GLM with Distributions\n\n- The distributional assumptions of the GLM are the reason why we do not need to worry if our dependent variable is normally distributed\n\n- Our dependent variable should be conditionally normal\n\n- We can check this assumption by checking our assumption about the residuals,\n$e_p \\sim N(0, \\sigma^2_e)$ \n\n\n\n\n::: {.cell output-location='default'}\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Assessing Distributional Assumptions Graphically\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod5)\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-17-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-17-4.png){width=672}\n:::\n:::\n\n\n\n\n## Hypothesis Tests for Normality\n\nIf a given test is **significant**, then it is saying that your data do not come from a normal distribution\n\nIn practice, test will give diverging information quite frequently:\nthe best way to evaluate normality is to consider both plots and tests (approximate = good)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(mod5$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  mod5$residuals\nW = 0.95055, p-value = 0.3756\n```\n\n\n:::\n:::\n\n\n\n\n## Wrapping Up\n\n- Today was an introduction to mathematical statistics as a way to understand the implications statistical models make about data\n\n- Although many of these topics do not seem directly relevant, they help provide insights that untrained analysts may not easily attain\n",
    "supporting": [
      "Lecture04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}