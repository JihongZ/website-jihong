{
  "hash": "f9604dff0f3c3cd8f0e8cd9d29c1856a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 11: Introduction to Bayesian Statistics and Markov Chain Monte Carlo Estimation\"\nsubtitle: \"\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-10-09\"\ndate-modified: \"2024-10-11\"\ndraft: false\nsidebar: false\nexecute: \n  echo: true\n  warning: false\noutput-location: default\ncode-annotations: below\nhighlight-style: \"nord\"\nformat: \n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"ESRM 64503 - Lecture 11: Absolute Model fit and Path Analysis\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n    fig-align: center\nfilters:\n  - quarto\n  - line-highlight\n---\n\n\n\n\n## Today's Lecture Objectives\n\n1.  Bayes' Theorem\n\n2.  Likelihood function vs. Posterior distribution\n\n3.  How to report posterior distribution of parameters\n\n4.  Bayesian update\n\n5.  Bayesian Linear Regression and Multivariate Regression\n\n# Brief Introduction to Bayesian\n\n## What is Bayesian?\n\n1.  What are key components of Bayesian models?\n\n    1.  Likelihood function - likelihood of **data given assumed probability distribution**\n    2.  Prior distribution - belief / previous evidences of **parameters**\n    3.  Posterior distribution - updated information of parameters given our data and modeling\n    4.  Posterior predictive distribution - future / predicted data\n\n2.  What are the differences between Bayesian with Frequentist analysis?\n\n    1.  **Input**: prior distribution: **Bayesian** vs. hypothesis of fixed parameters: **frequentist**\n    2.  **Estimation process**: Markov Chain Monte Carlo (MCMC) vs. Maximum Likelihood Estimation (MLE)\n    3.  **Result**: posterior distribution vs. point estimates of parameters\n    4.  **Accuracy check**: credible interval (plausibility of the parameters having those values) vs. confidence interval (the proportion of infinite samples having the fixed parameters)\n\n## Bayes' Theorem: How Bayesian Statistics Work\n\nBayesian methods rely on Bayes' Theorem\n\n$$\nP(\\boldsymbol{\\theta} | Data) = \\frac{P(Data|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(Data)} \\propto P(Data|\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta})\n$$\n\nWhere:\n\n1.  P: probability distribution function (PDF)\n2.  $P(\\boldsymbol{\\theta}|Data)$ : the [posterior distribution]{.underline} of parameter $\\boldsymbol{\\theta}$, given the observed data\n3.  $P(Data|\\boldsymbol{\\theta})$: the likelihood function (conditional distributin) of the observed data, given the parameters\n4.  $P(\\boldsymbol{\\theta})$: the [prior distribution]{.underline} of parameter $\\boldsymbol{\\theta}$\n5.  $P(Data)$: the marginal distribution of the observed data\n\n------------------------------------------------------------------------\n\n**A Live Bayesian Example**\n\n-   Suppose we want to assess the probability of rolling a \"1\" on a six-sided die:\n\n    $$\n    \\boldsymbol{\\theta} \\equiv p_{1} = P(D = 1)\n    $$\n\n-   Suppose we collect a sample of data (N = 5):\\\n    $$Data \\equiv X = \\{0, 1, 0, 1, 1\\}$$\n\n-   The [prior distribution]{.underline} of parameters is denoted as $P(p_1)$: how much the probability of \"1\" will occur we \"believe\" ;\n\n-   The [likelihood function]{.underline} is denoted as $P(X | p_1)$;\n\n-   The [posterior distribution]{.underline} is denoted as $P(p_1|X)$\n\n-   The aim of Bayesian is to estimate [the probability of rolling a \"1\" give the Data]{.underline} (posterior distribution) as:\n\n    $$\n    P(p_1|X) = \\frac{P(X|p_1)P(p_1)}{P(X)} \\propto P(X|p_1) P(p_1)\n    $$\n\n------------------------------------------------------------------------\n\n## Why use Bayesian\n\n1.  Bayesian approach to statistics is more intuitive; it resembles how we think about probability in everyday life (our prior belif and our updated belief after observing the data) â€“ in the odds of hypotheses, not those of data. In comparison, the frequentist conclusion sounds complex and difficult to comprehend.\n2.  Bayesian approach is much easier to construct very complicated model: using MCMC, we did not need to derive likelihood function by ourselves.\n3.  Bayesian approach is very flexible to incorporate prior (old, history) data; while the frequentist inference is static, which only use current data.\n\n## Step 1: Choose the Likelihood function (Model)\n\n-   The Likelihood function $P(X|p_1)$ follows Binomial distribution of 3 successes out of 5 samples:\\\n    $$P(X|p_1) = \\prod_{i =1}^{N=5} p_1^{X_i}(1-p_1)^{X_i} \n    \\\\= (1-p_i) \\cdot p_i \\cdot (1-p_i) \\cdot p_i \\cdot p_i$$\n\n-   We use Bernoulli (Binomial) Distribution (feat. *Jacob Bernoulli, 1654-1705*) because Bernoulli dist. has nice statistical probability.\n\n    -   \"Nice\" means making totally sense in normal life-- a common belief. For example, the $p_1$ value that maximizes the Bernoulli-based likelihood function is $Mean(X)$, and the $p_1$ values that minimizes the Bernoulli-based likelihood function is 0 or 1\n\n------------------------------------------------------------------------\n\n### Log-likelihood Function along \"Parameter\"\n\n$$\nLL(p_1, Data =\\{0, 1, 0, 1, 1\\})\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\np1 = seq(0, 1, length.out = 21)\nLikelihood = (p1)^3 * (1-p1)^2\nplot(x = p1, y = log(Likelihood))\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code  code-line-numbers=\"2\"}\np1[which.max(Likelihood)] # p1 = 0.6 = 3 / 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Log-likelihood Function along \"Data\"\n\n$$\nLL(p_1 \\in \\{0, 0.2,0.4, 0.6, 0.8,1\\}, Data \\in \\{0/5, 1/5, 2/5, 3/5, 4/5, 5/5\\})\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-line-numbers=\"5\"}\nlibrary(tidyverse)\np1 = c(0, 2, 4, 6, 8, 10) / 10\nnTrails = 5\nnSuccess = 0:nTrails\nLikelihood = sapply(p1, \\(x) choose(nTrails,nSuccess)*(x)^nSuccess*(1-x)^(nTrails - nSuccess))\nLikelihood_forPlot <- as.data.frame(Likelihood)\ncolnames(Likelihood_forPlot) <- p1\nLikelihood_forPlot$Sample = factor(paste0(nSuccess, \" out of \", nTrails), levels = paste0(nSuccess, \" out of \", nTrails))\n# plot\nLikelihood_forPlot %>% \n  pivot_longer(-Sample, names_to = \"p1\") %>% \n  mutate(`log(Likelihood)` = log(value)) %>% \n  ggplot(aes(x = Sample, y = `log(Likelihood)`, color = p1)) +\n  geom_point(size = 2) +\n  geom_path(aes(group=p1), size = 1.3)\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Step 2: Choose the Prior Distribution for $p_1$\n\nWe must now pick the prior distribution of $p_1$:\n\n$$\nP(p_1)\n$$\n\n-   Compared to likelihood function, we have much more choices. Many distributions to choose from\n\n-   To choose [prior distribution]{.underline}, think about what we know about a \"fair\" die.\n\n    -   the probability of rolling a \"1\" is about $\\frac{1}{6}$\n\n    -   the probabilities much higher/lower than $\\frac{1}{6}$ are very unlikely\n\n-   Let's consider a Beta distribution:\n\n    $$\n    p_1 \\sim Beta(\\alpha, \\beta)\n    $$\n\n------------------------------------------------------------------------\n\n### Prior for probability parameter: The Beta Distribution\n\nFor parameters that range between 0 and 1, the beta distribution with two hyperparameters {$\\alpha$, $\\beta$} makes a good choice for prior distribution:\n\n$$\nP(p_1) = \\frac{(p_1)^{a-1} (1-p_1)^{\\beta-1}}{B(\\alpha,\\beta)}\n$$\n\nWhere denominator $B$ is:\n\n$$\nB(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n$$\n\nand (fun fact: derived by *Daniel Bernoulli (1700-1782)*),\n\n$$\n\\Gamma(\\alpha) = \\int_0^{\\infty}t^{\\alpha-1}e^{-t}dt\n$$\n\n------------------------------------------------------------------------\n\n### More nice properties: Beta Distribution\n\n-   The Beta distribution has a mean of $\\frac{\\alpha}{\\alpha+\\beta}$ and a mode of $\\frac{\\alpha -1}{\\alpha + \\beta -2}$ for $\\alpha > 1$ & $\\beta > 1$; (fun fact: when $\\alpha\\ \\&\\ \\beta< 1$, pdf is U-shape, what that mean?)\n\n-   $\\alpha$ and $\\beta$ are called [hyperparameters]{.underline} of the parameter $p_1$;\n\n-   [**Hyperparameters**]{.underline} are parameters of prior parameters ;\n\n-   When $\\alpha = \\beta = 1$, the distribution is uniform distribution;\n\n-   To make sure $P(p_1 = \\frac{1}{6})$ is the largest, we can:\n\n    -   Many choices: $\\alpha =2$ and $\\beta = 6$ has the same mode as $\\alpha = 100$ and $\\beta = 496$; (hint: $\\beta = 5\\alpha - 4$)\n\n    -   The differences between choices is how strongly we feel in our beliefs\n\n![](images/beta_distribution.png)\n\n------------------------------------------------------------------------\n\n### Conjugacy of Beta Distribution\n\nWhen the binomial likelihood is multiplied by the beta prior, the result is proportional to another beta distribution:\n\n-   $\\text{Posterior} \\propto \\theta^x (1 - \\theta)^{n-x} \\times \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}$\n-   Simplifies to: $\\theta^{x + \\alpha - 1} (1 - \\theta)^{n - x + \\beta - 1}$\n\nThis is the kernel of a beta distribution with updated parameters $\\alpha' = x + \\alpha$ and $\\beta' = n - x + \\beta$. The fact that the posterior is still a beta distribution is what makes the beta distribution a conjugate prior for the binomial likelihood.\n\n> Human langauge: Both beta (prior) and binomial (likelihood) are so-called \"exponetial family\". The muliplication of them is still a \"exponential family\" distribution.\n\n------------------------------------------------------------------------\n\n### How strongly we believe in the prior\n\n::: columns\n::: {.column width=\"40%\"}\n-   The Beta distribution has a variance of $\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 (\\alpha + \\beta + 1)}$\n-   The smaller prior variance means the prior is more [informative]{.underline}\n    -   [Informative]{.underline} priors are those that have relatively small variances\n    -   [Uninformative]{.underline} priors are those that have relatively large variances\n-   Suppose we have four sets of hyperparameters choices: (2,6);(20,96);(100,496);(200,996)\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Function for variance of Beta distribution\nvarianceFun = function(alpha_beta){\n  alpha = alpha_beta[1]\n  beta = alpha_beta[2]\n  return((alpha * beta)/((alpha + beta)^2*(alpha + beta + 1)))\n}\n\n# Multiple prior choices from uninformative to informative\nalpha_beta_choices = list(\n  I_dont_trust = c(2, 6), \n  not_strong =   c(20, 96), \n  litte_strong = c(100, 496), \n  very_strong =  c(200, 996))\n\n## Transform to data.frame for plot\nalpha_beta_variance_plot <- t(sapply(alpha_beta_choices, varianceFun)) %>% \n  as.data.frame() %>% \n  pivot_longer(everything(), names_to = \"Degree\", values_to = \"Variance\") %>% \n  mutate(Degree = factor(Degree, levels = unique(Degree))) %>% \n  mutate(Alpha_Beta = c(\n    \"(2, 6)\",\n    \"(20, 96)\",\n    \"(100, 496)\",\n    \"(200, 996)\"\n  ))\n\nalpha_beta_variance_plot %>% \n  ggplot(aes(x = Degree, y = Variance)) +\n  geom_col(aes(fill = Degree)) +\n  geom_text(aes(label = Alpha_Beta), size = 6) +\n  labs(title = \"Variances along difference choices of alpha and beta\") +\n  theme(text = element_text(size = 21)) \n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Visualize prior distribution $P(p_1)$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndbeta <- function(p1, alpha, beta) {\n  # probability density function\n  PDF = ((p1)^(alpha-1)*(1-p1)^(beta-1)) / beta(alpha, beta)\n  return(PDF)\n}\n\ncondition <- data.frame(\n  alpha = c(2, 20, 100, 200),\n  beta = c(6, 96, 496, 996)\n)\npdf_bycond <- condition %>% \n  nest_by(alpha, beta) %>% \n  mutate(data = list(\n    dbeta(p1 = (1:99)/100, alpha = alpha, beta = beta)\n  ))\n\n## prepare data for plotting pdf by conditions\npdf_forplot <- Reduce(cbind, pdf_bycond$data) %>% \n  t() %>% \n  as.data.frame() ## merge conditions together\ncolnames(pdf_forplot) <- (1:99)/100 # add p1 values as x-axis\npdf_forplot <- pdf_forplot %>% \n  mutate(Alpha_Beta = c( # add alpha_beta conditions as colors\n    \"(2, 6)\",\n    \"(20, 96)\",\n    \"(100, 496)\",\n    \"(200, 996)\"\n  )) %>% \n  pivot_longer(-Alpha_Beta, names_to = 'p1', values_to = 'pdf') %>% \n  mutate(p1 = as.numeric(p1),\n         Alpha_Beta = factor(Alpha_Beta,levels = unique(Alpha_Beta)))\n\npdf_forplot %>% \n  ggplot() +\n  geom_vline(xintercept = 0.17, col = \"black\") +\n  geom_point(aes(x = p1, y = pdf, col = Alpha_Beta)) +\n  geom_path(aes(x = p1, y = pdf, col = Alpha_Beta, group = Alpha_Beta)) + \n  scale_color_discrete(name = \"Prior choices\")\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n**Question: WHY NOT USE NORMAL DISTRIBUTION?**\n\n------------------------------------------------------------------------\n\n## Step 3: The Posterior Distribution\n\nChoose a Beta distribution as the prior distribution of $p_1$ is very convenient:\n\n-   When combined with Bernoulli (Binomial) data likelihood, the [posterior distribution]{.underline} ($P(p_1|Data)$) can be derived analytically\n\n-   The posterior distribution is also a Beta distribution\n\n    -   $\\alpha' = \\alpha + \\sum_{i=1}^{N}X_i$ ($\\alpha'$ is parameter of the posterior distribution)\n\n    -   $\\beta' = \\beta + (N - \\sum_{i=1}^{N}X_i)$ ($\\beta'$ is parameter of the posterior distribution)\n\n-   The Beta distribution is said to be a [conjugate prior]{.underline} in Bayesian analysis: A prior distribution that leads to posterior distribution of the same family\n\n    -   Prior and Posterior distribution are all Beta distribution\n\n------------------------------------------------------------------------\n\n### Visualize the posterior distribution $P(p_1|Data)$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndbeta_posterior <- function(p1, alpha, beta, data) {\n  alpha_new = alpha + sum(data) \n  beta_new = beta + (length(data) - sum(data) )\n  # probability density function\n  PDF = ((p1)^(alpha_new-1)*(1-p1)^(beta_new-1)) / beta(alpha_new, beta_new)\n  return(PDF)\n}\n# Observed data\ndat = c(0, 1, 0, 1, 1)\n\ncondition <- data.frame(\n  alpha = c(2, 20, 100, 200),\n  beta = c(6, 96, 496, 996)\n)\n\npdf_posterior_bycond <- condition %>% \n  nest_by(alpha, beta) %>% \n  mutate(data = list(\n    dbeta_posterior(p1 = (1:99)/100, alpha = alpha, beta = beta,\n                    data = dat)\n  ))\n\n## prepare data for plotting pdf by conditions\npdf_posterior_forplot <- Reduce(cbind, pdf_posterior_bycond$data) %>% \n  t() %>% \n  as.data.frame() ## merge conditions together\ncolnames(pdf_posterior_forplot) <- (1:99)/100 # add p1 values as x-axis\npdf_posterior_forplot <- pdf_posterior_forplot %>% \n  mutate(Alpha_Beta = c( # add alpha_beta conditions as colors\n    \"(2, 6)\",\n    \"(20, 96)\",\n    \"(100, 496)\",\n    \"(200, 996)\"\n  )) %>% \n  pivot_longer(-Alpha_Beta, names_to = 'p1', values_to = 'pdf') %>% \n  mutate(p1 = as.numeric(p1),\n         Alpha_Beta = factor(Alpha_Beta,levels = unique(Alpha_Beta)))\n\npdf_posterior_forplot %>% \n  ggplot() +\n  geom_vline(xintercept = 0.17, col = \"black\") +\n  geom_point(aes(x = p1, y = pdf, col = Alpha_Beta)) +\n  geom_path(aes(x = p1, y = pdf, col = Alpha_Beta, group = Alpha_Beta)) + \n  scale_color_discrete(name = \"Prior choices\") +\n  labs( y = '')\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Manually calculate summaries of the posterior distribution\n\nTo determine the estimate of $p_1$, we need to summarize the posterior distribution:\n\n-   With prior hyperparameters $\\alpha = 2$ and $\\beta = 6$:\n\n    -   $\\hat p_1 = \\frac{2+3}{2+3+6+2}=\\frac{5}{13} = .3846$\n\n    -   SD = 0.1300237\n\n-   With prior hyperparameters $\\alpha = 100$ and $\\beta = 496$:\n\n    -   $\\hat p_1 = \\frac{100+3}{100+3+496+2}=\\frac{103}{601} = .1714$\n\n    -   SD = 0.0153589\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuninfo <- pdf_posterior_forplot %>% filter(Alpha_Beta == \"(2, 6)\")\npaste0(\"Sample Posterior Mean: \", mean(uninfo$p1 * uninfo$pdf)) # .3885\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sample Posterior Mean: 0.388500388484552\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuninfo2 <- pdf_posterior_forplot %>% filter(Alpha_Beta == \"(100, 496)\")\npaste0(\"Sample Posterior Mean: \", mean(uninfo2$p1 * uninfo2$pdf)) # 0.1731\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sample Posterior Mean: 0.173112153145432\"\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n# Bayesian Analysis in R: Stan Program\n\n## Introduction to Stan\n\n-   Stan is a widely used MCMC estimation program\n\n    -   Most recent; has many convenient features\n\n    -   Actually does several methods of estimation (ML, Variational Bayes)\n\n-   You create a model using Stan's syntax\n\n    -   Stan translates your model to a custom-built C++ syntax\n\n    -   Stan then compiles your model into its own executable program\n\n-   You then run the program to estimate your model\n\n    -   If you use R, the interface can be seamless\n\n------------------------------------------------------------------------\n\n### Stan and Rstudio\n\n::: columns\n::: {.column width=\"50%\"}\n-   Option 1: Stan has its own syntax which can be built in stand-alone text files (.stan)\n\n    -   Rstudio will let you create a *stan* file in the new `File` menu\n\n    -   Rstudio also has syntax highlighting in Stan files\n\n    -   Save `.stan` to the same path of your `.r`\n\n-   Option 2: You can also use Stan in a interactive way in **qmd** file\n\n    -   Which is helpful when you want to test multiple stan models and report your results at the same time\n:::\n\n------------------------------------------------------------------------\n\n::: {.column width=\"50%\"}\n![Create a new Stan file in Rstudio](Fig2_CreateStanFile.png){width=\"200\"}\n\n![Stan and R code blocks in qmd file](Fig2_StanCodeinQmd.png){width=\"450\"}\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Install CmdStan\n\n-   [cmdstanr Installation](https://mc-stan.org/cmdstanr/articles/cmdstanr.html#installing-cmdstan)\n\n-   [cmdstanr Users' Guide](https://mc-stan.org/docs/cmdstan-guide/cmdstan-installation.html#troubleshooting-the-installation)\n\n------------------------------------------------------------------------\n\n### Stan Code for the example model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\nregister_knitr_engine(override = FALSE)\n```\n:::\n\n::: {.cell output.var='dice_model'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> N; // sample size\n  array[N] int<lower=0, upper=1> y; // observed data\n  real<lower=1> alpha; // hyperparameter alpha\n  real<lower=1> beta; // hyperparameter beta\n}\nparameters {\n  real<lower=0,upper=1> p1; // parameters\n}\nmodel {\n  p1 ~ beta(alpha, beta); // prior distribution\n  for(n in 1:N){\n    y[n] ~ bernoulli(p1); // model\n  }\n}\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# mod <- cmdstan_model('dice_model.stan')\ndata_list1 <- list(\n  y = c(0, 1, 0, 1, 1),\n  N = 5,\n  alpha = 2,\n  beta = 6\n)\ndata_list2 <- list(\n  y = c(0, 1, 0, 1, 1),\n  N = 5,\n  alpha = 100,\n  beta = 496\n)\nfit1 <- dice_model$sample(\n  data = data_list1,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nfit2 <- dice_model$sample(\n  data = data_list2,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n#### Uninformative prior distribution (2, 6) and posterior distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbayesplot::mcmc_dens(fit1$draws('p1')) +\n  geom_path(aes(x = p1, y = pdf), data = pdf_posterior_forplot %>% filter(Alpha_Beta == '(2, 6)'), col = \"red\") +\n  geom_vline(xintercept = 1/6, col = \"green\") +\n  geom_vline(xintercept = 3/5, col = \"purple\") +\n  labs(title = \"Posterior distribution using uninformative prior vs. the conjugate prior (The Gibbs sampler) method\", caption = 'Green: mode of prior distribution; Purple: expected value from observed data; Red: Gibbs sampling method')\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nfit1$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 10\n  variable   mean median    sd   mad      q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -9.19  -8.91  0.751 0.337 -10.7   -8.66   1.00    1812.    2400.\n2 p1        0.383  0.377 0.131 0.137   0.180  0.609  1.00    1593.    1629.\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n#### Informative prior distribution (100, 496) and posterior distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbayesplot::mcmc_dens(fit2$draws('p1')) +\n  geom_path(aes(x = p1, y = pdf), data = pdf_posterior_forplot %>% filter(Alpha_Beta == '(100, 496)'), col = \"red\") +\n  geom_vline(xintercept = 1/6, col = \"green\") +\n  geom_vline(xintercept = 3/5, col = \"purple\") +\n  labs(title = \"Posterior distribution using informative prior vs. the conjugate prior (The Gibbs sampler) method\", caption = 'Green: mode of prior distribution; Purple: expected value from observed data; Red: Gibbs sampling method')\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nfit2$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -276.    -276.    0.716  0.317  -277.    -275.     1.00    1816.\n2 p1          0.171    0.171 0.0154 0.0155    0.146    0.197  1.00    1627.\n# â„¹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Bayesian updating\n\nWe can use the posterior distribution as a prior!\n\n1.  data {0, 1, 0, 1, 1} with the prior hyperparameter {2, 6} -\\> posterior parameter {5, 9}\n\n2.  new data {1, 1, 1, 1, 1} with the prior hyperparameter {5, 9} -\\> posterior parameter {10, 9}\n\n3.  one more new data {0, 0, 0, 0, 0} with the prior hyperparameter {10, 9} -\\> posterior parameter {10, 14}\n\n# Bayesian Linear Regression\n\n## Our Data Today\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ESRM64503)\ndat <- dataSexHeightWeight\n\n# Linear Model with Least Squares\n## Center independent variable - HeightIN for better interpretation\ndat$heightIN <- dat$heightIN - 60\n\n## an empty model suggested by data\nEmptyModel <- lm(weightLB ~ 1, data = dat)\n\n## Examine assumptions and leverage of fit\n### Residual plot, Q-Q residuals, Scale-Location\n# plot(EmptyModel)\n\n## Look at ANOVA table\n### F-values, Sum/Mean of square of residuals\n# anova(EmptyModel)\n\n## look at parameter summary\nsummary(EmptyModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weightLB ~ 1, data = dat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -74.4  -52.4   -8.4   53.1   85.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   183.40      12.61   14.55 9.43e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.38 on 19 degrees of freedom\n```\n\n\n:::\n:::\n\n\n\n\n## Stan Syntax for Empty Model\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nA Stan file saved as `EmptyModel.stan`\n\n-   Each line ends with a semi colon `;`\n-   Comments are put in with `//`\n-   Each block starts with `block type` and is surrounded by curly brackets `{}`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstanModel <- \"\ndata {\n  int<lower=0> N;\n  vector[N] weightLB;\n}\nparameters {\n  real beta0;\n  real<lower=0> sigma;\n}\nmodel {\n  beta0 ~ normal(0, 1000);\n  sigma ~ uniform(0, 100000);\n  weightLB ~ normal(beta0, sigma);\n}\n\"\nmodel00.fromString = cmdstan_model(stan_file = write_stan_file(stanModel)) #<1>\n```\n:::\n\n\n\n\n1.  the `data` block include the observed information\n2.  sample size (declared as a integer value)\n3.  dependent variable (declared as vector with the length as sample size)\n4.  the `parameter` block includes all parameters we want to estimate\n5.  intercept parameter\n6.  standard deviation of residuals\n7.  the `model` block includes the prior information and likelihood\n8.  prior distribution for `beta_0`\n9.  prior distribution for SD of residuals\n10. likelihood function for observed data\n\n## Prepare data for Empty Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# build R list containing data for Stan: Must be named what \"data\" are listed in analysis\nstanData = list(\n  N = nrow(dat), #<1>\n  weightLB = dat$weightLB #<2>\n)\n```\n:::\n\n\n\n\n1.  Sample size, `nrow()` calculates the number of rows of data\n2.  Vector of dependent variable\n\n-   Stan needs the data you declared in you syntax to be able to run\n\n-   Within R, we can pass this data list to Stan via a list object\n\n-   The entries in the list should correspond to the data block of the Stan syntax\n\n## Run Markov Chains in CmdStanr\n\n\n\n\n::: {.cell result='hide'}\n\n```{.r .cell-code}\n# run MCMC chain (sample from posterior)\nmodel00.samples = model00.fromString$sample(\n  data = stanData, #<1>\n  seed = 1, #<2>\n  chains = 3, #<3>\n  parallel_chains = 4, #<4>\n  iter_warmup = 2000, #<5>\n  iter_sampling = 1000 #<6>\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 3 chains, at most 4 in parallel...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Warmup) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Warmup) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Warmup) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Warmup) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Warmup) \nChain 1 Iteration: 2001 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Warmup) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Warmup) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Warmup) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Warmup) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Warmup) \nChain 2 Iteration: 2001 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Warmup) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Warmup) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Warmup) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Warmup) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Warmup) \nChain 3 Iteration: 2001 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\n\nAll 3 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n```\n\n\n:::\n:::\n\n\n\n\n1.  Data list\n2.  Random number seed\n3.  Number of chains (and parallel chains)\n4.  Number of warmup iterations (more details shortly)\n5.  Number of sampling iterations\n\n::: nonincremental\n-   Within the compiled program and the data, the next step is to run the Markov Chain Monte Carlo (MCMC) sampling (this is basically equivalent to drawing from a posterior distribution)\n\n-   In `cmdstanr`, running the MCMC sampling comes from the `$sample()` function that is part of the compiled program object\n:::\n\n## Results of Bayesian Empty Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel00.samples$summary()[c('variable','mean', 'rhat')]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  variable  mean  rhat\n  <chr>    <dbl> <dbl>\n1 lp__     -87.2  1.00\n2 beta0    183.   1.00\n3 sigma     60.2  1.00\n```\n\n\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(model00.samples$draws(\"beta0\"))\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n## Bayesian full model using `blavaan`\n\n![](images/clipboard-1983629374.png)\n\n## Bayesian Full Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(blavaan)\nlibrary(lavaan)\n\nmodel01.syntax = \n\" \n#endogenous variable equations\nweightLB ~ sex\nheightIN ~ sex\n\nheightIN ~~ weightLB\n\"\n\n#estimate model with Bayesian\nmodel01.fit_mcmc = bsem(model01.syntax, data=dat)\n#estimate model with MCMC\nmodel01.fit_ML = sem(model01.syntax, data=dat)\n```\n:::\n\n\n\n\n## Trace Plots of Parameters\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model01.fit_mcmc)\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n## Posterior Distribution of coefficient\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameterestimates(model01.fit_ML)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       lhs op      rhs    est     se      z pvalue ci.lower ci.upper\n1 weightLB  ~      sex 105.00  7.265 14.453  0.000   90.761  119.239\n2 heightIN  ~      sex   8.60  2.612  3.293  0.001    3.482   13.718\n3 weightLB ~~ heightIN  92.34 29.602  3.119  0.002   34.322  150.358\n4 weightLB ~~ weightLB 263.89 83.449  3.162  0.002  100.332  427.448\n5 heightIN ~~ heightIN  34.10 10.783  3.162  0.002   12.965   55.235\n6      sex ~~      sex   0.25  0.000     NA     NA    0.250    0.250\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npost_coefs <- blavaan::blavInspect(model01.fit_mcmc, \"mcmc\")\ncoda::densplot(post_coefs)\n```\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-22-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-22-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture11_files/figure-html/unnamed-chunk-22-5.png){width=672}\n:::\n:::\n\n\n\n\n## Summary of coefficients\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model01.fit_mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nblavaan 0.5.6 ended normally after 1000 iterations\n\n  Estimator                                      BAYES\n  Optimization method                             MCMC\n  Number of model parameters                         5\n\n  Number of observations                            20\n\n  Statistic                                 MargLogLik         PPP\n  Value                                             NA       0.000\n\nParameter Estimates:\n\n\nRegressions:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n  weightLB ~                                                                   \n    sex              50.022   10.671   27.753   69.846    1.004    normal(0,10)\n  heightIN ~                                                                   \n    sex              -9.942    3.813  -17.959   -3.056    1.003    normal(0,10)\n\nCovariances:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n .weightLB ~~                                                                  \n   .heightIN        200.165   66.793   99.567  358.636    1.002       beta(1,1)\n\nVariances:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n   .weightLB        592.995  197.930  295.492 1063.741    1.003 gamma(1,.5)[sd]\n   .heightIN         70.551   23.794   35.065  126.077    1.002 gamma(1,.5)[sd]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstandardizedsolution(model01.fit_mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       lhs op      rhs est.std\n1 weightLB  ~      sex   0.716\n2 heightIN  ~      sex  -0.509\n3 weightLB ~~ heightIN   0.979\n4 weightLB ~~ weightLB   0.487\n5 heightIN ~~ heightIN   0.741\n6      sex ~~      sex   1.000\n```\n\n\n:::\n:::\n\n\n\n\n## Wrapping up\n\nToday is a quick introduction to Bayesian Concept\n\n-   Bayes' Theorem: Fundamental theorem of Bayesian Inference\n-   Prior distribution: What we know about the parameter before seeing the data\n    -   hyperparameter: parameter of the prior distribution\n    -   Uninformative prior: Prior distribution that does not convey any information about the parameter\n    -   Informative prior: Prior distribution that conveys information about the parameter\n    -   Conjugate prior: Prior distribution that makes the posterior distribution the same family as the prior distribution\n-   Likelihood: What the data tell us about the parameter\n    -   Likelihood function: Probability of the data given the parameter\n    -   Likelihood principle: All the information about the data is contained in the likelihood function\n    -   Likelihood is a sort of \"scientific judgment\" on the generation process of the data\n-   Posterior distribution: What we know about the parameter after seeing the data\n",
    "supporting": [
      "Lecture11_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}