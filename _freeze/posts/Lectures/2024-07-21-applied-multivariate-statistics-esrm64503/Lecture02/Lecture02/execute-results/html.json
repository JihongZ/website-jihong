{
  "hash": "1e638d408131b902ecf0236d649e22ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 02: General Linear Model\"\nsubtitle: \"Descriptive Statistics and Basic Statistics\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-08-26\"\nsidebar: false\nexecute: \n  echo: true\n  warning: false\noutput-location: column\nformat: \n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: true\n    number-depth: 1\n    footer: \"ESRM 64503: Lecture 02 - Descriptive Statistics\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\n    include-in-header:\n      - file: mathjax-color.html\n---\n\n\n\n\n## Some notes before our class\n\n1.  Vote for late class time (5:15?) for free parking\n2.  The R code I'll show in the slides is just for illustration. Don't be worried when you hardly follow.\n3.  We will practice more in the R practice unit (Unit 3).\n\n## Learning Objectives\n\n1.  Review Homework 0\n2.  Unit 1: Descriptive Statistics\n    1.  Introduce R package – **ESRM64503**\n    2.  Univariate Descriptive Statistics\n        -   Central tendency: Mean, Median, Mode\n        -   Variation/Spread: Standard deviation (SD), Variance, Range\n    3.  Bivariate descriptive statistics\n        -   Correlation\n        -   Covariance\n    4.  Types of variable distributions\n        -   Marginal\n        -   Joint\n        -   Conditional\n    5.  Bias in estimators\n3.  Unit 2: General Linear Model\n\n## Installation of *ESRM64503* R package\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\npak::pak(\"JihongZ/ESRM64503\") # Install the Github package\npak::pak(\"JihongZ/ESRM64503\", upgrade = TRUE) # If you already install the package, try to upgrade\n```\n:::\n\n\n\n\n## Test *ESTM64503* Package\n\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"package version\"}\nlibrary(ESRM64503)\ndevtools::package_info(\"ESRM64503\") # Make sure the version number is 2024.08.20\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n package   * version    date (UTC) lib source\n ESRM64503 * 2024.08.20 2024-08-21 [1] Github (JihongZ/ESRM64503@ed78e3d)\n\n [1] /Users/jihong/Rlibs\n [2] /Users/jihong/Library/R/arm64/4.2/library\n [3] /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library\n```\n\n\n:::\n:::\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"homework information\"}\nhomework() # You can call \"homework()\" function to access homework info\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nThere will be four homeworks in total for ESRM 64503.\nYou can work on HW0 now, please click the following link to have access:\nhttps://jihongzhang.org/posts/Lectures/2024-07-21-applied-multivariate-statistics-esrm64503/HWs/HW_demo.html\nNext homework (Homework 1) will be posted on 09/09/2024.\nFor more details, please refer to\nhttps://jihongzhang.org/posts/Lectures/2024-07-21-applied-multivariate-statistics-esrm64503/\n```\n\n\n:::\n:::\n\n\n\n\n## Data Files of *ESRM64503*\n\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Data dataSexHeightWeight automate loaded\"}\ndataSexHeightWeight # You should find data named \"dataSexHeightWeight\" already loaded \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id sex heightIN weightLB\n1   1   F       56      117\n2   2   F       60      125\n3   3   F       64      133\n4   4   F       68      141\n5   5   F       72      149\n6   6   F       54      109\n7   7   F       62      128\n8   8   F       65      131\n9   9   F       65      131\n10 10   F       70      145\n11 11   M       64      211\n12 12   M       68      223\n13 13   M       72      235\n14 14   M       76      247\n15 15   M       80      259\n16 16   M       62      201\n17 17   M       69      228\n18 18   M       74      245\n19 19   M       75      241\n20 20   M       82      269\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Type ? to check variable information of data\"}\n?dataSexHeightWeight\n```\n:::\n\n\n\n\n## Data for Today's Lecture\n\n-   To help demonstrate the concepts of today's lecture, we will be using a toy data set with three variables\n\n    -   **Female (Gender)**: Coded as Male (= 0) or Female (= 1)\n\n    -   **Height**: in inches\n\n    -   **Weight**: in pounds\n\n-   The goal of lecture 02 will be to build a general **linear model** that predicts a person's weight\n\n    -   **Linear (regression) model**: a statistical model for an outcome that uses a linear combination (a weighted sum) of one or more predictor variables to produce an estimate of an observation's predicted value\n\n    -   $$\n        \\mathbb{y} = \\beta_0+\\beta_1 \\mathbf{X}\n        $$\n\n-   All models we learnt today will follow this framework.\n\n## Recoding Variables\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ESRM64503) # INSTALL: pak::pak(\"JihongZ/ESRM64504\")\nlibrary(kableExtra) # INSTALL: pak::pak(\"JihongZ/ESRM64504\")\n\ndataSexHeightWeight$female = dataSexHeightWeight$sex == \"F\"\nkable(dataSexHeightWeight,\n      caption = 'toy data set') |> \n  kable_styling(font_size = 30)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 30px; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">toy data set</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:left;\"> sex </th>\n   <th style=\"text-align:right;\"> heightIN </th>\n   <th style=\"text-align:right;\"> weightLB </th>\n   <th style=\"text-align:left;\"> female </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 56 </td>\n   <td style=\"text-align:right;\"> 117 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 60 </td>\n   <td style=\"text-align:right;\"> 125 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 64 </td>\n   <td style=\"text-align:right;\"> 133 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 68 </td>\n   <td style=\"text-align:right;\"> 141 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 72 </td>\n   <td style=\"text-align:right;\"> 149 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 54 </td>\n   <td style=\"text-align:right;\"> 109 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 62 </td>\n   <td style=\"text-align:right;\"> 128 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 65 </td>\n   <td style=\"text-align:right;\"> 131 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 65 </td>\n   <td style=\"text-align:right;\"> 131 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:right;\"> 70 </td>\n   <td style=\"text-align:right;\"> 145 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 64 </td>\n   <td style=\"text-align:right;\"> 211 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 68 </td>\n   <td style=\"text-align:right;\"> 223 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 72 </td>\n   <td style=\"text-align:right;\"> 235 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 14 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 76 </td>\n   <td style=\"text-align:right;\"> 247 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 80 </td>\n   <td style=\"text-align:right;\"> 259 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 62 </td>\n   <td style=\"text-align:right;\"> 201 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 69 </td>\n   <td style=\"text-align:right;\"> 228 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 74 </td>\n   <td style=\"text-align:right;\"> 245 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 75 </td>\n   <td style=\"text-align:right;\"> 241 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:right;\"> 82 </td>\n   <td style=\"text-align:right;\"> 269 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n# Unit 1: Descriptive Statistics\n\n## Quick inspection I\n\n-   Check (1) if any case has **missing** value (2) **distributions** for continuous and categorical variables\n\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code  code-fold=\"show\"}\ntable(complete.cases(dataSexHeightWeight))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTRUE \n  20 \n```\n\n\n:::\n:::\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code  code-fold=\"show\"}\ndataWithMissing <- dataSexHeightWeight\ndataWithMissing[11, 2] <- NA\ntable(complete.cases(dataWithMissing))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFALSE  TRUE \n    1    19 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Distribution of continous variables\"}\npsych::describe(dataSexHeightWeight[,-1], omit = TRUE, trim = 0) |> kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|         | vars|  n|  mean|     sd| median| trimmed|    mad| min| max| range|  skew| kurtosis|     se|\n|:--------|----:|--:|-----:|------:|------:|-------:|------:|---:|---:|-----:|-----:|--------:|------:|\n|heightIN |    2| 20|  67.9|  7.440|     68|    67.9|  7.413|  54|  82|    28| 0.048|   -0.812|  1.664|\n|weightLB |    3| 20| 183.4| 56.383|    175|   183.4| 72.647| 109| 269|   160| 0.110|   -1.804| 12.608|\n\n\n:::\n:::\n\n\n\n\n## Quick Inspect II\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Distribution of categorical variables\"}\ntable(dataSexHeightWeight$female)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFALSE  TRUE \n   10    10 \n```\n\n\n:::\n:::\n\n\n\n\n## Visualization: Pairwise Scatterplot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(dataSexHeightWeight[, c('female', 'heightIN', 'weightLB')])\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n## Histograms of Height and Weight\n\n::: columns\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dataSexHeightWeight$weightLB, main = 'Weight', xlab = 'Pounds')\n```\n\n::: {.cell-output-display}\n![Pairwise Scatter Points](Lecture02_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dataSexHeightWeight$heightIN, main = 'Height', xlab = 'Inches')\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n:::\n:::\n\n## Descriptive Statistics\n\n-   First, we can inspect each variable individually (**marginal distribution**) through a set of descriptive statistics\n\n    -   Visual way: histogram plot or density plot\n\n    -   Statistical way: Central tendency and Variability\n\n        -   Mean, Median, Mode\n\n        -   SD, Range\n\n-   Second, we can also summarize the **joint (bivariate) distribution** of two variables through a set of descriptive statistics:\n\n    -   **Joint vs. Marginal**: joint distribution describes more than one variable simultaneously\n\n    -   Common bivariate descriptive statistics:\n\n        -   Correlation and covariance\n\n## Descriptive Statistics for Toy Data: Marginal\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n## wide-format marginal description\nwide_marginal_desp <- dataSexHeightWeight |> \n  summarise(across(c(heightIN, weightLB, female), list(mean = mean, sd = sd, var = var)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nwide_marginal_desp |> \n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Value\") |> \n  separate(Variable, sep = \"_\", into = c(\"Variable\", \"Stats\")) |> \n  pivot_wider(names_from = Stats, values_from = Value) |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Variable |  mean|     sd|      var|\n|:--------|-----:|------:|--------:|\n|heightIN |  67.9|  7.440|   55.358|\n|weightLB | 183.4| 56.383| 3179.095|\n|female   |   0.5|  0.513|    0.263|\n\n\n:::\n:::\n\n\n\n\n## Descriptive Statistics for Toy Data: Joint\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_cov_mat <- matrix(nrow = 3, ncol = 3)\ncolnames(cor_cov_mat) <- rownames(cor_cov_mat) <- c(\"heightIN\", \"weightLB\", \"female\")\ncov_mat <- cov(dataSexHeightWeight[, c(\"heightIN\", \"weightLB\", \"female\")])\ncor_mat <- cor(dataSexHeightWeight[, c(\"heightIN\", \"weightLB\", \"female\")])\n## Assign values\ncor_cov_mat[lower.tri(cor_cov_mat)] <- cor_mat[lower.tri(cor_mat)]\ncor_cov_mat[upper.tri(cor_cov_mat)] <- cov_mat[upper.tri(cov_mat)]\ndiag(cor_cov_mat) <- diag(cov_mat)\nkable(cor_cov_mat, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|         | heightIN| weightLB|  female|\n|:--------|--------:|--------:|-------:|\n|heightIN |   55.358|  334.832|  -2.263|\n|weightLB |    0.798| 3179.095| -27.632|\n|female   |   -0.593|   -0.955|   0.263|\n\n\n:::\n:::\n\n\n\n\n-   Note:\n\n    -   Diagonal: Variance\n\n    -   Above Diagonal (upper triangle): Covaraince\n\n    -   Below Diagonal (lower triangle): Correlation\n\n-   **Question:** What we can tell regarding the relationships among three variables?\n\n# Variance, Correlation, Covariance\n\n## Re-examining the Concept of Variance\n\n-   Variability is a central concept in advanced statistics\n\n    -   In multivariate statistic, covariance is also central\n\n-   Two formulas for the variance (about the same when $N$ is larger):\n\n    ::: columns\n    ::: {.column width=\"50%\"}\n    $$\n    S^2_Y= \\frac{\\Sigma_{p=1}^{N}(Y_p-\\bar Y)}{N-1}\n    $$ {#eq-unbiased-var}\n    :::\n\n    ::: {.column width=\"50%\"}\n    $$\n    S^2_Y= \\frac{\\Sigma_{p=1}^{N}(Y_p-\\bar Y)}{N}$$ {#eq-biased-var}\n    :::\n    :::\n\n-   @eq-unbiased-var: **Unbiased** or \"sample\"\n\n-   @eq-biased-var: **Biased/ML** or \"population\"\n\nHere: $p$ = person;\n\n## Biased VS. Unbiased Variability\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nset.seed(1234)\nsample_sizes = seq(10, 200, 10)\npopulation_var = 100\nvariance_mat <- matrix(NA, nrow = length(sample_sizes), ncol = 4)\niter = 0\nfor (sample_size in sample_sizes) {\n  iter = iter + 1\n  sample_points <- rnorm(n = sample_size, mean = 0, sd = sqrt(population_var))\n  sample_var_biased <- var(sample_points) * (sample_size-1) / sample_size\n  sample_var_unbiased <- var(sample_points)\n  variance_mat[iter, 1] <- sample_size\n  variance_mat[iter, 2] <- sample_var_biased\n  variance_mat[iter, 3] <- sample_var_unbiased\n  variance_mat[iter, 4] <- population_var\n}\ncolnames(variance_mat) <- c(\n  \"sample_size\",\n  \"sample_var_biased\",\n  \"sample_var_unbiased\",\n  \"population_var\"\n)\n```\n:::\n\n\n\n\n**Take home note**: Unbiased variance estimators can get more accurate estimate of variance than the biased one.\n\n## Biased VS. Unbiased Estimator of Variance (Cont.)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nvariance_mat |> \n  as.data.frame() |> \n  pivot_longer(-sample_size) |> \n  ggplot() +\n  geom_point(aes(x = sample_size, y = value, color = name), linewidth = 1.1) +\n  geom_line(aes(x = sample_size, y = value, color = name), linewidth = 1.1) +\n  labs(x = \"Sample Size\", y = \"Estimates of Variance\") +\n  scale_color_manual(values = 1:3, labels = c(\"Population Variance\", \"Sample Biased Variance (ML)\", \"Sample Unbiased Variance\"), name = \"Estimator\") +\n  theme_classic() +\n  theme(text = element_text(size = 25)) \n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=1920}\n:::\n:::\n\n\n\n\n**Take home note**: When sample size is small, unbiased variance estimators can get the estimate of variance closer to the population variance than the biased one.\n\n## Interpretation of Variance\n\n-   The variance describes the spread of a variable in squared units (which come from $(Y_p - \\bar Y)^2$ term in the equation)\n\n-   Variance: **the average squared distance of an observation from the mean**\n\n    -   For the toy sample, the **variance of height** is 55.358 inches squared\n\n    -   For the toy sample, the **variance of weight** is 3179.095 pounds squared\n\n    -   The **variance of female** — not applicable in the sample way!\n\n        -   How is the sample equally distributed across different groups: 50/50 -\\> largest variance\n\n-   Because squared units are difficult to work with, we typically use the standard deviation – which is reported in units\n\n-   Standard deviation: the average distance of an observation from the mean\n\n    -   SD of Height: 7.44 inches\n\n    -   SD of Weight: 56.383 pounds\n\n## Variance/SD as a More General Statistical Concept\n\n-   Variance (and the standard deviation) is a concept that is applied across statistics – not just for data\n    -   Statistical parameters (slope, intercept) have variance\n        -   e.g., The sample mean $\\bar Y$ has a \"standard error\" (SE) of $S_{\\bar Y} = S_Y / \\sqrt{N}$\n        -   How accurately we can estimate the sample mean $\\neq$ How dispersed the samples are\n-   The standard error is another name for standard deviation\n    -   So \"standard error of the mean\" is equivalent to \"standard deviation of the mean\"\n    -   Usually \"**error**\" refers to **parameters**; \"**deviation**\" refers to **data**\n    -   Variance of the mean would be $S_{\\bar Y}^2  = S^2_Y / N$\n\n## Example: Table of Descriptive Statistics\n\n![Table 1 (Xiong et al., 2023)](figures/Exp_Descrip_Stats_Table.png){fig-align=\"center\"}\n\n-   Key information that be reprted\n\n    -   All variables that you think relevant to the study: (1) demographic (2) context factors (3) outcomes or predictors\n\n    -   Categorical Variables: (1) Percentage of each level (2) Sample size of each level (3) Range\n\n    -   Continuous Variables: Mean + SD + Range\n\n## Correlation of Variables\n\n-   Moving from marginal summaries of each variable to joint (bivariate) summaries, we can use the **Pearson Correlation** to describe the association between a pair of **continuous** variables:\n\n$$\nr_{Y_1, Y_2} = \\frac{\\frac{1}{N-1}\\sum_{p=1}^N (Y_{1p}-\\bar Y_1) (Y_{2p}-\\bar Y_2)}{S_{Y_1}S_{Y_2}} \\\\\n= \\frac{\\sum_{p=1}^N (Y_{1p}-\\bar Y_1) (Y_{2p}-\\bar Y_2)}{\\sqrt{\\sum_{p=1}^{N} (Y_{1p}-\\bar Y)^2}\\sqrt{\\sum_{p=1}^{N} (Y_{2p}-\\bar Y)^2}}\n$$\n\n-   **Human words**: how much Variable 1 vary with Variable 2 relative to their own variability.\n\n-   Note that pearson correlation does not take other variables into account\n\n## R: Pearson Correlation\n\n#### Method 1\n\n`cor()` function with the argument method = \"pearson\"\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nX = 1:10\nY = 10:1\nZ = 1:10\ncor(X, Y, method = \"pearson\")\ncor(X, Z, method = \"pearson\")\ncor(cbind(X, Y, Z), method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1\n[1] 1\n   X  Y  Z\nX  1 -1  1\nY -1  1 -1\nZ  1 -1  1\n```\n\n\n:::\n:::\n\n\n\n\n#### Method 2: Manual way\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncor_X_Y = sum((X-mean(X))*(Y-mean(Y)))/(length(X)-1)/(sd(X)*sd(Y))\ncor_X_Y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1\n```\n\n\n:::\n:::\n\n\n\n\n-   *sum*() adds up all numbers of vector within the parenthesis\n\n-   X-mean(X) returns the mean centered values of X\n\n-   Two vectors can be multiplied with each other and return a new vector: X\\*Y\n\n-   *length*() return the number of values of X\n\n## More on the Correlation Coefficient\n\n-   The Pearson correlation is a **biased** estimator\n\n    -   **Biased estimator**: the expected value differs from the true value for a statistic\n\n-   The unbiased version of correlation estimation would be:\n\n$$\nr_{Y_1, Y_2}^U= r_{Y_1,Y_2}(1+\\frac{1-r^2_{Y_1,Y_2}}{2N})\n$$\n\n-   Properties of biased estimator:\n\n    -   As N gets large bias goes away;\n\n    -   Bias is largest when $r_{Y_1, Y_2} =0$; Pearson Corr. is unbiased when $r_{Y_1, Y_2} =1$\n\n    -   Pearson correlation is an underestimate of true correlation\n\n## Covariance: Association with Units\n\n-   The **numerator** of the Pearson correlation ($r_{Y_1Y_2}$) is the **Covariance**\n\n    -   **Human words**: \"Unscaled (Unstandardized)\" Correlation\n\n    ::: columns\n    ::: {.column width=\"50%\"}\n    $$\n    S^2_{Y_1}= \\frac{\\sum_{p=1}^{N}(Y_{1p}-\\bar Y_1)(Y_{2p}-\\bar Y_2)}{N-1}\n    $$\n    :::\n\n    ::: {.column width=\"50%\"}\n    $$\n    S^2_{Y_2}= \\frac{\\sum_{p=1}^{N}(Y_{1p}-\\bar Y_1)(Y_{2p}-\\bar Y_2)}{N}$$\n    :::\n    :::\n\n-   The covariance uses the units of the original variables (but now they are multiples):\n\n    -   Covariance of height and weight: 334.832 *inch-pounds*\n\n-   The covariance of a variable with itself is the variance\n\n-   The covariance if often used in multivariate analyses because it ties directly into multivariate distribution\n\n    -   Covariance and correlation are easy to switch between\n\n## Going from Covariance to Correlation\n\n-   If you have the covariance matrix (variances and covariances):\n\n    $$\n    r_{Y_1,Y_2}=\\frac{S_{Y_1,Y_2}}{S_{Y_1}S_{Y_2}}\n    $$\n\n-   If you have the correlation matrix and the standard deviations:\n\n    $$\n    S_{Y_1, Y_2} = r_{Y_1, Y_2} S_{Y_1} S_{Y_2}\n    $$\n\n## Let's Look at R code together\n\n## Summary of Unit 1\n\n1.  **Descriptive statistics**: describe central tendency and variability of variables in a visual or statistical way.\n    -   Visual way: Histogram, Scatter plot, Density plot\n    -   Statistical way: mean/mode/median; variance/standard deviation\n2.  **Alternatively**, we cab describe the relationships between two or more variables using their joint distributions\n    -   Visual way: Scatter plot, Group-level Histogram\n    -   Statistical way: Covariance, Pearson Correlation, Chi-square\n\n# Unit 2: General Linear Model\n\n## Learning Objectives\n\n-   Types of distributions:\n\n    -   Conditional distribution: a special joint distribution condition on other variable\n\n-   The General Linear Model\n\n    -   Regression\n\n    -   Analysis of Variance (ANOVA)\n\n    -   Analysis of Covariance (ANCOVA)\n\n    -   Beyond – Interactions\n\n## Taxonomy of GLM\n\n-   The general linear model (GLM) incorporates many different labels of analysis under one unifying umbrella:\n\n|                 | Categorical Xs | Continuous Xs           | Both Types of Xs |\n|-----------------|----------------|-------------------------|------------------|\n| Univariate Y    | ANOVA          | Regression              | ANCOVA           |\n| Multivariate Ys | MANOVA         | Multivariate Regression | MANCOVA          |\n\n-   The typical assumption is that error term (residual or $\\epsilon$) is normally distribution – meaning that the data are conditionally normally distributed\n\n-   Models for non-normal outcomes (e.g., dichotomous, categorical, count) fall under the *Generalized Linear Model*, of which general linear model is a special case\n\n## Property of GLM: Conditional Normality of Outcome\n\nThe general form of GLM with two predictors:\n\n$$\nY_p = {\\color{blue}\\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p} + {\\color{red}e_p}\n$$\n\n-   ::: {style=\"color: blue;\"}\n    Model for the Means (Predicted Values):\n    :::\n\n    -   Each person's expected (predicted) outcome is a function of his/her values x and z (and their interaction)\n\n    -   y, x, and z are each measured only once per person (*p* subscript)\n\n-   ::: {style=\"color: red;\"}\n    Model for the Variance:\n    :::\n\n    -   $e_p \\sim N(0, \\sigma_e^2)$ $\\rightarrow$ One residual (unexplained) deviation\n\n    -   $e_p$ has a mean of 0 and variance of $\\sigma^2_e$ and is normally distributed, unrelated to x and z, unrelated across observation\n\n    -   Model for the variance is important for **model evaluation**\n\n## Example: Building a GLM for Weight Prediction\n\n-   **Goal**: build a GLM for predicting a person's weight, using height and gender as predictors\n\n-   **Plan**: we proposed several models for didactic reasons — to show how regression and ANOVA work with GLM\n\n    -   In practice, you wouldn't necessarily run these models in this sequence\n\n-   **Beginning model** (Model 1): An **empty model –** no predictors for weight (an **unconditional** model)\n\n-   **Final model** (Model 5): A **full model** – include all predictors and their interaction model\n\n## Example: All models\n\n::: columns\n::: {.column width=\"50%\"}\n-   Model 1\n\n$$\n\\hat{ \\text{Weight}_p }= \\beta_0\n$$\n\n-   Model 2:\n\n$$\n\\hat{\\text{Weight}_p} =\\beta_0 + \\beta_1\\text{Height}_p\n$$\n\n-   Model 2a:\n\n$$\n\\hat{\\text{Weight}_p} =\\beta_0 + \\beta_1\\text{HeightC}_p\n$$\n:::\n\n::: {.column width=\"50%\"}\n-   Model 3:\n\n$$\n\\hat{\\text{Weight}_p}=\\beta_0+\\beta_2\\text{Female}_p\n$$\n\n-   Model 4:\n\n$$\\hat{\\text{Weight}_p}=\\beta_0+\\beta_1\\text{HeightC}_p+\\beta_2\\text{Female}_p$$\n\n-   Model 5:\n\n$$\\hat{\\text{Weight}_p}=\\beta_0+\\beta_1\\text{HeightC}_p+\\beta_2\\text{Female}_p \\\\ + \\beta_3\\text{HeigthCxFemale}_p$$\n:::\n:::\n\n## Model 1: Empty Model\n\n$$\n\\text{Weight}_p = \\beta_0 + e_p\n$$ $$\ne_p \\sim N(0, \\sigma^2_e)\n$$\n\n-   Estimated parameters:\n\n    -   $\\beta_0$: Overall intercept - the \"grand\" mean of weight across all people\n\n    -   $e_p$: Residual error - how each one's observes deviate from $\\beta_0$\n\n    -   $\\sigma^2_e$: Residual variance - variability of residual error across all people\n\n------------------------------------------------------------------------\n\n### R Code\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nlibrary(ESRM64503)\nlibrary(kableExtra)\nmodel1 <- lm(weightLB ~ 1, data = dataSexHeightWeight) # model formula\nsummary(model1)$coefficients |> kable() # regression cofficients table\n```\n\n::: {.cell-output-display}\n\n\n|            | Estimate| Std. Error|  t value| Pr(>&#124;t&#124;)|\n|:-----------|--------:|----------:|--------:|------------------:|\n|(Intercept) |    183.4|   12.60773| 14.54664|                  0|\n\n\n:::\n\n```{.r .cell-code}\nanova(model1) |> kable() # F-statistic table\n```\n\n::: {.cell-output-display}\n\n\n|          | Df|  Sum Sq|  Mean Sq| F value| Pr(>F)|\n|:---------|--:|-------:|--------:|-------:|------:|\n|Residuals | 19| 60402.8| 3179.095|      NA|     NA|\n\n\n:::\n:::\n\n\n\n\n-   Interpretation\n\n    -   $\\beta_0 = 183.4$ is the predicted value of a Weight for all people is 183.4 pound\n        -   Just the mean of weight\n    -   $SE(\\beta_0) = 12.60$ is the standard error of the mean for weight with higher value suggesting more inaccuracy\n    -   $t = 14.55, p < .001$ is t-test of the parameter suggesting the mean of weight significantly deviate from 0\n    -   **Error term / Residual**: $\\sigma^2_e =  3179.095$ (variance of the residuals)\n        -   Equal to the unbiased variance of weight in empty model\n        -   Also know as Mean Square Error in F-table\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nvar(dataSexHeightWeight$weightLB)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3179.095\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(residuals(model1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3179.095\n```\n\n\n:::\n:::\n\n\n\n\n## Model 2: Predicting Weight from Height\n\n$$\n\\text{Weight}_p = \\beta_0 + \\beta_1 \\text{Height}_p + e_p\n$$ $$\ne_p \\sim N(0, \\sigma^2_e)\n$$\n\n-   Estimated parameters:\n\n    -   $\\beta_0$: Intercept - is the predicted value of a Weight for a people with [Height is 0]{style=\"color: red;\"}\n\n    -   $\\beta_1$: [Slope of Height]{style=\"color: red;\"} - the predicted increase of Weight for one-unit increase in Height\n\n    -   $e_p$: Residual error - how each one's observes deviate from $\\beta_0$\n\n    -   $\\sigma^2_e$: Residual variance - variability of residual error across all people\n\n------------------------------------------------------------------------\n\n### R code\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmodel2 <- lm(weightLB ~ heightIN, data = dataSexHeightWeight) # model formula\nsummary(model2)$coefficients |> kable(digit = 3) # regression cofficients table\n```\n\n::: {.cell-output-display}\n\n\n|            | Estimate| Std. Error| t value| Pr(>&#124;t&#124;)|\n|:-----------|--------:|----------:|-------:|------------------:|\n|(Intercept) | -227.292|     73.483|  -3.093|              0.006|\n|heightIN    |    6.048|      1.076|   5.621|              0.000|\n\n\n:::\n\n```{.r .cell-code}\nanova(model2) |> kable(digit = 3) # F-statistic table\n```\n\n::: {.cell-output-display}\n\n\n|          | Df|   Sum Sq|   Mean Sq| F value| Pr(>F)|\n|:---------|--:|--------:|---------:|-------:|------:|\n|heightIN  |  1| 38479.27| 38479.273|  31.593|      0|\n|Residuals | 18| 21923.53|  1217.974|      NA|     NA|\n\n\n:::\n:::\n\n\n\n\n-   Interpretation\n\n    -   $\\beta_0 = -227.3$ is the predicted value of a Weight for a people with [Height is 0]{style=\"color: red;\"}\n        -   Nonsensical – but we can fix it by [centering Height]{style=\"color: #ff0040;\"}\n    -   $\\beta_1 = 6.048$: Weight goes up 6.048 per inch\n    -   $SE(\\beta_1) = 1.076$: the inaccuracy of $\\beta_1$; $CI = 6.048 \\pm 1.96\\times 1.076$\n    -   $t = 5.621, p < .001$ is t-test of the parameter suggesting the slope of height significantly deviate from 0\n    -   $F(1, 18) = 31.593 = \\frac{38479.273}{1217.974} = t^2$\n    -   **Error term / Residual**: $\\sigma^2_e =  1217.974$ (variance of the residuals)\n        -   Height explains $\\frac{3179.095-1217.974}{3179.095}=61.7\\%$ of variance of weight\n\n## Model 2a: Predicting Weight from [Centered]{style=\"color: red;\"} Height\n\n$$\n\\text{Weight}_p = \\beta_0 + \\beta_1 (\\text{Height}_p - \\bar{Height}) + e_p \\\\\n= \\beta_0 + \\beta_1 \\text{HeightC}_p + e_p\n$$ $$\ne_p \\sim N(0, \\sigma^2_e)\n$$\n\n-   Estimated parameters:\n\n    -   $\\beta_0$: Intercept - is the predicted value of a Weight for a people with [Height is Mean Height]{style=\"color: red;\"}\n\n    -   $\\beta_1$: Slope of Height - the predicted increase of Weight for one-unit increase in Height\n\n    -   $e_p$: Residual error - how each one's observes deviate from $\\beta_0$\n\n    -   $\\sigma^2_e$: Residual variance - variability of residual error across all people\n\n------------------------------------------------------------------------\n\n### R code\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\ndataSexHeightWeight$heightC = dataSexHeightWeight$heightIN - mean(dataSexHeightWeight$heightIN)\nmodel2a <- lm(weightLB ~ heightC, data = dataSexHeightWeight) # model formula\nsummary(model2a)$coefficients |> kable(digit = 3) # regression cofficients table\n```\n\n::: {.cell-output-display}\n\n\n|            | Estimate| Std. Error| t value| Pr(>&#124;t&#124;)|\n|:-----------|--------:|----------:|-------:|------------------:|\n|(Intercept) |  183.400|      7.804|  23.501|                  0|\n|heightC     |    6.048|      1.076|   5.621|                  0|\n\n\n:::\n\n```{.r .cell-code}\nanova(model2a) |> kable(digit = 3) # F-statistic table\n```\n\n::: {.cell-output-display}\n\n\n|          | Df|   Sum Sq|   Mean Sq| F value| Pr(>F)|\n|:---------|--:|--------:|---------:|-------:|------:|\n|heightC   |  1| 38479.27| 38479.273|  31.593|      0|\n|Residuals | 18| 21923.53|  1217.974|      NA|     NA|\n\n\n:::\n:::\n\n\n\n\n-   Interpretation\n\n    -   $\\beta_0 = 183.4$ pounds (previously -227.3) is the predicted value of a Weight for a people with [Height is 67.9]{style=\"color: red;\"} inches\n    -   Everything except $\\beta_0$, $SE(\\beta_0)$, and t-value of intercept should be same as the previous model\n\n------------------------------------------------------------------------\n\n### Visualization\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code}\nggplot(dataSexHeightWeight) +\n  geom_point(aes(x = heightC, y = weightLB)) +\n  geom_smooth(aes(x = heightC, y = weightLB), method = \"lm\") +\n  geom_hline(aes(yintercept = 183.4), color = \"red\") +\n  labs(title = \"Model 2a: Fit Plot for weight\")\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Wrap up: Hypothesis Tests for Parameters\n\n-   To determine if the regression slope is significantly different from zero, we must use a hypothesis test:\n\n$$\nH_0: \\beta_1 = 0 \n$$\n\n$$\nH_1: \\beta_1 \\neq 0\n$$\n\n-   We have two options for this test (both are same here)\n\n    -   Use ANOVA table: sums of squares – F-test\n\n    -   Use \"Wald\" test for parameter: $t = \\frac{\\beta_1}{SE(\\beta_1)}$\n\n    -   Here $t^2 = F$\n\n-   p \\< 0.001 $\\rightarrow$ reject null ($H_0$) $\\rightarrow$ Conclusion: slope is significant\n\n## Model 3: Predicting Weight from Gender\n\n$$\n\\text{Weight}_p = \\beta_0 + \\beta_1 \\text{Female}_p + e_p\n$$ $$\ne_p \\sim N(0, \\sigma^2_e)\n$$\n\n-   Note: because gender is a categorical predictor, we must first code it into a number before entering it into model (typically done automatically in software)\n\n    -   Here we code the variable as [Female = 1 for females; Female = 0 for males]{style=\"color: blue;\"}\n\n<!-- -->\n\n-   Estimated parameters:\n\n    -   $\\beta_0$: Intercept - predicted value of Weight for a person with [Female = 0 (males)]{style=\"color: red;\"}\n\n    -   $\\beta_2$: Slope of Female - Change in predicted value of Weight between Males and Famales\n\n------------------------------------------------------------------------\n\n### R Code\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmodel3 <- lm(weightLB ~ female, data = dataSexHeightWeight) # model formula\nsummary(model3)$coefficients |> kable(digit = 3) # regression cofficients table\n```\n\n::: {.cell-output-display}\n\n\n|            | Estimate| Std. Error| t value| Pr(>&#124;t&#124;)|\n|:-----------|--------:|----------:|-------:|------------------:|\n|(Intercept) |    235.9|      5.415|  43.565|                  0|\n|femaleTRUE  |   -105.0|      7.658| -13.711|                  0|\n\n\n:::\n\n```{.r .cell-code}\nanova(model3) |> kable(digit = 3) # F-statistic table\n```\n\n::: {.cell-output-display}\n\n\n|          | Df|  Sum Sq|   Mean Sq| F value| Pr(>F)|\n|:---------|--:|-------:|---------:|-------:|------:|\n|female    |  1| 55125.0| 55125.000| 188.004|      0|\n|Residuals | 18|  5277.8|   293.211|      NA|     NA|\n\n\n:::\n:::\n\n\n\n\n-   Interpretation\n\n    -   $\\beta_0 = 235.9$ pounds is the predicted value of a Weight for a male\n    -   $\\beta_0 +\\beta_1 = 235.9 - 105.0 = 130.9$ pounds is the predicted value of a Weight for a female\n    -   $\\beta_1 = 105$ expected difference between gender, which is significant based on t-test and F-test\n\n------------------------------------------------------------------------\n\n### Visualization:\n\nLine Plot (not frequently used in this case)\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code}\nggplot(dataSexHeightWeight) +\n  geom_point(aes(x = as.numeric(female), y = weightLB)) +\n  geom_abline(intercept = coef(model3)[1], slope = coef(model3)[2]) +\n  scale_x_continuous(breaks = 0:1) +\n  labs(title = \"Model 3: Fit Plot for Female\",\n       x = \"Female\")\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Visualization: **box-and-whisker plot (Basic Way)**\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code}\nboxplot(weightLB ~ female, data = dataSexHeightWeight, \n        names = c(\"Male\", \"Female\"), ylab = \"Weight\", xlab = \"\",boxwex = .3)    # Basic boxplot in R\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Visualization: **box-and-whisker plot (Fancy Way)**\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"ggplot2 package\"}\nggplot(dataSexHeightWeight, aes(x = female, y = weightLB)) +\n  geom_dotplot(aes(fill = female, color = female), binaxis='y', stackdir='center') +\n  stat_summary(fun.data= \"mean_cl_normal\", fun.args = list(conf.int=.75), geom=\"crossbar\", width=0.3, fill = \"yellow\", alpha = .5) +\n  stat_summary(fun.data= \"median_hilow\", geom=\"errorbar\", fun.args = list(conf.int=1), width = .1, color=\"black\") + # Mean +- 2SD \n  stat_summary(fun.data= \"mean_sdl\",  geom=\"point\", shape = 5, size = 3) +\n  scale_x_discrete(labels = c(\"Male\", \"Female\")) +\n  labs(x = \"\", y = \"Weight\") +\n  theme_bw() +\n  theme(legend.position = \"none\", text = element_text(size = 20)) \n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Model 4: Predicting Weight from Height and Gender\n\n$$\n\\text{Weight}_p = \\beta_0 + \\beta_1 \\text{HeightC}_p + \\beta_2 \\text{Female}_p  + e_p\n$$ $$\ne_p \\sim N(0, \\sigma^2_e)\n$$\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmodel4 <- lm(weightLB ~ heightC + female, data = dataSexHeightWeight) # model formula\nsummary(model4)$coefficients |> kable(digit = 3) # regression cofficients table\n```\n\n::: {.cell-output-display}\n\n\n|            | Estimate| Std. Error| t value| Pr(>&#124;t&#124;)|\n|:-----------|--------:|----------:|-------:|------------------:|\n|(Intercept) |  224.256|      1.439| 155.876|                  0|\n|heightC     |    2.708|      0.155|  17.525|                  0|\n|femaleTRUE  |  -81.712|      2.241| -36.461|                  0|\n\n\n:::\n\n```{.r .cell-code}\nanova(model4) |> kable(digit = 3) # F-statistic table\n```\n\n::: {.cell-output-display}\n\n\n|          | Df|    Sum Sq|   Mean Sq|  F value| Pr(>F)|\n|:---------|--:|---------:|---------:|--------:|------:|\n|heightC   |  1| 38479.273| 38479.273| 2363.103|      0|\n|female    |  1| 21646.710| 21646.710| 1329.376|      0|\n|Residuals | 17|   276.817|    16.283|       NA|     NA|\n\n\n:::\n:::\n\n\n\n\n-   Interpretation\n\n    -   $\\beta_0 = 224.256 (SE = 1.439)$\n\n        -   The predicted weight is 224.256 pounds for a person with Female = 0 (males) and has Height as Mean Height 67.9 inches\n\n    -   $\\beta_1 = 2.708 (SE = 0.155)$\n\n        -   $t = \\frac{2.708}{0.155}=17.525; p <.001$\n\n        -   The expected change in weights for every one-unit increase in height holding gender constant\n\n    -   $\\beta_2=-81.713 (SE = 2.241)$\n\n        -   The expected difference between the mean of weights for males and the mean for females holding height constant\n\n    -   $\\sigma_e^2 = 16.283$\n\n        -   The residual variance of weight\n\n## Model 4: By-Gender Regression Lines\n\nModel 4 assumes identical regression slopes for both genders but has different intercepts\n\n-   This assumption of different slopes will be tested statistically by model 5\n\nPredicted Weight for Females with the height as 68.9 inch:\n\n$$\nW_p=224.256+2.708\\times\\color{blue}{(68.9-67.9)} - 81.713*\\color{red}{1} \\\\\n= 224.256+2.708-81.712 = 145.252\n$$\n\nPredicted Weight for Males with the height as 68.9 inch:\n\n$$\nW_p=224.256+2.708\\times\\color{blue}{(68.9-67.9)} - 81.713*\\color{red}{0} \\\\\n= 224.256+2.708 = 226.964\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(model4, data.frame(heightC = 1, female = TRUE))\npredict(model4, data.frame(heightC = 1, female = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      1 \n145.252 \n       1 \n226.9639 \n```\n\n\n:::\n:::\n\n\n\n\n### Visualization: Different intercept and Same slope across groups\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"ggplot2 package\"}\ndataSexHeightWeight |> \n  mutate(predictedWeight = predict(model4, dataSexHeightWeight)) |> \n  ggplot(aes(x = heightC, y = predictedWeight, color = female)) +\n  geom_line() +\n  geom_point(aes(y = weightLB, shape = female), size = 2) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Model 5: By-Gender Regression Lines\n\n$$\\text{Weight}_p = \\beta_0 + \\beta_1 \\text{HeightC}_p + \\beta_2 \\text{Female}_p +\\beta_3 \\text{HeightCxFemale}_p+ e_p$$\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmodel5 <- lm(weightLB ~ heightC * female, data = dataSexHeightWeight) # model formula\nsummary(model5)$coefficients |> kable(digit = 3) # regression cofficients table\n```\n\n::: {.cell-output-display}\n\n\n|                   | Estimate| Std. Error| t value| Pr(>&#124;t&#124;)|\n|:------------------|--------:|----------:|-------:|------------------:|\n|(Intercept)        |  222.184|      0.838| 265.107|                  0|\n|heightC            |    3.190|      0.111|  28.646|                  0|\n|femaleTRUE         |  -82.272|      1.211| -67.932|                  0|\n|heightC:femaleTRUE |   -1.094|      0.168|  -6.520|                  0|\n\n\n:::\n\n```{.r .cell-code}\nanova(model5) |> kable(digit = 3) # F-statistic table\n```\n\n::: {.cell-output-display}\n\n\n|               | Df|    Sum Sq|   Mean Sq|  F value| Pr(>F)|\n|:--------------|--:|---------:|---------:|--------:|------:|\n|heightC        |  1| 38479.273| 38479.273| 8132.723|      0|\n|female         |  1| 21646.710| 21646.710| 4575.104|      0|\n|heightC:female |  1|   201.115|   201.115|   42.506|      0|\n|Residuals      | 16|    75.703|     4.731|       NA|     NA|\n\n\n:::\n:::\n\n\n\n\n-   Interpretation\n\n    -   $\\beta_0 = 222.184 (SE = 0.838)$\n\n        -   SE gets smaller compared to Model 4.\n\n        -   The predicted weight is 224.184 pounds for a person with Female = 0 (males) and has Height as Mean Height 67.9 inches\n\n    -   $\\beta_1 = 3.189 (SE = 0.111)$\n\n        -   SE gets smaller compared to Model 4.\n\n        -   **Simple main effect of Height:** the expected change in weights for every one-unit increase in height [for males only]{style=\"color: red\"}\n\n    -   $\\beta_2=-82.271 (SE = 1.211)$\n\n        -   SE gets smaller compared to Model 4.\n\n        -   **Simple main effect of gender:** The expected difference between the mean of weights for males and the mean for females for people with mean height\n\n    -   $\\beta_3 = -1.093 (SE = 1.211)$ , $t = -6.52, p < .001$\n\n        -   **Gender-by-Height Interaction**: Additional change in slope of height for change in gender.\n\n        -   The slope of heights for males is 3.189; the slope of heights decreases 1.093 (2.096) inches/pound for females\n\n    -   $\\sigma_e^2 = 5$\n\n        -   Compared to 16.283 in model 4\n\n------------------------------------------------------------------------\n\n### Visualization: Different slopes and different intercepts\n\nModel 5 does not assume identical regression slopes\n\n-   Because $\\beta_3$ was significantly different from zero, the data supports different slopes for the gender\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(dataSexHeightWeight, aes(x = heightC, y = weightLB, color = female)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n## Model Comparison\n\n-   In practice, the empty model (Model 0) and the full model (Model 5) would be the only models to run\n\n    -   The aim of model comparison is to decide on whether we want more complex model or the simple one\n\n    -   The **trick** is to describe the impact of all and each of the predictors – typically using variance accounted for\n\n-   Residual Variance and $R^2$\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_res_var <- function(model) {\n  anova(model)$`Mean Sq` |>  tail(1)\n}\n\ndata.frame(\n  Model = paste(\"Model\", c(1, 2, \"2a\", 3, 4, 5)),\n  ResidualVariance = sapply(list(model1, model2, model2a, model3, model4, model5), get_res_var)\n) |> \n  ggplot(aes(x = Model, y = ResidualVariance))  +\n  geom_point() +\n  geom_path(group = 1) +\n  geom_text(aes(label = round(ResidualVariance, 2)),nudge_y = 200) +\n  labs(title = \"Residual Variance for six models\")\n```\n\n::: {.cell-output-display}\n![](Lecture02_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n\n## Comparison Across Models\n\n1.  **Are height and gender are good predictors?**\n    -   Total explained variances in weight by **height and gender:** Multiple $R^2$ of Model 5\n        -   $(3179.09-4.73)/3179.09 = 0.9985$ $\\rightarrow$ 99.85% variances in weights can be explained by height and gender\n    -   F-test comparing Model 5 to Model 1\n        -   $F_{3, 16} = 4250.1$, p \\< .001\n\n\n\n\n            ::: {.cell output-location='default'}\n            \n            ```{.r .cell-code  code-fold=\"true\"}\n            summary(model5)\n            ```\n            \n            ::: {.cell-output .cell-output-stdout}\n            \n            ```\n            \n            Call:\n            lm(formula = weightLB ~ heightC * female, data = dataSexHeightWeight)\n            \n            Residuals:\n                Min      1Q  Median      3Q     Max \n            -3.8312 -1.7797  0.4958  1.3575  3.3585 \n            \n            Coefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n            (Intercept)        222.1842     0.8381  265.11  < 2e-16 ***\n            heightC              3.1897     0.1114   28.65 3.55e-15 ***\n            femaleTRUE         -82.2719     1.2111  -67.93  < 2e-16 ***\n            heightC:femaleTRUE  -1.0939     0.1678   -6.52 7.07e-06 ***\n            ---\n            Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n            \n            Residual standard error: 2.175 on 16 degrees of freedom\n            Multiple R-squared:  0.9987,\tAdjusted R-squared:  0.9985 \n            F-statistic:  4250 on 3 and 16 DF,  p-value: < 2.2e-16\n            ```\n            \n            \n            :::\n            :::\n\n\n\n\n------------------------------------------------------------------------\n\n2.  **Are Height alone a good predictor?**\n    -   Explained **remaining variances** in weight by **height:** Multiple $R^2$ of Model 5 to Model 3\n\n        -   $(293.21-4.73)/293.21 = 0.9839$ $\\rightarrow$ 98.39% variances in weights [remaining after gender]{style=\"color: orange\"} can be explained by **the main and interaction effects of height**\n\n    -   F-test comparing Model 5 to Model 3\n\n        -   $F_{2, 16} = 548.74$, p \\< .001 suggests the effect of hight on weight is significant after controlling the effect of gender\n\n\n\n\n            ::: {.cell output-location='default'}\n            \n            ```{.r .cell-code  code-fold=\"true\"}\n            anova(model3, model5)\n            ((5277.8 - 75.7)/ (18-16))/(75.7 / 16)\n            ```\n            :::\n\n\n\n\n    -   True explained variances out of total variances in weight: **Unique contribution** of adding Height into the model\n\n        -   Check model 3, we can found that gender explained 90.78% variance of weight\n\n        -   $0.9839 * (1 - 0.9078) = 0.0907$ $\\rightarrow$ 9.07% more variances of weights can be explained by height after gender is already in the model\n\n        -   90.78% + 9.07% = 99.85% is the total variance explained by height and gender\n\n------------------------------------------------------------------------\n\n3.  **Are gender alone a good predictor?**\n\n-   Explained **remaining variances** in weight by **gender:** Multiple $R^2$ of Model 5 to Model 2a\n\n    -   $(1217.97-4.73)/1217.97 = 0.9961$ $\\rightarrow$ 99.61% variances in weights [remaining after height]{style=\"color: orange\"} can be explained by **the main and interaction effects of gender**\n\n-   F-test comparing Model 5 to Model 2a\n\n    -   $F_{2, 16} = 2308.8$, p \\< .001 suggests the effect of gender on weight is significant after controlling the effect of height\n\n\n\n\n        ::: {.cell output-location='default'}\n        \n        ```{.r .cell-code  code-fold=\"true\"}\n        anova(model2a, model5)\n        ((21923.5 - 75.7)/ (18-16))/(75.7 / 16)\n        ```\n        :::\n\n\n\n\n-   True explained variances out of total variances in weight: **Unique contribution** of adding Height into the model\n\n    -   Check model 3, we can found that gender explained 90.78% variance of weight\n\n    -   $0.9839 * (1 - 0.9078) = 0.0907$ $\\rightarrow$ 9.07% more variances of weights can be explained by height after gender is already in the model\n\n    -   90.78% + 9.07% = 99.85% is the total variance explained by height and gender\n\n## Summary of Unit 2\n\n1.  We learned GLM using the 2-factor model\n2.  For specific effect, we examine the t-test of coefficient\n3.  For the total contribution of a bunch of effects (main + interaction), we look at R-square and F-test\n4.  We sometime need to report predicted regression lines, which can be done in R with ggplot2\n\n# Unit 3: Let's Look at R code together\n",
    "supporting": [
      "Lecture02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}