{
  "hash": "893ec1ce5c6845707ef0157dd715873d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Brief Introducation to JAGS and R2jags in R'\nauthor: Jihong Zhang\ndate: '2018-09-12'\ndate-modified: 'Mar 11 2024'\ncategories:\n  - Manual\n  - R\n  - Jags\n  - MCMC\nformat: \n  html: \n    code-fold: false\n    code-line-numbers: false\n---\n\n\n\n> This post is aimed to introduce the basics of using jags in R programming. Jags is a frequently used program for conducting Bayesian statistics.Most of information below is borrowed from [Jeromy Anglim’s Blog](http://jeromyanglim.blogspot.com/2012/04/getting-started-with-jags-rjags-and.html). I will keep editing this post if I found more resources about jags.\n\n## What is JAGS?\n\nJAGS stands for Just Another Gibbs Sampler. To quote the program author, Martyn Plummer, \"It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation...\" It uses a dialect of the BUGS language, similar but a little different to OpenBUGS and WinBUGS.\n\n## Installation\n\nTo run jags with R, There is an interface with R called `rjags`.\n\n1\\. [Download and install Jags](http://mcmc-jags.sourceforge.net/) based on your operating system.\n\n2\\. Install additional R packages: type in `install.packages(c(“rjags”,”coda”))` in R console. `rjags` is to interface with JAGS and `coda` is to process MCMC output.\n\n## JAGS Examples\n\nThere are a lot of examples online. The following provides links or simple codes to JAGS code.\n\n-   Justin Esarey\n    -   An entire course on [Bayesian Statistics](http://jee3.web.rice.edu/teaching.htm) with examples in R and JAGS. It includes 10 lectures and each lecture lasts around 2 hours. The content is designed for a social science audience and it includes a syllabus linking with Simon Jackman's text. The videos are linked from above or available direclty on [YouTube](http://www.youtube.com/playlist?list=PLAFC5F02F224FA59F).\n-   Jeromy Anglim\n    -   The author of this blog also provides a few examples. He shared the codes on his [github account](https://github.com/jeromyanglim)\n-   John Myles White\n    -   A course on statistical models that is under development with [JAGS scripts on github](https://github.com/johnmyleswhite/JAGSExamples).\n    -   [Simple introductory examples of fitting a normal distribution, linear regression, and logistic regression](http://www.johnmyleswhite.com/notebook/2010/08/20/using-jags-in-r-with-the-rjags-package/)\n    -   A follow-up post demonstrating the use of the coda package with rjags to perform [MCMC diagnostics](http://www.johnmyleswhite.com/notebook/2010/08/29/mcmc-diagnostics-in-r-with-the-coda-package/).\n-   A simple simulation sample:\n\n\n\n\n\n\n\nFirst, simulate the Data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(R2jags)\nn.sim <- 100; set.seed(123)\nx1 <- rnorm(n.sim, mean = 5, sd = 2)\nx2 <- rbinom(n.sim, size = 1, prob = 0.3)\ne <- rnorm(n.sim, mean = 0, sd = 1)\n```\n:::\n\n\n\nNext, we create the outcome y based on coefficients $b_1$ and $b_2$ for the respective predictors and an intercept a:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb1 <- 1.2\nb2 <- -3.1\na <- 1.5\ny <- b1*x1 + b2*x2 + e\n```\n:::\n\n\n\nNow, we combine the variables into one dataframe for processing later:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim.dat <- data.frame(y, x1, x2)\n```\n:::\n\n\n\nAnd we create and summarize a (frequentist) linear model fit on these data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq.mod <- lm(y ~ x1 + x2, data = sim.dat)\nsummary(freq.mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = sim.dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3432 -0.6797 -0.1112  0.5367  3.2304 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.34949    0.28810   1.213    0.228    \nx1           1.13511    0.05158  22.005   <2e-16 ***\nx2          -3.09361    0.20650 -14.981   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9367 on 97 degrees of freedom\nMultiple R-squared:  0.8772,\tAdjusted R-squared:  0.8747 \nF-statistic: 346.5 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n### Beyesian Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayes.mod <- function() {\n for(i in 1:N){\n y[i] ~ dnorm(mu[i], tau)\n mu[i] <- alpha + beta1 * x1[i] + beta2 * x2[i]\n }\n alpha ~ dnorm(0, .01)\n beta1 ~ dunif(-100, 100)\n beta2 ~ dunif(-100, 100)\n tau ~ dgamma(.01, .01)\n}\n```\n:::\n\n\n\nNow define the vectors of the data matrix for JAGS:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- sim.dat$y\nx1 <- sim.dat$x1\nx2 <- sim.dat$x2\nN <- nrow(sim.dat)\n```\n:::\n\n\n\nRead in the data frame for JAGS\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim.dat.jags <- list(\"y\", \"x1\", \"x2\", \"N\")\n```\n:::\n\n\n\nDefine the parameters whose posterior distributions you are interested in summarizing later:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayes.mod.params <- c(\"alpha\", \"beta1\", \"beta2\")\n```\n:::\n\n\n\nSetting up starting values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayes.mod.inits <- function(){\n list(\"alpha\" = rnorm(1), \"beta1\" = rnorm(1), \"beta2\" = rnorm(1))\n}\n\n# inits1 <- list(\"alpha\" = 0, \"beta1\" = 0, \"beta2\" = 0)\n# inits2 <- list(\"alpha\" = 1, \"beta1\" = 1, \"beta2\" = 1)\n# inits3 <- list(\"alpha\" = -1, \"beta1\" = -1, \"beta2\" = -1)\n# bayes.mod.inits <- list(inits1, inits2, inits3)\n```\n:::\n\n\n\n### Fitting the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nbayes.mod.fit <- jags(data = sim.dat.jags, inits = bayes.mod.inits,\n  parameters.to.save = bayes.mod.params, n.chains = 3, n.iter = 9000,\n  n.burnin = 1000, model.file = bayes.mod)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nmodule glm loaded\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 4\n   Total graph size: 511\n\nInitializing model\n```\n\n\n:::\n:::\n\n\n\n### Diagnostics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(bayes.mod.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Bugs model at \"/var/folders/9t/ryz7lf_s7ts720p_lwdttfhm0000gn/T//Rtmp2X0viI/model11dd2c0cf069.txt\", fit using jags,\n 3 chains, each with 9000 iterations (first 1000 discarded), n.thin = 8\n n.sims = 3000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nalpha      0.362   0.293  -0.205   0.166   0.358   0.562   0.958 1.009   250\nbeta1      1.133   0.053   1.025   1.099   1.134   1.169   1.236 1.009   250\nbeta2     -3.090   0.205  -3.496  -3.231  -3.090  -2.950  -2.685 1.002  1700\ndeviance 271.830   2.899 268.167 269.718 271.122 273.198 279.223 1.000  3000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 4.2 and DIC = 276.0\nDIC is an estimate of expected predictive error (lower deviance is better).\n```\n\n\n:::\n:::\n\n\n```{.r .cell-code}\nplot(bayes.mod.fit)\n```\n\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n\n\n```{.r .cell-code}\ntraceplot(bayes.mod.fit)\n```\n\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}![](index_files/figure-html/unnamed-chunk-13-3.png){width=672}![](index_files/figure-html/unnamed-chunk-13-4.png){width=672}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}