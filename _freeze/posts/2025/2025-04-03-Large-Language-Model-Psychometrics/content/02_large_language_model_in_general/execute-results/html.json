{
  "hash": "3b3452216d6d1c8522616b0d21fcf48e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Large Language Models\"\nsubtitle: \"Examples with R and Python\"\ndate: \"2025-04-29\"\ndate-modified: \"2025-08-26\"\ndraft: false\nbibliography: ../references.bib\nimage: ../images/thumbnail_chatgpt.png\ntbl-cap-location: top\ncitation:\n  type: webpage\n  issued: 2025-03-07\nexecute: \n  cache: true  \nformat: \n  html:\n    code-tools: false\n    code-line-numbers: false\n    code-fold: false\n    code-summary: \"Click this to see code\"\n---\n\n\n\n::: macwindow\n\n**Topic: Strategies for Enhancing and Validating AI-Generated Response Diversity in Psychological Surveys**\n\n1. Methods to Increase Response Diversity\n\n-   **Fine-tune AI models** with datasets that include diverse cultural, demographic, and psychological profiles to better reflect broad human variability.\n-   **Adjust model parameters** (e.g., increase temperature or use top-k sampling) to introduce randomness and mimic varied human response patterns.\n-   **Combine responses from multiple AI chatbots** trained on distinct datasets to capture a wider range of subgroup perspectives.\n\n2. Checking AI Response Quality\n\n-   **Analyze AI-generated responses** to ensure they do not skew toward specific groups (e.g., highly educated English speakers) by examining output distributions.\n-   **Cluster AI responses and compare to real human data** using metrics such as response variability or linguistic diversity to confirm authenticity.\n\n3. Validating Measurement Tools\n\n-   **Mix synthetic AI responses with real human responses** to create a larger dataset, then test the measurement toolâ€™s reliability and validity across both groups.\n\n:::\n\n# Model architecture\n\n## Transformer\n\nThe original Transformer architecture was introduced in the paper [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) by Vaswani et al. in 2017.\nThe Transformer model has since become the foundation for many state-of-the-art natural language processing (NLP) models, including BERT, GPT-3, and T5.\n\n### Terminology\n\n-   Self-attention: an attention mechanism relating different positions of a single sequence to compute a representation of the sequence.\n\n    -   Q,K,V matrix: query, keys, values.\n        The output is computed as a weighted sum of the values.\n        All these three are key component of Attention function.\n        The following formula is also known as Scaled dot-product attention.\n\n        $$\n        \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n        $$\n\n    -   where queries and keys have the dimension $d_k$\n\n> the dot products get larger variances when the dimension of q and k increase.\n> So they scale the dot product by $\\frac{1}{\\sqrt{d_k}}$ to make sure the dot product has close variance with q and k.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nN_rep = 1000\ndot_product <- function(i, d_k, var_q = 1) {\n  set.seed(i)\n  q = rnorm(d_k, 0, sd = sqrt(var_q))\n  k = rnorm(d_k, 0, sd = sqrt(var_q))\n  crossprod(q, k)\n}\n\nvar_dot_product <- function(N_rep, d_k, var_q = 1) {\n  dot_product_values <- sapply(1:N_rep, \\(x) dot_product(x, d_k = d_k, var_q))\n  var(dot_product_values)\n}\n\nto_plot <- data.frame(\n  d_k_values = c(1, 10, 100, 1000),\n  var_dot_product_values = sapply(c(1, 10, 100, 1000), \\(x) var_dot_product(N_rep = N_rep, \n                                                                   d_k = x))\n)\n\nggplot(to_plot, aes(x = d_k_values, y = var_dot_product_values)) +\n  geom_path() +\n  geom_point() +\n  labs(x = \"d_k\", y = \"Variance of dot product of q and k\")\n```\n\n::: {.cell-output-display}\n![](02_large_language_model_in_general_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_q_values = c(1, 10, 25, 100, 400) # variances of q and k\nd_k = 2 # dimension of k\n\n# function to generate the variaance of scaled dot products\nvar_scaled_dot_product <- function(N_rep, d_k, var_q = 1) {\n  scaled_dot_product_values <- sapply(1:N_rep,\\(x){dot_product(x, d_k = d_k, var_q = var_q)/sqrt(d_k)} )\n  var(scaled_dot_product_values)\n}\n\nvar_scaled_dot_product_values <- sapply(var_q_values, \n                                        \\(x) var_scaled_dot_product(N_rep = N_rep, \n                                                                    d_k = d_k, \n                                                                    var_q = x) )\n\ndata.frame(\n  var_q_values = var_q_values,\n  var_scaled_dot_product_values = var_scaled_dot_product_values\n) |> \nggplot(aes(x = var_q_values, y = var_scaled_dot_product_values)) +\n  geom_path() +\n  geom_point() +\n  labs(x = \"variances of q and k\", y = \"Variance of scaled dot product of q and k\")\n```\n\n::: {.cell-output-display}\n![](02_large_language_model_in_general_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n-   Multi-Head Attention:\n\n-   Encoder-decoder structure: The basic structure of most neural sequence transduction models.\n\n    -   Encoder maps an input to a sequence of continuous representations.\n\n    -   Decoder generates an output sequence given the continuous representation of encoder\n\n![The Transformer - model architecture in the original paper](images/clipboard-1162313639.png){fig-align=\"center\" width=\"500\"}\n\n# Evaluation metrics\n\n## Item Similarity\n\nVarious metrics can be used to evaluate the similarity between two sentences (e.g., math items) at the lexical level.\nThey are typically called vector similarity measures.\n\n### BLEU\n\nBLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of text that has been machine-translated from one natural language to another (see [geeksforgeeks](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)).\nIt compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of n items) between the candidate and reference translations.\nHigher values indicate better quality translations.\n\nThe formula of BLEU score is:\n\n$$\n\\text{BLEU}(C, R) = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\cdot \\log p_n\\right)\n$$\n\nwhere:\n\n-   $C$ is the candidate translation,\n-   $R$ is the reference translation,\n-   $BP$ is the brevity penalty, which penalizes translations that are shorter than the reference translation,\n-   $p_n$ is the precision of n-grams in the candidate translation,\n-   $w_n$ is the weight assigned to the n-gram precision, typically set to $\\frac{1}{N}$ for $n=1,2,\\ldots,N$.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\ndef calculate_bleu(candidate, reference):\n    # Tokenize the sentences\n    candidate_tokens = candidate.split()\n    reference_tokens = reference.split()\n    \n    # Calculate BLEU score\n    smoothing_function = SmoothingFunction().method1\n    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing_function)\n    \n    return bleu_score\n\ncandidate = \"The cat sat on the mat.\"\nreference = \"The cat is sitting on the mat.\"\n\nbleu_score = calculate_bleu(candidate, reference)\nprint(f\"BLEU score: {bleu_score:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBLEU score: 0.2151\n```\n\n\n:::\n:::\n\n\n\n### Cosine Similarity\n\nThe cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\nIn NLP, it is often used to compare the similarity of two text documents or sentences by representing them as vectors in a high-dimensional space.\n\nThe formula of the cosine similarity is:\n\n$$\n\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\n$$\n\nwhere $A$ and $B$ are the two vectors, $A \\cdot B$ is the dot product of the vectors, and $||A||$ and $||B||$ are the magnitudes (or norms) of the vectors.\n\nTo convert words into vectors, we can use a simple method called **word2vec**.\nTo be more specific, we can **represent a word as a vector of character frequencies**, where each character in the word is counted and represented as a dimension in the vector space.\n\n::: panel-tabset\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef word2vec(word):\n    from collections import Counter\n    from math import sqrt\n\n    # count the characters in word\n    cw = Counter(word)\n    # precomputes a set of the different characters\n    sw = set(cw)\n    # precomputes the \"length\" of the word vector\n    lw = sqrt(sum(c*c for c in cw.values()))\n\n    # return a tuple\n    return cw, sw, lw\n\ndef cosdis(v1, v2):\n    # which characters are common to the two words?\n    common = v1[1].intersection(v2[1])\n    # by definition of cosine distance we have\n    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]\n  \na = 'Problem: Ten people are sitting around a round  table. Three of them are chosen at random to give  a presentation. What is the probability that the  three chosen people were sitting in consecutive  seats?'\nb = 'Problem: Ten people are sitting around a round table. Three of  them are chosen at random to give a presentation in a specific  order. What is the probability that the first and second presenter  were sitting in consecutive seats and at the same time the second  and third presenter were sitting in consecutive seats?'\nc = 'Problem: Ten people are sitting around a round table. Four of them are chosen at random to give a presentation in a specific order. What is the probability that the first and second presenter were sitting in consecutive seats and at the same time the second and third presenter were sitting in consecutive seats?'\n\nva = word2vec(a)\nvb = word2vec(b)\nvc = word2vec(c)\n\nprint(cosdis(va,vb))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9901018675724259\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(cosdis(vb,vc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9990936880256146\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(cosdis(vc,va))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.988220284030085\n```\n\n\n:::\n:::\n\n\n\n## R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = 'Problem: Ten people are sitting around a round  table. Three of them are chosen at random to give  a presentation. What is the probability that the  three chosen people were sitting in consecutive  seats?'\nb = 'Problem: Ten people are sitting around a round table. Three of  them are chosen at random to give a presentation in a specific  order. What is the probability that the first and second presenter  were sitting in consecutive seats and at the same time the second  and third presenter were sitting in consecutive seats?'\nc = 'Problem: Ten people are sitting around a round table. Four of them are chosen at random to give a presentation in a specific order. What is the probability that the first and second presenter were sitting in consecutive seats and at the same time the second and third presenter were sitting in consecutive seats?'\n\ncosine_similarity <- function(a, b) {\n  a <- strsplit(a, \"\")[[1]]\n  b <- strsplit(b, \"\")[[1]]\n  \n  a_freq <- table(a)\n  b_freq <- table(b)\n  \n  common_chars <- intersect(names(a_freq), names(b_freq))\n  \n  dot_product <- sum(a_freq[common_chars] * b_freq[common_chars])\n  norm_a <- sqrt(sum(a_freq^2))\n  norm_b <- sqrt(sum(b_freq^2))\n  \n  return(dot_product / (norm_a * norm_b))\n}\n\ncosine_similarity(a, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9901019\n```\n\n\n:::\n\n```{.r .cell-code}\ncosine_similarity(b, c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9990937\n```\n\n\n:::\n\n```{.r .cell-code}\ncosine_similarity(a, c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9882203\n```\n\n\n:::\n:::\n\n\n:::\n\n### Edit Similarity\n\n# Fail Mode Analysis\n\nFail mode analysis (FMA) is a systematic approach to identify potential failure modes in a system, process, or product and assess their impact on performance, safety, and reliability.\nIt is commonly used in engineering and quality management to improve designs and processes by proactively addressing potential issues.\nFMA of LLMs can help identify potential failure modes in the model's architecture, training data, and inference process, leading to improved performance and reliability.\n\nThere are various failure modes that can occur in LLMs, including:\n\n1.  **Potemkin Understanding**: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept[@mancoridisPotemkinUnderstandingLarge2025].\n\n2.  **Overfitting**: The model may become too specialized to the training data, resulting in poor generalization to new or unseen data.\n    This can lead to a lack of robustness and adaptability in real-world applications.\n    Recent works focus on using benchmarks with perturbations to test the robustness of LLMs, such as @huangMATHPerturbBenchmarkingLLMs2025.\n\n3.  **Context Window Limitation**: large reasoning models may fail to reason consistently in complex task, such as Tower of Hanai as shown in @shojaeeIllusionThinkingUnderstanding.\n    This may be because Chain-of-Thoughts (CoT) technique used by LRMs needs more steps in reasoning steps which is limited by the length of output tokens.\n\n## Potemkin understanding\n\nRegarding concept understanding ability of LLMs, traditional measures use accuracy (%) of human assessments (AP exams, AIME math competitions, and coding challenges) as the indicator.\n\n::: rmdquote\nHuman benchmarks are only valid tests for LLMs if LLMs misunderstand concepts in the same way that humans do.\n:::\n\nHowever, LLMs may have different ways of understanding concepts than humans, leading to a mismatch in evaluation---LLMs may achieve high accuracy on human benchmarks while still misunderstanding concepts.\n\n### Reason\n\nPotemkins arise when there is a misalignment between the way LLMs understand concepts and the way humans do.\nIn the experiment of Mancoridis et al. (2025), they used the definition of the concept as keystone element.\nThen, the Potemkin refers to the incorrectly answering other types of *instances* (e.g., example, generation, and edit) while being able to correctly define the concept.\nThey argued that human's misconception is more structured, they may answers those *instances* correctly or incorrectly while they do not understand the underlying concept.\n\nIn the diagnostic classification models, human may also answer those questions incorrectly given his/her latent ability level.\nThe chance of incorrectly answering the questions when they actually understanding the concepts is called slipping rate, denoted as $P(X = 0 | \\alpha = 1)$.\nBased on the conception of Mancoridis et al. (2025), the slipping rate should be inconsistent across instance.\n\n![Keynote indicator for AI potemkins and Human misconceptions](images/potemkins.png){fig-align=\"center\"}\n\n::: macwindow\n**Experiment: correct concept understanding**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nchat <- chat_openai()\n\nuser_prompt1 <- \"\nFill in the blank so that the resultant poem uses the ABAB rhyming scheme:  \nWondrous winter calls out \nShivering under the frost \nLies a lonely cat, sitting ? \nAlone but hardly lost\"\nchat$chat(user_prompt1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSure! To fit the ABAB rhyme scheme, the missing word at the end of the third \nline should rhyme with \"out\" (from the first line). The word \"about\" would fit \nwell:\n\nWondrous winter calls out  \nShivering under the frost  \nLies a lonely cat, sitting **about**  \nAlone but hardly lost\n```\n\n\n:::\n\n```{.r .cell-code}\nuser_prompt2 <- \"What is an ABAB rhyming scheme?\"\nchat$chat(user_prompt2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn **ABAB rhyming scheme** is a pattern in poetry where the first and third \nlines rhyme with each other, and the second and fourth lines rhyme with each \nother. This means lines 1 and 3 share a rhyme, as do lines 2 and 4. The letters\nA and B represent the end sounds of each line.\n\n**Example of ABAB rhyme scheme:**\n\nThe sky is blue and bright (A)  \nThe sun shines overhead (B)  \nBirds take their morning flight (A)  \nAs dreamers leave their bed (B)  \n\nHere, \"bright\" and \"flight\" rhyme (A), while \"overhead\" and \"bed\" rhyme (B).  \n\nIn summary:  \n- 1st line rhymes with 3rd (A)  \n- 2nd line rhymes with 4th (B)\n```\n\n\n:::\n\n```{.r .cell-code}\nuser_prompt3 <- \"Does 'out' rhyme with 'soft'?\"\nchat$chat(user_prompt3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNo, **'out'** and **'soft'** do **not** rhyme.\n\n- **'out'** ends with the \"owt\" sound, like \"shout\" or \"about.\"\n- **'soft'** ends with the \"awft\" sound, like \"loft\" or \"cough\" (though the \n\"ough\" spelling can vary).\n\nSince their ending sounds are different, they don't form a rhyme. For an ABAB \nrhyme scheme, you want words at the end of the A lines to have matching or \nsimilar ending sounds, and the same for the B lines.\n```\n\n\n:::\n:::\n\n\n:::\n\n::: maccaution\n**Experiment: incorrect concept understanding**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat2 <- chat_openai(model = \"gpt-3.5-turbo-1106\")\n\n#> Wondrous winter calls out \n#> Shivering under the frost \n#> Lies a lonely cat, sitting proud \n#> Alone but hardly lost\n\nuser_prompt2.3 <- \"Does 'proud' rhyme with 'soft'?\"\n\n#> No, \"proud\" and \"soft\" do not rhyme with each other. Rhyme occurs when two or more words have similar sounds, especially at the end of the words. In the ABAB rhyming scheme, the second and fourth lines should rhyme with each other.\n```\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}