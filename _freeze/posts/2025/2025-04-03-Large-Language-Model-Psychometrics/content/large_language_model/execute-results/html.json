{
  "hash": "a6384e5d79243a88456b7e6353874e66",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Large Language Models\"\nsubtitle: \"Examples with R and Python\"\ndate: \"2025-04-29\"\ndate-modified: \"2025-04-29\"\ndraft: false\nbibliography: ../references.bib\nimage: ../images/thumbnail_chatgpt.png\ntbl-cap-location: top\ncitation:\n  type: webpage\n  issued: 2025-03-07\nexecute: \n  cache: true  \nformat: \n  html:\n    code-tools: false\n    code-line-numbers: false\n    code-fold: true\n    code-summary: \"Click this to see R code\"\n---\n\n\n\n\n# Model architecture\n\n## Transformer\n\nThe original Transformer architecture was introduced in the paper [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) by Vaswani et al. in 2017.\nThe Transformer model has since become the foundation for many state-of-the-art natural language processing (NLP) models, including BERT, GPT-3, and T5.\n\n### Terminology\n\n-   Self-attention: an attention mechanism relating different positions of a single sequence to compute a representation of the sequence.\n\n    -   Q,K,V matrix: query, keys, values.\n        The output is computed as a weighted sum of the values.\n        All these three are key component of Attention function.\n        The following formula is also known as Scaled dot-product attention.\n\n        $$\n        \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n        $$\n\n    -   where queries and keys have the dimension $d_k$\n\n> the dot products get larger variances when the dimension of q and k increase.\n> So they scale the dot product by $\\frac{1}{\\sqrt{d_k}}$ to make sure the dot product has close variance with q and k.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nN_rep = 1000\ndot_product <- function(i, d_k, var_q = 1) {\n  set.seed(i)\n  q = rnorm(d_k, 0, sd = sqrt(var_q))\n  k = rnorm(d_k, 0, sd = sqrt(var_q))\n  crossprod(q, k)\n}\n\nvar_dot_product <- function(N_rep, d_k, var_q = 1) {\n  dot_product_values <- sapply(1:N_rep, \\(x) dot_product(x, d_k = d_k, var_q))\n  var(dot_product_values)\n}\n\nto_plot <- data.frame(\n  d_k_values = c(1, 10, 100, 1000),\n  var_dot_product_values = sapply(c(1, 10, 100, 1000), \\(x) var_dot_product(N_rep = N_rep, \n                                                                   d_k = x))\n)\n\nggplot(to_plot, aes(x = d_k_values, y = var_dot_product_values)) +\n  geom_path() +\n  geom_point() +\n  labs(x = \"d_k\", y = \"Variance of dot product of q and k\")\n```\n\n::: {.cell-output-display}\n![](large_language_model_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_q_values = c(1, 10, 25, 100, 400) # variances of q and k\nd_k = 2 # dimension of k\n\n# function to generate the variaance of scaled dot products\nvar_scaled_dot_product <- function(N_rep, d_k, var_q = 1) {\n  scaled_dot_product_values <- sapply(1:N_rep,\\(x){dot_product(x, d_k = d_k, var_q = var_q)/sqrt(d_k)} )\n  var(scaled_dot_product_values)\n}\n\nvar_scaled_dot_product_values <- sapply(var_q_values, \n                                        \\(x) var_scaled_dot_product(N_rep = N_rep, \n                                                                    d_k = d_k, \n                                                                    var_q = x) )\n\ndata.frame(\n  var_q_values = var_q_values,\n  var_scaled_dot_product_values = var_scaled_dot_product_values\n) |> \nggplot(aes(x = var_q_values, y = var_scaled_dot_product_values)) +\n  geom_path() +\n  geom_point() +\n  labs(x = \"variances of q and k\", y = \"Variance of scaled dot product of q and k\")\n```\n\n::: {.cell-output-display}\n![](large_language_model_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n-   Multi-Head Attention:\n\n-   Encoder-decoder structure: The basic structure of most neural sequence transduction models.\n\n    -   Encoder maps an input to a sequence of continuous representations.\n\n    -   Decoder generates an output sequence given the continuous representation of encoder\n\n![The Transformer - model architecture in the original paper](images/clipboard-1162313639.png){fig-align=\"center\" width=\"500\"}\n\n# Evaluation metrics\n\n## BLEU\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}