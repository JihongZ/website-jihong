{
  "hash": "0364219074b2d140808292c33758a616",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Large Language Models\"\nsubtitle: \"Examples with R and Python\"\ndate: \"2025-04-29\"\ndate-modified: \"2025-06-30\"\ndraft: false\nbibliography: ../references.bib\nimage: ../images/thumbnail_chatgpt.png\ntbl-cap-location: top\ncitation:\n  type: webpage\n  issued: 2025-03-07\nexecute: \n  cache: true  \nformat: \n  html:\n    code-tools: false\n    code-line-numbers: false\n    code-fold: false\n    code-summary: \"Click this to see code\"\n---\n\n\n\n\n# Model architecture\n\n## Transformer\n\nThe original Transformer architecture was introduced in the paper [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) by Vaswani et al. in 2017.\nThe Transformer model has since become the foundation for many state-of-the-art natural language processing (NLP) models, including BERT, GPT-3, and T5.\n\n### Terminology\n\n-   Self-attention: an attention mechanism relating different positions of a single sequence to compute a representation of the sequence.\n\n    -   Q,K,V matrix: query, keys, values.\n        The output is computed as a weighted sum of the values.\n        All these three are key component of Attention function.\n        The following formula is also known as Scaled dot-product attention.\n\n        $$\n        \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n        $$\n\n    -   where queries and keys have the dimension $d_k$\n\n> the dot products get larger variances when the dimension of q and k increase.\n> So they scale the dot product by $\\frac{1}{\\sqrt{d_k}}$ to make sure the dot product has close variance with q and k.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nN_rep = 1000\ndot_product <- function(i, d_k, var_q = 1) {\n  set.seed(i)\n  q = rnorm(d_k, 0, sd = sqrt(var_q))\n  k = rnorm(d_k, 0, sd = sqrt(var_q))\n  crossprod(q, k)\n}\n\nvar_dot_product <- function(N_rep, d_k, var_q = 1) {\n  dot_product_values <- sapply(1:N_rep, \\(x) dot_product(x, d_k = d_k, var_q))\n  var(dot_product_values)\n}\n\nto_plot <- data.frame(\n  d_k_values = c(1, 10, 100, 1000),\n  var_dot_product_values = sapply(c(1, 10, 100, 1000), \\(x) var_dot_product(N_rep = N_rep, \n                                                                   d_k = x))\n)\n\nggplot(to_plot, aes(x = d_k_values, y = var_dot_product_values)) +\n  geom_path() +\n  geom_point() +\n  labs(x = \"d_k\", y = \"Variance of dot product of q and k\")\n```\n\n::: {.cell-output-display}\n![](large_language_model_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_q_values = c(1, 10, 25, 100, 400) # variances of q and k\nd_k = 2 # dimension of k\n\n# function to generate the variaance of scaled dot products\nvar_scaled_dot_product <- function(N_rep, d_k, var_q = 1) {\n  scaled_dot_product_values <- sapply(1:N_rep,\\(x){dot_product(x, d_k = d_k, var_q = var_q)/sqrt(d_k)} )\n  var(scaled_dot_product_values)\n}\n\nvar_scaled_dot_product_values <- sapply(var_q_values, \n                                        \\(x) var_scaled_dot_product(N_rep = N_rep, \n                                                                    d_k = d_k, \n                                                                    var_q = x) )\n\ndata.frame(\n  var_q_values = var_q_values,\n  var_scaled_dot_product_values = var_scaled_dot_product_values\n) |> \nggplot(aes(x = var_q_values, y = var_scaled_dot_product_values)) +\n  geom_path() +\n  geom_point() +\n  labs(x = \"variances of q and k\", y = \"Variance of scaled dot product of q and k\")\n```\n\n::: {.cell-output-display}\n![](large_language_model_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n-   Multi-Head Attention:\n\n-   Encoder-decoder structure: The basic structure of most neural sequence transduction models.\n\n    -   Encoder maps an input to a sequence of continuous representations.\n\n    -   Decoder generates an output sequence given the continuous representation of encoder\n\n![The Transformer - model architecture in the original paper](images/clipboard-1162313639.png){fig-align=\"center\" width=\"500\"}\n\n# Evaluation metrics\n\n## Item Similarity\n\nVarious metrics can be used to evaluate the similarity between two sentences (e.g., math items) at the lexical level.\nThey are typically called vector similarity measures.\n\n### BLEU\n\nBLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of text that has been machine-translated from one natural language to another (see [geeksforgeeks](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)). It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of n items) between the candidate and reference translations. Higher values indicate better quality translations.\n\nThe formula of BLEU score is:\n\n$$\n\\text{BLEU}(C, R) = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\cdot \\log p_n\\right)\n$$\n\nwhere:\n\n- $C$ is the candidate translation,\n- $R$ is the reference translation,\n- $BP$ is the brevity penalty, which penalizes translations that are shorter than the reference translation,\n- $p_n$ is the precision of n-grams in the candidate translation,\n- $w_n$ is the weight assigned to the n-gram precision, typically set to $\\frac{1}{N}$ for $n=1,2,\\ldots,N$.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\ndef calculate_bleu(candidate, reference):\n    # Tokenize the sentences\n    candidate_tokens = candidate.split()\n    reference_tokens = reference.split()\n    \n    # Calculate BLEU score\n    smoothing_function = SmoothingFunction().method1\n    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing_function)\n    \n    return bleu_score\n\ncandidate = \"The cat sat on the mat.\"\nreference = \"The cat is sitting on the mat.\"\n\nbleu_score = calculate_bleu(candidate, reference)\nprint(f\"BLEU score: {bleu_score:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBLEU score: 0.2151\n```\n\n\n:::\n:::\n\n\n\n\n\n### Cosine Similarity\n\nThe cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. In NLP, it is often used to compare the similarity of two text documents or sentences by representing them as vectors in a high-dimensional space.\n\nThe formula of the cosine similarity is:\n\n$$\n\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\n$$\n\nwhere $A$ and $B$ are the two vectors, $A \\cdot B$ is the dot product of the vectors, and $||A||$ and $||B||$ are the magnitudes (or norms) of the vectors.\n\nTo convert words into vectors, we can use a simple method called **word2vec**. To be more specific, we can **represent a word as a vector of character frequencies**, where each character in the word is counted and represented as a dimension in the vector space.\n\n::: panel-tabset\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef word2vec(word):\n    from collections import Counter\n    from math import sqrt\n\n    # count the characters in word\n    cw = Counter(word)\n    # precomputes a set of the different characters\n    sw = set(cw)\n    # precomputes the \"length\" of the word vector\n    lw = sqrt(sum(c*c for c in cw.values()))\n\n    # return a tuple\n    return cw, sw, lw\n\ndef cosdis(v1, v2):\n    # which characters are common to the two words?\n    common = v1[1].intersection(v2[1])\n    # by definition of cosine distance we have\n    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]\n  \na = 'safasfeqefscwaeeafweeaeawaw'\nb = 'tsafdstrdfadsdfdswdfafdwaed'\nc = 'optykop;lvhopijresokpghwji7'\n\nva = word2vec(a)\nvb = word2vec(b)\nvc = word2vec(c)\n\nprint(cosdis(va,vb))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.5518436623205278\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(cosdis(vb,vc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.11374657965622856\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(cosdis(vc,va))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.1534943780776824\n```\n\n\n:::\n:::\n\n\n\n\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = 'safasfeqefscwaeeafweeaeawaw'\nb = 'tsafdstrdfadsdfdswdfafdwaed'\nc = 'optykop;lvhopijresokpghwji7'\n\ncosine_similarity <- function(a, b) {\n  a <- strsplit(a, \"\")[[1]]\n  b <- strsplit(b, \"\")[[1]]\n  \n  a_freq <- table(a)\n  b_freq <- table(b)\n  \n  common_chars <- intersect(names(a_freq), names(b_freq))\n  \n  dot_product <- sum(a_freq[common_chars] * b_freq[common_chars])\n  norm_a <- sqrt(sum(a_freq^2))\n  norm_b <- sqrt(sum(b_freq^2))\n  \n  return(dot_product / (norm_a * norm_b))\n}\n\ncosine_similarity(a, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5518437\n```\n\n\n:::\n\n```{.r .cell-code}\ncosine_similarity(b, c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1137466\n```\n\n\n:::\n\n```{.r .cell-code}\ncosine_similarity(a, c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1534944\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n\n### Edit Similarity\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}