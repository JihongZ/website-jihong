{
  "hash": "172c8edf58e04cd3cd2a157a79234b53",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"OpenAI Cookbook with R and Python\"\ndate: \"2025-03-07\"\ndate-modified: \"2025-04-18\"\ndraft: false\nbibliography: ../references.bib\nimage: ../images/thumbnail_chatgpt.png\ntbl-cap-location: top\ncitation:\n  type: webpage\n  issued: 2025-03-07\nexecute: \n  cache: true  \nformat: \n  html:\n    code-tools: false\n    code-line-numbers: false\n    code-fold: false\n    code-summary: \"Click this to see R code\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n![The generated Likert-scale motivation item by GPT-4.1](images/clipboard-1126306144.png){#fig-gpt-item}\n\n## GPT-4.1\n\nOpenAI published [GPT-4.1](https://platform.openai.com/docs/models/gpt-4.1) (Model: `gpt-4.1-2025-04-14`) on April 2025. The price is \\$2 / 1M token. As a reference, in my recent project of using LLM to generate survey responses, the number of tokens per prompt ranges from 5K to 10K, which means 100 calls cost around \\$2 or \\$.02 / calling.\n\nNext, I used one tiny experiment to examine the differences between GPT-4o and GPT-4.1 when answering to the same prompt. The prompt is a general prompt from my recent project.\n\nAs shown in @fig-gpt-compr, two items show differences in generated survey responses. It may be because the emotional words such as \"ashamed\" or \"failure\".\n\n-   Item6: I feel **ashamed** when I miss an exercise session.\n\n-   Item7: I feel like a **failure** when I haven't exercised in a while.\n\nIn contrast to choosing \"**Somewhat disagree**\" by GPT-4o, GPT-4.1 tend to choose \"**Disagree**\" in these two items with negatively emotional words. Does GPT-4.1 tend to be more emotionally similar to humans — mentally avoiding negative emotion?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Create meta prompts without personal information\"}\ninstruction <- \"\nInstructions:\n\nFor your output,\n  - Only provide the numerical responses.\n  - Format your responses as a semicolon-separated list: X;X;X;X;X;X;X, where X represents your selected option for each question, the total number of X depends on number of items.\n  - Example: '1;3;2' corresponds to:\n  \t- 1 (Strongly Agree) for the first question\n  \t- 3 (Somewhat Agree) for the second question\n  \t- 2 (Agree) for the third question\n  \t\nThe scale for all items is: 1 = Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Somewhat agree, 5 = Agree, 6 = Strongly agree.  \t\n\"\nprompt <- \"\nRole & Task:\n\nYou are a staff member of club serving underserved communities. The program enrolls at least 50% of youth (ages 9–14) from low-income households (as defined by free/reduced lunch status) and minority backgrounds.\n\nYour task is to construct a persona. Based on this persona, generate numerical responses to survey items in Survey Questions section using a Likert scale in Response Scale section that aligns with your expressed views.\n\nSurvey Questions:\nItem1: I exercise because other people say I should.\nItem2: I take part in exercise because my friends/family/spouse say I should.\nItem3: I exercise because others will not be pleased with me if I don't.\nItem4: I feel under pressure from my friends/family to exercise.\nItem5: I feel guilty when I don't exercise.\nItem6: I feel ashamed when I miss an exercise session.\nItem7: I feel like a failure when I haven't exercised in a while.\nItem8: I value the benefits of exercise.\nItem9: It's important to me to exercise regularly.\nItem10: I think it is important to make the effort to exercise regularly.\nItem11: I get restless if I don't exercise regularly.\nItem12: I exercise because it's fun.\nItem13: I enjoy my exercise sessions.\nItem14: I find exercise a pleasurable activity.\nItem15: I get pleasure and satisfaction from participating in exercise\n\"\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python code to call GPT API\"}\nimport time  # Import the time module\nfrom dis import Instruction\nimport os\nimport csv\nfrom openai import OpenAI\n\n# ChatGPT\ndef get_response_gpt4o(instructions, input, temp):\n    # Initialize the OpenAI API client\n    client = OpenAI()\n    response = client.responses.create(\n        model=\"gpt-4o\",\n        instructions=instructions,\n        input=input,\n        temperature=temp,\n    )\n    return response.output_text\n\ndef get_response_gpt41(instructions, input, temp):\n    # Initialize the OpenAI API client\n    client = OpenAI()\n    response = client.responses.create(\n        model=\"gpt-4.1-2025-04-14\",\n        instructions=instructions,\n        input=input,\n        temperature=temp,\n    )\n    return response.output_text\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"ggplot2 for generated responses\"}\ngpt4o_resp <- py$get_response_gpt4o(instructions=instruction, \n                                    input=prompt, temp = 0)\ngpt41_resp <- py$get_response_gpt41(instructions=instruction, \n                                    input=prompt, temp = 0)\n\nres <- data.frame(\n  gpt4o = unlist(str_split(gpt4o_resp, \";\")),\n  gpt41 = unlist(str_split(gpt41_resp, \";\")),\n  Item = factor(1:15)\n) |> \n  pivot_longer(starts_with(\"gpt\"), names_to = \"Model\", values_to = \"Response\")\n\nggplot(res) +\n  geom_path(aes(x = Item, y = Response, color = Model, group = Model) ) +\n  scale_color_manual(values = c(\"royalblue\", \"red3\"), \n                     labels = c(\"GPT4.1\", \"GPT4o\")) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Generated survey output for GPT-4o and GPT-4.1](chatgpt_files/figure-html/fig-gpt-compr-1.png){#fig-gpt-compr width=672}\n:::\n:::\n\n\n\n\n### Response probability distribution vs. responses value\n\nInstead of asking LLM to generate survey responses directly, we can also instruct LLMs to generate probability distributions of responses for each item. For example, instead of letting LLM output \"1;2;2\" for three 5-point Liker-scale items, we ask LLMs to output `Xi_k1:pi1,Xi_k2:pi2,Xi_k3:pi3,Xi_k4:pi4,Xi_k5:pi5,Xi_k6:pi6;`.\n\nwhere:\n\n  - `Xi_kj` denotes response option j (from 1 to 6) for item i\n  - `pij` is the probability (between 0 and 1) of selecting that option. \n  - Each line (separated by semicolons) corresponds to a different item\n  - The probabilities for each item must sum to 1\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Use response probabities for generated responses\"}\ninstruction2 <- \"\nInstructions:\nGenerate probabilistic responses for each survey item using the following format. Each response should represent a probability distribution across six ordinal options of 15 items. The pribabilities of all options of each item should be two or three decimal places. Ensure that for each item, the sum of all six probabilities equals 1.\n\n- Output format:\n\nProvide only numerical responses. Use a semicolon-separated list to indicate the probability of selecting each response option for each item.\n\n- Structure:\n\nFor Item i, the response should follow this pattern:\n\nXi_k1:pi1,Xi_k2:pi2,Xi_k3:pi3,Xi_k4:pi4,Xi_k5:pi5,Xi_k6:pi6;\n\nwhere:\n  - Xi_kj denotes response option j (from 1 to 6) for item i\n  - pij is the probability (between 0 and 1) of selecting that option. \n  - Each line (separated by semicolons) corresponds to a different item\n  - The probabilities for each item must sum to 1\n\n- Example:\n  \n  X1_k1:0.50,X1_k2:0.10,X1_k3:0.10,X1_k4:0.10,X1_k5:0.10,X1_k6:0.10;\n\nThis indicates that for Item 1, the probabilities of selecting options 1 through 6 are 50%, 10%, 10%, 10%, 10%, and 10%, respectively.\n\n- Scale (applies to all items):\n  1 = Strongly Disagree\n  2 = Disagree\n  3 = Somewhat Disagree\n  4 = Somewhat Agree\n  5 = Agree\n  6 = Strongly Agree\n\"\n\ngpt4o_resp2 <- py$get_response_gpt4o(instructions=instruction2, \n                                    input=prompt, temp = 0)\ngpt41_resp2 <- py$get_response_gpt41(instructions=instruction2, \n                                    input=prompt, temp = 0)\n```\n:::\n\n\n\n\nYou can find that GPT models can understand the prompt very well\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(gpt4o_resp2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX1_k1:0.300,X1_k2:0.250,X1_k3:0.200,X1_k4:0.150,X1_k5:0.050,X1_k6:0.050;\nX2_k1:0.250,X2_k2:0.250,X2_k3:0.200,X2_k4:0.150,X2_k5:0.100,X2_k6:0.050;\nX3_k1:0.350,X3_k2:0.250,X3_k3:0.200,X3_k4:0.100,X3_k5:0.050,X3_k6:0.050;\nX4_k1:0.300,X4_k2:0.250,X4_k3:0.200,X4_k4:0.150,X4_k5:0.050,X4_k6:0.050;\nX5_k1:0.200,X5_k2:0.200,X5_k3:0.200,X5_k4:0.200,X5_k5:0.100,X5_k6:0.100;\nX6_k1:0.250,X6_k2:0.250,X6_k3:0.200,X6_k4:0.150,X6_k5:0.100,X6_k6:0.050;\nX7_k1:0.300,X7_k2:0.250,X7_k3:0.200,X7_k4:0.150,X7_k5:0.050,X7_k6:0.050;\nX8_k1:0.050,X8_k2:0.050,X8_k3:0.100,X8_k4:0.200,X8_k5:0.300,X8_k6:0.300;\nX9_k1:0.050,X9_k2:0.050,X9_k3:0.100,X9_k4:0.200,X9_k5:0.300,X9_k6:0.300;\nX10_k1:0.050,X10_k2:0.050,X10_k3:0.100,X10_k4:0.200,X10_k5:0.300,X10_k6:0.300;\nX11_k1:0.100,X11_k2:0.100,X11_k3:0.150,X11_k4:0.200,X11_k5:0.250,X11_k6:0.200;\nX12_k1:0.050,X12_k2:0.050,X12_k3:0.100,X12_k4:0.200,X12_k5:0.300,X12_k6:0.300;\nX13_k1:0.050,X13_k2:0.050,X13_k3:0.100,X13_k4:0.200,X13_k5:0.300,X13_k6:0.300;\nX14_k1:0.050,X14_k2:0.050,X14_k3:0.100,X14_k4:0.200,X14_k5:0.300,X14_k6:0.300;\nX15_k1:0.050,X15_k2:0.050,X15_k3:0.100,X15_k4:0.200,X15_k5:0.300,X15_k6:0.300;\n```\n\n\n:::\n:::\n\n\n\n\nLet's do some data cleaning and then visualize the item response probabilities.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Visualize the response probability for two LLM mdoels\"}\nget_df_from_output <- function(raw_gpt_op) {\n  all_item_resp_name <- NULL\n  all_item_resp_prob <- NULL\n  item_resps <- str_split(raw_gpt_op, \"\\n\")[[1]]\n  for (i in 1:length(item_resps)) { # i = 1\n    each_resp_foritem <- str_split(str_remove(item_resps[i], \";\"), \",\")[[1]]\n    for (r in 1:length(each_resp_foritem)) { # for each response, r = 1\n      each_resp <- str_split(each_resp_foritem[r], \":\")[[1]]\n      each_resp_name <- each_resp[1]\n      each_resp_prob <- as.numeric(each_resp[2])\n      all_item_resp_name <- c(all_item_resp_name, each_resp_name)\n      all_item_resp_prob <- c(all_item_resp_prob, each_resp_prob)\n    }\n  }\n  res <- data.frame(\n    item_resps = all_item_resp_name,\n    probability = all_item_resp_prob\n  ) |> \n    separate(item_resps, into = c(\"Item\", \"Option\"), sep = \"_\") |> \n    mutate(Option = factor(str_remove(Option, \"k\"), levels = 1:6),\n           Item = factor(Item, levels = paste0(\"X\", 15:1)))\n  return(res)\n}\n\ngpt4o_resp2_dt <- get_df_from_output(gpt4o_resp2)\ngpt41_resp2_dt <- get_df_from_output(gpt41_resp2)\n\ndt_comb <- rbind(\n  gpt4o_resp2_dt |> mutate(Model = \"GPT-4o\"),\n  gpt41_resp2_dt |> mutate(Model = \"GPT-4.1\")\n) \n\nmycolors = c(\"#4682B4\", \"#B4464B\", \"#B4AF46\", \n             \"#1B9E77\", \"#D95F02\", \"#7570B3\",\n             \"#E7298A\", \"#66A61E\", \"#B4F60A\")\n\nggplot(dt_comb) +\n  geom_col(aes(x=Item, y = probability, fill = Option), \n           position =  position_stack(reverse = TRUE)) +\n  geom_text(aes(x=Item, y = probability, \n                label = probability, group = Item),\n            size = 2,\n            position = position_stack(reverse = TRUE, vjust = 0.5)) +\n  facet_wrap(~ Model) +\n  coord_flip() +\n  scale_fill_manual(values = mycolors)\n```\n\n::: {.cell-output-display}\n![Item response probabity distribution for GPT-4o and GPT-4.1](chatgpt_files/figure-html/fig-gpt-prob-1.png){#fig-gpt-prob width=672}\n:::\n:::\n\n\n\n\nAs @fig-gpt-prob shows, GPT-4.1 has varied item probability in Item 6 and Item 7. Taking item 7 for example, choosing 2 (Disagree) and 3 (Somewhat disagree) has same probability (p = .22) for GPT-4.1, while GPT-4o tend to have more in 1-Strongly disagree (p = .3) than 2-Disagree (p = .25) and 1-Somewhat disagree (p = .2).\n\nIn addition, GPT-4o seems to have exactly same item response distribution for item 8-10 and item 12-15. In contrast, GPT-4.1 has different item response distribution for these items.\n\n::: callout-important\n#### Takehome note\n-   GPT-4.1 is more sensitive to emotional words than GPT-4o.\n-   GPT-4.1 is more likely to have different response probability distribution across items that GPT-4o, suggesting GPT-4.1 can have more diversity in generation item responses.\n:::\n\n## Response API Walk-through\n\nAs shown in @fig-gpt-item, OpenAI provides [Prompts Playground](https://platform.openai.com/playground/prompts?models=gpt-4o) to test your prompt. OpenAI has recently updated their API from Chat to [Response API](https://platform.openai.com/docs/api-reference/responses).\n\nTo use the Response API, update `OpenAI` python module to the latest using the following command:\n\n``` bash\npip install openai --upgrade\n```\n\n::: macwindow\n**Responses vs. Chat Completions**\n\nFor OpenAI, there are two types of API interfaces (see [Responses vs. Chat Completions](https://platform.openai.com/docs/guides/responses-vs-chat-completions)). Previously, I use `client.chat.completions` for example. In the following example, I will use `client.responses`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    instructions=\"You are a coding assistant that talks like a pirate.\",\n    input=\"How do I check if a Python object is an instance of a class?\",\n)\n\nprint(response.output_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nArrr, to check if a Python object be an instance of a class, ye can use the `isinstance()` function. Here be the syntax, matey:\n\n```python\nif isinstance(obj, SomeClass):\n    print(\"Arrr, the object be an instance of the class!\")\nelse:\n    print(\"Nay, it be not.\")\n```\n\nJust replace `obj` with yer object and `SomeClass` with the class in question, savvy? Now hoist the sails and code away!\n````\n\n\n:::\n:::\n\n\n\n:::\n\n### Image example\n\n![](https://campusdata.uark.edu/resources/images/FacultyStaffProfile/jzhang.jpg?1743825231865&w=440){width=\"40%\"}\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Response API with image as input\"}\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"role\": \"user\", \"content\": \"Describe the person  in the image using 5-6 sentences?\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"input_image\",\n                    \"image_url\": \"https://campusdata.uark.edu/resources/images/FacultyStaffProfile/jzhang.jpg?1743825231865&w=440\",\n                }\n            ],\n        },\n    ],\n)\n\n\nprint(response.output_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe person in the image appears to be a man with short, dark hair, styled neatly. He is wearing glasses and has a slight smile, giving a friendly and approachable appearance. He is dressed in a professional manner, wearing a gray pinstripe suit and a light-colored shirt. The background is neutral, which keeps the focus on his facial expression and attire. His posture is upright, suggesting confidence and professionalism.\n```\n\n\n:::\n:::\n\n\n\n\n#### Use AI to reproduce academic figure\n\nLet's try a more academic figure --- a histogram plot from the web:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"LLM-generated R code\"}\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"role\": \"user\", \"content\": \"You are a R programming assistant. Provide R code to generate this figure. Make sure only R code is provide.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"input_image\",\n                    \"image_url\": \"https://seaborn.pydata.org/_images/distributions_3_0.png\",\n                }\n            ],\n        },\n    ],\n)\n\n\nprint(response.output_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n```r\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# Create the histogram\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", alpha = 0.6) +\n  theme_minimal() +\n  labs(x = \"flipper_length_mm\", y = \"Count\") +\n  theme(text = element_text(size = 12))\n```\n````\n\n\n:::\n:::\n\n\n\n\n::::: columns\n::: column\n##### Original Plot\n\n![](https://seaborn.pydata.org/_images/distributions_3_0.png)\n:::\n\n::: column\n##### AI-generated Plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R Code\"}\n# Required packages\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# Plot\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"black\", alpha = 0.6) +\n  theme_minimal(base_size = 15) +\n  labs(x = \"flipper_length_mm\", y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](chatgpt_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n:::\n:::::\n\n### Conversation State\n\nResource: <https://platform.openai.com/docs/guides/conversation-state?api-mode=responses>\n\nThe idea is creating a dynamic R vectors. The appended R string can change LLM conversation state including history.\n\nIn OpenAI, the `previous_response_id` parameter was used to chain responses and create a threaded conversation.\n\n\n\n\n\n```{.python .cell-code}\nfrom openai import OpenAI\n\nclient = OpenAI()\ndef get_conversation(history): \n  response = client.responses.create(\n      model=\"gpt-4o-mini\", input=history, store=False\n  )\n  \n  history += [\n    {\"role\": output.role, \"content\": output.content} for output in response.output]\n    \n  return history\n\nhistory = [{\"role\": \"user\", \"content\": \"Can your generate one 6-Likert scale item response with item difficult as 2 for a persoon with 1 standard deviation factor score?\"}]\nres = get_conversation(history)\nprint(res[1]['content'][0].text)\n```\n\nTo generate a response on a 6-point Likert scale using item difficulty and a person's factor score, we can use the following framework:\n\n1. **Likert Scale Points**: Typically ranges from 1 to 6, where:\n   - 1 = Strongly Disagree\n   - 2 = Disagree\n   - 3 = Slightly Disagree\n   - 4 = Slightly Agree\n   - 5 = Agree\n   - 6 = Strongly Agree\n\n2. **Item Difficulty**: Given as 2, meaning the item is relatively easy for most respondents.\n\n3. **Factor Score**: A factor score of 1 standard deviation above the mean suggests a higher-than-average endorsement of positive responses.\n\n### Generation of the Response:\n\n- **Item Difficulty** (2): This means the item is likely to be endorsed by a majority of respondents.\n- **Factor Score** (1 standard deviation above the mean): This indicates that the person would tend to agree more readily with agreeable statements.\n\n### Likert Scale Selection:\n\nConsidering the item difficulty (2) and the person's factor score (1 standard deviation above the mean), it would be reasonable to predict a response of **4** (Slightly Agree) or **5** (Agree).\n\n**Final Response**: \n\n**Likert Scale Item Response**: **5 (Agree)** \n\nThis suggests that the individual finds the statement agreeable, aligning well with their factor score above average despite the item's relative ease.\n\n```{.python .cell-code}\n# A second response will be like:\nres.append({\"role\": \"user\", \"content\": \"Can your generate another 6-Likert scale item response with item difficult as 2 and item discrimination as 1 for a persoon with 2 standard deviation factor score?\"})\nres2 = get_conversation(res)\nprint(res2[3]['content'][0].text)\n```\n\nTo generate a response for a 6-point Likert scale item with the specifications you've provided:\n\n1. **Item Difficulty**: 2 (indicating the item is fairly easy).\n2. **Item Discrimination**: 1 (suggesting that the item effectively differentiates between individuals with varying levels of the trait being measured).\n3. **Factor Score**: 2 standard deviations above the mean (indicating a strong endorsement of positive responses).\n\n### Likert Scale Points:\n- 1 = Strongly Disagree\n- 2 = Disagree\n- 3 = Slightly Disagree\n- 4 = Slightly Agree\n- 5 = Agree\n- 6 = Strongly Agree\n\n### Analysis:\n- **Item Difficulty (2)**: Means the item is expected to be endorsed by most respondents.\n- **Item Discrimination (1)**: Indicates the item can distinguish well between different levels of the trait.\n- **Factor Score (2 standard deviations above the mean)**: Indicates a strong tendency to agree with positive statements.\n\n### Likert Scale Response:\nGiven that the item is easy (difficulty of 2) and the individual is at 2 standard deviations above the mean, they would likely respond positively to the item.\n\n### Likely Response:\nA response of **6 (Strongly Agree)** is fitting in this scenario.\n\n**Final Response**: \n\n**Likert Scale Item Response**: **6 (Strongly Agree)** \n\nThis response reflects the individual's high endorsement of positive sentiments, consistent with their strong factor score and the item characteristics.\n\n\n\n\n## Example 1: Construct LLM function calling with Python and R\n\nThe following section is inspired by the fascinating guide in the [post](https://pavelbazin.com/post/the-essential-guide-to-large-language-models-structured-output-and-function-calling/?utm_source=reddit&utm_medium=social&utm_campaign=structured_output&utm_content=sub_python) authored by Pavel Bazin.\n\nFirst of all, type in the following bash command in the terminal to install `openai` python module if you already have Python installed.\n\n``` bash\npip install openai\npython3 -m pip install openai\npip install openai --upgrade\n```\n\nThe next step is to either create a new Python file (`.py`) or use `{python}` code chunk in Quarto, in which importing the `OpenAI` module and construct a python function called `llm_eval()`. This function can call OpenAI ChatGPT (e.g., `gpt-4o`) via the API key:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Further down these imports will be ommited for brevity\nimport os\nfrom openai import OpenAI\n\n\ndef llm_eval(prompt: str, message: str, model: str = \"gpt-4o\"):\n    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": message},\n    ]\n\n    return client.chat.completions.create(\n        model=model,\n        messages=messages\n    )\n```\n:::\n\n\n\n\nThen, we should be able to call the function either in R (use `reticulate` package) or in Python:\n\n:::: panel-tabset\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n\nprompt = \"\nYou are a data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output it as JSON.\n\"\nmessage = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n\nres = py$llm_eval(prompt=prompt, message=message) #<1>\n\njson_data = res$choices[[1]]$message$content\n\ncat(json_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n```json\n{\n  \"groceries\": [\n    \"bread\",\n    \"pack of eggs\",\n    \"few apples\",\n    \"bottle of milk\"\n  ]\n}\n```\n````\n\n\n:::\n:::\n\n\n\n\n::: rmdnote\nThere are two things that should be noted:\n\n1.  If you are using Quarto, use `py$<python object>` in R code chunk to have access to the python object. If you put python code and R code in multiple files, use `source_python(\"<Path to Python file>\")` to load the python function into R session.\n\n2.  In contrast to use `res.choices[0].message.content` in Python, note that the nested list in R is extract using `$` with the starting index as `1`.\n:::\n\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprompt = \"\"\"\nYou are a data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output it as JSON.\n\"\"\"\nmessage = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n\nres = eval(prompt=prompt, message=message)\njson_data = res.choices[0].message.content\n\nprint(json_data)\n```\n:::\n\n\n\n::::\n\n### `response_format`: Structured Response\n\nLLM returned a markdown formatted string containing JSON by default. The reason is that we didn’t enable structured output in the API call.\n\nTo return the plain JSON text, we can structure output in Python function through setting up `response_format`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef llm_eval2(prompt: str, message: str, model: str = \"gpt-4o\"):\n    \n    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": message},\n    ]\n    \n    return client.chat.completions.create(\n        model=model,\n        messages=messages,\n        # Enable structured output capability\n        response_format={\"type\": \"json_object\"},\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# A R function to request LLM's response given prompt and message.\nget_response <- function(prompt, message) {\n  res <- py$llm_eval2(prompt=as.character(prompt), \n                      message=as.character(message))\n  output <- res$choices[[1]]$message$content\n  return(output)\n}\n```\n:::\n\n\n\n\nNow, running the same code will return plain JSON. That is great not only because we don’t need to parse anything extra, but it also guarantees that the LLM won’t include any free-form text such as “Sure, here is your data! {…}”.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njson_data2 = get_response(prompt=prompt, message=message)\n\ncat(json_data2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{\n  \"groceries\": [\n    \"bread\",\n    \"pack of eggs\",\n    \"apples\",\n    \"bottle of milk\"\n  ]\n}\n```\n\n\n:::\n:::\n\n\n\n\nWe can further transform the JSON text into multiple format:\n\n-   Data Frame\n-   List\n-   Vector\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data2 <- jsonlite::fromJSON(json_data2)\n```\n:::\n\n\n\n\n::: panel-tabset\n### Data Frame\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(as.data.frame(list_data2))\n```\n\n::: {.cell-output-display}\n\n\n|groceries      |\n|:--------------|\n|bread          |\n|pack of eggs   |\n|apples         |\n|bottle of milk |\n\n\n:::\n:::\n\n\n\n\n### Vector\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data2$groceries\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"bread\"          \"pack of eggs\"   \"apples\"         \"bottle of milk\"\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Structure as a HTML list\n\nTo further structure the output as HTML list (e.g., `<ul></ul>`), we can construct a simple sparser in Python and print it in R like this:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport json\ndef render(data: str):\n    data_dict = json.loads(data)\n    backslash =\"\\n\\t\"\n\n    return f\"\"\"\n    <ul>\n        {\"\".join([ f\"<li>{x}</li>\" for x in data_dict[\"groceries\"]])}\n    </ul>\n    \"\"\"\n```\n:::\n\n\n```{.r .cell-code}\nlibrary(htmltools)\nHTML(py$render(json_data2))\n```\n\n```{=html}\n\n    <ul>\n        <li>bread</li><li>pack of eggs</li><li>apples</li><li>bottle of milk</li>\n    </ul>\n    \n```\n\n\n\n\n### Utilize Schema to Define Data Shape\n\nThe problem is, we don’t have the data shape defined; let’s call it **JSON schema**. Our schema is now up to the LLM, and it might change based on user input. Let’s rephrase the user query to see it in action. Instead of asking, “I’d like to buy some bread, a pack of eggs, a few apples, and a bottle of milk,” let’s ask, “12 eggs, 2 bottles of milk, 6 sparkling waters.”\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage = \"12 eggs, 2 bottles of milk, 6 sparkling waters\"\n\nres = py$llm_eval2(prompt=prompt, message=message)\n\njson_data = res$choices[[1]]$message$content\n\ncat(json_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{\n  \"groceries\": [\n    {\n      \"item\": \"eggs\",\n      \"quantity\": 12,\n      \"unit\": \"pieces\"\n    },\n    {\n      \"item\": \"milk\",\n      \"quantity\": 2,\n      \"unit\": \"bottles\"\n    },\n    {\n      \"item\": \"sparkling water\",\n      \"quantity\": 6,\n      \"unit\": \"bottles\"\n    }\n  ]\n}\n```\n\n\n:::\n:::\n\n\n\n\nAs seen in the output, ChatGPT can incorporate inputs with quantities into outputs with extra quantity attribute.\n\n::: rmdnote\nOpenAI introduced the next step for [Structured Output](https://platform.openai.com/docs/guides/structured-outputs/structured-outputs). You can get the same results using `response_format={\"type\": \"json_object\"}` and parse data yourself without using beta version of API which was not performing reliably in our products.\n:::\n\nFor the following experiment, let's create two prompts (No schema vs. With schema) and two inputs (Vague quantity vs. Precise quantity). We let LLM to generate the outputs for these 4 conditions:\n\n::: panel-tabset\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprompt_old = \"\nYou are data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output is as JSON.\n\"\n\nprompt_schema = \"\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    'groceries': [\n        { 'name': ITEM_NAME, 'quantity': ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\"\n\nvague_input <- \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\nprecise_input <- \"12 eggs, 2 bottles of milk, 6 sparkling waters.\"\n\ntabset_condition <- apply(expand.grid(c(\"No_Schema\", \"With_Schema\"), c(\"Vague_Input\", \"Precise_Input\")), 1, \\(x) paste0(x[1], \"+\", x[2]))\n\ndat_prompt_input <- data.frame(\n  expand.grid(\n    prompts = c(prompt_old, prompt_schema),\n    inputs = c(vague_input, precise_input), stringsAsFactors = FALSE\n  )\n  \n)\n\ndat_output <- dat_prompt_input |> \n  rowwise() |> \n  mutate(\n    json_data =get_response(prompt = prompts, message = inputs) \n  )\n```\n:::\n\n\n\n\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprompt = \"\"\"\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    \"groceries\": [\n        { \"name\": ITEM_NAME, \"quantity\": ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\"\"\"\n\ninputs = [\n    \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\",\n    \"12 eggs, 2 bottles of milk, 6 sparkling waters.\",\n]\n\nfor message in inputs:\n    res = eval(prompt=prompt, message=message)\n    json_data = res.choices[0].message.content\n    print(json_data)\n```\n:::\n\n\n\n:::\n\nThe outputs for 4 conditions show that the condition of the prompt with schema and the input with precise quantity has the most clear LLM output.\n\n::: macwindow\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function to convert JSON to Markdown Table\"}\njson_to_markdownTBL <- function(x) {\n  dat <- as.data.frame(fromJSON(x)) # convert JSON into data.frame\n  md_dat <- paste0(kableExtra::kable(dat, format = \"simple\"), collapse =\"\\n\")\n  return(md_dat)\n}\ndata_frame_output <- sapply(dat_output$json_data, json_to_markdownTBL) |> \n  unlist()\n```\n:::\n\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Compile pieces of informatiom into output\"}\ncontent <- paste0(glue(\"\n### {tabset_condition}\n**Prompt**:\\n{dat_output$prompts}\\n\\n\n**Input**:\\n\\n{dat_output$inputs}\\n\\n\n**LLMOutput**:\\n\\n{dat_output$json_data}\\n\\n\n**R DataFrame**:\\n\\n{data_frame_output}\\n\\n\n\"), collapse = \"\")\nglue(\"::: panel-tabset\\n{content}:::\")\n```\n\n::: panel-tabset\n### No_Schema+Vague_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output is as JSON.\n\n\n\n**Input**:\n\nI'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\n\n\n**LLMOutput**:\n\n{\n  \"groceries\": [\n    \"bread\",\n    \"pack of eggs\",\n    \"apples\",\n    \"bottle of milk\"\n  ]\n}\n\n\n**R DataFrame**:\n\n groceries      \n ---------------\n bread          \n pack of eggs   \n apples         \n bottle of milk \n\n### With_Schema+Vague_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    'groceries': [\n        { 'name': ITEM_NAME, 'quantity': ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\n\n\n**Input**:\n\nI'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\n\n\n**LLMOutput**:\n\n{ \n    \"groceries\": [\n        { \"name\": \"bread\", \"quantity\": 1 }, \n        { \"name\": \"pack of eggs\", \"quantity\": 1 }, \n        { \"name\": \"apples\", \"quantity\": 3 },\n        { \"name\": \"bottle of milk\", \"quantity\": 1 }\n    ] \n}\n\n\n**R DataFrame**:\n\ngroceries.name    groceries.quantity\n---------------  -------------------\nbread                              1\npack of eggs                       1\napples                             3\nbottle of milk                     1\n\n### No_Schema+Precise_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output is as JSON.\n\n\n\n**Input**:\n\n12 eggs, 2 bottles of milk, 6 sparkling waters.\n\n\n**LLMOutput**:\n\n{\n  \"groceries\": [\n    {\n      \"item\": \"eggs\",\n      \"quantity\": 12,\n      \"unit\": \"pieces\"\n    },\n    {\n      \"item\": \"milk\",\n      \"quantity\": 2,\n      \"unit\": \"bottles\"\n    },\n    {\n      \"item\": \"sparkling waters\",\n      \"quantity\": 6,\n      \"unit\": \"bottles\"\n    }\n  ]\n}\n\n\n**R DataFrame**:\n\ngroceries.item      groceries.quantity  groceries.unit \n-----------------  -------------------  ---------------\neggs                                12  pieces         \nmilk                                 2  bottles        \nsparkling waters                     6  bottles        \n\n### With_Schema+Precise_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    'groceries': [\n        { 'name': ITEM_NAME, 'quantity': ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\n\n\n**Input**:\n\n12 eggs, 2 bottles of milk, 6 sparkling waters.\n\n\n**LLMOutput**:\n\n{\n    \"groceries\": [\n        { \"name\": \"eggs\", \"quantity\": 12 },\n        { \"name\": \"bottles of milk\", \"quantity\": 2 },\n        { \"name\": \"sparkling waters\", \"quantity\": 6 }\n    ]\n}\n\n\n**R DataFrame**:\n\ngroceries.name      groceries.quantity\n-----------------  -------------------\neggs                                12\nbottles of milk                      2\nsparkling waters                     6\n\n:::\n\n\n\n:::\n\n## Resources\n\n-   [YouTube Video](https://www.youtube.com/watch?v=0pGxoubWI6s): OpenAI Just Changed Everything (Responses API Walkthrough)\n    -   [GitHub Repo](https://github.com/daveebbelaar/ai-cookbook/tree/main/models/openai/05-responses)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}