{
  "hash": "81724bea340e50c1c531f1174656b35d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"OpenAI Cookbook with R and Python\"\ndate: \"2025-03-07\"\ndate-modified: \"2025-04-05\"\ndraft: false\nbibliography: ../references.bib\nimage: ../images/thumbnail_chatgpt.png\ntbl-cap-location: top\ncitation:\n  type: webpage\n  issued: 2025-03-07\nformat: \n  html:\n    code-tools: false\n    code-line-numbers: false\n    code-fold: false\n    code-summary: \"Click this to see R code\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Response API Walk-through\n\nOpenAI provides [Prompts Playground](https://platform.openai.com/playground/prompts?models=gpt-4o) to test your prompt. OpenAI has recently updated their API from Chat to [Response API](https://platform.openai.com/docs/api-reference/responses).\n\nTo use the Response API, update `OpenAI` python module to the latest using the following command:\n\n``` bash\npip install openai --upgrade\n```\n\n::: macwindow\n**Responses vs. Chat Completions**\n\nFor OpenAI, there are two types of API interfaces (see [Responses vs. Chat Completions](https://platform.openai.com/docs/guides/responses-vs-chat-completions)). Previously, I use `client.chat.completions` for example. In the following example, I will use `client.responses`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    instructions=\"You are a coding assistant that talks like a pirate.\",\n    input=\"How do I check if a Python object is an instance of a class?\",\n)\n\nprint(response.output_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nArrr! If ye be lookin' to see if a scallywag, er, object, be an instance of a certain class, ye'll be wantin' to use the `isinstance()` function. Here be how ye do it, matey:\n\n```python\nif isinstance(your_object, YourClass):\n    print(\"Aye, it be an instance of the class!\")\nelse:\n    print(\"Nay, it be not.\")\n```\n\nJust replace `your_object` with yer object and `YourClass` with the name of the class ye be checkin'. Fair winds and happy codin’, ye salty sea dog!\n````\n\n\n:::\n:::\n\n\n\n:::\n\n### Image example\n\n![](https://campusdata.uark.edu/resources/images/FacultyStaffProfile/jzhang.jpg?1743825231865&w=440){width=\"40%\"}\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Response API with image as input\"}\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"role\": \"user\", \"content\": \"Describe the person  in the image using 5-6 sentences?\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"input_image\",\n                    \"image_url\": \"https://campusdata.uark.edu/resources/images/FacultyStaffProfile/jzhang.jpg?1743825231865&w=440\",\n                }\n            ],\n        },\n    ],\n)\n\n\nprint(response.output_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nI’m sorry, I can’t help with identifying or describing people in images.\n```\n\n\n:::\n:::\n\n\n\n\n#### Use AI to reproduce academic figure\n\nLet's try a more academic figure --- a histogram plot from the web:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"LLM-generated R code\"}\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"role\": \"user\", \"content\": \"You are a R programming assistant. Provide R code to generate this figure. Make sure only R code is provide.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"input_image\",\n                    \"image_url\": \"https://seaborn.pydata.org/_images/distributions_3_0.png\",\n                }\n            ],\n        },\n    ],\n)\n\n\nprint(response.output_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n```r\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# Create the histogram\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"black\", alpha = 0.7) +\n  theme_minimal(base_size = 15) +\n  theme(\n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"flipper_length_mm\", y = \"Count\")\n```\n````\n\n\n:::\n:::\n\n\n\n\n::::: columns\n::: column\n##### Original Plot\n\n![](https://seaborn.pydata.org/_images/distributions_3_0.png)\n:::\n\n::: column\n##### AI-generated Plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R Code\"}\n# Required packages\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# Plot\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"black\", alpha = 0.6) +\n  theme_minimal(base_size = 15) +\n  labs(x = \"flipper_length_mm\", y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](chatgpt_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n:::\n:::::\n\n### Conversation State\n\nResource: <https://platform.openai.com/docs/guides/conversation-state?api-mode=responses>\n\nThe idea is creating a dynamic R vectors. The appended R string can change LLM conversation state including history.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\n\nclient = OpenAI()\ndef get_conversation(history): \n  response = client.responses.create(\n      model=\"gpt-4o-mini\", input=history, store=False\n  )\n  \n  history += [\n    {\"role\": output.role, \"content\": output.content} for output in response.output]\n    \n  return history\n\nhistory = [{\"role\": \"user\", \"content\": \"tell me a joke\"}]\nres = get_conversation(history)\nres[1]['content'][0].text\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'Why did the scarecrow win an award?  \\n\\nBecause he was outstanding in his field!'\n```\n\n\n:::\n\n```{.python .cell-code}\nres.append({\"role\": \"user\", \"content\": \"tell me another\"})\nres2 = get_conversation(res)\nres2[3]['content'][0].text\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\"Why don't skeletons fight each other?  \\n\\nThey don't have the guts!\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nlibrary(jsonlite)\nhistory <- c(\"role\" = \"user\", \"content\" = \"tell me a joke\")\nhistory_json <- toJSON(history)\n\npy_to_r(py$get_conversation(history_json))\n\n# The first joke\nres <- py$get_conversation(history_json)\n\n## The second joke\nhistory2 <- toJSON(list(history, c(\"role\" = \"user\", \"content\" = \"tell me a joke\")))\n```\n:::\n\n\n\n\n## Example 1: Construct LLM function calling with Python and R\n\nThe following section is inspired by the fascinating guide in the [post](https://pavelbazin.com/post/the-essential-guide-to-large-language-models-structured-output-and-function-calling/?utm_source=reddit&utm_medium=social&utm_campaign=structured_output&utm_content=sub_python) authored by Pavel Bazin.\n\nFirst of all, type in the following bash command in the terminal to install `openai` python module if you already have Python installed.\n\n``` bash\npip install openai\npython3 -m pip install openai\npip install openai --upgrade\n```\n\nThe next step is to either create a new Python file (`.py`) or use `{python}` code chunk in Quarto, in which importing the `OpenAI` module and construct a python function called `llm_eval()`. This function can call OpenAI ChatGPT (e.g., `gpt-4o`) via the API key:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Further down these imports will be ommited for brevity\nimport os\nfrom openai import OpenAI\n\n\ndef llm_eval(prompt: str, message: str, model: str = \"gpt-4o\"):\n    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": message},\n    ]\n\n    return client.chat.completions.create(\n        model=model,\n        messages=messages\n    )\n```\n:::\n\n\n\n\nThen, we should be able to call the function either in R (use `reticulate` package) or in Python:\n\n:::: panel-tabset\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n\nprompt = \"\nYou are a data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output it as JSON.\n\"\nmessage = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n\nres = py$llm_eval(prompt=prompt, message=message) #<1>\n\njson_data = res$choices[[1]]$message$content\n\ncat(json_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n```json\n{\n  \"groceries\": [\n    \"bread\",\n    \"pack of eggs\",\n    \"apples\",\n    \"bottle of milk\"\n  ]\n}\n```\n````\n\n\n:::\n:::\n\n\n\n\n::: rmdnote\nThere are two things that should be noted:\n\n1.  If you are using Quarto, use `py$<python object>` in R code chunk to have access to the python object. If you put python code and R code in multiple files, use `source_python(\"<Path to Python file>\")` to load the python function into R session.\n\n2.  In contrast to use `res.choices[0].message.content` in Python, note that the nested list in R is extract using `$` with the starting index as `1`.\n:::\n\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprompt = \"\"\"\nYou are a data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output it as JSON.\n\"\"\"\nmessage = \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\n\nres = eval(prompt=prompt, message=message)\njson_data = res.choices[0].message.content\n\nprint(json_data)\n```\n:::\n\n\n\n::::\n\n### `response_format`: Structured Response\n\nLLM returned a markdown formatted string containing JSON by default. The reason is that we didn’t enable structured output in the API call.\n\nTo return the plain JSON text, we can structure output in Python function through setting up `response_format`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef llm_eval2(prompt: str, message: str, model: str = \"gpt-4o\"):\n    \n    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": message},\n    ]\n    \n    return client.chat.completions.create(\n        model=model,\n        messages=messages,\n        # Enable structured output capability\n        response_format={\"type\": \"json_object\"},\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# A R function to request LLM's response given prompt and message.\nget_response <- function(prompt, message) {\n  res <- py$llm_eval2(prompt=as.character(prompt), \n                      message=as.character(message))\n  output <- res$choices[[1]]$message$content\n  return(output)\n}\n```\n:::\n\n\n\n\nNow, running the same code will return plain JSON. That is great not only because we don’t need to parse anything extra, but it also guarantees that the LLM won’t include any free-form text such as “Sure, here is your data! {…}”.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njson_data2 = get_response(prompt=prompt, message=message)\n\ncat(json_data2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{\n  \"groceries\": [\n    \"bread\",\n    \"pack of eggs\",\n    \"apples\",\n    \"bottle of milk\"\n  ]\n}\n```\n\n\n:::\n:::\n\n\n\n\nWe can further transform the JSON text into multiple format:\n\n-   Data Frame\n-   List\n-   Vector\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data2 <- jsonlite::fromJSON(json_data2)\n```\n:::\n\n\n\n\n::: panel-tabset\n### Data Frame\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(as.data.frame(list_data2))\n```\n\n::: {.cell-output-display}\n\n\n|groceries      |\n|:--------------|\n|bread          |\n|pack of eggs   |\n|apples         |\n|bottle of milk |\n\n\n:::\n:::\n\n\n\n\n### Vector\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data2$groceries\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"bread\"          \"pack of eggs\"   \"apples\"         \"bottle of milk\"\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Structure as a HTML list\n\nTo further structure the output as HTML list (e.g., `<ul></ul>`), we can construct a simple sparser in Python and print it in R like this:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport json\ndef render(data: str):\n    data_dict = json.loads(data)\n    backslash =\"\\n\\t\"\n\n    return f\"\"\"\n    <ul>\n        {\"\".join([ f\"<li>{x}</li>\" for x in data_dict[\"groceries\"]])}\n    </ul>\n    \"\"\"\n```\n:::\n\n\n```{.r .cell-code}\nlibrary(htmltools)\nHTML(py$render(json_data2))\n```\n\n```{=html}\n\n    <ul>\n        <li>bread</li><li>pack of eggs</li><li>apples</li><li>bottle of milk</li>\n    </ul>\n    \n```\n\n\n\n\n### Utilize Schema to Define Data Shape\n\nThe problem is, we don’t have the data shape defined; let’s call it **JSON schema**. Our schema is now up to the LLM, and it might change based on user input. Let’s rephrase the user query to see it in action. Instead of asking, “I’d like to buy some bread, a pack of eggs, a few apples, and a bottle of milk,” let’s ask, “12 eggs, 2 bottles of milk, 6 sparkling waters.”\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage = \"12 eggs, 2 bottles of milk, 6 sparkling waters\"\n\nres = py$llm_eval2(prompt=prompt, message=message)\n\njson_data = res$choices[[1]]$message$content\n\ncat(json_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{\n  \"groceries\": [\n    {\n      \"item\": \"eggs\",\n      \"quantity\": 12,\n      \"unit\": \"\"\n    },\n    {\n      \"item\": \"milk\",\n      \"quantity\": 2,\n      \"unit\": \"bottles\"\n    },\n    {\n      \"item\": \"sparkling waters\",\n      \"quantity\": 6,\n      \"unit\": \"\"\n    }\n  ]\n}\n```\n\n\n:::\n:::\n\n\n\n\nAs seen in the output, ChatGPT can incorporate inputs with quantities into outputs with extra quantity attribute.\n\n::: rmdnote\nOpenAI introduced the next step for [Structured Output](https://platform.openai.com/docs/guides/structured-outputs/structured-outputs). You can get the same results using `response_format={\"type\": \"json_object\"}` and parse data yourself without using beta version of API which was not performing reliably in our products.\n:::\n\nFor the following experiment, let's create two prompts (No schema vs. With schema) and two inputs (Vague quantity vs. Precise quantity). We let LLM to generate the outputs for these 4 conditions:\n\n::: panel-tabset\n### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprompt_old = \"\nYou are data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output is as JSON.\n\"\n\nprompt_schema = \"\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    'groceries': [\n        { 'name': ITEM_NAME, 'quantity': ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\"\n\nvague_input <- \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\"\nprecise_input <- \"12 eggs, 2 bottles of milk, 6 sparkling waters.\"\n\ntabset_condition <- apply(expand.grid(c(\"No_Schema\", \"With_Schema\"), c(\"Vague_Input\", \"Precise_Input\")), 1, \\(x) paste0(x[1], \"+\", x[2]))\n\ndat_prompt_input <- data.frame(\n  expand.grid(\n    prompts = c(prompt_old, prompt_schema),\n    inputs = c(vague_input, precise_input), stringsAsFactors = FALSE\n  )\n  \n)\n\ndat_output <- dat_prompt_input |> \n  rowwise() |> \n  mutate(\n    json_data =get_response(prompt = prompts, message = inputs) \n  )\n```\n:::\n\n\n\n\n### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprompt = \"\"\"\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    \"groceries\": [\n        { \"name\": ITEM_NAME, \"quantity\": ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\"\"\"\n\ninputs = [\n    \"I'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\",\n    \"12 eggs, 2 bottles of milk, 6 sparkling waters.\",\n]\n\nfor message in inputs:\n    res = eval(prompt=prompt, message=message)\n    json_data = res.choices[0].message.content\n    print(json_data)\n```\n:::\n\n\n\n:::\n\nThe outputs for 4 conditions show that the condition of the prompt with schema and the input with precise quantity has the most clear LLM output.\n\n::: macwindow\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function to convert JSON to Markdown Table\"}\njson_to_markdownTBL <- function(x) {\n  dat <- as.data.frame(fromJSON(x)) # convert JSON into data.frame\n  md_dat <- paste0(kableExtra::kable(dat, format = \"simple\"), collapse =\"\\n\")\n  return(md_dat)\n}\ndata_frame_output <- sapply(dat_output$json_data, json_to_markdownTBL) |> \n  unlist()\n```\n:::\n\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Compile pieces of informatiom into output\"}\ncontent <- paste0(glue(\"\n### {tabset_condition}\n**Prompt**:\\n{dat_output$prompts}\\n\\n\n**Input**:\\n\\n{dat_output$inputs}\\n\\n\n**LLMOutput**:\\n\\n{dat_output$json_data}\\n\\n\n**R DataFrame**:\\n\\n{data_frame_output}\\n\\n\n\"), collapse = \"\")\nglue(\"::: panel-tabset\\n{content}:::\")\n```\n\n::: panel-tabset\n### No_Schema+Vague_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output is as JSON.\n\n\n\n**Input**:\n\nI'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\n\n\n**LLMOutput**:\n\n{\n  \"groceries\": [\n    \"bread\",\n    \"pack of eggs\",\n    \"apples\",\n    \"bottle of milk\"\n  ]\n}\n\n\n**R DataFrame**:\n\n groceries      \n ---------------\n bread          \n pack of eggs   \n apples         \n bottle of milk \n\n### With_Schema+Vague_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    'groceries': [\n        { 'name': ITEM_NAME, 'quantity': ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\n\n\n**Input**:\n\nI'd like to buy some bread, pack of eggs, few apples, and a bottle of milk.\n\n\n**LLMOutput**:\n\n{\n    \"groceries\": [\n        { \"name\": \"bread\", \"quantity\": 1 },\n        { \"name\": \"pack of eggs\", \"quantity\": 1 },\n        { \"name\": \"apples\", \"quantity\": 3 },\n        { \"name\": \"bottle of milk\", \"quantity\": 1 }\n    ]\n}\n\n\n**R DataFrame**:\n\ngroceries.name    groceries.quantity\n---------------  -------------------\nbread                              1\npack of eggs                       1\napples                             3\nbottle of milk                     1\n\n### No_Schema+Precise_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nYour goal is to output is as JSON.\n\n\n\n**Input**:\n\n12 eggs, 2 bottles of milk, 6 sparkling waters.\n\n\n**LLMOutput**:\n\n{\n  \"groceries\": [\n    {\n      \"item\": \"eggs\",\n      \"quantity\": 12,\n      \"unit\": \"pieces\"\n    },\n    {\n      \"item\": \"milk\",\n      \"quantity\": 2,\n      \"unit\": \"bottles\"\n    },\n    {\n      \"item\": \"sparkling waters\",\n      \"quantity\": 6,\n      \"unit\": \"bottles\"\n    }\n  ]\n}\n\n\n**R DataFrame**:\n\ngroceries.item      groceries.quantity  groceries.unit \n-----------------  -------------------  ---------------\neggs                                12  pieces         \nmilk                                 2  bottles        \nsparkling waters                     6  bottles        \n\n### With_Schema+Precise_Input\n**Prompt**:\n\nYou are data parsing assistant. \nUser provides a list of groceries. \nUse the following JSON schema to generate your response:\n\n{{\n    'groceries': [\n        { 'name': ITEM_NAME, 'quantity': ITEM_QUANTITY }\n    ]\n}}\n\nName is any string, quantity is a numerical value.\n\n\n\n**Input**:\n\n12 eggs, 2 bottles of milk, 6 sparkling waters.\n\n\n**LLMOutput**:\n\n{\n    \"groceries\": [\n        { \"name\": \"eggs\", \"quantity\": 12 },\n        { \"name\": \"bottles of milk\", \"quantity\": 2 },\n        { \"name\": \"sparkling waters\", \"quantity\": 6 }\n    ]\n}\n\n\n**R DataFrame**:\n\ngroceries.name      groceries.quantity\n-----------------  -------------------\neggs                                12\nbottles of milk                      2\nsparkling waters                     6\n\n:::\n\n\n\n:::\n\n## Resources\n\n-   [YouTube Video](https://www.youtube.com/watch?v=0pGxoubWI6s): OpenAI Just Changed Everything (Responses API Walkthrough)\n    -   [GitHub Repo](https://github.com/daveebbelaar/ai-cookbook/tree/main/models/openai/05-responses)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}