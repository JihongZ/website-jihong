{
  "hash": "f6c9b98c5f15f281986b23bc226e732a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 10\"\nsubtitle: \"Generalized Measurement Models: Modeling Observed Polytomous Data\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\nexecute: \n  echo: false\n  eval: false\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: false  # choose \"false \"if want to show all together\n    transition: slide\n    background-transition: fade\n    theme: [simple, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    scrollable: true\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-annotations: below\n    code-copy: true\n    code-summary: ''\n    highlight-style: arrow\n    view: 'scroll' # Activate the scroll view\n    scrollProgress: true # Force the scrollbar to remain visible\n    mermaid:\n      theme: neutral\n#bibliography: references.bib\n---\n\n\n\n## Previous Class\n\n1.  Dive deep into factor scoring\n2.  Show how different initial values affect Bayesian model estimation\n3.  Show how parameterization differs for standardized latent variables vs. marker item scale identification\n\n## Today's Lecture Objectives\n\n1.  Show how to estimate unidimensional latent variable models with polytomous data\n    -   Also know as [Polytomous]{.underline} I[tem response theory]{.underline} (IRT) or [Item factor analysis]{.underline} (IFA)\n2.  Distributions appropriate for polytomous (discrete; data with lower/upper limits)\n\n## Example Data: Conspiracy Theories\n\n-   Today's example is from a bootstrap resample of 177 undergraduate students at a large state university in the Midwest.\n-   The survey was a measure of 10 questions about their beliefs in various conspiracy theories that were being passed around the internet in the early 2010s\n-   All item responses were on a 5-point Likert scale with:\n    1.  Strong Disagree $\\rightarrow$ 0\n    2.  Disagree $\\rightarrow$ 0\n    3.  Neither Agree nor Disagree $\\rightarrow$ 0\n    4.  Agree $\\rightarrow$ 1\n    5.  Strongly Agree $\\rightarrow$ 1\n-   The purpose of this survey was to study individual beliefs regarding conspiracies.\n-   Our purpose in using this instrument is to provide a context that we all may find relevant as many of these conspiracies are still prevalent.\n\n## From Previous Lectures: CFA (Normal Outcomes)\n\nFor comparisons today, we will be using the model where we assumed each outcome was (conditionally) normally distributed:\n\nFor an item $i$ the model is:\n\n$$\nY_{pi}=\\mu_i+\\lambda_i\\theta_p+e_{pi}\\\\\ne_{pi}\\sim N(0,\\psi_i^2)\n$$\n\nRecall that this assumption wasn't good one as the type of data (discrete, bounded, some multimodality) did not match the normal distribution assumption.\n\n## Polytomous Data Characteristics\n\nAs we have done with each observed variable, we must decide which distribution to use\n\n-   To do this, we need to map the characteristics of our data on to distributions that share those characteristics\n\nOur observed data:\n\n-   Discrete responses\n-   Small set of known categories: 1,2,3,4,5\n-   Some observed item responses may be multimodal\n\n## Discrete Data Distributions\n\n[Stan](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html) has a list of distributions for bounded discrete data\n\n1.  Binomial distribution\n    -   Pro: Easy to use / code\n    -   Con: Unimodal distribution\n2.  Beta-binomial distribution\n    -   Not often used in psychometrics (but could be)\n    -   Generalizes binomial distribution to have different probability for each trial\n3.  Hypergeometric distribution\n    -   Not often used in psychometrics\n4.  Categorical distribution (sometimes called multinomial)\n    -   Most frequently used\n    -   Base distribution for graded response, partial credit, and nominal response models\n5.  Discrete range distribution (sometimes called uniform)\n    -   Not useful–doesn't have much information about latent variables\n\n# Binomial Distribution Models\n\n## Binomial Distribution Models\n\nThe binomial distribution is one of the easiest to use for polytomous items\n\n-   However, it assumes the distribution of responses are unimodal\n\nBinomial probability mass function (i.e., pdf):\n\n$$\nP(Y=y)=\\begin{pmatrix}n\\\\y\\end{pmatrix}p^y(1-p)^{(n-y)}\n$$\n\nParameters:\n\n-   n - \"number of trials\" (range: $n \\in \\{0, 1, \\dots\\}$)\n-   y - \"number of successes\" out of $n$ \"trials\" (range: $y\\in\\{0,1,\\dots,n\\}$)\n-   p - \"probability of success\" (range: \\[0, 1\\])\n-   Mean: $np$\n-   Variance: $np(1-p)$\n\n## Probability Mass Function\n\nFixing `n = 4` and probability of \"brief in conspiracy theory\" for each item `p = {.1, .3, .5, .7}` , we can know probability mass function across each item response from 0 to 4.\n\nQuestion: which shape of distribution matches our data's empirical distribution most?\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture10_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture10_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Adapting the Binomial for Item Response Models\n\nAlthough it doesn't seem like our items fit with a binomial, we can actually use this distribution\n\n-   Item response: number of successes $y_i$\n    -   Needed: recode data so that lowest category is 0 (subtract one from each item)\n-   Highest (recoded) item response: number of trials $n$\n    -   For all out items, once recoded, $n_i = 4$\n-   Then, use a link function to model each item's $p_i$ as a function of the latent trait:\n\n$$\nP(Y_i=y_i)=\\begin{pmatrix}4\\\\y_i\\end{pmatrix}p^{y_i}(1-p)^{(4-y_i)}\n$$\n\nwhere probability of success of item $i$ for individual $p$ is as:\n\n$$\np_{pi}=\\frac{\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}{1+\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}\n$$\n\n------------------------------------------------------------------------\n\nNote:\n\n-   Shown with a logit link function (but could be any link)\n-   Shown in slope/intercept form (but could be distrimination/difficulty for unidimensional items)\n-   Could also include asymptote parameters ($c_i$ or $d_i$)\n\n## Binomial Item Response Model\n\nThe item response model, put into the PDF of the binomial is then:\n\n$$\nP(Y_{pi}|\\theta_p)=\\begin{pmatrix}n_i\\\\Y_{pi}\\end{pmatrix}(\\frac{\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}{1+\\text{exp}(\\mu_i+\\lambda_i\\theta_p)})^{y_{pi}}(1-\\frac{\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}{1+\\text{exp}(\\mu_i+\\lambda_i\\theta_p)})^{4-y_{pi}}\n$$\n\nFurther, we can use the same priors as before on each of our item parameters\n\n-   $\\mu_i$: Normal Prior $N(0, 100)$\n\n-   $\\lambda_i$: Normal prior $N(0, 100)$\n\nLikewise, we can identify the scale of the latent variable as before, too:\n\n-   $\\theta_p \\sim N(0,1)$\n\n## `model{}` for the Binomial Model in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  lambda ~ multi_normal(meanLambda, covLambda); // Prior for item discrimination/factor loadings\n  mu ~ multi_normal(meanMu, covMu);             // Prior for item intercepts\n  theta ~ normal(0, 1);                         // Prior for latent variable (with mean/sd specified)\n  for (item in 1:nItems){\n    Y[item] ~ binomial(maxItem[item], inv_logit(mu[item] + lambda[item]*theta));\n  }\n}\n```\n:::\n\n\n\nHere, the binomial [item response function]{.underline} has two arguments:\n\n-   The **first** part: (`maxItem[Item]`) is the number of \"trials\" $n_i$ (here, our maximum score minus one – 4)\n-   The **second** part: (`inv_logit(mu[item] + lambda[item]*theta)`) is the probability from our model ($p_i$)\n\nThe data `Y[item]` must be:\n\n-   Type: integer\n-   Range: 0 through `maxItem[item]`\n\n## `parameters{}` for the Binomial Model in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nparameters{\n  vector[nObs] theta;                // the latent variables (one for each person)\n  vector[nItems] mu;                 // the item intercepts (one for each item)\n  vector[nItems] lambda;             // the factor loadings/item discriminations (one for each item)\n}\n```\n:::\n\n\n\nNo changes from any of our previous slope/intercept models\n\n## `data{}` for the Binomial Model in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> nObs;                     // number of observations\n  int<lower=0> nItems;                   // number of items\n  array[nItems] int<lower=0> maxItem;    // maximum value of Item (should be 4 for 5-point Likert)\n  \n  array[nItems, nObs] int<lower=0>  Y;   // item responses in an array\n\n  vector[nItems] meanMu;                 // prior mean vector for intercept parameters\n  matrix[nItems, nItems] covMu;          // prior covariance matrix for intercept parameters\n  \n  vector[nItems] meanLambda;             // prior mean vector for discrimination parameters\n  matrix[nItems, nItems] covLambda;      // prior covariance matrix for discrimination parameters\n}\n```\n:::\n\n\n\nNote:\n\n-   Need to supply `maxItem` (maximum score minus one for each item)\n\n-   The data are the same (integer) as in the binary/dichotomous item syntax\n\n## Preparing Data for Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# note: data must start at zero\nconspiracyItemsBinomial = conspiracyItems\nfor (item in 1:ncol(conspiracyItemsBinomial)){\n  conspiracyItemsBinomial[, item] = conspiracyItemsBinomial[, item] - 1\n}\n\n# check first item\ntable(conspiracyItemsBinomial[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 0  1  2  3  4 \n49 51 47 23  7 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# determine maximum value for each item\nmaxItem = apply(X = conspiracyItemsBinomial,\n                MARGIN = 2, \n                FUN = max)\nmaxItem\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n item1  item2  item3  item4  item5  item6  item7  item8  item9 item10 \n     4      4      4      4      4      4      4      4      4      4 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### R Data List for Binomial Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Prepare data list -----------------------------\n# data dimensions\nnObs = nrow(conspiracyItems)\nnItems = ncol(conspiracyItems)\n\n# item intercept hyperparameters\nmuMeanHyperParameter = 0\nmuMeanVecHP = rep(muMeanHyperParameter, nItems)\n\nmuVarianceHyperParameter = 1000\nmuCovarianceMatrixHP = diag(x = muVarianceHyperParameter, nrow = nItems)\n\n# item discrimination/factor loading hyperparameters\nlambdaMeanHyperParameter = 0\nlambdaMeanVecHP = rep(lambdaMeanHyperParameter, nItems)\n\nlambdaVarianceHyperParameter = 1000\nlambdaCovarianceMatrixHP = diag(x = lambdaVarianceHyperParameter, nrow = nItems)\n\n\nmodelBinomial_data = list(\n  nObs = nObs,\n  nItems = nItems,\n  maxItem = maxItem,\n  Y = t(conspiracyItemsBinomial), \n  meanMu = muMeanVecHP,\n  covMu = muCovarianceMatrixHP,\n  meanLambda = lambdaMeanVecHP,\n  covLambda = lambdaCovarianceMatrixHP\n)\n```\n:::\n\n\n\n## Binomial Model Stan Call\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelBinomial_samples = modelBinomial_stan$sample(\n  data = modelBinomial_data,\n  seed = 12112022,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 5000,\n  iter_sampling = 5000,\n  init = function() list(lambda=rnorm(nItems, mean=5, sd=1))\n)\n```\n:::\n\n\n\n1.  Seed as 12112022\n2.  Number of Markov Chains as 4\n3.  Warmup Iterations as 5000\n4.  Sampling Iterations as 5000\n5.  Initial values of $\\lambda$s as $N(5, 1)$\n\n## Binomial Model Results\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# checking convergence\nmax(modelBinomial_samples$summary(.cores = 4)$rhat, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.003589\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# item parameter results\nprint(modelBinomial_samples$summary(variables = c(\"mu\", \"lambda\"), .cores = 4) ,n=Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 10\n   variable     mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n   <chr>       <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n 1 mu[1]      -0.840 -0.838 0.126 0.126 -1.05  -0.637  1.00    2506.    6240.\n 2 mu[2]      -1.84  -1.83  0.215 0.212 -2.21  -1.50   1.00    2374.    6009.\n 3 mu[3]      -1.99  -1.98  0.223 0.221 -2.38  -1.65   1.00    2662.    6596.\n 4 mu[4]      -1.73  -1.72  0.208 0.206 -2.08  -1.40   1.00    2341.    5991.\n 5 mu[5]      -2.00  -1.99  0.247 0.244 -2.43  -1.62   1.00    2046.    4880.\n 6 mu[6]      -2.10  -2.09  0.241 0.240 -2.51  -1.72   1.00    2370.    6109.\n 7 mu[7]      -2.44  -2.43  0.261 0.257 -2.90  -2.04   1.00    2735.    5943.\n 8 mu[8]      -2.16  -2.15  0.240 0.239 -2.58  -1.79   1.00    2452.    6329.\n 9 mu[9]      -2.40  -2.39  0.278 0.274 -2.88  -1.97   1.00    2412.    6524.\n10 mu[10]     -3.97  -3.93  0.517 0.511 -4.88  -3.19   1.00    3523.    7434.\n11 lambda[1]   1.11   1.10  0.143 0.142  0.881  1.35   1.00    5249.    8927.\n12 lambda[2]   1.90   1.89  0.246 0.241  1.53   2.33   1.00    4555.    6779.\n13 lambda[3]   1.92   1.90  0.254 0.253  1.52   2.36   1.00    4719.    7465.\n14 lambda[4]   1.89   1.88  0.235 0.230  1.53   2.30   1.00    4539.    7441.\n15 lambda[5]   2.28   2.26  0.281 0.277  1.84   2.76   1.00    4041.    7534.\n16 lambda[6]   2.15   2.13  0.273 0.269  1.72   2.62   1.00    4082.    6668.\n17 lambda[7]   2.13   2.11  0.293 0.292  1.68   2.64   1.00    4389.    6712.\n18 lambda[8]   2.08   2.07  0.268 0.263  1.67   2.55   1.00    4109.    6184.\n19 lambda[9]   2.35   2.33  0.315 0.308  1.87   2.90   1.00    4302.    7267.\n20 lambda[10]  3.40   3.36  0.559 0.541  2.56   4.39   1.00    4807.    6536.\n```\n\n\n:::\n:::\n\n\n\n## Option Characteristic Curves\n\nExpected probability of certain response across a range of latent variable $\\theta$\n\n![](Code/OPC_item10.png){fig-align=\"center\"}\n\n## ICC for Item 10: The expected scores with latent variable\n\n![](Code/ICC_item10.png){fig-align=\"center\"}\n\n## ICC for all items: which items have high expected score given theta\n\n![](Code/ICC_itemAll.png){fig-align=\"center\"}\n\n## Investigate Latent Variable Estimates\n\nCompare factors scores of Binomial model to 2PL-IRT\n\n![](Code/FactorScore_TwoModels.png){fig-align=\"center\"}\n\n## Comparing EAP Estimates to Sum Scores\n\n![](Code/FactorScore_Scatters.png){fig-align=\"center\"}\n\n# Categorical / Multinomial Distribution Models\n\n## Multinomial Distribution Models\n\nAlthough the binomial distribution is easy, it may not fit our data well\n\n-   Instead, we can use the categorical distribution, with following PMF:\n\n$$\nP(Y=y)=\\frac{n!}{y_i!\\dots y_C!}p_1^{y_1}\\dots p_C^{y_C}\n$$\n\nHere,\n\n-   $n$: number of \"trials\"\n\n-   $y_c$: number of events in each of $c$ categories (c $\\in \\{1, \\cdots, C\\}$; $\\Sigma_cy_c = n$)\n\n-   $p_c$: probability of observing an event in category $c$\n\n## Graded Response Model (GRM)\n\nGRM is one of the most popular model for ordered categorical response\n\n$$\nP(Y_{ic}|\\theta) = 1 - P^*(Y_{i1}|\\theta) \\ \\ \\text{if}\\ c=1\\\\\nP(Y_{ic}|\\theta) = P^*(Y_{i,c-1}|\\theta)-P^*(Y_{i,c}|\\theta) \\ \\ \\text{if}\\ 1<c<C\\\\\nP(Y_{ic}|\\theta) = P^*(Y_{i,C-1}|\\theta) \\ \\ \\text{if}\\ c=C_i\\\\\n$$where probability of response larger than $c$:\n\n$$\nP^*(Y_{i,c}|\\theta) = P(Y_{i,c}>c|\\theta)=\\frac{\\text{exp}(\\mu_{ic}+\\lambda_u\\theta_p)}{1+\\text{exp}(\\mu_{ic}+\\lambda_u\\theta_p)}\n$$\n\nWith:\n\n-   $C_i-1$ has ordered intercept intercepts: $\\mu_i > \\mu_2 > \\cdots > \\mu_{C_i-1}$\n\n## `model{}` for GRM in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  \n  lambda ~ multi_normal(meanLambda, covLambda); // Prior for item discrimination/factor loadings\n  theta ~ normal(0, 1);                         // Prior for latent variable (with mean/sd specified)\n  \n  for (item in 1:nItems){\n    thr[item] ~ multi_normal(meanThr[item], covThr[item]);             // Prior for item intercepts\n    Y[item] ~ ordered_logistic(lambda[item]*theta, thr[item]);\n  }\n  \n}\n```\n:::\n\n\n\n-   `ordered_logistic` is the built-in Stan function for ordered categorical outcomes\n\n    -   Two arguments neeeded for the function\n\n    -   `lambda[item]*theta` is the muliplications\n\n    -   `thr[item]` is the thresholds for the item, which is negative values of item intercept\n\n-   The function expects the response of Y starts from 1\n\n## `parameters{}` for GRM in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nparameters {\n  vector[nObs] theta;                // the latent variables (one for each person)\n  array[nItems] ordered[maxCategory-1] thr; // the item thresholds (one for each item category minus one)\n  vector[nItems] lambda;             // the factor loadings/item discriminations (one for each item)\n}\n```\n:::\n\n\n\nNote that we need to declare threshould as: `ordered[maxCategory-1]` so that\n\n$$\n\\tau_{i1} < \\tau_{i2} <\\cdots < \\tau_{iC-1} \n$$\n\n## `generated quantities{}` for GRM in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ngenerated quantities{\n  array[nItems] vector[maxCategory-1] mu;\n  for (item in 1:nItems){\n    mu[item] = -1*thr[item];\n  }\n}\n```\n:::\n\n\n\nConvert threshold back into item intercept\n\n## `data{}` for GRM in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> nObs;                            // number of observations\n  int<lower=0> nItems;                          // number of items\n  int<lower=0> maxCategory; \n  \n  array[nItems, nObs] int<lower=1, upper=5>  Y; // item responses in an array\n\n  array[nItems] vector[maxCategory-1] meanThr;   // prior mean vector for intercept parameters\n  array[nItems] matrix[maxCategory-1, maxCategory-1] covThr;      // prior covariance matrix for intercept parameters\n  \n  vector[nItems] meanLambda;         // prior mean vector for discrimination parameters\n  matrix[nItems, nItems] covLambda;  // prior covariance matrix for discrimination parameters\n}\n```\n:::\n\n\n\nNote that the input for the prior mean/covariance matrix for threshold parameters is now an array (one mean vector and covariance matrix per item)\n\n## Prepare Data for GRM\n\nTo match the array for input for the threshold hyperparameter matrices, a little data manipulation is needed\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# item threshold hyperparameters\nthrMeanHyperParameter = 0\nthrMeanVecHP = rep(thrMeanHyperParameter, maxCategory-1)\nthrMeanMatrix = NULL\nfor (item in 1:nItems){\n  thrMeanMatrix = rbind(thrMeanMatrix, thrMeanVecHP)\n}\n\nthrVarianceHyperParameter = 1000\nthrCovarianceMatrixHP = diag(x = thrVarianceHyperParameter, nrow = maxCategory-1)\nthrCovArray = array(data = 0, dim = c(nItems, maxCategory-1, maxCategory-1))\nfor (item in 1:nItems){\n  thrCovArray[item, , ] = diag(x = thrVarianceHyperParameter, nrow = maxCategory-1)\n}\n```\n:::\n\n\n\nR array matches Stan's array type\n\n## GRM Results\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# checking convergence\nmax(modelOrderedLogit_samples$summary(.cores = 5)$rhat, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.003504\n```\n\n\n:::\n\n```{.r .cell-code}\n# item parameter results\nprint(modelOrderedLogit_samples$summary(variables = c(\"lambda\", \"mu\"), .cores = 5) ,n=Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 10\n   variable       mean   median    sd   mad      q5     q95  rhat ess_bulk\n   <chr>         <dbl>    <dbl> <dbl> <dbl>   <dbl>   <dbl> <dbl>    <dbl>\n 1 lambda[1]    2.13     2.11   0.286 0.282   1.68   2.61    1.00    4435.\n 2 lambda[2]    3.06     3.04   0.428 0.422   2.40   3.80    1.00    4504.\n 3 lambda[3]    2.57     2.55   0.382 0.376   1.98   3.23    1.00    5053.\n 4 lambda[4]    3.09     3.07   0.423 0.416   2.44   3.82    1.00    4762.\n 5 lambda[5]    4.98     4.92   0.762 0.746   3.84   6.34    1.00    3883.\n 6 lambda[6]    5.01     4.95   0.760 0.738   3.88   6.35    1.00    4065.\n 7 lambda[7]    3.22     3.19   0.487 0.483   2.47   4.06    1.00    4260.\n 8 lambda[8]    5.33     5.26   0.863 0.839   4.06   6.87    1.00    3751.\n 9 lambda[9]    3.14     3.10   0.480 0.472   2.40   3.97    1.00    4633.\n10 lambda[10]   2.64     2.61   0.473 0.463   1.92   3.47    1.00    5129.\n11 mu[1,1]      1.49     1.48   0.286 0.285   1.03   1.97    1.00    2189.\n12 mu[2,1]      0.182    0.185  0.332 0.325  -0.374  0.712   1.00    1430.\n13 mu[3,1]     -0.447   -0.442  0.297 0.296  -0.944  0.0271  1.00    1641.\n14 mu[4,1]      0.348    0.350  0.336 0.334  -0.204  0.895   1.00    1427.\n15 mu[5,1]      0.332    0.338  0.494 0.483  -0.484  1.14    1.00    1188.\n16 mu[6,1]     -0.0321  -0.0198 0.498 0.488  -0.859  0.769   1.00    1166.\n17 mu[7,1]     -0.809   -0.793  0.360 0.353  -1.42  -0.245   1.00    1424.\n18 mu[8,1]     -0.381   -0.365  0.530 0.509  -1.28   0.454   1.00    1187.\n19 mu[9,1]     -0.740   -0.726  0.353 0.346  -1.34  -0.185   1.00    1487.\n20 mu[10,1]    -1.97    -1.95   0.394 0.385  -2.65  -1.37    1.00    2329.\n21 mu[1,2]     -0.654   -0.649  0.258 0.256  -1.09  -0.244   1.00    1816.\n22 mu[2,2]     -2.21    -2.19   0.390 0.381  -2.88  -1.61    1.00    1918.\n23 mu[3,2]     -1.67    -1.66   0.327 0.321  -2.23  -1.15    1.00    1901.\n24 mu[4,2]     -1.79    -1.78   0.366 0.361  -2.42  -1.22    1.00    1668.\n25 mu[5,2]     -3.18    -3.14   0.627 0.608  -4.28  -2.23    1.00    1807.\n26 mu[6,2]     -3.25    -3.21   0.612 0.599  -4.30  -2.31    1.00    1660.\n27 mu[7,2]     -2.72    -2.71   0.435 0.425  -3.48  -2.04    1.00    1879.\n28 mu[8,2]     -3.26    -3.22   0.649 0.637  -4.41  -2.29    1.00    1655.\n29 mu[9,2]     -2.67    -2.64   0.433 0.424  -3.42  -1.99    1.00    2109.\n30 mu[10,2]    -3.25    -3.22   0.463 0.455  -4.05  -2.54    1.00    2987.\n31 mu[1,3]     -2.51    -2.50   0.317 0.319  -3.04  -2.00    1.00    2449.\n32 mu[2,3]     -4.13    -4.11   0.500 0.494  -5.00  -3.36    1.00    3009.\n33 mu[3,3]     -4.35    -4.32   0.517 0.503  -5.26  -3.56    1.00    4073.\n34 mu[4,3]     -4.59    -4.56   0.538 0.533  -5.52  -3.75    1.00    3361.\n35 mu[5,3]     -6.04    -5.98   0.867 0.852  -7.56  -4.73    1.00    3042.\n36 mu[6,3]     -7.47    -7.39   1.02  1.00   -9.25  -5.93    1.00    3599.\n37 mu[7,3]     -5.11    -5.08   0.636 0.629  -6.21  -4.12    1.00    3421.\n38 mu[8,3]     -9.02    -8.89   1.32  1.29  -11.4   -7.07    1.00    4280.\n39 mu[9,3]     -4.00    -3.98   0.521 0.512  -4.90  -3.19    1.00    2803.\n40 mu[10,3]    -4.48    -4.44   0.560 0.551  -5.45  -3.62    1.00    4067.\n41 mu[1,4]     -4.53    -4.50   0.507 0.506  -5.39  -3.74    1.00    5718.\n42 mu[2,4]     -5.76    -5.72   0.683 0.676  -6.94  -4.69    1.00    5189.\n43 mu[3,4]     -5.52    -5.48   0.660 0.649  -6.67  -4.50    1.00    5692.\n44 mu[4,4]     -5.55    -5.52   0.635 0.623  -6.65  -4.57    1.00    4458.\n45 mu[5,4]     -8.62    -8.53   1.17  1.14  -10.7   -6.85    1.00    4420.\n46 mu[6,4]    -10.4    -10.3    1.46  1.44  -13.0   -8.23    1.00    6549.\n47 mu[7,4]     -6.87    -6.81   0.871 0.851  -8.36  -5.52    1.00    5615.\n48 mu[8,4]    -12.1    -11.9    1.91  1.86  -15.5   -9.26    1.00    7019.\n49 mu[9,4]     -5.79    -5.75   0.714 0.703  -7.04  -4.70    1.00    4707.\n50 mu[10,4]    -4.74    -4.71   0.589 0.583  -5.77  -3.83    1.00    4319.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Option Characteristics Curve\n\n![](Code/OPC_item10_GRM.png){fig-align=\"center\"}\n\n## OPC for other 9 items\n\n![](Code/OPC_item1-9_GRM.png){fig-align=\"center\"}\n\n## EAP of Factor Scores for Four Models\n\n![](Code/FactorScore_ThreeModels.png){fig-align=\"center\"}\n\n## Posterior SDs around Factor Scores\n\n![](Code/FactorScoreSD_ThreeModels.png){fig-align=\"center\"}\n\n## Resources\n\n-   [Dr. Templin's slide](https://jonathantemplin.github.io/Bayesian-Psychometric-Modeling-Course-Fall2022/lectures/lecture04d/04d_Modeling_Observed_Polytomous_Data#/binomial-model-data-block)\n",
    "supporting": [
      "Lecture10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}