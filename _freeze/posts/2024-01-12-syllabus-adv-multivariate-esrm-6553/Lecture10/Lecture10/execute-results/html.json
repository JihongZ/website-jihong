{
  "hash": "ca4af42a9687d78083b1f62eeaa904dd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 10\"\nsubtitle: \"Generalized Measurement Models: Modeling Observed Polytomous Data\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\nexecute: \n  echo: false\n  eval: false\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: false  # choose \"false \"if want to show all together\n    transition: slide\n    background-transition: fade\n    theme: [simple, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    scrollable: true\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-annotations: below\n    code-copy: true\n    code-summary: ''\n    highlight-style: arrow\n    view: 'scroll' # Activate the scroll view\n    scrollProgress: true # Force the scrollbar to remain visible\n    mermaid:\n      theme: neutral\n#bibliography: references.bib\n---\n\n\n\n## Previous Class\n\n1.  Dive deep into factor scoring\n2.  Show how different initial values affect Bayesian model estimation\n3.  Show how parameterization differs for standardized latent variables vs. marker item scale identification\n\n## Today's Lecture Objectives\n\n1.  Show how to estimate unidimensional latent variable models with polytomous data\n    -   Also know as [Polytomous]{.underline} I[tem response theory]{.underline} (IRT) or [Item factor analysis]{.underline} (IFA)\n2.  Distributions appropriate for polytomous (discrete; data with lower/upper limits)\n\n## Example Data: Conspiracy Theories\n\n-   Today's example is from a bootstrap resample of 177 undergraduate students at a large state university in the Midwest.\n-   The survey was a measure of 10 questions about their beliefs in various conspiracy theories that were being passed around the internet in the early 2010s\n-   All item responses were on a 5-point Likert scale with:\n    1.  Strong Disagree $\\rightarrow$ 0\n    2.  Disagree $\\rightarrow$ 0\n    3.  Neither Agree nor Disagree $\\rightarrow$ 0\n    4.  Agree $\\rightarrow$ 1\n    5.  Strongly Agree $\\rightarrow$ 1\n-   The purpose of this survey was to study individual beliefs regarding conspiracies.\n-   Our purpose in using this instrument is to provide a context that we all may find relevant as many of these conspiracies are still prevalent.\n\n## From Previous Lectures: CFA (Normal Outcomes)\n\nFor comparisons today, we will be using the model where we assumed each outcome was (conditionally) normally distributed:\n\nFor an item $i$ the model is:\n\n$$\nY_{pi}=\\mu_i+\\lambda_i\\theta_p+e_{pi}\\\\\ne_{pi}\\sim N(0,\\psi_i^2)\n$$\n\nRecall that this assumption wasn't good one as the type of data (discrete, bounded, some multimodality) did not match the normal distribution assumption.\n\n## Polytomous Data Characteristics\n\nAs we have done with each observed variable, we must decide which distribution to use\n\n-   To do this, we need to map the characteristics of our data on to distributions that share those characteristics\n\nOur observed data:\n\n-   Discrete responses\n-   Small set of known categories: 1,2,3,4,5\n-   Some observed item responses may be multimodal\n\n## Discrete Data Distributions\n\n[Stan](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html) has a list of distributions for bounded discrete data\n\n1.  Binomial distribution\n    -   Pro: Easy to use / code\n    -   Con: Unimodal distribution\n2.  Beta-binomial distribution\n    -   Not often used in psychometrics (but could be)\n    -   Generalizes binomial distribution to have different probability for each trial\n3.  Hypergeometric distribution\n    -   Not often used in psychometrics\n4.  Categorical distribution (sometimes called multinomial)\n    -   Most frequently used\n    -   Base distribution for graded response, partial credit, and nominal response models\n5.  Discrete range distribution (sometimes called uniform)\n    -   Not useful–doesn't have much information about latent variables\n\n# Binomial Distribution Models\n\n## Binomial Distribution Models\n\nThe binomial distribution is one of the easiest to use for polytomous items\n\n-   However, it assumes the distribution of responses are unimodal\n\nBinomial probability mass function (i.e., pdf):\n\n$$\nP(Y=y)=\\begin{pmatrix}n\\\\y\\end{pmatrix}p^y(1-p)^{(n-y)}\n$$\n\nParameters:\n\n-   n - \"number of trials\" (range: $n \\in \\{0, 1, \\dots\\}$)\n-   y - \"number of successes\" out of $n$ \"trials\" (range: $y\\in\\{0,1,\\dots,n\\}$)\n-   p - \"probability of success\" (range: \\[0, 1\\])\n-   Mean: $np$\n-   Variance: $np(1-p)$\n\n## Probability Mass Function\n\nFixing `n = 4` and probability of \"brief in conspiracy theory\" for each item `p = {.1, .3, .5, .7}` , we can know probability mass function across each item response from 0 to 4.\n\nQuestion: which shape of distribution matches our data's empirical distribution most?\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture10_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture10_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Adapting the Binomial for Item Response Models\n\nAlthough it doesn't seem like our items fit with a binomial, we can actually use this distribution\n\n-   Item response: number of successes $y_i$\n    -   Needed: recode data so that lowest category is 0 (subtract one from each item)\n-   Highest (recoded) item response: number of trials $n$\n    -   For all out items, once recoded, $n_i = 4$\n-   Then, use a link function to model each item's $p_i$ as a function of the latent trait:\n\n$$\nP(Y_i=y_i)=\\begin{pmatrix}4\\\\y_i\\end{pmatrix}p^{y_i}(1-p)^{(4-y_i)}\n$$\n\nwhere probability of success of item $i$ for individual $p$ is as:\n\n$$\np_{pi}=\\frac{\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}{1+\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}\n$$\n\n------------------------------------------------------------------------\n\nNote:\n\n-   Shown with a logit link function (but could be any link)\n-   Shown in slope/intercept form (but could be distrimination/difficulty for unidimensional items)\n-   Could also include asymptote parameters ($c_i$ or $d_i$)\n\n## Binomial Item Response Model\n\nThe item response model, put into the PDF of the binomial is then:\n\n$$\nP(Y_{pi}|\\theta_p)=\\begin{pmatrix}n_i\\\\Y_{pi}\\end{pmatrix}(\\frac{\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}{1+\\text{exp}(\\mu_i+\\lambda_i\\theta_p)})^{y_{pi}}(1-\\frac{\\text{exp}(\\mu_i+\\lambda_i\\theta_p)}{1+\\text{exp}(\\mu_i+\\lambda_i\\theta_p)})^{4-y_{pi}}\n$$\n\nFurther, we can use the same priors as before on each of our item parameters\n\n-   $\\mu_i$: Normal Prior $N(0, 100)$\n\n-   $\\lambda_i$: Normal prior $N(0, 100)$\n\nLikewise, we can identify the scale of the latent variable as before, too:\n\n-   $\\theta_p \\sim N(0,1)$\n\n## `model{}` for the Binomial Model in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  lambda ~ multi_normal(meanLambda, covLambda); // Prior for item discrimination/factor loadings\n  mu ~ multi_normal(meanMu, covMu);             // Prior for item intercepts\n  theta ~ normal(0, 1);                         // Prior for latent variable (with mean/sd specified)\n  for (item in 1:nItems){\n    Y[item] ~ binomial(maxItem[item], inv_logit(mu[item] + lambda[item]*theta));\n  }\n}\n```\n:::\n\n\n\nHere, the binomial [item response function]{.underline} has two arguments:\n\n-   The **first** part: (`maxItem[Item]`) is the number of \"trials\" $n_i$ (here, our maximum score minus one – 4)\n-   The **second** part: (`inv_logit(mu[item] + lambda[item]*theta)`) is the probability from our model ($p_i$)\n\nThe data `Y[item]` must be:\n\n-   Type: integer\n-   Range: 0 through `maxItem[item]`\n\n## `parameters{}` for the Binomial Model in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nparameters{\n  vector[nObs] theta;                // the latent variables (one for each person)\n  vector[nItems] mu;                 // the item intercepts (one for each item)\n  vector[nItems] lambda;             // the factor loadings/item discriminations (one for each item)\n}\n```\n:::\n\n\n\nNo changes from any of our previous slope/intercept models\n\n## `data{}` for the Binomial Model in Stan\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> nObs;                     // number of observations\n  int<lower=0> nItems;                   // number of items\n  array[nItems] int<lower=0> maxItem;    // maximum value of Item (should be 4 for 5-point Likert)\n  \n  array[nItems, nObs] int<lower=0>  Y;   // item responses in an array\n\n  vector[nItems] meanMu;                 // prior mean vector for intercept parameters\n  matrix[nItems, nItems] covMu;          // prior covariance matrix for intercept parameters\n  \n  vector[nItems] meanLambda;             // prior mean vector for discrimination parameters\n  matrix[nItems, nItems] covLambda;      // prior covariance matrix for discrimination parameters\n}\n```\n:::\n\n\n\nNote:\n\n-   Need to supply `maxItem` (maximum score minus one for each item)\n\n-   The data are the same (integer) as in the binary/dichotomous item syntax\n\n## Preparing Data for Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# note: data must start at zero\nconspiracyItemsBinomial = conspiracyItems\nfor (item in 1:ncol(conspiracyItemsBinomial)){\n  conspiracyItemsBinomial[, item] = conspiracyItemsBinomial[, item] - 1\n}\n\n# check first item\ntable(conspiracyItemsBinomial[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 0  1  2  3  4 \n49 51 47 23  7 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# determine maximum value for each item\nmaxItem = apply(X = conspiracyItemsBinomial,\n                MARGIN = 2, \n                FUN = max)\nmaxItem\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n item1  item2  item3  item4  item5  item6  item7  item8  item9 item10 \n     4      4      4      4      4      4      4      4      4      4 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n### R Data List for Binomial Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Prepare data list -----------------------------\n# data dimensions\nnObs = nrow(conspiracyItems)\nnItems = ncol(conspiracyItems)\n\n# item intercept hyperparameters\nmuMeanHyperParameter = 0\nmuMeanVecHP = rep(muMeanHyperParameter, nItems)\n\nmuVarianceHyperParameter = 1000\nmuCovarianceMatrixHP = diag(x = muVarianceHyperParameter, nrow = nItems)\n\n# item discrimination/factor loading hyperparameters\nlambdaMeanHyperParameter = 0\nlambdaMeanVecHP = rep(lambdaMeanHyperParameter, nItems)\n\nlambdaVarianceHyperParameter = 1000\nlambdaCovarianceMatrixHP = diag(x = lambdaVarianceHyperParameter, nrow = nItems)\n\n\nmodelBinomial_data = list(\n  nObs = nObs,\n  nItems = nItems,\n  maxItem = maxItem,\n  Y = t(conspiracyItemsBinomial), \n  meanMu = muMeanVecHP,\n  covMu = muCovarianceMatrixHP,\n  meanLambda = lambdaMeanVecHP,\n  covLambda = lambdaCovarianceMatrixHP\n)\n```\n:::\n\n\n\n## Binomial Model Stan Call\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelBinomial_samples = modelBinomial_stan$sample(\n  data = modelBinomial_data,\n  seed = 12112022,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 5000,\n  iter_sampling = 5000,\n  init = function() list(lambda=rnorm(nItems, mean=5, sd=1))\n)\n```\n:::\n\n\n\n1.  Seed as 12112022\n2.  Number of Markov Chains as 4\n3.  Warmup Iterations as 5000\n4.  Sampling Iterations as 5000\n5.  Initial values of $\\lambda$s as $N(5, 1)$\n\n## Binomial Model Results\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# checking convergence\nmax(modelBinomial_samples$summary(.cores = 4)$rhat, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.003589\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# item parameter results\nprint(modelBinomial_samples$summary(variables = c(\"mu\", \"lambda\"), .cores = 4) ,n=Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 10\n   variable     mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n   <chr>       <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n 1 mu[1]      -0.840 -0.838 0.126 0.126 -1.05  -0.637  1.00    2506.    6240.\n 2 mu[2]      -1.84  -1.83  0.215 0.212 -2.21  -1.50   1.00    2374.    6009.\n 3 mu[3]      -1.99  -1.98  0.223 0.221 -2.38  -1.65   1.00    2662.    6596.\n 4 mu[4]      -1.73  -1.72  0.208 0.206 -2.08  -1.40   1.00    2341.    5991.\n 5 mu[5]      -2.00  -1.99  0.247 0.244 -2.43  -1.62   1.00    2046.    4880.\n 6 mu[6]      -2.10  -2.09  0.241 0.240 -2.51  -1.72   1.00    2370.    6109.\n 7 mu[7]      -2.44  -2.43  0.261 0.257 -2.90  -2.04   1.00    2735.    5943.\n 8 mu[8]      -2.16  -2.15  0.240 0.239 -2.58  -1.79   1.00    2452.    6329.\n 9 mu[9]      -2.40  -2.39  0.278 0.274 -2.88  -1.97   1.00    2412.    6524.\n10 mu[10]     -3.97  -3.93  0.517 0.511 -4.88  -3.19   1.00    3523.    7434.\n11 lambda[1]   1.11   1.10  0.143 0.142  0.881  1.35   1.00    5249.    8927.\n12 lambda[2]   1.90   1.89  0.246 0.241  1.53   2.33   1.00    4555.    6779.\n13 lambda[3]   1.92   1.90  0.254 0.253  1.52   2.36   1.00    4719.    7465.\n14 lambda[4]   1.89   1.88  0.235 0.230  1.53   2.30   1.00    4539.    7441.\n15 lambda[5]   2.28   2.26  0.281 0.277  1.84   2.76   1.00    4041.    7534.\n16 lambda[6]   2.15   2.13  0.273 0.269  1.72   2.62   1.00    4082.    6668.\n17 lambda[7]   2.13   2.11  0.293 0.292  1.68   2.64   1.00    4389.    6712.\n18 lambda[8]   2.08   2.07  0.268 0.263  1.67   2.55   1.00    4109.    6184.\n19 lambda[9]   2.35   2.33  0.315 0.308  1.87   2.90   1.00    4302.    7267.\n20 lambda[10]  3.40   3.36  0.559 0.541  2.56   4.39   1.00    4807.    6536.\n```\n\n\n:::\n:::\n\n\n\n## Option Characteristic Curves\n\nExpected probability of certain response across a range of latent variable $\\theta$\n\n![](Code/OPC_item10.png){fig-align=\"center\"}\n\n## ICC for Item 10: The expected scores with latent variable\n\n![](Code/ICC_item10.png){fig-align=\"center\"}\n\n## ICC for all items: which items have high expected score given theta\n\n![](Code/ICC_itemAll.png){fig-align=\"center\"}\n\n## Investigate Latent Variable Estimates\n\nCompare factors scores of Binomial model to 2PL-IRT\n\n![](Code/FactorScore_TwoModels.png){fig-align=\"center\"}\n\n## Comparing EAP Estimates to Sum Scores\n\n![](Code/FactorScore_Scatters.png){fig-align=\"center\"}\n\n## Discriminatin/Difficulty Parameterization\n\nThe slope/intercept form\n\n## Resources\n\n-   [Dr. Templin's slide](https://jonathantemplin.github.io/Bayesian-Psychometric-Modeling-Course-Fall2022/lectures/lecture04d/04d_Modeling_Observed_Polytomous_Data#/binomial-model-data-block)\n",
    "supporting": [
      "Lecture10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}