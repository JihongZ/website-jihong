{
  "hash": "1d2e4dd99dd4bf1f9335eda855905e36",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 11\"\nsubtitle: \"Multidimensionality and Missing Data\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\nexecute: \n  echo: false\n  eval: false\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: false  # choose \"false \"if want to show all together\n    transition: slide\n    background-transition: fade\n    theme: [simple, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    scrollable: true\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-annotations: below\n    code-copy: true\n    code-summary: ''\n    highlight-style: arrow\n    view: 'scroll' # Activate the scroll view\n    scrollProgress: true # Force the scrollbar to remain visible\n    mermaid:\n      theme: neutral\n#bibliography: references.bib\n---\n\n\n\n## Previous Class\n\n1.  Show how to estimate unidimensional latent variable models with polytomous data\n    -   Also know as [Polytomous]{.underline} I[tem response theory]{.underline} (IRT) or [Item factor analysis]{.underline} (IFA)\n2.  Distributions appropriate for polytomous (discrete; data with lower/upper limits)\n\n## Today's Lecture Objectives\n\n1.  Course evaluation\n2.  How to model multidimensional factor structures\n3.  How to estimate models with missing data\n\n## Course evaluation\n\nThe course evaluation will be open on April 23 and end on May 3. It is important for me. Please make sure fill out the survey.\n\n## Example Data: Conspiracy Theories\n\n-   Today's example is from a bootstrap resample of 177 undergraduate students at a large state university in the Midwest.\n-   The survey was a measure of 10 questions about their beliefs in various conspiracy theories that were being passed around the internet in the early 2010s\n-   All item responses were on a 5-point Likert scale with:\n    1.  Strong Disagree\n    2.  Disagree\n    3.  Neither Agree nor Disagree\n    4.  Agree\n    5.  Strongly Agree\n-   The purpose of this survey was to study individual beliefs regarding conspiracies.\n-   Our purpose in using this instrument is to provide a context that we all may find relevant as many of these conspiracies are still prevalent.\n\n# Multidimensionality\n\n## More than one Latent Variable - Latent Parameter Space\n\nWe need to create latent variables by specifying which items measure which latent variables in an analysis model\n\n-   This procedure Called different names by different fields:\n\n    -   Alignment (education measurement)\n\n    -   Factor pattern matrix (factor analysis)\n\n    -   Q-matrix (Question matrix; diagnostic models and multidimensional IRT)\n\n## From Q-matrix to Model\n\nThe alignment provides a specification of which latent variables are measured by which items\n\n-   Sometimes we say items \"load onto\" factors\n\nThe math definition of either of these terms is simply whether or not a latent variable appears as a predictor for an item\n\n-   For instance, item one appears to measure nongovernment conspiracies, meaning its alignment (row vector of the Q-matrix)\n\n    ``` r\n             Gov NonGov \n    item1     0      1 \n    ```\n\n## From Q-matrix to Model (Cont.)\n\nThe model for the first item is then built with only the factors measured by the item as being present:\n\n$$\n\\newcommand{\\bmatrix}[1]{\\begin{bmatrix}#1\\end{bmatrix}}\n\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n\\newcommand{\\green}[1]{\\color{green}{#1}}\n\\newcommand{\\red}[1]{\\color{red}{#1}}\nf(E(Y_{p1}|\\bb{\\theta_p})=\\mu_1 + \\bb{0}*\\lambda_{11}\\theta_{p1} +\\bb{1}*\\lambda_{21}\\theta_{p2} \\\\=\\mu_1 + \\lambda_{21}\\theta_{p2} \n$$\n\nWhere:\n\n-   $\\mu_1$ is the item intercept\n\n-   $\\lambda_{\\bf{1}1}$ is the factor loading for item 1 (the first subscript) loaded on factor 1 (the second subscript)\n\n-   $\\theta_{p1}$ is the value of the latent variable for person *p* and factor 1\n\nThe second factor is not included in the model for the item\n\n## More Q-matrix\n\nWe could generalize the previous function with Q-matrix\n\n$$\nQ=\\begin{bmatrix}q_{11}\\ \\ \\ 0\\\\0\\ \\ \\  q_{12}\\end{bmatrix}\n$$\n\nIf item 1 is only measured by `NonGov`, we can map item responses to factor loadings and theta via Q-matrix\n\n$$\n\\begin{align}\nf(E(Y_{p1}|\\boldsymbol{\\theta_p})\n&=\\mu_1+q_{11}(\\lambda_{11}\\theta_{p1})+q_{12}(\\lambda_{12}\\theta_{p2})\\\\\n&= \\mu_1 + \\boldsymbol{\\theta_p\\text{diag}(q)\\lambda_1}\\\\\n&= \\mu_1 + [\\theta_{p1}\\ \\ \\theta_{p2}]\\begin{bmatrix}0\\ \\ 0\\\\0\\ \\ 1\\end{bmatrix}\\begin{bmatrix}\\lambda_{11}\\\\\\lambda_{12}\\end{bmatrix}\\\\\n&= \\mu_1 + [\\theta_{p1}\\ \\ \\theta_{p2}]\\begin{bmatrix}0\\\\\\lambda_{12}\\end{bmatrix}\\\\\n&= \\mu_1 + \\lambda_{12}\\theta_{p2}\n\\end{align} \n$$\n\nWhere:\n\n-   $\\boldsymbol\\lambda_1$ = $\\begin{bmatrix}\\lambda_{11}\\\\\\lambda_{12}\\end{bmatrix}$ contains all possible factor loadings for item 1 (size 2 $\\times$ 1)\n\n-   $\\boldsymbol\\theta_p=[\\theta_{p1}\\ \\ \\theta_{p2}]$ contains the factor scores for person *p*\n\n-   $\\text{diag}(\\boldsymbol q_i)=\\boldsymbol q_i \\begin{bmatrix}1\\ 0 \\\\0\\ 1 \\end{bmatrix}=[0\\ \\ 1]\\begin{bmatrix}1\\ 0 \\\\0\\ 1 \\end{bmatrix}=\\begin{bmatrix}0\\ \\ 0 \\\\0\\ \\ 1 \\end{bmatrix}$ is a diagonal matrix of ones times the vector of Q-matrix entries for item 1.\n\n## Today's Q-matrix\n\n``` r\n        Gov NonGov\nitem1    0      1\nitem2    1      0\nitem3    0      1\nitem4    0      1\nitem5    1      0\nitem6    0      1\nitem7    1      0\nitem8    1      0\nitem9    1      0\nitem10   0      1\n```\n\nGiven the Q-matrix each item has its own model using the alignment specifications.\n\n$$\nf(E(Y_{p1}|\\boldsymbol{\\theta}_p))=\\mu_1+\\lambda_{12}\\theta_{p2}\\\\\nf(E(Y_{p\\red2}|\\boldsymbol{\\theta}_p))=\\mu_1+\\lambda_{\\red{2}1}\\theta_{p\\red1}\\\\\n\\cdots\\\\\nf(E(Y_{p\\red{10}}|\\bb{\\theta}_p))=\\mu_1+\\lambda_{\\red{10,}\\green1}\\theta_{p\\red{10}}\\\\\n$$\n\n## Multidimensional Graded Response Model\n\nGRM is one of the most popular model for ordered categorical response\n\n$$\nP(Y_{ic}|\\theta) = 1 - P^*(Y_{i1}|\\theta) \\ \\ \\text{if}\\ c=1\\\\\nP(Y_{ic}|\\theta) = P^*(Y_{i,c-1}|\\theta)-P^*(Y_{i,c}|\\theta) \\ \\ \\text{if}\\ 1<c<C\\\\\nP(Y_{ic}|\\theta) = P^*(Y_{i,C-1}|\\theta) \\ \\ \\text{if}\\ c=C_i\\\\\n$$where probability of response larger than $c$:\n\n$$\nP^*(Y_{i,c}|\\theta) = P(Y_{i,c}>c|\\theta)=\\frac{\\text{exp}(\\red{\\mu_{ic}+\\lambda_{ij}\\theta_{pj}})}\n{1+\\text{exp}(\\red{\\mu_{ic}+\\lambda_{ij}\\theta_{pj}})}\n$$\n\nWith:\n\n-   *j* denotes which factor the item loads on\n-   $C_i-1$ has ordered intercept intercepts: $\\mu_i > \\mu_2 > \\cdots > \\mu_{C_i-1}$\n-   In `Stan`, we model item thresholds (difficulty) $\\tau_{c}$ =$\\mu_c$, so that $\\tau_1<\\tau_2<\\cdots<\\tau_{C_i-1}$\n\n## Stan Parameter Block\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nparameters {\n  array[nObs] vector[nFactors] theta;                // the latent variables (one for each person)\n  array[nItems] ordered[maxCategory-1] thr; // the item thresholds (one for each item category minus one)\n  vector[nLoadings] lambda;             // the factor loadings/item discriminations (one for each item)\n  cholesky_factor_corr[nFactors] thetaCorrL;\n}\n```\n:::\n\n\n\nNote:\n\n1.  `theta` is a array (for the MVN prior)\n2.  `thr` is the same as the unidimensional model\n3.  `lambda` is the vector of all factor loadings to be estimated (needs `nLoadings`)\n4.  `thetaCorrL` is of type `chelesky_factor_corr`, a built in type that identifies this as lower diagonal of a Cholesky-factorized correlation matrix\n\n## Stan Transformed Data Block\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ntransformed data{\n  int<lower=0> nLoadings = 0;                                      // number of loadings in model\n  \n  for (factor in 1:nFactors){\n    nLoadings = nLoadings + sum(Qmatrix[1:nItems, factor]);\n  }\n\n  array[nLoadings, 2] int loadingLocation;                     // the row/column positions of each loading\n  int loadingNum=1;\n  \n  for (item in 1:nItems){\n    for (factor in 1:nFactors){\n      if (Qmatrix[item, factor] == 1){\n        loadingLocation[loadingNum, 1] = item;\n        loadingLocation[loadingNum, 2] = factor;\n        loadingNum = loadingNum + 1;\n      }\n    }\n  }\n\n}\n```\n:::\n\n\n\nNote:\n\n1.  The `transformed data {}` block runs prior to the Markov Chain;\n    -   We use it to create variables that will stay constant throughout the chain\n2.  Here, we count the number of loadings in the Q-matrix `nLoadings`\n    -   We then process the Q-matrix to tell `Stan` the row and column position of each loading in `loadingMatrix` used in the `model {}` block\n3.  This syntax works for any Q-matrix (but only has main effects in the model)\n\n## Stan Transformed Parameters Block\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ntransformed parameters{\n  matrix[nItems, nFactors] lambdaMatrix = rep_matrix(0.0, nItems, nFactors);\n  matrix[nObs, nFactors] thetaMatrix;\n  \n  // build matrix for lambdas to multiply theta matrix\n  for (loading in 1:nLoadings){\n    lambdaMatrix[loadingLocation[loading,1], loadingLocation[loading,2]] = lambda[loading];\n  }\n  \n  for (factor in 1:nFactors){\n    thetaMatrix[,factor] = to_vector(theta[,factor]);\n  }\n  \n}\n```\n:::\n\n\n\nNote:\n\n1.  The `transformed parameters {}` block runs prior to each iteration of the Markov chain\n    -   This means it affects the estimation of each type of parameters\n2.  We use it to create:\n    -   `thetaMatrix` (converting `theta` from an array to a matrix)\n    -   `lambdaMatrix` (puts the loadings and zeros from the Q-matrix into correct position)\n        -   `lambdaMatrix` initialized at zero so we just have to add the loadings in the correct position\n\n## Stan Model Block - Factor Scores\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code  code-line-numbers=\"1,3-4,8\"}\nmodel {\n  lambda ~ multi_normal(meanLambda, covLambda); \n  thetaCorrL ~ lkj_corr_cholesky(1.0);\n  theta ~ multi_normal_cholesky(meanTheta, thetaCorrL);    \n  \n  for (item in 1:nItems){\n    thr[item] ~ multi_normal(meanThr[item], covThr[item]);            \n    Y[item] ~ ordered_logistic(thetaMatrix*lambdaMatrix[item,1:nFactors]', thr[item]);\n  }\n}\n```\n:::\n\n\n\n-   `thetaMatrix` is a matrix of latent variables for each person with $N \\times J$. Generated by MVN with factor mean and factor covaraince\n\n    $$\n    \\bb{\\Theta} \\sim \\text{MVM}(\\bb\\mu_\\Theta, \\Sigma_\\Theta)\n    $$\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ntransformed parameters{\n  matrix[nObs, nFactors] thetaMatrix;\n  for (factor in 1:nFactors){\n      thetaMatrix[,factor] = to_vector(theta[,factor]);\n  }\n}\n```\n:::\n\n\n\nWe need to convert `theta` from array to a matrix in `transformed parameters` block.\n\n## Stan Model Block - Factor Loadings\n\nloading location matrix `Loc`\n\n$$\nQ= \\bmatrix{\n 0\\      1\\\\ \n 1\\      0\\\\\n 0\\      1\\\\\n 0\\      1\\\\\n 1\\      0\\\\\n 0\\      1\\\\\n 1\\      0\\\\\n 1\\      0\\\\\n 1\\      0\\\\\n 1\\      1\\\\\n}\n$$\n\n$$\n\\text{Loc} = \\bmatrix{\n1\\ 2\\\\\n2\\ 1\\\\\n3\\ 2\\\\\n4\\ 2\\\\\n5\\ 1\\\\\n6\\ 2\\\\\n7\\ 1\\\\\n8\\ 1\\\\\n9\\ 1\\\\\n10\\ 1\\\\\n10\\ 2\\\\\n}\n$$\n\n$$\n\\bb\\Lambda_{[\\text{Loc}[j, 1], \\text{Loc[j,2]}]}=\\lambda_{jk}\n$$\n\nwhere j denotes the index of lambda\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code  code-line-numbers=\"1-2,8\"}\nmodel {\n  lambda ~ multi_normal(meanLambda, covLambda); \n  thetaCorrL ~ lkj_corr_cholesky(1.0);\n  theta ~ multi_normal_cholesky(meanTheta, thetaCorrL);    \n  \n  for (item in 1:nItems){\n    thr[item] ~ multi_normal(meanThr[item], covThr[item]);            \n    Y[item] ~ ordered_logistic(thetaMatrix*lambdaMatrix[item,1:nFactors]', thr[item]);\n  }\n}\n```\n:::\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ntransformed data{\n  int<lower=0> nLoadings = 0;                   // number of loadings in model\n  for (factor in 1:nFactors){\n    nLoadings = nLoadings + sum(Qmatrix[1:nItems, factor]);\n  }\n  array[nLoadings, 2] int loadingLocation;      // the row/column positions of each loading\n  int loadingNum=1;\n  for (item in 1:nItems){\n    for (factor in 1:nFactors){\n      if (Qmatrix[item, factor] == 1){\n        loadingLocation[loadingNum, 1] = item;\n        loadingLocation[loadingNum, 2] = factor;\n        loadingNum = loadingNum + 1;\n      }\n    }\n  }\n}\n```\n:::\n\n\n\n-   Create a location matrix for factor loadings `loadingLocation` with first column as item index and second column as factor index;\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ntransformed parameters{\n  matrix[nItems, nFactors] lambdaMatrix = rep_matrix(0.0, nItems, nFactors);\n  for (loading in 1:nLoadings){\n    lambdaMatrix[loadingLocation[loading,1], loadingLocation[loading,2]] = lambda[loading];\n  }\n}\n```\n:::\n\n\n\n-   `lambdaMatrix` puts the loadings and zeros from the Q-matrix into correct position\n\n## Stan Data Block\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  \n  // data specifications  =============================================================\n  int<lower=0> nObs;                            // number of observations\n  int<lower=0> nItems;                          // number of items\n  int<lower=0> maxCategory;       // number of categories for each item\n  \n  // input data  =============================================================\n  array[nItems, nObs] int<lower=1, upper=5>  Y; // item responses in an array\n\n  // loading specifications  =============================================================\n  int<lower=1> nFactors;                                       // number of loadings in the model\n  array[nItems, nFactors] int<lower=0, upper=1> Qmatrix;\n  \n  // prior specifications =============================================================\n  array[nItems] vector[maxCategory-1] meanThr;                // prior mean vector for intercept parameters\n  array[nItems] matrix[maxCategory-1, maxCategory-1] covThr;  // prior covariance matrix for intercept parameters\n  \n  vector[nItems] meanLambda;         // prior mean vector for discrimination parameters\n  matrix[nItems, nItems] covLambda;  // prior covariance matrix for discrimination parameters\n  \n  vector[nFactors] meanTheta;\n}\n```\n:::\n\n\n\nNote:\n\n1.  Changes from unidimensional model:\n    -   `meanTheta`: Factor means (hyperparameters) are added (but we will set these to zero)\n    -   `nFactors`: Number of latent variables (needed for Q-matrix)\n    -   `Qmatrix`: Q-matrix for model\n\n## Stan Generated Quantities Block\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ngenerated quantities{ \n  array[nItems] vector[maxCategory-1] mu;\n  corr_matrix[nFactors] thetaCorr;\n   \n  for (item in 1:nItems){\n    mu[item] = -1*thr[item];\n  }\n  \n  \n  thetaCorr = multiply_lower_tri_self_transpose(thetaCorrL);\n  \n}\n```\n:::\n\n\n\nNote:\n\n-   Converts thresholds to intercepts `mu`\n\n-   Creates `thetaCorr` by multiplying Cholesky-factor lower triangle with upper triangle\n\n    -   We will only use the parameters of `thetaCorr` when looking at model output\n\n## Estimation in Stan\n\nWe run `Stan` the same way we have previously:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelOrderedLogit_samples = modelOrderedLogit_stan$sample(\n  data = modelOrderedLogit_data,\n  seed = 191120221,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 2000,\n  iter_sampling = 2000,\n  init = function() list(lambda=rnorm(nItems, mean=5, sd=1))\n)\n```\n:::\n\n\n\nNote:\n\n-   Smaller chain (model takes a lot longer to run)\n-   Still need to keep loadings positive initially\n\n## Analysis Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# checking convergence\nmax(modelOrderedLogit_samples$summary(.cores = 6)$rhat, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.082978\n```\n\n\n:::\n\n```{.r .cell-code}\n# item parameter results\nprint(modelOrderedLogit_samples$summary(variables = c(\"lambda\", \"mu\", \"thetaCorr\"), .cores = 6) ,n=Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 54 × 10\n   variable          mean  median      sd     mad       q5    q95  rhat ess_bulk\n   <chr>            <dbl>   <dbl>   <dbl>   <dbl>    <dbl>  <dbl> <dbl>    <dbl>\n 1 lambda[1]       2.00    1.99   0.265   0.259     1.59    2.46   1.00   3041. \n 2 lambda[2]       2.84    2.82   0.382   0.372     2.25    3.51   1.00   2718. \n 3 lambda[3]       2.40    2.38   0.342   0.338     1.87    2.99   1.00   3657. \n 4 lambda[4]       2.93    2.90   0.389   0.380     2.32    3.60   1.00   3332. \n 5 lambda[5]       4.34    4.30   0.597   0.606     3.41    5.38   1.00   2542. \n 6 lambda[6]       4.23    4.20   0.570   0.560     3.36    5.23   1.00   2611. \n 7 lambda[7]       2.86    2.84   0.410   0.402     2.24    3.57   1.00   3546. \n 8 lambda[8]       4.14    4.10   0.566   0.552     3.26    5.12   1.00   2429. \n 9 lambda[9]       2.91    2.89   0.423   0.421     2.25    3.64   1.00   3071. \n10 lambda[10]      2.45    2.42   0.429   0.417     1.80    3.19   1.00   4312. \n11 mu[1,1]         1.88    1.87   0.291   0.292     1.42    2.37   1.00   1552. \n12 mu[2,1]         0.819   0.811  0.304   0.302     0.325   1.33   1.01    864. \n13 mu[3,1]         0.0995  0.102  0.263   0.260    -0.328   0.530  1.01   1005. \n14 mu[4,1]         0.998   0.991  0.320   0.317     0.490   1.53   1.00    893. \n15 mu[5,1]         1.32    1.31   0.430   0.421     0.641   2.05   1.01    729. \n16 mu[6,1]         0.973   0.966  0.418   0.406     0.305   1.67   1.01    741. \n17 mu[7,1]        -0.103  -0.103  0.293   0.289    -0.578   0.376  1.01    877. \n18 mu[8,1]         0.704   0.697  0.397   0.388     0.0702  1.37   1.01    760. \n19 mu[9,1]        -0.0561 -0.0587 0.297   0.294    -0.540   0.435  1.00    877. \n20 mu[10,1]       -1.36   -1.35   0.310   0.304    -1.89   -0.869  1.00   1370. \n21 mu[1,2]        -0.209  -0.206  0.235   0.229    -0.602   0.172  1.00   1015. \n22 mu[2,2]        -1.52   -1.51   0.320   0.321    -2.05   -1.01   1.00   1106. \n23 mu[3,2]        -1.10   -1.10   0.276   0.270    -1.56   -0.662  1.00   1075. \n24 mu[4,2]        -1.14   -1.13   0.312   0.311    -1.66   -0.637  1.00    940. \n25 mu[5,2]        -1.95   -1.94   0.451   0.438    -2.72   -1.24   1.00    974. \n26 mu[6,2]        -1.99   -1.98   0.431   0.428    -2.72   -1.30   1.01    947. \n27 mu[7,2]        -1.95   -1.94   0.341   0.336    -2.54   -1.41   1.00   1167. \n28 mu[8,2]        -1.82   -1.81   0.418   0.412    -2.54   -1.17   1.01    900. \n29 mu[9,2]        -1.95   -1.93   0.346   0.339    -2.53   -1.40   1.00   1251. \n30 mu[10,2]       -2.60   -2.58   0.368   0.371    -3.23   -2.02   1.00   1954. \n31 mu[1,3]        -2.03   -2.02   0.281   0.283    -2.50   -1.58   1.00   1398. \n32 mu[2,3]        -3.38   -3.36   0.412   0.409    -4.09   -2.74   1.00   1855. \n33 mu[3,3]        -3.68   -3.66   0.437   0.433    -4.43   -3.00   1.00   2948. \n34 mu[4,3]        -3.85   -3.83   0.456   0.448    -4.64   -3.13   1.00   1934. \n35 mu[5,3]        -4.56   -4.53   0.606   0.589    -5.62   -3.61   1.00   2031. \n36 mu[6,3]        -5.61   -5.59   0.675   0.682    -6.75   -4.55   1.00   2530. \n37 mu[7,3]        -4.14   -4.13   0.505   0.503    -5.01   -3.35   1.00   2298. \n38 mu[8,3]        -6.41   -6.37   0.777   0.766    -7.77   -5.20   1.00   3221. \n39 mu[9,3]        -3.24   -3.23   0.422   0.423    -3.96   -2.58   1.00   1797. \n40 mu[10,3]       -3.76   -3.74   0.451   0.450    -4.54   -3.04   1.00   2814. \n41 mu[1,4]        -3.98   -3.95   0.454   0.448    -4.76   -3.27   1.00   3838. \n42 mu[2,4]        -4.91   -4.88   0.573   0.568    -5.90   -4.02   1.00   3794. \n43 mu[3,4]        -4.76   -4.74   0.561   0.558    -5.73   -3.90   1.00   4789. \n44 mu[4,4]        -4.75   -4.73   0.548   0.542    -5.69   -3.90   1.00   2753. \n45 mu[5,4]        -6.83   -6.78   0.848   0.849    -8.31   -5.52   1.00   3543. \n46 mu[6,4]        -7.93   -7.88   0.976   0.974    -9.62   -6.40   1.00   5887. \n47 mu[7,4]        -5.70   -5.66   0.698   0.702    -6.91   -4.63   1.00   3865. \n48 mu[8,4]        -8.48   -8.43   1.10    1.07    -10.4    -6.78   1.00   6211. \n49 mu[9,4]        -4.95   -4.91   0.595   0.588    -5.98   -4.03   1.00   3195. \n50 mu[10,4]       -4.01   -3.99   0.477   0.472    -4.84   -3.27   1.00   3214. \n51 thetaCorr[1,1]  1       1      0       0         1       1     NA        NA  \n52 thetaCorr[2,1]  0.992   0.994  0.00650 0.00499   0.979   0.999  1.08     41.2\n53 thetaCorr[1,2]  0.992   0.994  0.00650 0.00499   0.979   0.999  1.08     41.2\n54 thetaCorr[2,2]  1       1      0       0         1       1     NA        NA  \n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Results: Factor Correlation\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture11_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture11_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n## More visualization: EAP Theta Estimates\n\nPlots of draws from person 1\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture11_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nRelationships of sample EAP of Factor 1 and Factor 2\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture11_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## Wrapping Up\n\n1.  Stan makes multidimensional latent variable models fairly easy to implement\n    -   LKJ priors allows for scale identification for standardized factors\n    -   Can use the code mentioned above to model any type of Q-matrix\n2.  But...\n    -   Estimation is relatively slower because latent variable correlation takes more time to converge.\n\n# Missing Data\n\n## Dealing with Missing Data in Stan\n\nIf you ever attempted to analyze missing data in `Stan`, you likely received an error message:\n\n`Error: Variable 'Y' has NA values.`\n\nThat is because, by default, `Stan` does not model missing data\n\n-   Instead, we have to get `Stan` to work with the data we have (the values that are not missing)\n\n-   That does not mean remove cases where any observed variables are missing\n\n## Example Missing Data\n\nTo make things a bit easier, I'm only turning one value into missing data (the first person's response to the first item)\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconspiracyItems = conspiracyData[,1:10]\nconspiracyItems[1, 1] = NA\n```\n:::\n\n\n\nNote that all code will work with as missing as you have\n\n-   Observed variables do have to have some values that are not missing\n\n## Stan Syntax: Multidimensional Model\n\nWe will use the previous syntax with graded response modeling.\n\nThe Q-matrix this time will be a single column vector (one dimension)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQmatrix = matrix(data = 1, nrow = ncol(conspiracyItems), ncol = 1)\n\nQmatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n [1,]    1\n [2,]    1\n [3,]    1\n [4,]    1\n [5,]    1\n [6,]    1\n [7,]    1\n [8,]    1\n [9,]    1\n[10,]    1\n```\n\n\n:::\n:::\n\n\n\n## Stan Model Block for Missing Values\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  \n  lambda ~ multi_normal(meanLambda, covLambda); \n  thetaCorrL ~ lkj_corr_cholesky(1.0);\n  theta ~ multi_normal_cholesky(meanTheta, thetaCorrL);    \n  \n  \n  for (item in 1:nItems){\n    thr[item] ~ multi_normal(meanThr[item], covThr[item]);            \n    Y[item, observed[item, 1:nObserved[item]]] ~ ordered_logistic(thetaMatrix[observed[item, 1:nObserved[item]],]*lambdaMatrix[item,1:nFactors]', thr[item]);\n  }\n  \n  \n}\n```\n:::\n\n\n\nNotes:\n\n-   Big change is in `Y`:\n\n    -   Previous: `Y[item]`\n\n    -   Now: `Y[item, observed[item,1:nObserveed[item]]]`\n\n        -   The part after the comma is a list of who provided responses to the item (input in the data block)\n\n-   Mirroring this is a change to `thetaMatrix[observed[item, 1:nObserved[item]],]`\n\n    -   Keeps only the latent variables for the persons who provide responses\n\n## Stan Data Block\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  \n  // data specifications  =============================================================\n  int<lower=0> nObs;                            // number of observations\n  int<lower=0> nItems;                          // number of items\n  int<lower=0> maxCategory;       // number of categories for each item\n  array[nItems] int nObserved;\n  array[nItems, nObs] array[nItems] int observed;\n  \n  // input data  =============================================================\n  array[nItems, nObs] int<lower=-1, upper=5>  Y; // item responses in an array\n\n  // loading specifications  =============================================================\n  int<lower=1> nFactors;                                       // number of loadings in the model\n  array[nItems, nFactors] int<lower=0, upper=1> Qmatrix;\n  \n  // prior specifications =============================================================\n  array[nItems] vector[maxCategory-1] meanThr;                // prior mean vector for intercept parameters\n  array[nItems] matrix[maxCategory-1, maxCategory-1] covThr;  // prior covariance matrix for intercept parameters\n  \n  vector[nItems] meanLambda;         // prior mean vector for discrimination parameters\n  matrix[nItems, nItems] covLambda;  // prior covariance matrix for discrimination parameters\n  \n  vector[nFactors] meanTheta;\n}\n```\n:::\n\n\n\n## Data Block Notes\n\n1.  Two new arrays added:\n    -   `array[nItems] int Observed` : The number of observations with non-missing data for each item\n    -   `array[nItems, nObs] array[nItems] int observed` : A listing of which observations have non-missing data for each item\n        -   Here, the size of the array is equal to the size of the data matrix\n        -   If there were no missing data at all, the listing of observations with non-missing data would equal this size\n2.  Stan uses these arrays to only model data that are not missing\n    -   The values of observed serve to select only cases in Y that are not missing\n\n## Building Non-Missing Data Arrays\n\nTo build these arrays, we can use a loop in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved = matrix(data = -1, nrow = nrow(conspiracyItems), ncol = ncol(conspiracyItems))\nnObserved = NULL\nfor (variable in 1:ncol(conspiracyItems)){\n  nObserved = c(nObserved, length(which(!is.na(conspiracyItems[, variable]))))\n  observed[1:nObserved[variable], variable] = which(!is.na(conspiracyItems[, variable]))\n}\n```\n:::\n\n\n\nFor the item that has the first case missing, this gives:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnObserved[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 176\n```\n\n\n:::\n\n```{.r .cell-code}\nobserved[,1] # row as person, column as number of observation of this item\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1]   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n [19]  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n [37]  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n [55]  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n [73]  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n [91]  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n[109] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n[127] 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n[145] 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n[163] 164 165 166 167 168 169 170 171 172 173 174 175 176 177  -1\n```\n\n\n:::\n:::\n\n\n\nThe item has 176 observed responses and one missing\n\n-   Entries 1 through 176 of `observed[,1]` list who has non-missing data\n\n-   The 177th entry of `observed[,1]` is -1 (but won't be used in Stan)\n\n## Array Indexing\n\nWe can use the values of `observed[,1]` to have Stan only select the corresponding data points that are non-missing\n\nTo demonstrate, in R, here is all of the data for the first item\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconspiracyItems[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] NA  3  4  2  2  1  4  2  3  2  1  3  2  1  2  2  2  2  3  2  1  1  1  1  4\n [26]  4  4  3  4  3  3  1  2  1  2  3  1  2  1  2  1  1  2  2  4  3  3  1  1  4\n [51]  1  2  1  3  1  1  2  1  4  2  2  2  1  5  3  2  3  3  1  3  2  1  2  1  1\n [76]  2  3  4  3  3  2  2  1  3  3  1  2  3  1  4  2  1  2  5  5  2  3  1  3  2\n[101]  3  5  2  4  1  3  3  4  3  2  2  4  3  3  4  3  2  2  1  1  3  4  3  2  1\n[126]  4  3  2  2  3  4  5  1  5  3  1  3  3  3  2  2  1  1  3  1  3  3  4  1  1\n[151]  4  1  4  2  1  1  1  2  2  1  4  2  1  2  4  1  2  5  3  2  1  3  3  3  2\n[176]  3  3\n```\n\n\n:::\n:::\n\n\n\nAnd here, we select the non-missing using the index values in `observed` :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved[1:nObserved[1], 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1]   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n [19]  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n [37]  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n [55]  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n [73]  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n [91]  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n[109] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n[127] 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n[145] 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n[163] 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n```\n\n\n:::\n\n```{.r .cell-code}\nconspiracyItems[observed[1:nObserved, 1], 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 3 4 2 2 1 4 2 3 2 1 3 2 1 2 2 2 2 3 2 1 1 1 1 4 4 4 3 4 3 3 1 2 1 2 3 1 2\n [38] 1 2 1 1 2 2 4 3 3 1 1 4 1 2 1 3 1 1 2 1 4 2 2 2 1 5 3 2 3 3 1 3 2 1 2 1 1\n [75] 2 3 4 3 3 2 2 1 3 3 1 2 3 1 4 2 1 2 5 5 2 3 1 3 2 3 5 2 4 1 3 3 4 3 2 2 4\n[112] 3 3 4 3 2 2 1 1 3 4 3 2 1 4 3 2 2 3 4 5 1 5 3 1 3 3 3 2 2 1 1 3 1 3 3 4 1\n[149] 1 4 1 4 2 1 1 1 2 2 1 4 2 1 2 4 1 2 5 3 2 1 3 3 3 2 3 3\n```\n\n\n:::\n:::\n\n\n\nThe values of `observed[1:nObserved,1]` leads to only using the non-missing data\n\n## Change Missing NA to Nonsense Values\n\nFinally, we must ensure all data into Stan have no NA values\n\n-   Dr. Templin's recommendation: Change all NA values to something that cannot be modeled\n\n    -   Picking `-1` here: it cannot be used with `ordered_logit()` likelihood\n\n-   This ensures that Stan won't model the data by accident\n\n    -   But, we must remember this if we are using the data in other steps like PPMC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fill in NA values in Y\nY = conspiracyItems\nfor (variable in 1:ncol(conspiracyItems)){\n  Y[which(is.na(Y[,variable])),variable] = -1\n}\n```\n:::\n\n\n\n## Running Stan\n\nWith our missing values denotes, we can run Stan as we have previously\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelOrderedLogit_data = list(\n  nObs = nObs,\n  nItems = nItems,\n  maxCategory = maxCategory,\n  nObserved = nObserved,\n  observed = t(observed),\n  Y = t(Y), \n  nFactors = ncol(Qmatrix),\n  Qmatrix = Qmatrix,\n  meanThr = thrMeanMatrix,\n  covThr = thrCovArray,\n  meanLambda = lambdaMeanVecHP,\n  covLambda = lambdaCovarianceMatrixHP,\n  meanTheta = thetaMean\n)\n\n\nmodelOrderedLogit_samples = modelOrderedLogit_stan$sample(\n  data = modelOrderedLogit_data,\n  seed = 191120221,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 2000,\n  iter_sampling = 2000,\n  init = function() list(lambda=rnorm(nItems, mean=5, sd=1))\n)\n```\n:::\n\n\n\n## Data Analysis Results\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# checking convergence\nmax(modelOrderedLogitNoMiss_samples$summary(.cores = 6)$rhat, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.008265\n```\n\n\n:::\n\n```{.r .cell-code}\n# item parameter results\nprint(modelOrderedLogitNoMiss_samples$summary(variables = c(\"lambda\", \"mu\"), .cores = 6) ,n=Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 10\n   variable     mean  median    sd   mad       q5    q95  rhat ess_bulk ess_tail\n   <chr>       <dbl>   <dbl> <dbl> <dbl>    <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n 1 lambda[1]  2.01    2.00   0.267 0.264   1.59    2.46   1.00    1816.    3503.\n 2 lambda[2]  2.85    2.83   0.381 0.374   2.26    3.51   1.00    2018.    3514.\n 3 lambda[3]  2.39    2.37   0.342 0.336   1.86    2.98   1.01    2224.    3805.\n 4 lambda[4]  2.90    2.88   0.386 0.383   2.31    3.57   1.00    2112.    4057.\n 5 lambda[5]  4.30    4.26   0.604 0.594   3.38    5.36   1.00    1686.    3370.\n 6 lambda[6]  4.18    4.15   0.555 0.542   3.32    5.13   1.00    1948.    3628.\n 7 lambda[7]  2.88    2.86   0.417 0.413   2.24    3.59   1.00    1979.    3752.\n 8 lambda[8]  4.16    4.13   0.565 0.548   3.30    5.16   1.00    1805.    3518.\n 9 lambda[9]  2.90    2.87   0.429 0.431   2.24    3.64   1.00    2515.    4050.\n10 lambda[1…  2.41    2.39   0.421 0.408   1.76    3.17   1.00    2734.    3889.\n11 mu[1,1]    1.87    1.87   0.287 0.286   1.41    2.36   1.00    1328.    2908.\n12 mu[2,1]    0.805   0.798  0.304 0.299   0.317   1.32   1.01     698.    1979.\n13 mu[3,1]    0.0942  0.0880 0.255 0.249  -0.322   0.525  1.01     855.    2328.\n14 mu[4,1]    0.982   0.976  0.309 0.300   0.487   1.50   1.01     825.    2510.\n15 mu[5,1]    1.29    1.28   0.418 0.422   0.629   1.98   1.01     545.    1641.\n16 mu[6,1]    0.947   0.935  0.392 0.388   0.324   1.61   1.01     651.    2064.\n17 mu[7,1]   -0.116  -0.115  0.288 0.284  -0.591   0.358  1.01     743.    1984.\n18 mu[8,1]    0.680   0.672  0.387 0.380   0.0485  1.33   1.01     536.    1353.\n19 mu[9,1]   -0.0700 -0.0693 0.295 0.295  -0.560   0.415  1.01     718.    2146.\n20 mu[10,1]  -1.35   -1.34   0.299 0.294  -1.86   -0.883  1.01     984.    3276.\n21 mu[1,2]   -0.197  -0.194  0.234 0.231  -0.584   0.191  1.01    1039.    2640.\n22 mu[2,2]   -1.53   -1.53   0.315 0.311  -2.06   -1.04   1.00    1022.    2850.\n23 mu[3,2]   -1.10   -1.10   0.270 0.267  -1.56   -0.665  1.00     811.    2432.\n24 mu[4,2]   -1.13   -1.12   0.308 0.305  -1.65   -0.638  1.01     809.    2367.\n25 mu[5,2]   -1.95   -1.94   0.434 0.431  -2.68   -1.26   1.00     951.    2479.\n26 mu[6,2]   -1.97   -1.96   0.419 0.412  -2.68   -1.31   1.01     800.    2150.\n27 mu[7,2]   -1.96   -1.95   0.336 0.338  -2.54   -1.43   1.00     906.    2466.\n28 mu[8,2]   -1.84   -1.83   0.408 0.410  -2.53   -1.19   1.01     807.    2086.\n29 mu[9,2]   -1.95   -1.94   0.339 0.330  -2.52   -1.40   1.01    1076.    2510.\n30 mu[10,2]  -2.58   -2.57   0.356 0.351  -3.19   -2.03   1.00    1800.    4116.\n31 mu[1,3]   -2.03   -2.02   0.281 0.280  -2.51   -1.58   1.00    1417.    3411.\n32 mu[2,3]   -3.40   -3.38   0.402 0.403  -4.08   -2.75   1.00    1660.    3827.\n33 mu[3,3]   -3.67   -3.66   0.427 0.426  -4.40   -3.00   1.00    2626.    5222.\n34 mu[4,3]   -3.82   -3.80   0.446 0.442  -4.59   -3.12   1.00    1672.    4166.\n35 mu[5,3]   -4.53   -4.50   0.585 0.583  -5.53   -3.61   1.00    1842.    3811.\n36 mu[6,3]   -5.58   -5.54   0.678 0.673  -6.74   -4.54   1.00    1989.    4938.\n37 mu[7,3]   -4.16   -4.14   0.498 0.493  -5.00   -3.37   1.00    1939.    4002.\n38 mu[8,3]   -6.43   -6.39   0.776 0.762  -7.79   -5.21   1.00    2306.    4827.\n39 mu[9,3]   -3.23   -3.22   0.412 0.409  -3.94   -2.58   1.00    1505.    3950.\n40 mu[10,3]  -3.74   -3.72   0.453 0.456  -4.50   -3.03   1.00    2764.    5306.\n41 mu[1,4]   -3.98   -3.96   0.464 0.457  -4.78   -3.24   1.00    3660.    5915.\n42 mu[2,4]   -4.93   -4.90   0.575 0.580  -5.89   -4.02   1.00    3111.    5402.\n43 mu[3,4]   -4.76   -4.73   0.561 0.553  -5.72   -3.88   1.00    3842.    6058.\n44 mu[4,4]   -4.72   -4.70   0.541 0.540  -5.65   -3.86   1.00    2396.    4966.\n45 mu[5,4]   -6.77   -6.71   0.822 0.818  -8.19   -5.51   1.00    3176.    4958.\n46 mu[6,4]   -7.88   -7.83   0.973 0.969  -9.57   -6.39   1.00    4493.    5380.\n47 mu[7,4]   -5.71   -5.67   0.694 0.685  -6.92   -4.63   1.00    3581.    5497.\n48 mu[8,4]   -8.49   -8.42   1.08  1.08  -10.3    -6.81   1.00    4347.    5604.\n49 mu[9,4]   -4.92   -4.88   0.585 0.577  -5.94   -4.03   1.00    2817.    5381.\n50 mu[10,4]  -3.99   -3.98   0.480 0.479  -4.82   -3.25   1.00    2984.    5612.\n```\n\n\n:::\n:::\n\n\n\n## Wrapping Up\n\nToday, we showed how to skip over missing data in Stan\n\n-   Slight modification needed to syntax\n\n-   Assumes missing at random\n\nOf note, we could (but didn't) also build models for missing data in Stan\n\n-   Using the transformed parameters block\n\nFinally, Stan's missing data methods are quite different from JAGS\n\n-   JAGS imputes any missing data at each step of a Markov chain using Gibbs sampling.\n\n## Resources\n\n-   [Dr. Templin's slide](https://jonathantemplin.github.io/Bayesian-Psychometric-Modeling-Course-Fall2022/lectures/lecture04e/04e_Modeling_Multidimensional_Latent_Variables#/from-q-matrix-to-model-1)\n",
    "supporting": [
      "Lecture11_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}