{
  "hash": "5a0607c07a3117f56d22417468193db3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 06\"\nsubtitle: \"Generalized Measurement Models: An Introduction\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\n  data-background-opacity: \"0.9\"\nexecute: \n  echo: true\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: true  # choose \"false \"if want to show all together\n    theme: [serif, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    transition: slide\n    background-transition: fade\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-link: true\n    code-annotations: hover\n    code-copy: true\n    highlight-style: arrow\n    code-block-border-left: true\n    code-block-background: \"#b22222\"\n    mermaid:\n      theme: neutral\n#bibliography: references.bib\n---\n\n\n\n\n\n\n## Today's Lecture Objectives\n\n1.  Introduce measurement (psychometric) models in general\n\n2.  Describe the steps needed in a psychometric model analysis\n\n3.  Dive deeper into the observed-variables modeling aspect\n\n# Measurement Model in general\n\n## Measurement Model Analysis Steps\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nsubgraph \"Measurement Procedure\"\n  subgraph Modeling\n  direction LR\n  Start --> id1\n  id1([Specify model]) --> id2([\"`Specify scale identification \n  method for latent variables`\"])\n  id2 --> id3([Estimate model])\n  id3 --> id4{Adequate fit indices}\n  id4 -- No --> id1\n  end\n  Modeling -- Model fit acceptable --> one\n  subgraph one[\"Evaluation: Measurement Model with Auxiliary Components\"]\n    direction LR\n    id5([\"Score estimation \n    and secondary analyses with scores\"]) --> id6([Item evaluation])\n    id6 --> id7([Scale construction])\n    id7 --> id8([Equating])\n    id8 --> id9([Measure Invariance/Differential item functioning])\n    id9 --> End\n  end\nend\nstyle Start fill:#f9f,stroke:#333,stroke-width:5px\nstyle End fill:#bbf,stroke:#333,stroke-width:5px\n```\n\n\n\n\n\n\n## Components of a Measurement Model\n\nThere are two components of a measurement model\n\n**Theory (what we cannot see but assume its existence):**\n\n::: nonincremental\n-   Latent variable(s)\n\n-   Other effects as needed by a model\n\n    ::: nonincremental\n    -   Random effects (e.g., initial status and slopes in Latent Growth Model)\n\n    -   Testlet effects (e.g., a group-level variation among items)\n\n    -   Effects of observed variables (e.g., gender differences, DIF, Measurement Invariance)\n    :::\n:::\n\n**Data (what we can see and we assume generated by theory):**\n\n::: nonincremental\n-   Outcomes\n\n    -   An assumed distribution for each outcome\n\n    -   A key statistic of outcome for the model (e.g., mean, sd)\n\n    -   A link function\n:::\n\n------------------------------------------------------------------------\n\n### General form for measurement model (SEM, IRT):\n\n$$\nf(E(\\mathbf{Y}\\mid\\Theta)) = \\boldsymbol{\\mu} +\\Theta\\Lambda^T\n$$\n\nand\n\n$$\n\\Lambda_j = Q \\odot \\boldsymbol{\\lambda_j}\n$$\n\nAssume N as sample size, P as number of factors, J as number of items. Then,\n\n::: nonincremental\n-   $f()$: link function. CFA: identity link; IRT: logistic/probit link\n-   $E(\\mathbf{Y}\\mid\\Theta)$: Expected/Predicted values of outcomes\n-   $\\Theta$: latent factor scores matrix (N $\\times$ P)\n-   $\\Lambda$: A factor loading matrix (J $\\times$ P)\n    -   $\\Lambda_j$: $j$th row vector of factor loading matrix\n\n    -   $\\textbf{Q}$: Q-matrix represents the connections between items and latent variables\n\n    -   $\\boldsymbol{\\lambda}_j$: a vector of factor loading vectors for item $j$\n-   $\\mu$: item intercepts (J $\\times$ 1)\n:::\n\n------------------------------------------------------------------------\n\n### Example 1 with general form\n\n::: columns\n::: {.column width=\"70%\"}\nLet's consider a measurement model with only one latent variable and five items:\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart TD\n  id1((θ)) --> id2[\"Y1\"]\n  id1 --> id3[\"Y2\"]\n  id1 --> id4[\"Y3\"]\n  id1 --> id5[\"Y4\"]\n  id1 --> id6[\"Y5\"]\n```\n\n\n\n\n\n\nThe model shows:\n\n::: nonincremental\n-   One latent variable ($\\theta$)\n\n-   Five observed variables ($\\mathbf{Y} = \\{Y_1, Y_2, Y_3, Y_4, Y_5\\}$)\n:::\n:::\n\n::: {.column width=\"30%\"}\nThen,\n\n-   $\\Theta$ = $\\begin{bmatrix} \\theta_1, \\\\\\theta_2,\\\\ \\cdots,\\\\ \\theta_N \\end{bmatrix}$\n\n-   $\\Lambda^T$ = $\\begin{bmatrix}\\lambda_1, \\lambda_2, ..., \\lambda_5 \\end{bmatrix}$\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Example 2 with general form\n\n::: columns\n::: {.column width=\"70%\"}\nLet's consider a measurement model with only two latent variables and five items:\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart TD\n  theta1((θ1)) --> id2[\"Y1\"]\n  theta1 --> id3[\"Y2\"]\n  theta1 --> id4[\"Y3\"]\n  theta1 --> id5[\"Y4\"]\n  theta1 --> id6[\"Y5\"]\n  theta2((θ2)) --> id2[\"Y1\"]\n  theta2 --> id3[\"Y2\"]\n  theta2 --> id4[\"Y3\"]\n  theta2 --> id5[\"Y4\"]\n  theta2 --> id6[\"Y5\"]\n```\n\n\n\n\n\n\nThe model shows:\n\n::: nonincremental\n-   Two latent variables ($\\theta_1$, $\\theta_2$)\n\n-   Five observed variables ($\\mathbf{Y} = \\{Y_1, Y_2, Y_3, Y_4, Y_5\\}$)\n:::\n:::\n\n::: {.column width=\"30%\"}\nThen,\n\n-   $\\Theta$ = $\\begin{bmatrix} \\theta_{1,1}, \\theta_{1,2}\\\\\\theta_{2,1}, \\theta_{2,2}\\\\ \\cdots,\\cdots \\\\ \\theta_{N, 1}, \\theta_{N,2} \\end{bmatrix}$ $\\sim [0, \\Sigma]$\n\n-   $\\Lambda^T$ = $\\begin{bmatrix}\\lambda_{1,1}, \\lambda_{1,2}, ..., \\lambda_{1,5}\\\\\\lambda_{2,1}, \\lambda_{2,2}, ..., \\lambda_{2,5}\\end{bmatrix}$\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Example 3 with general form\n\n::: columns\n::: {.column width=\"70%\"}\nLet's consider a measurement model with only two latent variables and five items:\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart TD\n  theta1((θ1)) --> id2[\"Y1\"]\n  theta1 --> id3[\"Y2\"]\n  theta1 --> id4[\"Y3\"]\n  theta2((θ2)) --> id4[\"Y3\"]\n  theta2 --> id5[\"Y4\"]\n  theta2 --> id6[\"Y5\"]\n```\n\n\n\n\n\n\nThe model shows:\n\n::: nonincremental\n-   Two latent variables ($\\theta_1$, $\\theta_2$)\n\n-   Five observed variables ($\\mathbf{Y} = \\{Y_1, Y_2, Y_3, Y_4, Y_5\\}$)\n:::\n:::\n\n::: {.column width=\"30%\"}\nThen,\n\n-   $\\Theta$ = $\\begin{bmatrix} \\theta_{1,1}, \\theta_{1,2}\\\\\\theta_{2,1}, \\theta_{2,2}\\\\ \\cdots,\\cdots \\\\ \\theta_{N, 1}, \\theta_{N,2} \\end{bmatrix}$ $\\sim [0, \\Sigma]$\n\n-   $\\Lambda^T$ = $\\begin{bmatrix}\\lambda_{1,1}, \\lambda_{1,2}, \\lambda_{1,3}, 0,0\\\\ 0, 0, \\lambda_{2,3}, \\lambda_{2,4}, \\lambda_{2,5}\\end{bmatrix}$\n\n-   Note that we only limit our model to main-effect models. Interaction effects of factors introduce more complexity.\n\n-   It is difficulty to specify factor loadings with $0$s directly\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Item-specific form\n\nFor each item j:\n\n$$\n\\mathbf{Y_j} \\sim N(\\mu_j+ \\boldsymbol{\\lambda}_{j}Q_j\\Theta, \\psi_j)\n$$\n\n## Bayesian view: latent variables\n\nLatent variables in Bayesian are built by following specification:\n\n1.  What are their distributions? (normal distribution or others)\n\n    -   For example, $\\theta_i$ values for one person and $\\theta$ values for samples. Factor score $\\theta$ is a mixture distribution of distributions of each individual's factor score $\\theta_i$\n\n    -   But, in MLE/WSLMV, we do not estimate mean and sd of each individual's factor score for model to be converged\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nset.seed(12)\nN = 15\nndraws = 200\nFS = matrix(NA, nrow = N, ndraws)\nFS_p = rnorm(N)\nFS_p = FS_p[order(FS_p)]\nfor (i in 1:N) {\n  FS[i,] = rnorm(ndraws, mean = FS_p[i], sd = 1)\n}\nFS_plot <- as.data.frame(t(FS))\ncolnames(FS_plot) <- paste0(\"Person\", 1:N)\nFS_plot <- FS_plot |> pivot_longer(everything(), names_to = \"Person\", values_to = \"Factor Score\")\nFS_plot$Person <- factor(FS_plot$Person, levels = paste0(\"Person\", 1:N))\nggplot() +\n  geom_density(aes(x = `Factor Score`, fill = Person, col = Person ), alpha = .5, data = FS_plot)  +\n  geom_density(aes(x = FS_p))\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n2.  Multidimensionality\n    -   How many factors to be measured?\n    -   If $\\geq$ 2 factors, we specify mean vectors and variance-covariance matrix\n    -   To link latent variables with observed variables, we need to create a indicator matrix of coefficient/effects of latent variable on items.\n        -   In diagnostic modeling and multidimensional IRT, we call it Q-matrix\n\n## Simulation Study 1\n\n-   Let's perform a small simulation study to see how to perform factor analysis in naive Stan.\n-   The model specification is a two-factor with each measured by 3 items. In total, there are 6 items with continuous responses. Sample size is 1000.\n\n![Model Specification](simulation_model1.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Data Simulation\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nN <- 1000\nJ <- 6\n# parameters\npsi <- .3 # factor correlation\nsigma <- .1 # residual varaince\nFS <- mvtnorm::rmvnorm(N, mean = c(0, 0), sigma = matrix(c(1, psi, psi, 1), 2, 2, byrow = T))\nLambda <- matrix(\n  c(\n    0.7, 0,\n    0.5, 0,\n    0.3, 0,\n    0, 0.7,\n    0, 0.5,\n    0, 0.3\n  ), 6, 2,\n  byrow = T\n)\nmu <- matrix(rep(0.1, J), nrow = 1, byrow = T)\nresidual <- mvtnorm::rmvnorm(N, mean = rep(0, J), sigma = diag(sigma^2, J))\nY <- t(apply(FS %*% t(Lambda), 1, \\(x) x + mu)) + residual\nQ <- matrix(\n  c(\n    1, 0,\n    1, 0,\n    1, 0,\n    0, 1,\n    0, 1,\n    0, 1\n  ), 6, 2,\n  byrow = T\n)\nloc <- Q |>\n  as.data.frame() |>\n  rename(`1` = V1, `2` = V2) |> \n  rownames_to_column(\"Item\") |>\n  pivot_longer(c(`1`, `2`), names_to = \"Theta\", values_to = \"q\") |> \n  mutate(across(Item:q, as.numeric)) |> \n  filter(q == 1) |> \n  as.matrix()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]        [,2]         [,3]       [,4]        [,5]        [,6]\n[1,] -0.8030701 -0.48545470 -0.256368448  0.2829436 -0.02007136  0.02274601\n[2,]  0.4271083  0.50926367  0.270175274 -1.5916892 -1.05896321 -0.64570802\n[3,]  0.4803976  0.40621781  0.122951997  0.5982296  0.42932001  0.29298711\n[4,] -0.3292829 -0.18227739  0.002010588 -0.3921847 -0.20918407  0.19121259\n[5,] -0.4938891 -0.21694927 -0.251691793 -0.5544642 -0.28519849 -0.42496295\n[6,] -0.3946768  0.01379814 -0.164355671 -0.7838352 -0.38385332 -0.29007225\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### Strategies for Stan: factor loadings\n\n::: nonincremental\n-   We will iterate over item response function across each item\n-   To benefit from the efficiency of vectorization, we specify a vector of factor loadings with length 6\n    -   $\\{\\lambda_1, \\lambda_2, \\cdots, \\lambda_6 \\}$\n\n    -   Optionally, a matrix with number of items by number of factor for $\\lambda$s can be specified like $\\lambda_{11}$ and $\\lambda_{62}$, that introduces flexibility but complexity\n-   We should have a location table telling `stan` the information about which factor each factor loading belong to\n    -   For example, $\\lambda_{1}$ belongs to factor 1 and $\\lambda_4$ belong to factor 2\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Item Theta q\n[1,]    1     1 1\n[2,]    2     1 1\n[3,]    3     1 1\n[4,]    4     2 1\n[5,]    5     2 1\n[6,]    6     2 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### Strategies for Stan: prior distribution and hyperparameters\n\n::: columns\n::: {.column width=\"80%\"}\n::: incremental\n-   residual variances for items: $\\sigma \\sim \\text{exponential}(sigmaRate)$\n    -   sigmaRate is set to 1\n-   item intercepts: $\\mu \\sim \\text{MVN}(meanMu, covMu)$\n    -   meanMu is set to a vector of 0s with length 6\n\n    -   covMu is set to a diagonol matrix of 1000 with 6 $\\times$ 6\n-   factor scores: $\\Theta \\sim \\text{MVN}(meanTheta, corrTheta)$\n    -   meanTheta is set to a vector of 0s with length 2\n\n    -   $corrTheta \\sim lkj\\_corr(eta)$ and eta is set to 1\n\n    -   Optionally, $L \\sim lkj\\_corr\\_cholesky(eta)$ and corrTheta = LL'\n-   factor loadings: $\\Lambda \\sim \\text{MVN}(meanLambda, covLambda)$\n    -   meanLambda is set to a vector of 0s with length 6 (number of items)\n\n    -   covLambdais set to a matrix of $\\begin{pmatrix}1000, 0,\\cdots,0\\\\ \\cdots\\\\0, 0, \\cdots1000\\end{pmatrix}$\n:::\n:::\n\n::: {.column width=\"20%\"}\n-   **Question: what about factor correlation** $\\psi$?\n\n-   See [this blog](http://stla.github.io/stlapblog/posts/StanLKJprior.html) and [Stan's reference](https://mc-stan.org/docs/functions-reference/cholesky-lkj-correlation-distribution.html) for Choleskey decomposition\n\n-   For eta = 1, the distribution is uniform over the space of all correlation matrices\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Data Structure and Data Block\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='Lecture06.R'}\n\n```{.r .cell-code}\ndata_list <- list(\n  N = 1000, # number of subjects/observations\n  J = J, # number of items\n  K = 2, # number of latent variables,\n  Y = Y,\n  Q = Q,\n  # location/index of lambda\n  kk = loc[,2],\n  #hyperparameter\n  sigmaRate = .01,\n  meanMu = rep(0, J),\n  covMu = diag(1000, J),\n  meanTheta = rep(0, 2),\n  eta = 1,\n  meanLambda = rep(0, J),\n  covLambda = diag(1000, J)\n)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='simulation_loc.stan' output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> N; // number of observations\n  int<lower=0> J; // number of items\n  int<lower=0> K; // number of latent variables\n  matrix[N, J] Y; // item responses\n  \n  //location/index of lambda\n  array[J] int<lower=0> kk;\n  \n  //hyperparameter\n  real<lower=0> sigmaRate;\n  vector[J] meanMu;\n  matrix[J, J] covMu;      // prior covariance matrix for coefficients\n  vector[K] meanTheta;\n  \n  vector[J] meanLambda;\n  matrix[J, J] covLambda;      // prior covariance matrix for coefficients\n  \n  real<lower=0> eta; // LKJ shape parameters\n}\n```\n:::\n\n\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Parameter and Transformed Parameter block\n\n\n\n\n\n\n::: {.cell filename='simulation_loc.stan' output.var='display'}\n\n```{.stan .cell-code}\nparameters {\n  vector[J] mu;                      // item intercepts\n  vector<lower=0,upper=1>[J] lambda; // factor loadings\n  vector<lower=0>[J] sigma;          // the unique residual standard deviation for each item\n  matrix[N, K] theta;                // the latent variables (one for each person)\n  cholesky_factor_corr[K] L;         // L of factor correlation matrix\n}\ntransformed parameters{\n  matrix[K,K] corrTheta = multiply_lower_tri_self_transpose(L);\n}\n```\n:::\n\n\n\n\n\n\nNote that `L` is the Cholesky decomposition factor of factor correlation matrix\n\n::: nonincremental\n-   It is also a lower triangle matrix\n-   use `cholesky_factor_corr[K]` to declare this parameter\n-   Sometime, parameters are those that are easy to sampling, `transformed parameters` block is used to transform your parameters to those that are difficulty to direct sampling.\n:::\n\n------------------------------------------------------------------------\n\n### Model block\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='Lecture06.R'}\n\n```{.r .cell-code}\ndata_list <- list(\n  N = 1000, # number of subjects/observations\n  J = J, # number of items\n  K = 2, # number of latent variables,\n  Y = Y,\n  Q = Q,\n  # location/index of lambda\n  kk = loc[,2],\n  #hyperparameter\n  sigmaRate = .01,\n  meanMu = rep(0, J),\n  covMu = diag(1000, J),\n  meanTheta = rep(0, 2),\n  eta = 1,\n  meanLambda = rep(0, J),\n  covLambda = diag(1000, J)\n)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='simulation_loc.stan' output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  mu ~ multi_normal(meanMu, covMu);\n  sigma ~ exponential(sigmaRate);   \n  lambda ~ multi_normal(meanLambda, covLambda);\n  L ~ lkj_corr_cholesky(eta);\n  for (i in 1:N) {\n    theta[i,] ~ multi_normal(meanTheta, corrTheta);\n  }\n  for (j in 1:J) { // loop over each item response function\n    Y[,j] ~ normal(mu[j]+lambda[j]*theta[,kk[j]], sigma[j]);\n  }\n}\n```\n:::\n\n\n\n\n\n:::\n:::\n\nNote that `kk[j]` selects which factor to be multiplied dependent on factor loading's index. That why we have a location matrix of factor loadings. `Theta` in `loc` table is kk.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Item Theta q\n[1,]    1     1 1\n[2,]    2     1 1\n[3,]    3     1 1\n[4,]    4     2 1\n[5,]    5     2 1\n[6,]    6     2 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### Generated quantities block\n\n\n\n\n\n\n::: {.cell filename='simulation_loc.stan' output.var='display'}\n\n```{.stan .cell-code}\ngenerated quantities {\n  vector[N * J] log_lik;\n  matrix[N, J] temp;\n  matrix[N, J] Y_rep;\n  vector[J] Item_Mean_rep;\n  for (i in 1:N) {\n    for (j in 1:J) {\n      temp[i, j] = normal_lpdf(Y[i, j] | mu[j]+lambda[j]*theta[i,kk[j]],  sigma[j]); \n    }\n  }\n  log_lik = to_vector(temp);\n  for (j in 1:J) {\n    Y_rep[,j] = to_vector(normal_rng(mu[j]+lambda[j]*theta[,kk[j]], sigma[j]));\n    Item_Mean_rep[j] = mean(Y_rep[,j]);\n  }\n}\n\n```\n:::\n\n\n\n\n\n\nTo obtain leave-one-out (LOO) model fitting, we need to generate log-likelihood:\n\n-   `log_lik` includes both person information and item information in factor analysis and IRT\n\n-   `log_lik` must be a vector in Stan\n\n-   Thus, the length of log-likelihood is a vector of length N $\\times$ J\n\nTo conduct posterior predictive model checking, we need to generate simulation data sets: `Y_rep`\n\n-   `Y_rep` can be generated using `normal_rng` and posterior draws of parameters\n\n-   `Item_Mean_rep` were generated to compared to observed item means\n\n------------------------------------------------------------------------\n\n### Model Estimation\n\nHere, my MCMC estimation is set to:\n\n::: nonincremental\n1.  Four MCMC chains that are running parallel\n2.  A seed as 1234 for replication\n3.  The warmup iteration is 1000 and the sampling iteration is 2000. Thus, the total iteration is 3000.\n4.  Since we have 4 chains, the total iteration for summary statistics is 12000 iterations.\n5.  The total estimation time is 220s (3.6 minutes)\n:::\n\n``` r\n#| eval: false\nmod_cfa_twofactor <- cmdstan_model(here::here(\"posts\", \"2024-01-12-syllabus-adv-multivariate-esrm-6553\", \"Lecture06\", \"Code\", \"simulation_loc.stan\"))\nfit_cfa_twofactor <- mod_cfa_twofactor$sample(\n  data = data_list,\n  seed = 1234,\n  chains = 4,\n  parallel_chains = 4, \n  iter_sampling = 2000,\n  iter_warmup = 1000\n)\n```\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$total\n[1] 220.9985\n\n$chains\n  chain_id  warmup sampling   total\n1        1  30.640   51.569  82.209\n2        2 178.157   42.665 220.822\n3        3  31.094   43.514  74.608\n4        4  29.858   43.240  73.098\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Result \\> Model diagnostic \\> PPMC\n\nAll items have PPP of item mean close to 0.5, suggesting great model-data fitting.\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nItem_Mean_rep_mat <- fit_cfa_twofactor$draws(\"Item_Mean_rep\", format = 'matrix')\nItem_Mean_obs <- colMeans(Y)\nPPP <- rep(NA, J)\n# colMeans(Item_Mean_rep_mat)\nfor (item in 1:J) {\n  PPP[item] <- mean(as.numeric(Item_Mean_rep_mat[,item]) > Item_Mean_obs[item])\n}\nPPP\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.491250 0.505750 0.497500 0.499875 0.503250 0.486500\n```\n\n\n:::\n\n```{.r .cell-code}\ndata.frame(\n  Item = factor(1:J, levels = 1:J),\n  PPP = PPP\n) |> \n  ggplot() +\n  geom_col(aes(x = Item, y = PPP)) + \n  geom_hline(aes(yintercept = .5), col = 'red', size = 1.3) + \n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='Posterior predictive distribution of item means'}\n\n```{.r .cell-code}\nobs_mean <- data.frame(\n  Item = paste0(\"Item_Mean_rep[\", 1:6,\"]\"),\n  y = Item_Mean_obs\n)\nItem_Mean_rep_mat |> as.data.frame() |> \n  pivot_longer(everything(), names_to = \"Item\", values_to = \"Posterior\") |> \n  ggplot() +\n  geom_density(aes(x = Posterior)) +\n  geom_vline(aes(xintercept = y), data = obs_mean, col ='red') +\n  facet_wrap(~ Item, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### MCMC Result \\> Model diagnostic \\> LOO\n\nWe firt examined the max/mean PSRF (rhat) for convergence. This is also called Gelman and Rubin diagnosis. The maximum RSRF is 1.03 suggesting MCMC estimation converged.\n\nThen, we examined the LOO with PSIS. According to Pareto K diagnostic, most log-likelihood estimation suggests good reliability.\n\nQuestion: Why we have 8000 $\\times$ 6000 log-likelihood elements? hints: our information in data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(\n  variables = NULL,\n  \"rhat\"\n) |> \n summarise(mean_rhat = mean(rhat, na.rm = T),\n           max_rhat = max(rhat, na.rm = T))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mean_rhat max_rhat\n      <dbl>    <dbl>\n1      1.00     1.03\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_res <- fit_cfa_twofactor$loo(variables = 'log_lik', save_psis = TRUE, cores = 2)\nprint(loo_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 8000 by 6000 log-likelihood matrix\n\n         Estimate    SE\nelpd_loo   4074.8  54.5\np_loo      1844.1  29.4\nlooic     -8149.5 108.9\n------\nMonte Carlo SE of elpd_loo is NA.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     3888  64.8%   677       \n (0.5, 0.7]   (ok)       1559  26.0%   174       \n   (0.7, 1]   (bad)       532   8.9%   12        \n   (1, Inf)   (very bad)   21   0.4%   13        \nSee help('pareto-k-diagnostic') for details.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Estimation \\> Factor Loadings\n\n::: columns\n::: {.column width=\"50%\"}\nBenchmark model using `lavaan`\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lavaan\nmod <- \"\nF1 =~ I1 + I2 + I3\nF2 =~ I4 + I5 + I6\n\"\ndat <- as.data.frame(Y)\ncolnames(dat) <- paste0('I', 1:6)\nfit <- cfa(mod, data = dat, std.lv = TRUE)\nlavaan::parameterestimates(fit) |> filter(op == \"=~\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lhs op rhs   est    se      z pvalue ci.lower ci.upper\n1  F1 =~  I1 0.720 0.016 43.685      0    0.688    0.752\n2  F1 =~  I2 0.514 0.012 43.082      0    0.491    0.538\n3  F1 =~  I3 0.310 0.008 40.922      0    0.295    0.325\n4  F2 =~  I4 0.673 0.015 43.699      0    0.643    0.703\n5  F2 =~  I5 0.483 0.011 42.975      0    0.461    0.505\n6  F2 =~  I6 0.293 0.007 40.248      0    0.279    0.307\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\nModel 1 using `Stan`\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(\"lambda\") |> select(variable,mean, median, sd, q5, q95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  variable   mean median      sd    q5   q95\n  <chr>     <dbl>  <dbl>   <dbl> <dbl> <dbl>\n1 lambda[1] 0.723  0.722 0.0175  0.694 0.752\n2 lambda[2] 0.516  0.516 0.0127  0.495 0.537\n3 lambda[3] 0.311  0.311 0.00791 0.298 0.324\n4 lambda[4] 0.674  0.674 0.0151  0.649 0.699\n5 lambda[5] 0.484  0.484 0.0111  0.466 0.502\n6 lambda[6] 0.294  0.294 0.00727 0.282 0.306\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(\"lambda\", \\(x) quantile(x, c(.025, .975)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  variable  `2.5%` `97.5%`\n  <chr>      <dbl>   <dbl>\n1 lambda[1]  0.689   0.757\n2 lambda[2]  0.492   0.541\n3 lambda[3]  0.296   0.326\n4 lambda[4]  0.645   0.704\n5 lambda[5]  0.462   0.505\n6 lambda[6]  0.280   0.308\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\nWe can easily notice how consistent between estimates of `lavaan` with `Stan` :\n\n::: nonincremental\n1.  `est` in `lavaan` corresponds to `mean` or `median` in `stan` . Their difference is around .001 - .002.\n2.  `se` in `lavaan` corresponds to `sd` in `stan`\n3.  `ci.lower` and `ci.upper` in lavaan is for 95% confidence interval, so they have larger range than `q5` and `q95` in Stan, which is for 90% Credible Interval\n4.  90% CI is the default setting, but we can calculate Bayesian 95% Credible Interval using `$summary()`\n:::\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Visualization \\> Factor Loadings\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::color_scheme_set(\"viridis\")\nbayesplot::mcmc_trace(fit_cfa_twofactor$draws(), regex_pars = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Estimation \\> Factor Correlation\n\nThe factor correlation (Psi = .3) is represented by the non-diagonal elements of factor correlation.\n\nThere are two ways to check estimation of factor correlation:\n\n-   Since we use LKJ sampling with `L` Cholesky factor, recall that `L` is lower triangle of factor correlation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(\"L[2,1]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  <chr>    <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 L[2,1]   0.318  0.318 0.0283 0.0286 0.272 0.365  1.00    1836.    4208.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   We can also look at the elements of factor correlation matrix\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(c(\"corrTheta[1,2]\", \"corrTheta[2,1]\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 10\n  variable        mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  <chr>          <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 corrTheta[1,2] 0.318  0.318 0.0283 0.0286 0.272 0.365  1.00    1836.    4208.\n2 corrTheta[2,1] 0.318  0.318 0.0283 0.0286 0.272 0.365  1.00    1836.    4208.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Visualization \\> Factor Correlation\n\n-   We can also visual inspect trace plots of MCMC draws of `L`\n-   What type of error leads to the deviation between posterior values of `L` and true value.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(fit_cfa_twofactor$draws(\"L[2,1]\"))\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Estimation \\> Item Intercepts\n\nItem intercepts are set to .1. Let's see what Bayesian model recovers the true values\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(\"mu\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  variable   mean median      sd     mad     q5   q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 mu[1]    0.106  0.106  0.0247  0.0231  0.0673 0.147  1.03     83.2     122.\n2 mu[2]    0.106  0.106  0.0177  0.0167  0.0777 0.135  1.03     85.8     144.\n3 mu[3]    0.101  0.101  0.0109  0.0102  0.0836 0.119  1.03     88.6     130.\n4 mu[4]    0.0842 0.0844 0.0208  0.0217  0.0488 0.118  1.02    128.      328.\n5 mu[5]    0.0966 0.0968 0.0151  0.0157  0.0707 0.121  1.02    129.      348.\n6 mu[6]    0.0989 0.0990 0.00952 0.00983 0.0828 0.114  1.02    138.      401.\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWe notices item 4 is little off with true values.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(fit_cfa_twofactor$draws(\"mu\"))\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Estimation \\> **Residual variances**\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_twofactor$summary(\"sigma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  variable   mean median      sd     mad     q5   q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 sigma[1] 0.107  0.108  0.00688 0.00674 0.0961 0.118  1.01     747.    1009.\n2 sigma[2] 0.0995 0.0995 0.00412 0.00412 0.0928 0.106  1.00    1178.    2166.\n3 sigma[3] 0.0961 0.0960 0.00254 0.00251 0.0919 0.100  1.00    5741.    6069.\n4 sigma[4] 0.0991 0.0993 0.00676 0.00644 0.0877 0.110  1.00     738.    1131.\n5 sigma[5] 0.0962 0.0962 0.00408 0.00416 0.0894 0.103  1.00    1221.    2231.\n6 sigma[6] 0.0998 0.0998 0.00256 0.00260 0.0957 0.104  1.00    6468.    5918.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(fit_cfa_twofactor$draws(\"sigma\"))\n```\n\n::: {.cell-output-display}\n![](Lecture06_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Simulation Study 2\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n::: nonincremental\n-   To illustrate how to model a more complex CFA with cross-loadings, let's generate a new simulation data with test length 7 and two latent variables\n-   The mean structure of model is like this, each latent variable was measured by 4 items. The other parameters are as same as Model 1.\n:::\n\n![Mean Structure of Model 2](simulation_model2.png)\n\n------------------------------------------------------------------------\n\n### Strategies for Model 2\n\nThe general idea is that we assign a uninformative prior distribution for \"main\" factor loadings, and assign informative \"close-to-zero\" prior distribution for \"zero\" factor loadings:\n\n![](simulation_priorsetting.png)\n\n------------------------------------------------------------------------\n\n### Strategies for Model 2 (Cont.)\n\n::: columns\n::: {.column width=\"50%\"}\n-   Have a more detailed location matrix for factor loading matrix\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(Lambda2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   V1  V2\n1 0.7 0.0\n2 0.5 0.0\n3 0.3 0.0\n4 0.5 0.5\n5 0.0 0.7\n6 0.0 0.5\n7 0.0 0.3\n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   We can transform the information of factor loading into:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Transform Q to location index\nloc2 <- Q2 |>\n  as.data.frame() |>\n  rename(`1` = V1, `2` = V2) |> \n  rownames_to_column(\"Item\") |>\n  pivot_longer(c(`1`, `2`), names_to = \"Theta\", values_to = \"q\") |> \n  mutate(across(Item:q, as.numeric)) |> \n  mutate(q = -q + 2) |> \n  as.matrix()\nas.data.frame(loc2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Item Theta q\n1     1     1 1\n2     1     2 2\n3     2     1 1\n4     2     2 2\n5     3     1 1\n6     3     2 2\n7     4     1 1\n8     4     2 1\n9     5     1 2\n10    5     2 1\n11    6     1 2\n12    6     2 1\n13    7     1 2\n14    7     2 1\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n-   Where `Item` represents the index of item a factor loading belongs to, and `Theta` represents the index of latent variables a factor loading belongs to.\n\n-   In Stan's data block, we denote `Item` of location matrix as `jj` and `Theta` as `kk` . `q` represents two types of prior distributions for factor loadings.\n\n\n\n\n\n\n::: {.cell filename='simulation_exp2.stan' output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  ...\n  int<lower=0> R; // number of rows in location matrix\n  array[R] int<lower=0>jj;\n  array[R] int<lower=0>kk;\n  array[R] int<lower=0>q;\n  ...\n}\n```\n:::\n\n\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Data Structure and Data Block\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='Lecture06.R'}\n\n```{.r .cell-code}\ndata_list2 <- list(\n  N = 1000, # number of subjects/observations\n  J = J2, # number of items\n  K = 2, # number of latent variables,\n  Y = Y2,\n  Q = Q2,\n  # location of lambda\n  R = nrow(loc2),\n  jj = loc2[,1],\n  kk = loc2[,2],\n  q = loc2[,3],\n  #hyperparameter\n  meanSigma = .1,\n  scaleSigma = 1,\n  meanMu = rep(0, J2),\n  covMu = diag(10, J2),\n  meanTheta = rep(0, 2),\n  corrTheta = matrix(c(1, .3, .3, 1), 2, 2, byrow = T)\n)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell filename='simulation_exp2.stan' output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> N; // number of observations\n  int<lower=0> J; // number of items\n  int<lower=0> K; // number of latent variables\n  matrix[N, J] Y; // item responses\n  \n  int<lower=0> R; // number of rows in location matrix\n  array[R] int<lower=0>jj;\n  array[R] int<lower=0>kk;\n  array[R] int<lower=0>q;\n  \n  //hyperparameter\n  real<lower=0> meanSigma;\n  real<lower=0> scaleSigma;\n  vector[J] meanMu;\n  matrix[J, J] covMu;      // prior covariance matrix for coefficients\n  vector[K] meanTheta;\n  matrix[K, K] corrTheta;\n  \n}\n```\n:::\n\n\n\n\n\n:::\n:::\n\nNote that for the simplicity of estimation, I specified the factor correlation matrix as fixed. If you are interested in estimating factor correlation, you can refer to the previous model using LKJ sampling.\n\n------------------------------------------------------------------------\n\n### Parameters block\n\n\n\n\n\n\n::: {.cell filename='simulation_exp2.stan' output.var='display'}\n\n```{.stan .cell-code}\nparameters {\n  vector<lower=0,upper=1>[J] mu;\n  matrix<lower=0>[J, K] lambda;\n  vector<lower=0,upper=1>[J] sigma; // the unique residual standard deviation for each item\n  matrix[N, K] theta;                // the latent variables (one for each person)\n  //matrix[K, K] corrTheta; // not use corrmatrix to avoid duplicancy of validation\n}\n\n```\n:::\n\n\n\n\n\n\nFor `parameters` block, the only difference is we specify `lambda` as a matrix with J $\\times$ K, which is 7 $\\times$ 2 in our case.\n\n------------------------------------------------------------------------\n\n### Model block\n\nAs you can see, in `Model` block, we need to use if_else in Stan to specify factor loadings in different locations\n\n::: nonincremental\n-   For type 1 (green), we specify a uninformative normal distribution\n-   For type 2 (red), we specify a informative shrinkage priors.\n-   For univariate residual, we can simply use `cauchy` or `exponential` prior\n-   The item response function estimate each item response using $\\mu+ \\Lambda * \\Theta$ as kernal and $\\sigma$ as variation\n:::\n\n\n\n\n\n\n::: {.cell filename='simulation_exp2.stan' output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  mu ~ multi_normal(meanMu, covMu);\n  // specify lambda's regulation\n  for (r in 1:R) {\n    if (q[r] == 1){\n      lambda[jj[r], kk[r]] ~ normal(0, 10);\n    }else{// student_t(nu, mu, sigma)\n      lambda[jj[r], kk[r]] ~ student_t(1, 0, 0.01);\n    }\n  }\n  //corrTheta ~ lkj_corr(eta);\n  for (i in 1:N) {\n    theta[i,] ~ multi_normal(meanTheta, corrTheta);\n  }\n  for (j in 1:J) {\n    sigma[j] ~ cauchy(meanSigma, scaleSigma);                   // Prior for unique standard deviations\n    Y[,j] ~ normal(mu[j]+to_row_vector(lambda[j,])*theta', sigma[j]);\n  }\n}\n```\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### Model Result \\> R-hat\n\nWe set up the MCMC as follows:\n\n::: nonincremental\n-   3000 warmups + 3000 samplings = 6000 iterations\n-   Total execution time: 202.7 seconds for my computers but it takes 20 minutes or more if I specify inproper priors\n-   4 chains\n-   R hat is acceptable suggesting convergence\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_exp2 <- mod_cfa_exp2$sample(\n  data = data_list2,\n  seed = 1234,\n  chains = 4,\n  parallel_chains = 4, \n  iter_sampling = 3000,\n  iter_warmup = 3000\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_exp2$summary(\n  variables = NULL,\n  \"rhat\"\n) |> \n summarise(mean_rhat = mean(rhat, na.rm = T),\n           max_rhat = max(rhat, na.rm = T))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mean_rhat max_rhat\n      <dbl>    <dbl>\n1      1.00     1.04\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### MCMC Results \\> Estimation \\> Factor Loadings\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cfa_exp2$summary('lambda') |> select(variable, mean, median, sd, q5, q95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 14 × 6\n   variable       mean  median      sd       q5    q95\n   <chr>         <dbl>   <dbl>   <dbl>    <dbl>  <dbl>\n 1 lambda[1,1] 0.714   0.714   0.0164  0.687    0.741 \n 2 lambda[2,1] 0.509   0.509   0.0119  0.489    0.529 \n 3 lambda[3,1] 0.305   0.305   0.00761 0.293    0.318 \n 4 lambda[4,1] 0.515   0.514   0.0130  0.493    0.536 \n 5 lambda[5,1] 0.00724 0.00582 0.00597 0.000607 0.0186\n 6 lambda[6,1] 0.00795 0.00726 0.00503 0.00128  0.0171\n 7 lambda[7,1] 0.00476 0.00422 0.00324 0.000559 0.0107\n 8 lambda[1,2] 0.00606 0.00479 0.00523 0.000414 0.0163\n 9 lambda[2,2] 0.00797 0.00734 0.00483 0.00142  0.0168\n10 lambda[3,2] 0.00618 0.00586 0.00355 0.000989 0.0126\n11 lambda[4,2] 0.491   0.491   0.0122  0.471    0.511 \n12 lambda[5,2] 0.680   0.680   0.0152  0.655    0.706 \n13 lambda[6,2] 0.482   0.482   0.0110  0.464    0.500 \n14 lambda[7,2] 0.285   0.285   0.00694 0.274    0.296 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nLambda2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.7  0.0\n[2,]  0.5  0.0\n[3,]  0.3  0.0\n[4,]  0.5  0.5\n[5,]  0.0  0.7\n[6,]  0.0  0.5\n[7,]  0.0  0.3\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Wrapping up\n\nWe simulated two models: one is 2-factor model without cross-loadings; another is 2-factor model with cross-loadings.\n\nIn real setting, the Bayesian modeling could be challenging because\n\n::: nonincremental\n-   Prior distributions are unsure\n\n-   Bad prior may leads to unconverge; So try multiple priors\n\n    -   Good priors will have nice converge very early (i.e, 500 or 1000 samples)\n\n-   MCMC sampling is computationally intensive, and you may not sure how many iterations are enough\n\n-   Hard to come up with a strategy of model building\n\n    -   For example, \"location matrix and different priors\" is a strategy I prefer\n\n    -   It may not works for any problems for cross-loadings\n\n-   You may try `blavaan` or other wrap-up package for Bayesian CFA, it saves some time for model building\n\n    -   But you have no idea how they set up MCMC\n:::\n\nAll of these topics will be with us when we start model complicated models in our future lecture.\n\n------------------------------------------------------------------------\n\n## Next Class\n\n1.  More strategies about measurement models with Stan\n\n## Other materials\n\n1.  [Jonathan Templin's Website](https://jonathantemplin.github.io/Bayesian-Psychometric-Modeling-Course-Fall2022/lectures/lecture04b/04b_Modeling_Observed_Data)\n2.  [Winston-Salem's Website](https://medewitt.github.io/resources/stan_cfa.html)\n3.  [Rick Farouni's Website](https://rfarouni.github.io/assets/projects/BayesianFactorAnalysis/BayesianFactorAnalysis.html)\n\n## Reference\n",
    "supporting": [
      "Lecture06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}