{
  "hash": "029ec7c5a139534c8e868856aae61c48",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'How to choose network analysis estimation for application research'\nauthor: 'Jihong Zhang'\nsubtitle: 'Compare multiple R packages for psychological network analysis'\ndate: 'April 4 2024'\ncategories:\n  - R\n  - Bayesian\n  - Tutorial\n  - Network Analysis\nexecute: \n  eval: true\n  echo: true\n  warning: false\nformat: \n  html: \n    code-fold: true\n    code-summary: 'Click to see the code'\nbibliography: references.bib\n---\n\n\n\n# Background\n\nMultiple estimation methods for network analysis have been proposed, from regularized to unregularized, from frequentist to Bayesian approach, from one-step to multiple steps. @isvoranuWhichEstimationMethod2023a provides an throughout illustration of current network estimation methods and simulation study. This post tries to reproduce the procedures and coding illustrated by the paper and compare the results for packages with up-to-date version. Five packages will be used for network modeling: (1) `qgraph`, (2)`psychonetrics`, (3)`MGM`, (4)`BGGM`, (5)`GGMnonreg`. Note that the tutorial of each R package is not the scope of this post. The specific variants of estimation is described in @fig-usedestimation. Please see [here](../2024-03-05-Bayesian-GGM/index.qmd) for `BGGM` R package for more detailed example.\n\n![Estimation methods used in Isvoranu and Epskamp (2023)](estimation_methods_table.png){#fig-usedestimation fig-align=\"center\"}\n\n# Data Generation\n\nFor the sake of simplicity, I will only pick one condition from the simulation settings. To simulate data, I used the same code from the paper. The function is a wrapper of `bootnet::ggmGenerator` function\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"dataGenerator() function\"}\nlibrary(\"bootnet\")\ndataGenerator <- function(\n  trueNet,  # true network models: DASS21, PTSD, BFI\n  sampleSize, # sample size\n  data = c(\"normal\",\"skewed\",\"uniform ordered\",\"skewed ordered\"), # type of data\n  nLevels = 4,\n  missing = 0\n){\n  data <- match.arg(data)\n  nNode <- ncol(trueNet)\n  \n  if (data == \"normal\" || data == \"skewed\"){\n    # Generator function:\n    gen <- ggmGenerator()\n    \n    # Generate data:\n    Data <- gen(sampleSize,trueNet)\n    \n    # Generate replication data:\n    Data2 <- gen(sampleSize,trueNet)\n    \n    if (data == \"skewed\"){ ## exponential transformation to make data skewed\n      for (i in 1:ncol(trueNet)){\n        Data[,i] <- exp(Data[,i])\n        Data2[,i] <- exp(Data2[,i])\n      }\n    }\n  \n  } else {\n    # Skew factor:\n    skewFactor <- switch(data,\n                         \"uniform ordered\" = 1,\n                         \"skewed ordered\" = 2,\n                         \"very skewed ordered\" = 4)\n    \n    # Generator function:\n    gen <- ggmGenerator(ordinal = TRUE, nLevels = nLevels)\n    \n    # Make thresholds:\n    thresholds <- lapply(seq_len(nNode),function(x)qnorm(seq(0,1,length=nLevels + 1)[-c(1,nLevels+1)]^(1/skewFactor)))\n    \n    # Generate data:\n    Data <- gen(sampleSize, list(\n      graph = trueNet,\n      thresholds = thresholds\n    ))\n    \n    # Generate replication data:\n    Data2 <- gen(sampleSize, list(\n      graph = trueNet,\n      thresholds = thresholds\n    ))\n  }\n  \n  # Add missings:\n  if (missing > 0){\n    for (i in 1:ncol(Data)){\n      Data[runif(sampleSize) < missing,i] <- NA\n      Data2[runif(sampleSize) < missing,i] <- NA\n    }\n  }\n  \n  return(list(\n    data1 = Data,\n    data2 = Data2\n  ))\n}\n```\n:::\n\n\n\nBFI contains 26 columns: first columns are the precision matrix with 25 $\\times$ 25 and second columns as clusters. We will use the structure of BFI for the simulation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\nBFI <- read.csv(here(\"notes\", \"2024-04-04-Network-Estimation-Methods\", \n                     \"weights matrices\", \"BFI.csv\"))\nround(BFI[, 1:25], 2)[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     A1    A2    A3   A4   A5\n1  0.00 -0.25 -0.11 0.00 0.00\n2 -0.25  0.00  0.25 0.15 0.14\n3 -0.11  0.25  0.00 0.17 0.26\n4  0.00  0.15  0.17 0.00 0.00\n5  0.00  0.14  0.26 0.00 0.00\n```\n\n\n:::\n:::\n\n\n\nTo generate data from `bootnet::ggmGenerator()`, we can generate a function `gen` from it with first argument as sample size and second argument as partial correlation matrix.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngen <- bootnet::ggmGenerator()\ngen(20, as.matrix(BFI[1:5, 1:5]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]        [,2]       [,3]        [,4]       [,5]\n [1,]  0.15197711 -1.13787272  0.4365272  1.25370880  0.7415676\n [2,]  1.00394084 -1.55776706 -0.1904807 -1.63553708  0.5012727\n [3,]  0.35532977  0.41376961  0.5614085 -0.36490970  0.5100830\n [4,]  1.72209193 -1.87313025 -2.5553354  0.04433577 -1.7107850\n [5,]  1.17712634 -0.05650101 -0.1445630 -1.22676984 -0.8419986\n [6,] -0.06754646 -1.43778433 -2.9205476  0.08320189 -1.4831299\n [7,]  0.39136245  0.63787571 -0.6373354  0.21237691 -0.4809887\n [8,] -1.95459264  2.18843913  0.7321798  2.23929334  0.4545828\n [9,] -0.90179919  0.83072851  1.1732905  0.03308385  0.1373834\n[10,]  0.73057818 -1.17243646  0.2524365 -0.77315191  0.8734144\n[11,] -0.48331282  0.31905867  1.5615210 -1.29417953  0.3508071\n[12,]  1.24242793 -0.70792647  0.3571831 -2.47895184  0.3658056\n[13,]  2.33328664 -0.37947712 -0.4858091 -0.68227955  0.2944840\n[14,] -0.12400009  0.28288101  1.2652303  1.09778408  0.7210398\n[15,]  0.03298341 -0.53387346 -2.1516198 -0.45143728 -0.6684614\n[16,]  0.01679120  0.39019556 -0.6698239  0.48480172  0.9524484\n[17,]  0.48816173 -0.35849573 -1.1935901  0.05618575 -0.5417355\n[18,] -0.64221519 -0.89328022 -0.0861652 -0.65498596 -0.8170173\n[19,]  0.26330120  1.28974166 -0.6943198 -0.32863254 -0.6595003\n[20,] -0.04002161 -0.57248009 -1.4656651  1.22536870 -0.7931569\n```\n\n\n:::\n\n```{.r .cell-code}\ndata <- gen(600, as.matrix(BFI[, 1:25]))\n```\n:::\n\n\n\n# Estimation\n\n## Method 1 - EBICglasso\n\nThis method uses the `bootnet::estimateNetwork` function.\n\n`EBICtuning <- 0.5` sets up the hyperparameter ($\\gamma$) as .5 that controls how much the EBIC prefers simpler models (fewer edges), which by default is .5.\n\n$$\n\\text{EBIC} =-2L+E(\\log(N))+4\\gamma E(\\log(P))\n$$\n\nAnother important setting is the tuning parameter ($\\lambda$) of EBICglasso that controls the sparsity level in the penalized likelihood function as:\n\n$$\n\\log \\det(\\boldsymbol{K}) - \\text{trace}(\\boldsymbol{SK})-\\lambda \\Sigma|\\kappa_{ij}|\n$$\n\nwhere $\\boldsymbol{S}$ represents the sample variance-covariance matrix and $\\boldsymbol{K}$ represents the precision matrix that *lasso* aims to estimate. Overall, the regularization limits the sum of absolute partial correlation coefficients.\n\n`sampleSize=\"pairwise_average\"` will set the sample size to the average of sample sizes used for each individual correlation in `EBICglasso`.\n\n@fig-bfi is the estimated network structure with BFI-25 data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 1: EBICglasso\"}\nlibrary(\"qgraph\")\nsampleAdjust  <-  \"pairwise_average\"\nEBICtuning <- 0.5\ntransformation <- \"polychoric/categorical\" # EBICglasso use cor_auto\nres <- estimateNetwork(data, \n                       default = \"EBICglasso\",\n                       sampleSize = sampleAdjust,\n                       tuning = EBICtuning,\n                       corMethod = ifelse(transformation == \"polychoric/categorical\",\n                                          \"cor_auto\",\n                                          \"cor\"))\n    \nestnet <- res$graph\nqgraph(estnet)\n```\n\n::: {.cell-output-display}\n![Network Plot for BFI-25](index_files/figure-html/fig-bfi-1.png){#fig-bfi width=768}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}