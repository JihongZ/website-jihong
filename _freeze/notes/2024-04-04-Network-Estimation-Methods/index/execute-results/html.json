{
  "hash": "b95148c6249b8c558f64aa919a9325f9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'How to choose network analysis estimation for application research'\nauthor: 'Jihong Zhang'\nsubtitle: 'Compare multiple R packages for psychological network analysis'\ndate: 'April 4 2024'\ncategories:\n  - R\n  - Bayesian\n  - Tutorial\n  - Network Analysis\nexecute: \n  eval: true\n  echo: true\n  warning: false\nformat: \n  html: \n    code-fold: true\n    code-summary: 'Click to see the code'\nbibliography: references.bib\ncsl: apa.csl\n---\n\n\n\n# Background\n\nMultiple estimation methods for network analysis have been proposed, from regularized to unregularized, from frequentist to Bayesian approach, from one-step to multiple steps. @isvoranuWhichEstimationMethod2023a provides an throughout illustration of current network estimation methods and simulation study. This post tries to reproduce the procedures and coding illustrated by the paper and compare the results for packages with up-to-date version. Five packages will be used for network modeling: (1) `qgraph`, (2)`psychonetrics`, (3)`MGM`, (4)`BGGM`, (5)`GGMnonreg`. Note that the tutorial of each R package is not the scope of this post. The specific variants of estimation is described in @fig-usedestimation. Please see [here](../2024-03-05-Bayesian-GGM/index.qmd) for `BGGM` R package for more detailed example.\n\n![Estimation methods used in Isvoranu and Epskamp (2023)](estimation_methods_table.png){#fig-usedestimation fig-align=\"center\"}\n\n# Data Generation\n\nFor the sake of simplicity, I will only pick one condition from the simulation settings. To simulate data, I used the same code from the paper. The function is a wrapper of `bootnet::ggmGenerator` function\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"dataGenerator() function\"}\nlibrary(\"bootnet\")\ndataGenerator <- function(\n  trueNet,  # true network models: DASS21, PTSD, BFI\n  sampleSize, # sample size\n  data = c(\"normal\",\"skewed\",\"uniform ordered\",\"skewed ordered\"), # type of data\n  nLevels = 4,\n  missing = 0\n){\n  data <- match.arg(data)\n  nNode <- ncol(trueNet)\n  \n  if (data == \"normal\" || data == \"skewed\"){\n    # Generator function:\n    gen <- ggmGenerator()\n    \n    # Generate data:\n    Data <- gen(sampleSize,trueNet)\n    \n    # Generate replication data:\n    Data2 <- gen(sampleSize,trueNet)\n    \n    if (data == \"skewed\"){ ## exponential transformation to make data skewed\n      for (i in 1:ncol(trueNet)){\n        Data[,i] <- exp(Data[,i])\n        Data2[,i] <- exp(Data2[,i])\n      }\n    }\n  \n  } else {\n    # Skew factor:\n    skewFactor <- switch(data,\n                         \"uniform ordered\" = 1,\n                         \"skewed ordered\" = 2,\n                         \"very skewed ordered\" = 4)\n    \n    # Generator function:\n    gen <- ggmGenerator(ordinal = TRUE, nLevels = nLevels)\n    \n    # Make thresholds:\n    thresholds <- lapply(seq_len(nNode),function(x)qnorm(seq(0,1,length=nLevels + 1)[-c(1,nLevels+1)]^(1/skewFactor)))\n    \n    # Generate data:\n    Data <- gen(sampleSize, list(\n      graph = trueNet,\n      thresholds = thresholds\n    ))\n    \n    # Generate replication data:\n    Data2 <- gen(sampleSize, list(\n      graph = trueNet,\n      thresholds = thresholds\n    ))\n  }\n  \n  # Add missings:\n  if (missing > 0){\n    for (i in 1:ncol(Data)){\n      Data[runif(sampleSize) < missing,i] <- NA\n      Data2[runif(sampleSize) < missing,i] <- NA\n    }\n  }\n  \n  return(list(\n    data1 = Data,\n    data2 = Data2\n  ))\n}\n```\n:::\n\n\n\nBFI contains 26 columns: first columns are the precision matrix with 25 $\\times$ 25 and second columns as clusters. We will use the structure of BFI for the simulation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\nbfi_truenetwork <- read.csv(here(\"notes\", \"2024-04-04-Network-Estimation-Methods\", \n                                 \"weights matrices\", \"BFI.csv\"))\nbfi_truenetwork <- bfi_truenetwork[,1:25]\nhead(psychTools::bfi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4\n61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3  6  3  4\n61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4  2  4  3\n61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4  2  5  5\n61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3  3  4  3\n61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3  3  4  3\n61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4  3  5  6\n      O5 gender education age\n61617  3      1        NA  16\n61618  3      2        NA  18\n61620  2      2        NA  17\n61621  5      2        NA  17\n61622  3      1        NA  17\n61623  1      2         3  21\n```\n\n\n:::\n:::\n\n\n\nTo generate data from `bootnet::ggmGenerator()`, we can generate a function `gen` from it with first argument as sample size and second argument as partial correlation matrix.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ngen <- bootnet::ggmGenerator()\ndata <- gen(2800, as.matrix(bfi_truenetwork))\n```\n:::\n\n\n\n# Measures\n\n## Overview of Centrality Measures\n\nCentrality measures are used as one characteristic of network model to summarize the importance of nodes/items given a network. Some ideas of centrality measures come from social network. Some may not make much sense in psychometric network. I also listed the centrality measures that have been used in literatures of psychological or psychometric modeling. Following the terminology of @christensen2018, centrality measures include: (1) betweenness centrality (BC); (2) the randomized shortest paths betweeness centrality (RSPBC); (3) closeness centrality (CC); (4)node degree (ND); (5) node strength (NS); (6) expected influence (EI); (7) engenvector centrality (EC); (8) Eccentricity (*k;* @hage1995); (9) hybrid centrality (HC; @christensen2018a).\n\nThey can be grouped into three categories:\n\n[*Type 1*]{.underline}—Number of path relevant metrics: BC measures how often a node is on the shortest path from one node to another. RSPBC is a adjusted BC measure which measures how oftern a node is on the random shortest path from one to another; CC measures the average number of paths from one node to all other nodes in the network. ND measures how many connections are connected to each node. Eccentricity (*k*) measures the maximum \"distance\" between one node with other nodes with lower values suggests higher centrality, where one node's distance is the length of a shortest path between this node to other nodes.\n\n[*Type 2*]{.underline}—Path strength relevant metrics: **NS** measures the sum of the absolute values of edge weights connected to a single node. **EI** measures the sum of the positive or negative values of edge weights connected to a single node.\n\n[*Type 3*]{.underline}—Composite of multiple centrality measures: **HC** measures nodes on the basis of their centrality values across multiple measures of centrality (BC, LC, *k*, EC, and NS) which describes highly central nodes with large values and highly peripheral nodes with small values [@christensen2018b].\n\nSome literature of network psychometrics criticized the use of Type 1 centrality measures and some Type II centrality measures [@hallquist2021; @neal2022]. For example, @hallquist2021 argued that some Type I centrality measures such as BC and CC derive from the concept of distance, which builds on the physical nature of many traditional graph theory applications, including railways and compute networks. In association networks, the idea of network distance does not have physical reference. @bringmann2019 also think distance-based centrality like closeness centrality (CC) and betweenness centrality (BC) do not have \"shortest path\", or \"node exchangeability\" assumptions in psychological networks so they are not suitable in the context of psychological network. In addition, ND (degree), CC (closeness), and BC (betweenness) do not take negative values of edge weights into account that may lose important information in the context of psychology.\n\n## Network level accuracy metrics\n\nStructure accuracy measures such as *sensitivity*, specificity, and precision investigate if an true edge is include or not or an false edge is include or not. Several edge weight accuracy measures include: (1) correlation between absolute values of estimated edge weights and true edge weights, (2) average absolute bias between estimated edge weights and true weights, (3) average absolute bias for true edges.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor0 <- function(x,y,...){\n  if (sum(!is.na(x)) < 2 || sum(!is.na(y)) < 2 || sd(x,na.rm=TRUE)==0 | sd(y,na.rm=TRUE) == 0){\n    return(0)\n  } else {\n    return(cor(x,y,...))\n  }\n}\n\nbias <- function(x,y) mean(abs(x-y),na.rm=TRUE)\n\n\n### Inner function:\ncomparison_metrics <- function(real, est, name = \"full\"){\n  \n  # Output list:\n  out <- list()\n  \n  # True positives:\n  TruePos <- sum(est != 0 &  real != 0)\n  \n  # False pos:\n  FalsePos <- sum(est != 0 & real == 0)\n  \n  # True Neg:\n  TrueNeg <- sum(est == 0 & real == 0)\n  \n  # False Neg:\n  FalseNeg <- sum(est == 0 & real != 0)\n  \n  # Sensitivity:\n  out$sensitivity <- TruePos / (TruePos + FalseNeg)\n  \n  # Sensitivity top 50%:\n  top50 <- which(abs(real) > median(abs(real[real!=0])))\n  out[[\"sensitivity_top50\"]] <- sum(est[top50]!=0 & real[top50] != 0) / sum(real[top50] != 0)\n  \n  # Sensitivity top 25%:\n  top25 <- which(abs(real) > quantile(abs(real[real!=0]), 0.75))\n  out[[\"sensitivity_top25\"]] <- sum(est[top25]!=0 & real[top25] != 0) / sum(real[top25] != 0)\n  \n  # Sensitivity top 10%:\n  top10 <- which(abs(real) > quantile(abs(real[real!=0]), 0.90))\n  out[[\"sensitivity_top10\"]] <- sum(est[top10]!=0 & real[top10] != 0) / sum(real[top10] != 0)\n  \n  # Specificity:\n  out$specificity <- TrueNeg / (TrueNeg + FalsePos)\n  \n  # Precision (1 - FDR):\n  out$precision <- TruePos / (FalsePos + TruePos)\n  \n  # precision top 50% (of estimated edges):\n  top50 <- which(abs(est) > median(abs(est[est!=0])))\n  out[[\"precision_top50\"]] <- sum(est[top50]!=0 & real[top50] != 0) / sum(est[top50] != 0)\n  \n  # precision top 25%:\n  top25 <- which(abs(est) > quantile(abs(est[est!=0]), 0.75))\n  out[[\"precision_top25\"]] <- sum(est[top25]!=0 & real[top25] != 0) / sum(est[top25] != 0)\n  \n  # precision top 10%:\n  top10 <- which(abs(est) > quantile(abs(est[est!=0]), 0.90))\n  out[[\"precision_top10\"]] <- sum(est[top10]!=0 & real[top10] != 0) / sum(est[top10] != 0)\n  \n  # Signed sensitivity:\n  TruePos_signed <- sum(est != 0 &  real != 0 & sign(est) == sign(real))\n  out$sensitivity_signed <- TruePos_signed / (TruePos + FalseNeg)\n  \n  # Correlation:\n  out$correlation <- cor0(est,real)\n  \n  # Correlation between absolute edges:\n  out$abs_cor <- cor0(abs(est),abs(real))\n  \n  #\n  out$bias <- bias(est,real)\n  \n  ## Some measures for true edges only:\n  if (TruePos > 0){\n    \n    trueEdges <- est != 0 & real != 0\n    \n    out$bias_true_edges <- bias(est[trueEdges],real[trueEdges])\n    out$abs_cor_true_edges <- cor0(abs(est[trueEdges]),abs(real[trueEdges]))\n  } else {\n    out$bias_true_edges <- NA\n    out$abs_cor_true_edges <- NA\n  }\n  \n  out$truePos <- TruePos\n  out$falsePos <- FalsePos\n  out$trueNeg <- TrueNeg\n  out$falseNeg <- FalseNeg\n  \n  # Mean absolute weight false positives:\n  false_edges <- (est != 0 &  real == 0) | (est != 0 & real != 0 & sign(est) != sign(real) )\n  out$mean_false_edge_weight <- mean(abs(est[false_edges]))\n  out$SD_false_edge_weight <- sd(abs(est[false_edges]))\n  \n  # Fading:\n  out$maxfade_false_edge <- max(abs(est[false_edges])) / max(abs(est))\n  out$meanfade_false_edge <- mean(abs(est[false_edges])) / max(abs(est))\n  \n  \n  # Set naname\n  if (name != \"\"){\n    names(out) <- paste0(names(out),\"_\",name)  \n  }\n  out\n}\n```\n:::\n\n\n\n# Estimation\n\n## Method 1 - EBICglasso in `bootnet`\n\nThis method uses the `bootnet::estimateNetwork` function.\n\n`EBICtuning <- 0.5` sets up the hyperparameter ($\\gamma$) as .5 that controls how much the EBIC prefers simpler models (fewer edges), which by default is .5.\n\n$$\n\\text{EBIC} =-2L+E(\\log(N))+4\\gamma E(\\log(P))\n$$\n\nAnother important setting is the tuning parameter ($\\lambda$) of EBICglasso that controls the sparsity level in the penalized likelihood function as:\n\n$$\n\\log \\det(\\boldsymbol{K}) - \\text{trace}(\\boldsymbol{SK})-\\lambda \\Sigma|\\kappa_{ij}|\n$$\n\nwhere $\\boldsymbol{S}$ represents the sample variance-covariance matrix and $\\boldsymbol{K}$ represents the precision matrix that *lasso* aims to estimate. Overall, the regularization limits the sum of absolute partial correlation coefficients.\n\n`sampleSize=\"pairwise_average\"` will set the sample size to the average of sample sizes used for each individual correlation in `EBICglasso`.\n\n@fig-bfi is the estimated network structure with BFI-25 data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 1: EBICglasso\"}\nlibrary(\"qgraph\")\nlibrary(\"bootnet\")\nsampleAdjust  <-  \"pairwise_average\"\nEBICtuning <- 0.5\ntransformation <- \"polychoric/categorical\" # EBICglasso use cor_auto\nres_m1 <- estimateNetwork(data, # input as polychoric correlation\n                          default = \"EBICglasso\",\n                          sampleSize = sampleAdjust,\n                          tuning = EBICtuning,\n                          corMethod = ifelse(transformation == \"polychoric/categorical\",\n                                             \"cor_auto\",\n                                             \"cor\"))\n    \nestnet_m1 <- res_m1$graph\n\nqgraph(estnet_m1)\n```\n\n::: {.cell-output-display}\n![Method 1: Network Plot for BFI-25 with EBICglasso](index_files/figure-html/fig-bfi-1.png){#fig-bfi width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance <- function(network) {\n  res_comparison <- comparison_metrics(real = as.matrix(bfi_truenetwork), est = as.matrix(network))\n  c(\n    sensitivity = res_comparison$sensitivity_full,\n    specificity = res_comparison$specificity_full,\n    precision = res_comparison$precision_full,\n    \n    abs_corr = res_comparison$abs_cor_true_edges_full, # The Pearson correlation between the absolute edge weights\n    bias = res_comparison$bias_full, # The average absolute deviation\n    bias_true = res_comparison$bias_true_edges_full # The average absolute deviation between the true edge weight and the estimated edge weight\n  )\n}\nperformance(estnet_m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsensitivity specificity   precision    abs_corr        bias   bias_true \n0.982142857 0.840399002 0.774647887 0.963820333 0.009005281 0.021625848 \n```\n\n\n:::\n:::\n\n\n\n## Method 2 - ggmModSelect in `bootnet`\n\nThe *ggmModSelect* algorithm is a non-regularized method.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 2: ggmModSelect\"}\n# With stepwise\nres_m2_1 <- estimateNetwork(data, default = \"ggmModSelect\",\n                           sampleSize = sampleAdjust, stepwise = TRUE,\n                           tuning = EBICtuning,\n                           corMethod = ifelse(transformation == \"polychoric/categorical\",\n                                              \"cor_auto\",\n                                              \"cor\"))\n\nestnet_m2_1 <- res_m2_1$graph\nqgraph(estnet_m2_1)\n```\n\n::: {.cell-output-display}\n![Method 2.1: Network Plot for BFI-25 with ggmModSelect and Stepwise](index_files/figure-html/fig-m2_1-1.png){#fig-m2_1 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 2: ggmModSelect\"}\n# Without stepwise\nres_m2_2 <- estimateNetwork(data, default = \"ggmModSelect\",\n                           sampleSize = sampleAdjust, stepwise = FALSE,\n                           tuning = EBICtuning,\n                           corMethod = ifelse(transformation == \"polychoric/categorical\",\n                                              \"cor_auto\",\n                                              \"cor\"))\nestnet_m2_2 <- res_m2_2$graph\nqgraph(estnet_m2_2)\n```\n\n::: {.cell-output-display}\n![Method 2.1: Network Plot for BFI-25 with ggmModSelect and No Stepwise](index_files/figure-html/fig-m2_2-1.png){#fig-m2_2 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance_compr <- Reduce(rbind, lapply(list(estnet_m1, estnet_m2_1, estnet_m2_2), performance))\nrownames(performance_compr) <- c(\"EBICglasso\", \"ggmModSelect_stepwise\", \"ggmModSelect_nostepwise\")\nkableExtra::kable(performance_compr)\n```\n\n::: {.cell-output-display}\n\n\n|                        | sensitivity| specificity| precision|  abs_corr|      bias| bias_true|\n|:-----------------------|-----------:|-----------:|---------:|---------:|---------:|---------:|\n|EBICglasso              |   0.9821429|   0.8403990| 0.7746479| 0.9638203| 0.0090053| 0.0216258|\n|ggmModSelect_stepwise   |   0.8839286|   1.0000000| 1.0000000| 0.9588360| 0.0081832| 0.0170543|\n|ggmModSelect_nostepwise |   0.9375000|   0.9002494| 0.8400000| 0.9669727| 0.0078012| 0.0148425|\n\n\n:::\n:::\n\n\n\n## Method 3 - FIML in `psychonetrics`\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 3: FIML\"}\nlibrary(psychonetrics)\nlibrary(magrittr)\n# With stepwise\nres_m3_1 <- ggm(data, estimator = \"FIML\", standardize = \"z\") %>% \n  prune(alpha = .01, recursive = FALSE) \n    \nestnet_m3_1 <- getmatrix(res_m3_1, \"omega\")\n\nqgraph(estnet_m3_1)\n```\n\n::: {.cell-output-display}\n![Method 3: Network Plot for BFI-25 with FIML and Prue](index_files/figure-html/fig-m3_1-1.png){#fig-m3_1 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 3: FIML\"}\n# With stepwise\nres_m3_2 <- ggm(data, estimator = \"FIML\", standardize = \"z\") %>% \n  prune(alpha = .01, recursive = FALSE) |> \n  stepup(alpha = .01)\n    \nestnet_m3_2 <- getmatrix(res_m3_2, \"omega\")\n\nqgraph(estnet_m3_2)\n```\n\n::: {.cell-output-display}\n![Method 3: Network Plot for BFI-25 with FIML and Stepup](index_files/figure-html/fig-m3_2-1.png){#fig-m3_2 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance_compr <- Reduce(rbind, lapply(list(estnet_m1, estnet_m2_1, estnet_m2_2, \n                                               estnet_m3_1, estnet_m3_2), performance))\nrownames(performance_compr) <- c(\"EBICglasso\", \"ggmModSelect_stepwise\", \"ggmModSelect_nostepwise\",\n                                 \"FIML_prune\", \"FIML_stepup\")\nkableExtra::kable(performance_compr, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|                        | sensitivity| specificity| precision| abs_corr|  bias| bias_true|\n|:-----------------------|-----------:|-----------:|---------:|--------:|-----:|---------:|\n|EBICglasso              |       0.982|        0.84|     0.775|    0.964| 0.009|     0.022|\n|ggmModSelect_stepwise   |       0.884|        1.00|     1.000|    0.959| 0.008|     0.017|\n|ggmModSelect_nostepwise |       0.938|        0.90|     0.840|    0.967| 0.008|     0.015|\n|FIML_prune              |       0.929|        0.99|     0.981|    0.966| 0.007|     0.015|\n|FIML_stepup             |       0.982|        0.99|     0.982|    0.972| 0.006|     0.014|\n\n\n:::\n:::\n\n\n\n## Method 4 - WLS in `psychonetrics`\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 4: WLS\"}\n# With stepwise\nres_m4_1 <- ggm(data, estimator = \"WLS\", standardize = \"z\") %>% \n  prune(alpha = .01, recursive = FALSE) \n    \nestnet_m4_1 <- getmatrix(res_m4_1, \"omega\")\n\nqgraph(estnet_m4_1)\n```\n\n::: {.cell-output-display}\n![Method 4: Network Plot for BFI-25 with WLS and prune](index_files/figure-html/fig-m4_1-1.png){#fig-m4_1 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 4: WLS\"}\nres_m4_2 <- ggm(data, estimator = \"WLS\", standardize = \"z\") %>% \n  prune(alpha = .01, recursive = FALSE) |> \n  modelsearch(prunealpha = .01, addalpha = .01)\n    \nestnet_m4_2 <- getmatrix(res_m4_2, \"omega\")\n\nqgraph(estnet_m4_2)\n```\n\n::: {.cell-output-display}\n![Method 4: Network Plot for BFI-25 with WLS, prune, and modelsearch](index_files/figure-html/fig-m4_2-1.png){#fig-m4_2 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance_compr <- Reduce(rbind, lapply(list(estnet_m1, estnet_m2_1, estnet_m2_2, \n                                               estnet_m3_1, estnet_m3_2,\n                                               estnet_m4_1, estnet_m4_2), performance))\nrownames(performance_compr) <- c(\"EBICglasso\", \"ggmModSelect_stepwise\", \"ggmModSelect_nostepwise\",\n                                 \"FIML_prune\", \"FIML_stepup\",\n                                 \"WLS_prune\", \"WLS_prune_modelsearch\")\nkableExtra::kable(performance_compr, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|                        | sensitivity| specificity| precision| abs_corr|  bias| bias_true|\n|:-----------------------|-----------:|-----------:|---------:|--------:|-----:|---------:|\n|EBICglasso              |       0.982|       0.840|     0.775|    0.964| 0.009|     0.022|\n|ggmModSelect_stepwise   |       0.884|       1.000|     1.000|    0.959| 0.008|     0.017|\n|ggmModSelect_nostepwise |       0.938|       0.900|     0.840|    0.967| 0.008|     0.015|\n|FIML_prune              |       0.929|       0.990|     0.981|    0.966| 0.007|     0.015|\n|FIML_stepup             |       0.982|       0.990|     0.982|    0.972| 0.006|     0.014|\n|WLS_prune               |       0.938|       0.995|     0.991|    0.973| 0.006|     0.014|\n|WLS_prune_modelsearch   |       0.938|       0.995|     0.991|    0.973| 0.006|     0.014|\n\n\n:::\n:::\n\n\n\n## Method 5 - mgm in `mgm`\n\n`mgm` package is used for estimating mixed graphical models. Node type can be \"g\" for Gaussian or \"c\" for categorical. For continuous variables, set `level` to 1 and `type` to g.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 5: mgm\"}\nlibrary(mgm)\n## mgm with CV\nres_m5_1 <- mgm(na.omit(data), type = rep(\"g\", ncol(data)), level = rep(1, ncol(data)), \n           lambdaFolds = 20,\n           lambdaSel = \"CV\", lambdaGam = EBICtuning, \n           pbar = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNote that the sign of parameter estimates is stored separately; see ?mgm\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 5: mgm\"}\ngetmatrix_mgm <- function(res) {\n  sign = ifelse(is.na(res$pairwise$signs), 1, res$pairwise$signs)\n  sign * res$pairwise$wadj\n}\nestnet_m5_1 <- getmatrix_mgm(res_m5_1)\nqgraph(estnet_m5_1)\n```\n\n::: {.cell-output-display}\n![Method 5: Network Plot for BFI-25 with mgm and CV](index_files/figure-html/fig-m5_1-1.png){#fig-m5_1 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 5: mgm\"}\n## mgm with EBIC\nres_m5_2 <- mgm(na.omit(data), type = rep(\"g\", ncol(data)), level = rep(1, ncol(data)), \n           lambdaFolds = 20,\n           lambdaSel = \"EBIC\", lambdaGam = EBICtuning,\n           pbar = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNote that the sign of parameter estimates is stored separately; see ?mgm\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 5: mgm\"}\nestnet_m5_2 <- getmatrix_mgm(res_m5_2)\nqgraph(estnet_m5_2)\n```\n\n::: {.cell-output-display}\n![Method 5: Network Plot for BFI-25 with mgm and EBIC](index_files/figure-html/fig-m5_2-1.png){#fig-m5_2 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance_compr <- Reduce(rbind, lapply(list(estnet_m1, estnet_m2_1, estnet_m2_2, \n                                               estnet_m3_1, estnet_m3_2,\n                                               estnet_m4_1, estnet_m4_2,\n                                               estnet_m5_1, estnet_m5_2), performance))\nrownames(performance_compr) <- c(\"EBICglasso\", \"ggmModSelect_stepwise\", \"ggmModSelect_nostepwise\",\n                                 \"FIML_prune\", \"FIML_stepup\",\n                                 \"WLS_prune\", \"WLS_prune_modelsearch\",\n                                 \"mgm_CV\", \"mgm_EBIC\")\nkableExtra::kable(performance_compr, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|                        | sensitivity| specificity| precision| abs_corr|  bias| bias_true|\n|:-----------------------|-----------:|-----------:|---------:|--------:|-----:|---------:|\n|EBICglasso              |       0.982|       0.840|     0.775|    0.964| 0.009|     0.022|\n|ggmModSelect_stepwise   |       0.884|       1.000|     1.000|    0.959| 0.008|     0.017|\n|ggmModSelect_nostepwise |       0.938|       0.900|     0.840|    0.967| 0.008|     0.015|\n|FIML_prune              |       0.929|       0.990|     0.981|    0.966| 0.007|     0.015|\n|FIML_stepup             |       0.982|       0.990|     0.982|    0.972| 0.006|     0.014|\n|WLS_prune               |       0.938|       0.995|     0.991|    0.973| 0.006|     0.014|\n|WLS_prune_modelsearch   |       0.938|       0.995|     0.991|    0.973| 0.006|     0.014|\n|mgm_CV                  |       0.964|       0.865|     0.800|    0.973| 0.009|     0.017|\n|mgm_EBIC                |       0.821|       0.980|     0.958|    0.965| 0.012|     0.026|\n\n\n:::\n:::\n\n\n\n## Method 6 - BGGM in `BGGM`\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 6: BGGM\"}\nres_m6_1 <- BGGM::explore(data, type = \"continuous\") |> \n  BGGM:::select.explore(BF_cut = 3)\nestnet_m6_1 <- res_m6_1$pcor_mat_zero\nqgraph(estnet_m6_1)\n```\n\n::: {.cell-output-display}\n![Method 6: Network Plot for BFI-25 with BGGM and explore](index_files/figure-html/fig-m6_1-1.png){#fig-m6_1 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Method 6: BGGM\"}\nres_m6_2 <- BGGM::explore(data, type = \"continuous\") |> \n  BGGM:::select.estimate(cred = 0.95)\nestnet_m6_2 <- res_m6_2$pcor_adj\nqgraph(estnet_m6_2)\n```\n\n::: {.cell-output-display}\n![Method 6: Network Plot for BFI-25 with BGGM and estimate](index_files/figure-html/fig-m6_2-1.png){#fig-m6_2 width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance_compr <- Reduce(rbind, lapply(list(estnet_m1, estnet_m2_1, estnet_m2_2, \n                                               estnet_m3_1, estnet_m3_2,\n                                               estnet_m4_1, estnet_m4_2,\n                                               estnet_m5_1, estnet_m5_2,\n                                               estnet_m6_1, estnet_m6_2), performance))\nrownames(performance_compr) <- c(\"EBICglasso\", \"ggmModSelect_stepwise\", \"ggmModSelect_nostepwise\",\n                                 \"FIML_prune\", \"FIML_stepup\",\n                                 \"WLS_prune\", \"WLS_prune_modelsearch\",\n                                 \"mgm_CV\", \"mgm_EBIC\",\n                                 \"BGGM_explore\", \"BGGM_estimate\")\nkableExtra::kable(performance_compr, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|                        | sensitivity| specificity| precision| abs_corr|  bias| bias_true|\n|:-----------------------|-----------:|-----------:|---------:|--------:|-----:|---------:|\n|EBICglasso              |       0.982|       0.840|     0.775|    0.964| 0.009|     0.022|\n|ggmModSelect_stepwise   |       0.884|       1.000|     1.000|    0.959| 0.008|     0.017|\n|ggmModSelect_nostepwise |       0.938|       0.900|     0.840|    0.967| 0.008|     0.015|\n|FIML_prune              |       0.929|       0.990|     0.981|    0.966| 0.007|     0.015|\n|FIML_stepup             |       0.982|       0.990|     0.982|    0.972| 0.006|     0.014|\n|WLS_prune               |       0.938|       0.995|     0.991|    0.973| 0.006|     0.014|\n|WLS_prune_modelsearch   |       0.938|       0.995|     0.991|    0.973| 0.006|     0.014|\n|mgm_CV                  |       0.964|       0.865|     0.800|    0.973| 0.009|     0.017|\n|mgm_EBIC                |       0.821|       0.980|     0.958|    0.965| 0.012|     0.026|\n|BGGM_explore            |       0.920|       1.000|     1.000|    0.970| 0.007|     0.015|\n|BGGM_estimate           |       0.991|       0.980|     0.965|    0.969| 0.006|     0.015|\n\n\n:::\n:::\n\n\n\n# Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nperformance_tbl <- performance_compr |> \n  as.data.frame() |> \n  mutate(method = rownames(performance_compr)) |> \n  mutate(method = factor(method, levels = rownames(performance_compr)))\nggplot(performance_tbl) +\n  geom_point(aes(x = sensitivity, y = specificity, col = method), size = 3) +\n  theme_bw() +\n  theme(legend.position = 'bottom')\n```\n\n::: {.cell-output-display}\n![Specificity and Sensitivity Balance](index_files/figure-html/fig-summary01-1.png){#fig-summary01 width=768}\n:::\n:::\n\n\n\n`FLML_prune` and `BGGM_estimation` seem to perform best among all methods regarding balance between sensitivity and specificity of network weights.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(performance_tbl) +\n  geom_col(aes(y = forcats::fct_reorder(method, precision), x = precision, fill = method)) +\n  theme_bw() +\n  labs(y = \"\") +\n  theme(legend.position = 'bottom')\n```\n\n::: {.cell-output-display}\n![Precision among all methods](index_files/figure-html/fig-summary02-1.png){#fig-summary02 width=960}\n:::\n:::\n\n\n\n`ggmMoSelect` with the stepwise procedure appears to have highest precision, followed by BGGM explore.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}