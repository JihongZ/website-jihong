{
  "hash": "52e5750f490fa6a17fe7ac3541fc1ca2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulate response patterns under the framework of IRT\"\nauthor: 'Jihong Zhang'\nsubtitle: \"Using mirt package\"\ndate: 'April 15 2024'\ncategories:\n  - R\n  - Simulation\n  - Mirt\nexecute: \n  eval: true\n  echo: true\n  warning: false\nformat: \n  html: \n    code-fold: false\n    code-summary: 'Click to see the code'\n    number-sections: true\n---\n\n\n\n## Research question:\n\n1.  How different methods of cutting item response affecting parameter estimation accuracy?\n    -   Merge first two categories\n    -   Merge the middle category into the neighboring category\n\nWe need to pre-specify `a` (item slopes) and `b` (item intercepts). `mu` (factor mean) and `sigma` (factor correlation) are optional.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mirt)\n# Simulation for 6 5-category items\nset.seed(1234)\nJ = 6 # number of items\nN = 1000 # number of participants\na <- matrix(rlnorm(J, mean = 0, sd = 1), ncol = 1)\na\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\n[1,] 0.29907355\n[2,] 1.31973273\n[3,] 2.95778648\n[4,] 0.09578035\n[5,] 1.53591253\n[6,] 1.65873604\n```\n\n\n:::\n\n```{.r .cell-code}\n# for the graded model, ensure that there is enough space between the intercepts,\n# otherwise closer categories will not be selected often (minimum distance of 0.3 here)\ndiffs <- t(apply(matrix(runif(J*4, .3, 1), nrow = J), 1, cumsum))\nd <- -diffs + rnorm(J) + 4\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]     [,3]      [,4]\n[1,] 2.664915 2.234209 1.781049 1.1617851\n[2,] 5.469432 5.006874 4.139455 3.6538239\n[3,] 3.629467 3.107838 2.439850 1.9265796\n[4,] 2.623207 2.111322 1.171061 0.5159463\n[5,] 3.059096 2.647764 1.765822 1.3390547\n[6,] 3.972815 3.644818 3.312779 2.4810091\n```\n\n\n:::\n:::\n\n\n\n## Data generation\n\n### Model 1: population Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Population data: 5-category: 0-4\ndat1 <- simdata(a, d, N, itemtype = rep('graded', 6))\nhead(dat1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Item_1 Item_2 Item_3 Item_4 Item_5 Item_6\n[1,]      4      4      0      2      2      3\n[2,]      4      4      0      3      1      1\n[3,]      4      4      4      3      4      4\n[4,]      4      4      4      4      3      4\n[5,]      4      4      2      2      4      4\n[6,]      4      4      0      2      4      4\n```\n\n\n:::\n\n```{.r .cell-code}\napply(dat1, 2, min)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem_1 Item_2 Item_3 Item_4 Item_5 Item_6 \n     0      0      0      0      0      0 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(dat1, 2, max)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem_1 Item_2 Item_3 Item_4 Item_5 Item_6 \n     4      4      4      4      4      4 \n```\n\n\n:::\n:::\n\n\n\n### Model 2: 4-category data with first two categories merged\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data2: Merged-category data: 4-category, 0-1->0,2->1,3->2,4->3\ndat2 <- dat1\ndat2[dat2%in%0:1] <- 0\ndat2[dat2==2] <- 1\ndat2[dat2==3] <- 2\ndat2[dat2==4] <- 3\napply(dat2, 2, min)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem_1 Item_2 Item_3 Item_4 Item_5 Item_6 \n     0      0      0      0      0      0 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(dat2, 2, max)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem_1 Item_2 Item_3 Item_4 Item_5 Item_6 \n     3      3      3      3      3      3 \n```\n\n\n:::\n:::\n\n\n\n### Model 3: 4-category data with middle 2 categories merged\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data3: Merged-category data: 4-category, 0->0,1-2->1,3->2,4->3\ndat3 <- dat1\ndat3[dat3%in%1:2] <- 1\ndat3[dat3==3] <- 2\ndat3[dat3==4] <- 3\napply(dat3, 2, min)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem_1 Item_2 Item_3 Item_4 Item_5 Item_6 \n     0      0      0      0      0      0 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(dat3, 2, max)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nItem_1 Item_2 Item_3 Item_4 Item_5 Item_6 \n     3      3      3      3      3      3 \n```\n\n\n:::\n:::\n\n\n\n## Data analysis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Population model\nmod1 <- mirt(dat1, model = 1)\nmod1_param <- coef(mod1, simplify = TRUE)$items\nmod1_itemfit <- itemfit(mod1)\n\n## Tail merge model: model2\nmod2 <- mirt(dat2, model = 1)\nmod2_param <- coef(mod2, simplify = TRUE)$items\nmod2_itemfit <- itemfit(mod2)\n\n## Middle category merge model: model3\nmod3 <- mirt(dat3, model = 1)\nmod3_param <- coef(mod3, simplify = TRUE)$items\nmod3_itemfit <- itemfit(mod3)\n```\n:::\n\n\n\n## Parameter recovery\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]     [,3]      [,4]\n[1,] 2.664915 2.234209 1.781049 1.1617851\n[2,] 5.469432 5.006874 4.139455 3.6538239\n[3,] 3.629467 3.107838 2.439850 1.9265796\n[4,] 2.623207 2.111322 1.171061 0.5159463\n[5,] 3.059096 2.647764 1.765822 1.3390547\n[6,] 3.972815 3.644818 3.312779 2.4810091\n```\n\n\n:::\n\n```{.r .cell-code}\nkableExtra::kable(mod1_param[,2:5], digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|       |    d1|    d2|    d3|    d4|\n|:------|-----:|-----:|-----:|-----:|\n|Item_1 | 2.736| 2.385| 1.904| 1.305|\n|Item_2 | 4.859| 4.593| 3.841| 3.362|\n|Item_3 | 3.313| 2.689| 2.066| 1.585|\n|Item_4 | 2.701| 2.203| 1.191| 0.533|\n|Item_5 | 2.984| 2.647| 1.815| 1.395|\n|Item_6 | 3.998| 3.611| 3.188| 2.445|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrev_bias <- function(true, est) {\n  diffs = abs(true - est)\n  mean(diffs)\n}\n\nrmse <- function(true, est) {\n  sqrt(mean((true - est)^2))\n}\n\n\nResults <- data.frame(\n  Bias = c(\n    rev_bias(true = as.numeric(a), est = mod1_param[,'a1']),\n    rev_bias(true = as.numeric(a), est = mod2_param[,'a1']),\n    rev_bias(true = as.numeric(a), est = mod3_param[,'a1'])\n  ),\n  RMSE = c(\n    rmse(true = as.numeric(a), est = mod1_param[,'a1']),\n    rmse(true = as.numeric(a), est = mod2_param[,'a1']),\n    rmse(true = as.numeric(a), est = mod3_param[,'a1'])\n  )\n)\nrownames(Results) <- paste0(\"Model \", 1:3)\nkableExtra::kable(Results, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|        |  Bias|  RMSE|\n|:-------|-----:|-----:|\n|Model 1 | 0.138| 0.181|\n|Model 2 | 0.148| 0.192|\n|Model 3 | 0.144| 0.199|\n\n\n:::\n:::\n\n\n\n## Item fit\n\nModel 1 (population model) seems to have overall best item fits. Model 2 (tail merged model) seems to have most worst-fitting items. Model 3 (middle-range merged model) seems to have the worst-fitting item (item 6).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRMSEA_mat <- rbind(\n  mod1 = mod1_itemfit$RMSEA.S_X2,\n  mod2 = mod2_itemfit$RMSEA.S_X2,\n  mod3 = mod3_itemfit$RMSEA.S_X2\n)\ncolnames(RMSEA_mat) <- paste0('Item', 1:6)\nrownames(RMSEA_mat) <- paste0('Model ', 1:3)\nkableExtra::kable(RMSEA_mat, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|        | Item1| Item2| Item3| Item4| Item5| Item6|\n|:-------|-----:|-----:|-----:|-----:|-----:|-----:|\n|Model 1 |     0|     0| 0.007| 0.000| 0.000| 0.015|\n|Model 2 |     0|     0| 0.000| 0.014| 0.014| 0.018|\n|Model 3 |     0|     0| 0.000| 0.000| 0.000| 0.026|\n\n\n:::\n:::\n\n\n\n## Global fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM2_df <- rbind(\n  M2(mod1, \"C2\"),\n  M2(mod2, \"C2\"),\n  M2(mod3, \"C2\")\n)\nrownames(M2_df) <- paste0(\"Model \", 1:3)\nkableExtra::kable(M2_df, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|        |    M2| df|     p| RMSEA| RMSEA_5| RMSEA_95| SRMSR|   TLI|   CFI|\n|:-------|-----:|--:|-----:|-----:|-------:|--------:|-----:|-----:|-----:|\n|Model 1 | 7.446|  9| 0.591| 0.000|       0|    0.031| 0.024| 1.006| 1.000|\n|Model 2 | 9.723|  9| 0.373| 0.009|       0|    0.037| 0.027| 0.997| 0.998|\n|Model 3 | 9.265|  9| 0.413| 0.005|       0|    0.036| 0.027| 0.999| 0.999|\n\n\n:::\n:::\n\n\n\n## MCMC simulation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parSim)\nResults <- parSim(\n  J = c(6, 10, 20),\n  N = c(300, 500, 1000),\n  # Number of repititions?\n  reps = 100, # more is better!\n  \n  # Parallel?\n  nCores = 8,\n  export = c(\"rev_bias\"),\n  expression = {\n      library(mirt)\n      a <- matrix(rlnorm(J, mean = 0, sd = 1), ncol = 1)\n      diffs <- t(apply(matrix(runif(J*4, .3, 1), nrow = J), 1, cumsum))\n      d <- -diffs + rnorm(J) + 4\n      dat1 <- simdata(a, d, N, itemtype = rep('graded', J))\n      dat2 <- dat1\n      dat2[dat2%in%0:1] <- 0\n      dat2[dat2==2] <- 1\n      dat2[dat2==3] <- 2\n      dat2[dat2==4] <- 3\n      dat3 <- dat1\n      dat3[dat3%in%1:2] <- 1\n      dat3[dat3==3] <- 2\n      dat3[dat3==4] <- 3\n      ## Population model\n      mod1 <- mirt(dat1, model = 1)\n      mod1_param <- coef(mod1, simplify = TRUE)$items\n      mod1_gfi <- M2(mod1, \"C2\")\n      \n      ## Tail merge model: model2\n      mod2 <- mirt(dat2, model = 1)\n      mod2_param <- coef(mod2, simplify = TRUE)$items\n      mod2_gfi <- M2(mod2, \"C2\")\n      \n      ## middle category merge model: model3\n      mod3 <- mirt(dat3, model = 1)\n      mod3_param <- coef(mod3, simplify = TRUE)$items\n      mod3_gfi <- M2(mod3, \"C2\")\n      ## output\n      data.frame(\n       bias_mod1 = rev_bias(true = as.numeric(a), est = mod1_param[,'a1']),\n       bias_mod2 = rev_bias(true = as.numeric(a), est = mod2_param[,'a1']),\n       bias_mod3 = rev_bias(true = as.numeric(a), est = mod3_param[,'a1']),\n       RMSEA_mod1 = mod1_gfi$RMSEA,\n       RMSEA_mod2 = mod2_gfi$RMSEA,\n       RMSEA_mod3 = mod3_gfi$RMSEA,\n       M2_mod1 = mod1_gfi$M2,\n       M2_mod2 = mod2_gfi$M2,\n       M2_mod3 = mod3_gfi$M2,\n       SRMSR_mod1 = mod1_gfi$SRMSR,\n       SRMSR_mod2 = mod2_gfi$SRMSR,\n       SRMSR_mod3 = mod3_gfi$SRMSR,\n       TLI_mod1 = mod1_gfi$TLI,\n       TLI_mod2 = mod2_gfi$TLI,\n       TLI_mod3 = mod3_gfi$TLI\n      )\n  }\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nResults_plot <- Results |> \n  pivot_longer(c(starts_with(\"bias\"), \n                 starts_with(\"RMSEA\"), \n                 starts_with(\"SRMSR\"), \n                 starts_with(\"TLI\")), \n               names_to = 'Model', values_to = 'Value') |> \n  separate(Model, into = c(\"Measure\", \"Model\")) |> \n  group_by(J, N, Model, Measure) |> \n  summarise(Value = mean(Value, na.rm = T)) |> \n  mutate(N = factor(N, levels = c(300, 500, 1000)))\n\nResults_plot |> \n  ggplot() +\n  geom_path(aes(x = N, y = Value, col = Model, group = Model)) +\n  geom_point(aes(x = N, y = Value, col = Model)) +\n  facet_grid(Measure ~ J, scales = \"free\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Performance measures across conditions](index_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Conclusion\n\n1.  There are not much differences in global model fits regarding varied cutting methods of Likert-scale item responses under the conditions in the simulation.\n2.  Increases of test length and sample sizes will increase estimation accuracy for both cutting methods.\n3.  Item fits will differ by varied cutting methods.\n4.  Goodness-of-fit may not perform well evaluating cutting methods of item responses.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}