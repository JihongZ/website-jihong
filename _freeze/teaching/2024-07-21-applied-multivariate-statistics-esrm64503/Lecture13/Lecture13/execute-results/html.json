{
  "hash": "648f40b9a56d453cf10990fbc44238de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 13: Confirmatory Factor Analysis and Psychological Network\"\nsubtitle: \"\"\nauthor: \"Jihong Zhang\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-11-22\"\nsidebar: false\nexecute: \n  echo: true\n  eval: true\n  warning: false\n  message: false\nformat: \n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"Lecture 13: Confirmatory Factor Analysis and Network Psychometrics\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Today's Objectives\n\n1.  Understand the relationship between Exploratory and Confirmatory Factor Analysis\n\n2.  Understand what is network analysis and confirmatory factor analysis\n\n3.  Understand how network model can be applied into real examples\n\n# Confirmatory Factor Analysis\n\n## CFA Approach to EFA\n\n-   EFA and CFA are same things except that one is to explore the factor structure (EFA) and one is to confirm pre-determined factor structure (CFA).\n\n-   But ... we can also conduct **exploratory analysis** using a CFA model\n<<<<<<< HEAD\n=======\n\n    -   Need to set the right number of constraints for identification\n    -   We set the value of factor loadings for a few items on a few of the factors\n        -   Typically to zero\n        -   Sometimes to one (Brown, 2002)\n    -   We keep the factor covariance matrix as an identity\n        -   Uncorrelated factors (as in EFA) with variances of one\n\n-   Benefits of using CFA for exploratory analyses:\n    -   CFA constraints remove rotational indeterminacy of factor loadings – no rotating is needed (or possible)\n    -   Defines factors with potentially less ambiguity \n        - Constraints are easy to see \n        - For some software (SAS and SPSS), we get much more model fit information\n    -   For some software (SAS and SPSS), we get much more model fit information    \n\n## CFA Example\n\n-   We can use `lavaan` to do CFA... here is the syntax for the one factor model\n    -   The `~=` is the symbol represent factors and their loaded items \n    \n-   We can see that the one-factor CFA has same loading with EFA\n    \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lavaan)\ndata02a = read.csv(file=\"gambling_lecture12.csv\", header=TRUE)\nhead(data02a, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X1 X3 X5 X9 X10 X13 X14 X18 X21 X23\n1  4  5  3  2   2   2   2   2   2   2\n2  1  1  1  1   1   1   1   1   1   1\n3  2  1  1  3   2   2   2   2   2   2\n```\n\n\n:::\n\n```{.r .cell-code}\n# One-factor CFA Model\nCFA_1factor.syntax = \"\nfactor1 =~ X1 + X3 + X5 + X9 + X10 + X13 + X14 + X18 + X21 + X23\n\"\n\n#for comparison with EFA we are using standardized factors (var = 1; mean = 0)\nCFA_1factor.model = cfa(model = CFA_1factor.syntax, data = data02a, estimator = \"MLR\", std.lv = TRUE)\nsummary(CFA_1factor.model, fit.measures = TRUE, standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6-19 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n                                                  Used       Total\n  Number of observations                          1333        1336\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               161.846     104.001\n  Degrees of freedom                                35          35\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.556\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4148.081    2238.585\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.853\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.969       0.969\n  Tucker-Lewis Index (TLI)                       0.960       0.960\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.974\n  Robust Tucker-Lewis Index (TLI)                            0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -16575.733  -16575.733\n  Scaling correction factor                                  3.031\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -16494.810  -16494.810\n  Scaling correction factor                                  2.092\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               33191.466   33191.466\n  Bayesian (BIC)                             33295.370   33295.370\n  Sample-size adjusted Bayesian (SABIC)      33231.839   33231.839\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.052       0.038\n  90 Percent confidence interval - lower         0.044       0.032\n  90 Percent confidence interval - upper         0.060       0.045\n  P-value H_0: RMSEA <= 0.050                    0.318       0.997\n  P-value H_0: RMSEA >= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.048\n  90 Percent confidence interval - lower                     0.037\n  90 Percent confidence interval - upper                     0.059\n  P-value H_0: Robust RMSEA <= 0.050                         0.604\n  P-value H_0: Robust RMSEA >= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028       0.028\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  factor1 =~                                                            \n    X1                0.581    0.039   15.002    0.000    0.581    0.569\n    X3                0.451    0.038   11.896    0.000    0.451    0.521\n    X5                0.647    0.041   15.684    0.000    0.647    0.670\n    X9                0.542    0.034   16.182    0.000    0.542    0.764\n    X10               0.598    0.034   17.355    0.000    0.598    0.688\n    X13               0.684    0.037   18.334    0.000    0.684    0.715\n    X14               0.562    0.038   14.601    0.000    0.562    0.378\n    X18               0.547    0.038   14.438    0.000    0.547    0.429\n    X21               0.564    0.036   15.774    0.000    0.564    0.680\n    X23               0.635    0.033   19.346    0.000    0.635    0.653\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .X1                0.706    0.063   11.130    0.000    0.706    0.677\n   .X3                0.546    0.043   12.732    0.000    0.546    0.728\n   .X5                0.513    0.047   10.888    0.000    0.513    0.550\n   .X9                0.210    0.016   12.932    0.000    0.210    0.417\n   .X10               0.399    0.043    9.297    0.000    0.399    0.527\n   .X13               0.447    0.047    9.541    0.000    0.447    0.488\n   .X14               1.894    0.078   24.226    0.000    1.894    0.857\n   .X18               1.326    0.096   13.842    0.000    1.326    0.816\n   .X21               0.370    0.036   10.303    0.000    0.370    0.538\n   .X23               0.542    0.047   11.570    0.000    0.542    0.573\n    factor1           1.000                               1.000    1.000\n```\n\n\n:::\n:::\n\n\n\n\n\n    \n## CFA Example: Two-factor model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#two factor CFA: one item removed from factor 2 and zero covariance between factors\n\nCFA_2factor.syntax = \"\nfactor1 =~ X1 + X3 + X5 + X9 + X10 + X13 + X14 + X18 + X21 + X23\nfactor2 =~      X3 + X5 + X9 + X10 + X13 + X14 + X18 + X21 + X23\n\nfactor1 ~ 0*factor2\n\"\n\n#for comparison with EFA we are using standardized factors (var = 1; mean = 0)\nCFA_2factor.model = cfa(model = CFA_2factor.syntax, data = data02a, estimator = \"MLR\", std.lv = TRUE)\nsummary(CFA_2factor.model, fit.measures = TRUE, standardized = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6-19 ended normally after 42 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n\n                                                  Used       Total\n  Number of observations                          1333        1336\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                56.704      39.189\n  Degrees of freedom                                26          26\n  P-value (Chi-square)                           0.000       0.047\n  Scaling correction factor                                  1.447\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4148.081    2238.585\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.853\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993       0.994\n  Tucker-Lewis Index (TLI)                       0.987       0.990\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.995\n  Robust Tucker-Lewis Index (TLI)                            0.992\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -16523.162  -16523.162\n  Scaling correction factor                                  2.671\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -16494.810  -16494.810\n  Scaling correction factor                                  2.092\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               33104.324   33104.324\n  Bayesian (BIC)                             33254.985   33254.985\n  Sample-size adjusted Bayesian (SABIC)      33162.865   33162.865\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.030       0.020\n  90 Percent confidence interval - lower         0.019       0.007\n  90 Percent confidence interval - upper         0.040       0.029\n  P-value H_0: RMSEA <= 0.050                    0.999       1.000\n  P-value H_0: RMSEA >= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.023\n  90 Percent confidence interval - lower                     0.003\n  90 Percent confidence interval - upper                     0.038\n  P-value H_0: Robust RMSEA <= 0.050                         0.999\n  P-value H_0: Robust RMSEA >= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017       0.017\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  factor1 =~                                                            \n    X1                0.578    0.039   14.876    0.000    0.578    0.566\n    X3                0.452    0.038   11.780    0.000    0.452    0.522\n    X5                0.659    0.041   16.022    0.000    0.659    0.682\n    X9                0.557    0.032   17.200    0.000    0.557    0.784\n    X10               0.613    0.035   17.707    0.000    0.613    0.705\n    X13               0.671    0.042   16.164    0.000    0.671    0.702\n    X14               0.559    0.039   14.279    0.000    0.559    0.376\n    X18               0.524    0.045   11.627    0.000    0.524    0.411\n    X21               0.555    0.043   13.021    0.000    0.555    0.669\n    X23               0.621    0.034   18.196    0.000    0.621    0.639\n  factor2 =~                                                            \n    X3               -0.023    0.049   -0.466    0.641   -0.023   -0.027\n    X5               -0.098    0.069   -1.424    0.154   -0.098   -0.101\n    X9               -0.076    0.069   -1.093    0.274   -0.076   -0.107\n    X10              -0.108    0.075   -1.430    0.153   -0.108   -0.124\n    X13               0.218    0.072    3.034    0.002    0.218    0.228\n    X14              -0.011    0.082   -0.140    0.888   -0.011   -0.008\n    X18               0.306    0.073    4.205    0.000    0.306    0.240\n    X21               0.297    0.089    3.344    0.001    0.297    0.358\n    X23               0.161    0.069    2.322    0.020    0.161    0.166\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  factor1 ~                                                             \n    factor2           0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .X1                0.709    0.065   10.958    0.000    0.709    0.680\n   .X3                0.545    0.043   12.668    0.000    0.545    0.727\n   .X5                0.489    0.053    9.306    0.000    0.489    0.525\n   .X9                0.189    0.020    9.463    0.000    0.189    0.375\n   .X10               0.369    0.043    8.665    0.000    0.369    0.488\n   .X13               0.417    0.048    8.621    0.000    0.417    0.456\n   .X14               1.896    0.078   24.415    0.000    1.896    0.858\n   .X18               1.258    0.101   12.423    0.000    1.258    0.774\n   .X21               0.291    0.041    7.040    0.000    0.291    0.424\n   .X23               0.534    0.051   10.465    0.000    0.534    0.564\n   .factor1           1.000                               1.000    1.000\n    factor2           1.000                               1.000    1.000\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Wrapping up\n\n1. CFA is an alternative model with EFA. \n\n2. The difference is that we have more control over the model structure using CFA, such as we can specify which item(s) loaded on which factors. \n\n3. EFA is not necessary and CFA can be the first step. You just keep revising the model structure until you get acceptable model fitting.\n\n4. Another alternative modeling for dependency among items is network modeling.\n>>>>>>> a79e7de (local changes)\n\n    -   Need to set the right number of constraints for identification\n    -   We set the value of factor loadings for a few items on a few of the factors\n        -   Typically to zero\n        -   Sometimes to one (Brown, 2002)\n    -   We keep the factor covariance matrix as an identity\n        -   Uncorrelated factors (as in EFA) with variances of one\n\n<<<<<<< HEAD\n-   Benefits of using CFA for exploratory analyses:\n\n-   CFA constraints remove rotational indeterminacy of factor loadings – no rotating is needed (or possible)\n\n    -   Defines factors with potentially less ambiguity\n    -   Constraints are easy to see\n\n-   For some software (SAS and SPSS), we get much more model fit information\n\n## Example of CFA\n\nSee the [example](https://www.lavaan.ugent.be/tutorial/cfa.html) from lavaan package\n\n# Background\n=======\n# Network Analysis\n>>>>>>> a79e7de (local changes)\n\n## Network Analysis\n\nNetwork analysis is a broad area. It has many names in varied fields:\n\n1.  Graphical Models (Computer Science, Machine Learning)\n2.  Bayesian Network (Computer Science, Educational Measurement)\n3.  Social Network (Sociology)\n4.  Psychological/Psychometric Network (Psychopathology, Psychology)\n5.  Structural Equation Model, Path Analysis (Psychology, Education)\n\n1 and 2 focus on the probabilistic relationships and further casual relationships among variables. 3 and 4 focuses on network structure and node importance. 5 focus on the regression coefficients of structural and measurement model.\n\nAll 5 analysis methods have a network-shaped diagram. [**Graphical modeling**]{.underline} is a more \"general\" term that can comprise of the other network models.\n\n## Examples of Varied Networks\n\n::: {layout=\"[[40,40], [100]]\" layout-halign=\"bottom\"}\n![Bayesian Network (Briganti et al., 2023)](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/BN_DAG.png){#fig-BN-DAG}\n\n![Facebook friendship network in a single undergraduate dorm (Lewis et al., 2008)](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/SocialNetwork.png){#fig-social-network}\n\n![Factor Analysis and Psychological Network (Borsboom et al., 2021)](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/GGM.png){#fig-GGM}\n:::\n\n## Research Aims\n\n1.  Bayesian Network (BN) aims to derive the casual relations between variables\n2.  Social Network aims to examine the network structure (community, density or centrality) of [individuals]{.underline}\n3.  Factor Analysis aims to identify latent variables\n4.  Psychological Network aims to examines the associations among observed variables (topological structures) and their positions in the network.\n\n## Network Psychometrics\n\n1.  Network psychometrics is a novel area that allows representing complex phenomena of measured constructs in terms of a set of elements that interact with each other.\n2.  It is inspired by the so-called [mutualism model]{.underline} and research in ecosystem modeling [@kan2019].\n    -   A mutualism model proposes that basic cognitive abilities directly and positively interact during development.\n3.  Psychometric network arises as [dynamics or reciprocal causation]{.underline} among variables are getting more attentions.\n4.  For example, individual differences in depression could arise from, and could be maintained by, vicious cycles of mutual relationships among symptoms.\n5.  A depression symptom such as insomnia can cause another symptom, such as fatigue, which in turn can determine concentration problems and worrying, which can result in more insomnia and so on.\n\n## Comparison to factor analysis\n\n[**Factor analysis**]{.underline} (common factor model) assumes the associations between observed features can be explained by one or more common factors.\n\n-   For example, higher \"depression\" level leads to increased frequency of depressive behaviors\n\n[**Psychometric network**]{.underline}, however, assumes the associations between observed features ARE the reason of the development of depression. Or \"depression\" is the network itself.\n\n-   unavoidable cycle of \"fatigue -\\> worrying -\\> insomnia -\\> fatigue\" will leads to higher \"depression\"\n\n## Utility of Psychometric Network\n\n1.  Explain the pathways of certain psychological phenomenon\n2.  Identify the most important problem that needed to be intervene in the treatment procedure\n3.  Examine group differences in interactions among observed features\n4.  Examine density of network: more dense network indicates more dynamic of certain problems\n5.  Examine clusters/communities of observed features: some symptoms are more likely to concur than other symptoms\n\n## Terminology I - Overall procedure\n\n1.  [Network structure estimation]{.underline}: the application of statistical models to assess the structure of pairwise (conditional) associations in multivariate data.\n2.  [Network description]{.underline}: characterization of the global topology and the position of individual nodes in that topology.\n3.  [Psychometric network analysis]{.underline}: the analysis of multivariate psychometric data using network structure estimation and network description.\n\n## Terminology II - Network description\n\n::: columns\n::: {.column width=\"50%\"}\n1.  [Node]{.underline}: psychometric variables that are selected in the network\n    -   such as responses to questionnaire items, symptom ratings, and cognitive test scores, background variables such as age and gender, experimental interventions.\n2.  [Edge (conditional association)]{.underline}: associations between variables taking into account other variables that may explain the association\n3.  [Edge weight]{.underline}: parameter estimates that represent the strength of conditional association between nodes\n4.  [Node centrality]{.underline}: the relative importance of a node in the context of other nodes, that can be calculated using different statistics\n:::\n\n::: {.column width=\"50%\"}\n![](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/girls_contemporaneous_MG.jpg)\n:::\n:::\n\n## Terminology III - Network structure estimation\n\n1.  [Node selection]{.underline}: the choice of which variables will function as nodes in the network model.\n2.  [Network stability analysis]{.underline}: the assessment of estimation precision and robustness to sampling error of psychometric networks.\n3.  [Pairwise Markov random field (PMRF)]{.underline}: an undirected network that represents variables as nodes and conditional associations as edges, in which unconnected nodes are conditionally independent.\n\n## Exploratory Nature\n\nPsychometric network is exploratory by nature. To obtain a meaningful network structure, psychometric networks need to drop weak edges but keep strong edges.\n\nThis procedure is typically called edge selection. One popular edge selection method is [regularization]{.underline}.\n\n-   Original network structure without regularization is called saturated network; vice versa regularized network\n\n::: columns\n::: {.column width=\"50%\"}\n![Emotion regulation (Awareness), Interpersonal problems and eating disorder](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/girls_betweensubject_MG.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n![](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/girls_contemporaneous_MG.jpg)\n:::\n:::\n\n## Workflow of psychometric network\n\n::: columns\n::: {.column width=\"60%\"}\n![](/posts/Lectures/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture12_Network/workflow.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"40%\"}\nPsychometric network analysis methodology includes steps of network structure estimation (to construct the network), network description (to characterize the network) and network stability analysis (to assess the robustness of results).\n:::\n:::\n\n## Types of data and network models\n\n1.  Cross-sectional data (N = large, T = 1)\n    -   **Ising** model for categorical variables\n    -   Gaussian graphical model (**GGM**; Foygel & Drton, 2010) for continuous variables\n    -   Mixed graphical model (**MGM**) for mixed types of variables: include both categorical variables and continuous variables\n2.  Panel data (N \\>\\> T)\n    -   Multilevel Graphical vector autoregressive model (**GVAR**)\n        -   i.e., longitudinal data, repeated measures\n3.  Time-series data ($N \\geq 1$, T = large)\n    -   Graphical vector autoregression\n        -   i.e., ecological momentary assessment, conducted via smartphones\n\n# Network Estimation\n\n## Gaussian graphical model\n\nGGM is one type of Pairwise Markov random field (PMRF) when data are continuous.\n\nIn a PMRF, the joint likelihood of multivariate data is modeled through the use of pairwise conditional associations, leading to a network representation that is undirected.\n\nFor $p$-dimensional data following multivariate normal distribution:\n\n$$\n\\boldsymbol{X} \\sim \\mathcal{N}(\\mu, \\boldsymbol{K}^{-1})\n$$\n\nWhere $K$ is a inverse covariance matrix of $\\boldsymbol{X}$ ($K = \\Sigma^{-1}$), also known as *precision*/*concentration* matrix.\n\nTo obtain sparse network structure, the $i$th row and $j$th column element of $\\boldsymbol{K}$, $k_{ij}=0$ when edge $\\{j, k\\}$ is not included in the network $G$,\n\n-   It means $X^{(i)}$ and $X^{(j)}$ are independent conditional on the other variables .\n\n## Partial correlation networks\n\nGGM can be standardized as the partial correlation network, in which each edge of GGM representing partial correlations between two nodes.\n\n$$\n\\rho_{ij}=\\rho(X^{(i)}, X^{(j)}|\\boldsymbol{X}^{-(i,j)}) = -\\frac{k_{ij}}{\\sqrt{k_{ii}}\\sqrt{k_{jj}}}\n$$\n\nAssume there are three variables: fatigue, insomnia, concentration\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"True Covariance Matrix - $\\boldsymbol{\\Sigma}$\"}\nSigma = matrix(c(\n  1,    -.26,  .31,\n  -.26,    1, -.08,\n  .31,  -.08,    1  \n), ncol = 3, byrow = T)\nSigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]  [,3]\n[1,]  1.00 -0.26  0.31\n[2,] -0.26  1.00 -0.08\n[3,]  0.31 -0.08  1.00\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"True Precision Matrix - K\"}\nK = solve(Sigma)\nround(K, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]  [,3]\n[1,]  1.18 0.28 -0.34\n[2,]  0.28 1.07  0.00\n[3,] -0.34 0.00  1.11\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Partial correlation matrix - R\"}\nR = K\nfor (i in 1:nrow(R)) {\n  for (j in 1:ncol(R)){\n    if (i != j) {\n      R[i, j] = - K[i, j] / (sqrt(K[i, i])*sqrt(K[j, j]))\n    }else{\n      R[i, j] = 1\n    }\n  }\n}\nround(R, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2] [,3]\n[1,]  1.00 -0.25  0.3\n[2,] -0.25  1.00  0.0\n[3,]  0.30  0.00  1.0\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n## Network Interpretation\n\n1.  Someone who is tired is also more likely to suffer from concentration problems and insomnia.\n2.  Concentration problems and insomnia are conditional independent given the level of fatigue\n    -   Or the correlation between insomnia and concentration can be totally explained by the relationships of both variables with fatigue\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqgraph::qgraph(R, labels = c(\"Fatigue\", \"Concentration\", \"Insomnia\"))\n```\n\n::: {.cell-output-display}\n![](Lecture13_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Estimation of partial correlation model for cross-sectional data\n\nFactor analysis model:\n\n$$\n\\boldsymbol{X}\\sim\\mathcal{N}(\\mu, \\boldsymbol{\\Lambda\\Psi\\Lambda^\\text{T}+\\Phi})\n$$\n\nGGM with partial correlation matrix:\n\n$$\n\\boldsymbol{X} \\sim \\mathcal{N}(0, \\boldsymbol{\\Delta(I-\\Omega)^{-1}\\Delta})\n$$\n\nWhere\n\n1.  $\\boldsymbol{\\Delta}$ is a diagonal scaling matrix that controls the variances\n2.  $\\boldsymbol{\\Omega}$ is a square symmetrical matrix with $0$s on the diagonal and partial correlation coefficients on the off diagonal.\n\n## `psychonetrics`: Partial correlation matrix estimation\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Population covariance matrix - $\\Sigma$\"}\nSigma |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]  [,3]\n[1,]  1.00 -0.26  0.31\n[2,] -0.26  1.00 -0.08\n[3,]  0.31 -0.08  1.00\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Population partial correlation matrix - R\"}\nR |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]  [,3]\n[1,]  1.000 -0.248 0.300\n[2,] -0.248  1.000 0.001\n[3,]  0.300  0.001 1.000\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Estimated sample partial correlation matrix - $\\hat\\Omega$\"}\nlibrary(psychonetrics)\nfit = ggm(covs = Sigma, nobs = 50) |> runmodel()\nOmega <- fit |> getmatrix(\"omega\") \nOmega |> round(3)# estimated Omega\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]  [,3]\n[1,]  0.000 -0.248 0.300\n[2,] -0.248  0.000 0.001\n[3,]  0.300  0.001 0.000\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Estimated sample scaling matrix - $\\hat\\Delta$\"}\nDelta <- fit |> getmatrix(matrix = \"delta\") \nDelta |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]  [,3]\n[1,] 0.921 0.000 0.000\n[2,] 0.000 0.966 0.000\n[3,] 0.000 0.000 0.951\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Estimated sample covaraicne matrix - $\\hat S$\"}\nS = Delta %*% solve(diag(1, 3) - Omega) %*% Delta\nS |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]  [,3]\n[1,]  1.00 -0.26  0.31\n[2,] -0.26  1.00 -0.08\n[3,]  0.31 -0.08  1.00\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n## `BGGM`: Bayesian approach\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Posterior means of partial correlation matrix\"}\nlibrary(BGGM)\nset.seed(1234)\ndat <- mvtnorm::rmvnorm(500, mean = rep(0, 3), sigma = Sigma)\nfit_bggm <- BGGM::estimate(dat, type = \"continuous\", iter = 1000, analytic = FALSE)\nfit_bggm$pcor_mat |> round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]  [,3]\n[1,]  0.000 -0.260 0.189\n[2,] -0.260  0.000 0.042\n[3,]  0.189  0.042 0.000\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Posterior distributions of partial correlations\"}\nsummary(fit_bggm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBGGM: Bayesian Gaussian Graphical Models \n--- \nType: continuous \nAnalytic: FALSE \nFormula:  \nPosterior Samples: 1000 \nObservations (n):\nNodes (p): 3 \nRelations: 3 \n--- \nCall: \nBGGM::estimate(Y = dat, type = \"continuous\", analytic = FALSE, \n    iter = 1000)\n--- \nEstimates:\n Relation Post.mean Post.sd Cred.lb Cred.ub\n     1--2    -0.260   0.042  -0.342  -0.178\n     1--3     0.189   0.043   0.104   0.272\n     2--3     0.042   0.047  -0.052   0.129\n--- \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Edge Selection: Regularization\n\nMultiple procedure and software can be used to perform edge selection:\n\n1.  `prune` function in `psychonetrics` package uses stepdown model search by pruning non-significant parameters\n\n    -   a edge with significance level lower than $\\alpha$ will be removed and re-fit the model until no nonsignificant edge existed\n    -   $p$ values of edges needed to be adjusted\n\n2.  `EBICglasso` in `qgraph` and `glasso` package uses Extended Bayesian Information Criterion (EBIC) to select best model\n\n    $$\n    \\text{EBIC}=-2\\text{L}+E(\\log(N))+4\\gamma E(log(P))\n    $$\n\n3.  `select` function in [`BGGM`](https://osf.io/preprints/psyarxiv/ypxd8) package uses Bayesian Hypothesis Testing — Bayes Factor (BF) to select model\n\n    -   $\\mathcal{H}_0: \\rho_{ij}=0$\n    -   $\\mathcal{H}_1: \\rho_{ij}\\in(-1, 1)$\n    -   $$\n        BF = \\frac{p(\\boldsymbol{X}|\\mathcal{H}_0)}{p(\\boldsymbol{X}|\\mathcal{H}_1)}\n        $$\n    -   By default, edges with BF \\< 3 will be included\n\n## Network Description\n\n1.  Network-level metrics:\n    -   **Network stability**: The assessment of estimation precision and robustness to sampling error of psychometric networks. Assess whether edge weights and node centrality changes with case dropping subset bootstrap.\n    -   **Network sensitivity**: The sensitivity analysis of the network by adding covariates (age, gender, hukou, education, marital status, and self-rated health) to the model to control their confounding effects. Report whether some central nodes are no longer central after adding some variables.\n2.  Node-level metrics:\n    -   **Total number of node**\n    -   **Node centrality**: the position of individual nodes within the network. A generic term that subsumes a family of measures that aim to assess how central a node is in a network topology, such as node strength, betweenness and closeness.\n    -   **Node bridge strength**: the degree to which one node connects two nodes from different facets. The bridge nodes (nodes with high bridge strength) that connect different communities of nodes (e.g., Neurotic factors is bridge nodes between personality traits and depressive symptoms @liPersonalityTraitsDepressive2024).\n    -   **Node centrality differences between groups**: node A is central in group 1's network but not so central in group 2's network.\n3.  Edge-level metrics:\n    -   **Total Number of edges**: proportions of non-zero edges\n    -   **Edge weight**: edge weights typically are parameter estimates that represent the strength of the conditional association between nodes. **Average edge weight** can be used to assess the overall strength of network connections.\n\n# Example: Big Five Personality Scale\n\n## Data Information\n\n25 self-reported personality items representing 5 factors.\n\n70.6% edges are nonzero using EBICglasso, while only 63.6% edges are nonzero using BF method and 62.0% edges are kept using $\\alpha =.01$ significance testing\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"EBICglasso\"}\nlibrary(psych)\nlibrary(qgraph)\ndata(bfi)\nbig5groups <- list(\n  Agreeableness = 1:5,\n  Conscientiousness = 6:10,\n  Extraversion = 11:15,\n  Neuroticism = 16:20,\n  Openness = 21:25\n)\nCorMat <- cor_auto(bfi[,1:25])\n\nEBICgraph <- EBICglasso(CorMat, nrow(bfi), 0.5, threshold = TRUE)\n\n## density\ndensity_nonzero_edge <- function(pcor_matrix){\n  N_nonzero_edge = (sum(pcor_matrix == 0) - ncol(pcor_matrix)) /2\n  N_all_edge = ncol(pcor_matrix)*(ncol(pcor_matrix)-1)/2\n  N_nonzero_edge/N_all_edge\n}\ndensity_nonzero_edge(EBICgraph)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7066667\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Significance testing\"}\nPruneFit <- ggm(bfi[,1:25]) |> prune(alpha = .01)\ndensity_nonzero_edge(getmatrix(PruneFit, \"omega\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.62\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Bayes Factor\"}\nBGGMfit <- BGGM::explore(bfi[,1:25], type = \"continuous\", iter = 1000, analytic = FALSE) |> \n  select()\ndensity_nonzero_edge(BGGMfit$pcor_mat_zero)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6666667\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Network Structure\n\n::: {layout-nrow=\"2\"}\n::: column\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![EBICglasso](Lecture13_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n::: column\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Sig. Test](Lecture13_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![BF](Lecture13_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n## Centrality - Strength\n\nStrength centrality measures suggest that `C4` and `E4` or `N1` have highest centrality indicating they play most imporatant roles in the networks.\n\n\n\n\n\n\n::: {.cell layout-ncol=\"3\"}\n::: {.cell-output-display}\n![Method 1: EBICglasso](Lecture13_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Method 2: Sig. Test](Lecture13_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Method 3: Bayes Factor](Lecture13_files/figure-html/unnamed-chunk-20-3.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Centrality - Bridge\n\n`E4` and `E5` has highest bridge strength, indicating they serve as bridges linking communities of personality. They are important elements connecting varied types of personality\n\n\n\n\n\n\n::: {.cell layout-ncol=\"3\"}\n::: {.cell-output-display}\n![Method 1: EBICglasso](Lecture13_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Method 2: Sig. Test](Lecture13_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Method 3: Bayes Factor](Lecture13_files/figure-html/unnamed-chunk-21-3.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Wrapping up\n\n1.  Network analysis is an alternative way of modeling dependency among variables.\n2.  Gaussian graphical model is used for multivariate continuous data.\n3.  The goal is to estimate network structure, node importance, and stability.\n\n## Other Materials:\n\n1.  [Jihong's post: how to choose network analysis estimation](https://jihongzhang.org/notes/2024-04-04-Network-Estimation-Methods)\n\n## Reference\n",
    "supporting": [
      "Lecture13_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}