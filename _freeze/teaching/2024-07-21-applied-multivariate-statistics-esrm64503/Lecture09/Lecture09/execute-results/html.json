{
  "hash": "1b68b299144705ea9a9afca43a69dba3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 09: Path Analysis\"\nsubtitle: \"Absolute Model fit and Model Interpretation\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-10-09\"\ndate-modified: \"2024-10-11\"\nsidebar: false\nexecute: \n  echo: true\n  warning: false\noutput-location: default\ncode-annotations: below\nhighlight-style: \"nord\"\nformat: \n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"ESRM 64503 - Lecture 09: Absolute Model fit and Path Analysis\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n    fig-align: center\nfilters:\n  - quarto\n  - line-highlight\nbibliography: references.bib\n---\n\n## Today's Class\n\n```{=html}\n<div class=\"card shadow\">\n    <div class=\"ml-3 mt-2\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"54\" height=\"14\" viewBox=\"0 0 54 14\">\n            <g fill=\"none\" fill-rule=\"evenodd\" transform=\"translate(1 1)\">\n                <circle cx=\"6\" cy=\"6\" r=\"6\" fill=\"#FF5F56\" stroke=\"#E0443E\" stroke-width=\".5\"></circle>\n                <circle cx=\"26\" cy=\"6\" r=\"6\" fill=\"#FFBD2E\" stroke=\"#DEA123\" stroke-width=\".5\"></circle>\n                <circle cx=\"46\" cy=\"6\" r=\"6\" fill=\"#27C93F\" stroke=\"#1AAB29\" stroke-width=\".5\"></circle>\n            </g>\n        </svg>\n    </div>\n    <div class=\"card-body\">\n        <h4 class=\"card-title\"><b>Today's Class</b></h4>\n        <ul>\n          <li>Multivariate regression via path model</li>\n          <li>Model modification</li>\n          <li>Comparing and contrasting path analysis</li>\n          <ul>\n            <li>Differences in model fit measures </li>\n          </ul>\n          <li>How to interpret the results of path model</li>\n        </ul>\n    </div>\n</div>\n```\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nlibrary(ESRM64503)\nlibrary(kableExtra)\nlibrary(tidyverse)\nlibrary(DescTools) # Desc() allows you to quick screen data\nlibrary(lavaan) # Desc() allows you to quick screen data\n# options(digits = 3)\nhead(dataMath)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id hsl cc use msc mas mse perf female\n1  1  NA  9  44  55  39  NA   14      1\n2  2   3  2  77  70  42  71   12      0\n3  3  NA 12  64  52  31  NA   NA      1\n4  4   6 20  71  65  39  84   19      0\n5  5   2 15  48  19   2  60   12      0\n6  6  NA 15  61  62  42  87   18      0\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(dataMath)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 350   9\n```\n\n\n:::\n:::\n\n\n# Path Analysis\n\n-   Previous models considered simple pairwise correlations among observed variables (`perf` and `use`). These models may be limited in answering more complex research questions, such as whether variable A mediates the relationship between variable B and variable C\n\n-   Path analysis is used to analyze multivariate linear models where outcomes can be also predictors\n\n-   Path analysis details:\n\n    -   Model identification\n\n    -   Modeling workflow\n\n-   Example Analyses\n\n-   Procedure:\n\n    - Model building\n    \n    - Model fitting\n    \n    - Model summary\n    \n## Today's Example Data\n\n-   Data are simulated based on the results reported in:\n\n-   Sample of 350 undergraduates (229 women, 121 men)\n\n    -   In simulation, 10% of data points were missing (using missing completely at random mechanism).\n\n::: callout-tip\n## Dictionary\n\n1.  Prior Experience at High School Level (HSL)\n2.  Prior Experience at College Level (CC)\n3.  Perceived Usefulness of Mathematics (USE)\n4.  Math Self-Concept (MSC)\n5.  Math Anxiety (MAS)\n6.  Math Self-Efficacy (MSE)\n7.  Math Performance (PERF)\n8.  Female (sex variable: 0 = male; 1 = female)\n:::\n\n## Multivariate Linear Regression Path Diagram\n\n![](path_diagram.png){fig-align=\"center\"}\n\n-   This path diagram illustrates a multivariate regression model examining relationships among six constructs related to mathematics education\n-   **Red solid arrows** represent hypothesized direct effects (regression paths) between variables\n-   **Blue dashed arrows** indicate residual variances for endogenous variables\n-   The model shows:\n    -   Female as the sole exogenous variable (predictor with no antecedents)\n    -   Five endogenous variables: High School Math Experience, College Math Experience, Mathematics Self-Efficacy, Mathematics Self-Concept, Mathematics Performance, and Perceived Usefulness\n    -   Multiple direct and indirect pathways through which gender and prior experiences influence mathematics outcomes\n    -   A complex network of relationships suggesting mediation effects through self-beliefs (self-efficacy and self-concept)\n\n## The Big Picture\n\n1.  **Path analysis** is a multivariate statistical method that, when using an _identity link_, assumes the variables in an analysis are multivariate normally distributed\n\n    -   Mean vectors\n    -   Covariance matrices\n\n2.  By specifying simultaneous regression equations (the core of path models), a very specific covariance matrix is implied\n\n    -   This is where things deviate from our familiar R matrix\n\n3.  Like multivariate models, the key to path analysis is finding an approximation to the unstructured (saturated) covariance matrix\n\n    -   With fewer parameters, if possible\n\n4.  The art to path analysis is in specifying models that blend theory and statistical evidence to produce valid, generalizable results\n\n## Types of Variables in Path Model\n\n- **Endogenous variable(s)**: variables whose variability is explained by one or more variables in a model\n  - In linear regression, the dependent variable is the only endogenous variable in an analysis\n      - Mathematics Performance (PERF) and Mathematics Usefulness (USE)\n- **Exogenous variable(s)**: variables whose variability is not explained by any variables in a model\n  - In linear regression, the independent variable(s) are the exogenous variables in the analysis\n      - Female (F)\n\n![](path_diagram.png){fig-align=\"center\"}\n\n## Procedure of Path Analysis Steps\n\n![](path_model.png){fig-align=\"center\"}\n\n## Identification of Path Models\n\n-   Model identification is necessary for statistical models to have \"meaningful\" results\n\n-   For path models, identification can be very difficult\n\n-   Because of their unique structure, path models must have identification in two ways:\n\n    -   \"Globally\" – so that the total number of parameters does not exceed the total number of means, variances, and covariances of the endogenous and exogenous variables\n\n    -   \"Locally\" – so that each individual equation is identified\n\n-   Model identification is guaranteed if a model is both \"globally\" and \"locally\" identified\n\n## Global Identification: \"T-rule\"\n\n-   A necessary but not sufficient condition for a path models is that of having equal to or fewer model parameters than there are \"[distributional parameters]{.underline}\"\n\n-   Distributional parameters: As the path models we discuss assume the multivariate normal distribution, we have two matrices of parameters\n\n    -   The mean vector\n    -   The covariance matrix\n\n-   For the MVN, the so-called **T-rule** states that a model must have equal to or fewer parameters than the unique elements of the covariance matrix of all endogenous and exogenous variables (the sum of all variables in the analysis)\n\n    -   Let $s = p+q$, the total of all endogenous (p) and exogenous (q) variables\n    -   Then the total unique elements are $\\frac{s(s+1)}{2}$\n\n## More on the \"T-rule\"\n\n-   The classical definition of the \"T-rule\" counts the following entities as model parameters:\n    1.  Direct effects (regression slopes)\n    2.  Residual variances\n    3.  Residual covariances\n    4.  Exogenous variances\n    5.  Exogenous covariances\n-   Missing from this list are:\n    1.  The set of exogenous variable means\n    2.  The set of intercepts for endogenous variables\n-   Each of the missing entities are part of the likelihood function, but are considered “saturated” so no additional parameters can be added (all parameters are estimated)\n    -   These do not enter into the equation for the covariance matrix of the endogenous and exogenous variables\n\n## T-rule Identification Status\n\n-   Just-identified: number of observed covariances = number of model parameters\n    -   Necessary for identification, but no model fit indices available\n-   Over-identified: number of observed covariances \\> number of model parameters\n    -   Necessary for identification; model fit indices available\n-   Under-identified: number of observed covariances \\< number of model parameters\n    -   Model is [NOT IDENTIFIED]{.underline}: No results available\n\n## Our Destination: Overall Path Model\n\n-   Based on the theory described in the introduction to @pajaresRoleSelfefficacySelfconcept1994, the following model was hypothesized – use this diagram to build your knowledge of path models\n\n![](path_diagram_labelled.png){fig-align=\"center\"}\n\n## Overall Path Model: How to Inspect\n\n![](path_diagram_indexed.png){fig-align=\"center\"}\n\n## Path Model Setup - Questions for the Analysis\n\n-   How many variables are in our model? $s = 7$\n\n    -   Gender, HSL, CC, MSC, MSE, PERF, and USE\n\n-   How many variables are endogenous? $p = 6$\n\n    -   HSL, CC, MSC, MSE, PERF and USE\n\n-   How many variables are exogenous? $q = 1$\n\n    -   Gender\n\n-   Is the model recursive or non-recursive?\n\n    -   Recursive – no feedback loops present\n\n## Path Model Setup – Questions for the Analysis\n\n-   Is the model identified?\n\n    -   Check the t-rule first\n\n    -   How many covariance terms are there in the all-variable matrix?\n\n        -   $\\frac{7*(7+1)}{2} = 28$\n\n    -   How many model parameters are to be estimated?\n\n        -   12 direct paths\n\n        -   6 residual variances (only endogenous variables have resid. var.)\n\n        -   1 variance of the exogenous variable\n\n        -   6 endogenous variance intercepts\n\n            -   Not relevant for T-rule identification, but counted in R matrix\n\n-   28 (total variances/covariances) \\> (12 + 6 + 1) parameters, thus this model is over-identified\n\n    -   We can use R to run analysis\n\n## Overall Hypothesized Path Model: Equation Form\n\n![](path_diagram_indexed.png){fig-align=\"center\" width=\"1000\"}\n\n-   The path model from can be re-expressed in the following 6 endogenous variable regression equations:\n\n$$\nHSL_i = \\beta_{0, HSL} + \\beta_{F, HSL}F_i + e_{i, HSL}\n$$ {#eq-reg1}\n\n$$\nCC_i = \\beta_{0,CC}+\\beta_{HSL,CC}HSL_i + e_{i,CC}\n$$ {#eq-reg2}\n\n$$\nMSE_i = \\beta_{0,MSE} + \\beta_{F,MSE}F_i +\\beta_{HSL,MSE}HSL_i + \\beta_{CC,MSE}CC_i +e_{i,MSE}\n$$ {#eq-reg3}\n\n$$\nMSC_i = \\beta_{0, MSC} + \\beta_{HSL,MSC}HSL_i + \\beta_{CC,MSC} CC_i +  \\beta_{MSE,MSC} MSE_i + e_{i,MSC}\n$$ {#eq-reg4}\n\n$$\nUSE_i = \\beta_{0,USE} + \\beta_{MSE,USE}MSE_{i} + e_{i, USE}\n$$ {#eq-reg5}\n\n$$\nPERF_i = \\beta_{0,PERF} + \\beta_{HSL,PERF}HSL_{i} +\\beta_{MSE,PERF}MSE_i + \\beta_{MSC, PERF}MSC_{i} + e_{i,PERF}\n$$ {#eq-reg6}\n\n## Data Analytic Plan\n\n1.  Model building: Constructed our model\n\n2.  Identification checking (new): Verified it was identified using the t-rule and that it is a recursive model\n\n3.  Model fitting: Estimate the model with R\n\n4.  Model evaluation (new): Check model fit\n\n5.  Model summary: extract results of models\n\n## Path Model: `lavaan` syntax\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 01-----------------------------------------------------------------------\nmodel01.syntax = \n\" \n#endogenous variable equations\nperf ~ hsl + msc + mse\nuse  ~ mse\nmse  ~ hsl + cc + female \nmsc  ~ mse + cc + hsl\ncc   ~ hsl\nhsl  ~ female\n\n#endogenous variable intercepts\nperf ~ 1\nuse  ~ 1\nmse  ~ 1\nmsc  ~ 1\ncc   ~ 1\nhsl  ~ 1\n\n#endogenous variable residual variances\nperf ~~ perf\nuse  ~~ use\nmse  ~~ mse\nmsc  ~~ msc\ncc   ~~ cc  \nhsl  ~~ hsl\n\n# endogenous variable residual covariances\n# this is optional, because by default lavaan will assume their residuals are zero-correlated\n# none specfied in the original model so these have zeros:\nperf ~~ 0*use + 0*mse + 0*msc + 0*cc + 0*hsl\nuse  ~~ 0*mse + 0*msc + 0*cc + 0*hsl\nmse  ~~ 0*msc + 0*cc + 0*hsl\nmsc  ~~ 0*cc + 0*hsl\ncc   ~~ 0*hsl\n\"\n```\n:::\n\n\n## Model Fit Evaluation\n\n-   First, we check convergence:\n    -   `lavaan`'s ML algorithm converged!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#estimate model\nmodel01.fit = sem(model01.syntax, data=dataMath, mimic = \"MPLUS\", estimator = \"MLR\")\n\n#see if model converged\ninspect(model01.fit, what=\"converged\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n-   Second, we check for abnormally large standard errors:\n\n    -   None too large, relative to the size of the parameter\n\n    -   Indicates identified model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameterestimates(model01.fit) |> filter(op != \"~~\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      lhs op    rhs     est    se      z pvalue ci.lower ci.upper\n1    perf  ~    hsl   0.172 0.107  1.600  0.110   -0.039    0.383\n2    perf  ~    msc   0.036 0.009  3.981  0.000    0.018    0.054\n3    perf  ~    mse   0.139 0.013 10.522  0.000    0.113    0.164\n4     use  ~    mse   0.290 0.072  4.014  0.000    0.149    0.432\n5     mse  ~    hsl   4.103 0.407 10.085  0.000    3.305    4.900\n6     mse  ~     cc   0.393 0.105  3.729  0.000    0.186    0.599\n7     mse  ~ female   4.285 1.159  3.697  0.000    2.013    6.557\n8     msc  ~    mse   0.721 0.068 10.557  0.000    0.587    0.855\n9     msc  ~     cc   0.557 0.132  4.211  0.000    0.298    0.817\n10    msc  ~    hsl   2.929 0.659  4.446  0.000    1.638    4.220\n11     cc  ~    hsl   0.681 0.246  2.771  0.006    0.199    1.163\n12    hsl  ~ female   0.201 0.154  1.307  0.191   -0.101    0.504\n13   perf ~1          1.009 0.788  1.281  0.200   -0.535    2.552\n14    use ~1         31.118 5.357  5.809  0.000   20.619   41.618\n15    mse ~1         47.885 2.246 21.319  0.000   43.482   52.287\n16    msc ~1        -23.322 4.542 -5.135  0.000  -32.224  -14.419\n17     cc ~1          6.979 1.263  5.524  0.000    4.503    9.455\n18    hsl ~1          4.844 0.091 53.434  0.000    4.666    5.021\n19 female ~1          0.346 0.000     NA     NA    0.346    0.346\n```\n\n\n:::\n:::\n\n\n-   Third, we look at the model fit statistics.\n\n## Model Fit Statistics\n\n-   Before we interpret the estimation result, we need to assess the fit of a multivariate linear model to the data, in an absolute sense\n\n-   If a model does not fit the data:\n\n    -   Parameter estimates may be biased\n    -   Standard errors of estimates may be biased\n    -   Inferences made from the model may be wrong\n    -   If the saturated model fit is wrong, then the LRTs will be inaccurate\n\n-   Not all “good-fitting” models are useful…\n\n    -   …model fit just allows you to talk about your model…there may be nothing of significance (statistically or practically) in your results, though\n\n## Global measures of Model fit\n\n1.  Root Mean Square Error of Approximation (RMSEA)\n2.  Likelihood ratio test\n    -   User model versus the saturated model: Testing if your model fits as well as the saturated model\n    -   The saturated model versue the baseline model: Testing whether any variables have non-zero covariances (significant correlations)\n3.  User model versus baseline model\n    -   CFI (the comparative fit index)\n    -   TLI (the Tucker–Lewis index)\n4.  Log-likelihood and Information Criteria\n5.  Standardized Root Mean Square Residual (SRMR)\n    -   How far off a model’s correlations are from the saturated model correlations\n\n## Model Fit Statistics: Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model01.fit, fit.measures = TRUE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel Test User Model: # <1>\n                                              Standard      Scaled\n  Test Statistic                                58.896      58.913\n  Degrees of freedom                                 9           9\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.000\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model: # <2>\n\n  Test statistic                               619.926     629.882\n  Degrees of freedom                                21          21\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.984\n\nRoot Mean Square Error of Approximation: # <3>\n\n  RMSEA                                          0.126       0.126\n  90 Percent confidence interval - lower         0.096       0.096\n  90 Percent confidence interval - upper         0.157       0.157\n  P-value H_0: RMSEA <= 0.050                    0.000       0.000\n  P-value H_0: RMSEA >= 0.080                    0.994       0.994  \n  \nUser Model versus Baseline Model: #<4>\n\n  Comparative Fit Index (CFI)                    0.917       0.918\n  Tucker-Lewis Index (TLI)                       0.806       0.809\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.918\n  Robust Tucker-Lewis Index (TLI)                            0.809\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5889.496   -5889.496\n  Scaling correction factor                                  0.965\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5860.048   -5860.048\n  Scaling correction factor                                  0.975\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11826.992   11826.992\n  Bayesian (BIC)                             11919.583   11919.583\n  Sample-size adjusted Bayesian (SABIC)      11843.446   11843.446\n```\n:::\n\n\n1.  This is a likelihood ratio (deviance) test comparing our model ($H_0$) with the saturated model – the saturated model fits much better ($\\chi^2(\\Delta df = 9) = 58.896$, p \\< .001)\n2.  This is a likelihood ratio (deviance) test comparing the baseline model with the saturated model\n3.  The RMSEA estimate is 0.126. Good fit is considered 0.05 or less.\n4.  The CFI estimate is .917 and the TLI is .806. Good fit is considered 0.95 or higher.\n\n-   Based on the model fit statistics, we conclude that our model does not adequately approximate the covariance matrix. Therefore, inferences from these results may be unreliable due to potentially biased standard errors and parameter estimates\n\n## Model Modification Method 1: Check Residual Covariance\n\n-   Having concluded that our model fit is inadequate, we must modify the model to improve fit\n\n    -   These modifications are data-driven rather than theory-driven, which raises concerns about their generalizability beyond this sample\n\n-   Ideally, model modifications should be guided by substantive theory\n\n    -   However, we can inspect the normalized residual covariance matrix (analogous to z-scores) to identify areas of greatest misfit\n    -   Several normalized residual covariance exceeds the absolute value of 1.96: MSC with USE and CC with Female\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    resid(model01.fit, type = \"normalized\")$cov\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n             perf    use    mse    msc     cc    hsl female\n    perf   -0.076                                          \n    use    -0.159  0.041                                   \n    mse    -0.071 -0.110 -0.086                            \n    msc     0.059  5.051 -0.039  0.043                     \n    cc     -0.028  0.720 -0.377 -0.161  0.046              \n    hsl     0.006  0.559  0.085  0.105 -0.034  0.039       \n    female -1.523 -0.027 -0.425 -1.452 -2.577  0.091  0.000\n    ```\n    \n    \n    :::\n    :::\n\n\n-  `use - msc` (5.051) and `female - cc` (-2.577) have large residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsampleCov <- inspect(model01.fit, \"sampstat\")$cov # covariance matrix from data\nmodelImpliedCov <- inspect(model01.fit, \"cov.ov\") # model implied covariance matrix\nsampleCov - modelImpliedCov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         perf    use    mse    msc     cc    hsl female\nperf   -0.056                                          \nuse    -0.438  0.792                                   \nmse    -0.167 -1.171 -0.908                            \nmsc     0.202 73.578 -0.517  0.996                     \ncc     -0.028  3.894 -1.540 -0.987  0.115              \nhsl     0.001  0.622  0.080  0.159 -0.015  0.005       \nfemale -0.120 -0.011 -0.128 -0.666 -0.393  0.003  0.000\n```\n\n\n:::\n:::\n\n\n## Our Destination: Overall Path Model\n\n![](path_diagram_corrected.png){fig-align=\"center\"}\n\n-   The largest normalized covariances suggest relationships that may be present that are not being modeled:\n\n    -   Add a direct effect between Female and CC\n    -   Add a direct effect (residual covariance) between MSC and USE\n\n## Model Modification Method 2: More Help for Fit\n\n-   As we used Maximum Likelihood to estimate our model, another useful feature is that of the modification indices\n\n    -   Modification indices (also called Score or LaGrangian Multiplier tests) that attempt to suggest the change in the log-likelihood for adding a given model parameter (larger values indicate a better fit for adding the parameter)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#calculate modification indices\nmodel01.mi = modificationindices(model01.fit, sort. = T)\n\n#display values of indices\nhead(model01.mi, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      lhs op    rhs     mi    epc sepc.lv sepc.all sepc.nox\n54    msc  ~    use 41.517  0.299   0.299    0.275    0.275\n31    use ~~    msc 41.517 70.912  70.912    0.386    0.386\n46    use  ~    msc 40.032  0.451   0.451    0.490    0.490\n63    hsl  ~    mse  6.486  1.139   1.139   10.265   10.265\n65    hsl  ~     cc  6.477  0.447   0.447    1.992    1.992\n39     cc ~~    hsl  6.477 15.131  15.131    1.974    1.974\n59     cc  ~    msc  6.477 -0.568  -0.568   -1.654   -1.654\n60     cc  ~ female  6.477 -1.756  -1.756   -0.142   -0.298\n70 female  ~     cc  6.477 -0.012  -0.012   -0.145   -0.145\n68 female  ~    mse  6.477 -0.030  -0.030   -0.748   -0.748\n```\n\n\n:::\n:::\n\n\n-   The modification indices have three large values:\n    -   A direct effect predicting MSC from USE\n    -   A direct effect predicting USE from MSC\n    -   A residual covariance between USE and MSC\n-   Note: the MI value is -2 times the change in the log-likelihood and the EPC is the expected parameter value\n    -   The MI is like a 1 DF Chi-Square Deviance test\n        -   Values greater than 3.84 are likely to be significant changes in the log-likelihood\n-   All three modifications involve the same variables; therefore, we can only select one\n    -   Substantive theory would ideally guide this decision\n-   As we do not know theory, we will choose to add a residual covariance between USE and MSC ( the “\\~\\~” symbol)\n    -   This acknowledges that their covariance remains unexplained by the specified model structure—not an ideal theoretical solution, but one that may enable valid inference if adequate model fit is achieved\n    -   MI = 41.517\n    -   EPC = 70.912\n\n# Model 2: New Model by adding MSE with USE\n\n## Model 2: `lavaan` syntax\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 02: Add residual covariance between USE and MSC-------------------------------------\nmodel02.syntax = \n\" \n#endogenous variable equations\nperf ~ hsl + msc + mse\nuse  ~ mse\nmse  ~ hsl + cc + female \nmsc  ~ mse + cc + hsl\ncc   ~ hsl\nhsl  ~ female\n\n#endogenous variable intercepts\nperf ~ 1\nuse  ~ 1\nmse  ~ 1\nmsc  ~ 1\ncc   ~ 1\nhsl  ~ 1\n\n#endogenous variable residual variances\nperf ~~ perf\nuse  ~~ use\nmse  ~~ mse\nmsc  ~~ msc\ncc   ~~ cc  \nhsl  ~~ hsl\n\n#endogenous variable residual covariances\n#none specfied in the original model so these have zeros:\nperf ~~ 0*use + 0*mse + 0*msc + 0*cc + 0*hsl\nuse  ~~ 0*mse + msc + 0*cc + 0*hsl      #<- the changed part of syntax here (no 0* in front of msc)\nmse  ~~ 0*msc + 0*cc + 0*hsl\nmsc  ~~ 0*cc + 0*hsl\ncc   ~~ 0*hsl\n\"\n```\n:::\n\n\n## Assessing Model fit of the Modified Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#estimate model\nmodel02.fit = sem(model02.syntax, data=dataMath, mimic = \"MPLUS\", estimator = \"MLR\")\n\n#see if model converged\ninspect(model02.fit, what=\"converged\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#show summary of model fit statistics and parameters\nsummary(model02.fit, standardized=TRUE, fit.measures=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6-20 ended normally after 80 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n\n  Number of observations                           350\n  Number of missing patterns                        26\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                14.827      14.393\n  Degrees of freedom                                 8           8\n  P-value (Chi-square)                           0.063       0.072\n  Scaling correction factor                                  1.030\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                               619.926     629.882\n  Degrees of freedom                                21          21\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.984\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.989       0.990\n  Tucker-Lewis Index (TLI)                       0.970       0.972\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.990\n  Robust Tucker-Lewis Index (TLI)                            0.973\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5867.461   -5867.461\n  Scaling correction factor                                  0.957\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5860.048   -5860.048\n  Scaling correction factor                                  0.975\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11784.923   11784.923\n  Bayesian (BIC)                             11881.371   11881.371\n  Sample-size adjusted Bayesian (SABIC)      11802.062   11802.062\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.049       0.048\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.088       0.086\n  P-value H_0: RMSEA <= 0.050                    0.457       0.484\n  P-value H_0: RMSEA >= 0.080                    0.104       0.089\n                                                                  \n  Robust RMSEA                                               0.053\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.096\n  P-value H_0: Robust RMSEA <= 0.050                         0.405\n  P-value H_0: Robust RMSEA >= 0.080                         0.170\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035       0.035\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  perf ~                                                                \n    hsl               0.153    0.107    1.432    0.152    0.153    0.068\n    msc               0.037    0.009    4.147    0.000    0.037    0.215\n    mse               0.139    0.013   10.700    0.000    0.139    0.557\n  use ~                                                                 \n    mse               0.277    0.073    3.803    0.000    0.277    0.209\n  mse ~                                                                 \n    hsl               4.138    0.406   10.203    0.000    4.138    0.459\n    cc                0.393    0.105    3.723    0.000    0.393    0.194\n    female            4.168    1.160    3.593    0.000    4.168    0.166\n  msc ~                                                                 \n    mse               0.736    0.066   11.119    0.000    0.736    0.512\n    cc                0.519    0.117    4.434    0.000    0.519    0.179\n    hsl               2.824    0.593    4.764    0.000    2.824    0.218\n  cc ~                                                                  \n    hsl               0.662    0.247    2.686    0.007    0.662    0.149\n  hsl ~                                                                 \n    female            0.208    0.154    1.348    0.178    0.208    0.075\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .perf ~~                                                               \n   .use               0.000                               0.000    0.000\n   .mse               0.000                               0.000    0.000\n   .msc               0.000                               0.000    0.000\n   .cc                0.000                               0.000    0.000\n   .hsl               0.000                               0.000    0.000\n .use ~~                                                                \n   .mse               0.000                               0.000    0.000\n   .msc              70.249   10.358    6.782    0.000   70.249    0.380\n   .cc                0.000                               0.000    0.000\n   .hsl               0.000                               0.000    0.000\n .mse ~~                                                                \n   .msc               0.000                               0.000    0.000\n   .cc                0.000                               0.000    0.000\n   .hsl               0.000                               0.000    0.000\n .msc ~~                                                                \n   .cc                0.000                               0.000    0.000\n   .hsl               0.000                               0.000    0.000\n .cc ~~                                                                 \n   .hsl               0.000                               0.000    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .perf              1.023    0.782    1.309    0.191    1.023    0.344\n   .use              32.162    5.393    5.964    0.000   32.162    2.035\n   .mse              47.738    2.237   21.339    0.000   47.738    4.006\n   .msc             -23.369    4.484   -5.211    0.000  -23.369   -1.364\n   .cc                7.072    1.268    5.576    0.000    7.072    1.201\n   .hsl               4.843    0.091   53.390    0.000    4.843    3.663\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .perf              3.763    0.309   12.173    0.000    3.763    0.427\n   .use             238.854   19.097   12.507    0.000  238.854    0.956\n   .mse              97.294    7.758   12.541    0.000   97.294    0.685\n   .msc             142.912   10.793   13.241    0.000  142.912    0.487\n   .cc               33.923    2.456   13.813    0.000   33.923    0.978\n   .hsl               1.738    0.126   13.813    0.000    1.738    0.994\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneeded_fitIndex <- c(\"rmsea\", \"srmr\",\"cfi\", \"tli\", \"chisq\", \"df\", \"pvalue\") \nfitmeasures(model02.fit)[needed_fitIndex]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rmsea        srmr         cfi         tli       chisq          df \n 0.04937683  0.03459811  0.98860192  0.97008004 14.82660076  8.00000000 \n     pvalue \n 0.06260610 \n```\n\n\n:::\n:::\n\n\n-   Now we must start over with our path model decision tree\n    -   The model is identified (now 20 parameters \\< 28 covariances)\n    -   Estimation converged; Standard errors look acceptable\n-   Model fit indices:\n    -   The comparison with the saturated model suggests our model fits statistically\n    -   The RMSEA is 0.049, which indicates good fit\n    -   The CFI and TLI both indicate good fit (CFI/TLI \\> .90)\n    -   The SRMR also indicates good fit\n-   Therefore, we conclude the model adequately approximates the covariance matrix, allowing us to proceed with parameter interpretation. However, we first examine the residual covariances and modification indices\n\n## Normalized Residual Covariances\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid(model02.fit, type = \"normalized\")$cov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         perf    use    mse    msc     cc    hsl female\nperf   -0.062                                          \nuse    -0.990  0.020                                   \nmse    -0.113  0.064 -0.103                            \nmsc    -0.003  0.337 -0.104  0.054                     \ncc      0.018  0.771 -0.355  0.050  0.034              \nhsl     0.062  0.638  0.020  0.154  0.037  0.017       \nfemale -1.500  0.026 -0.362 -1.456 -2.577  0.051  0.000\n```\n\n\n:::\n:::\n\n\n-   the normalized residual covariance of CC with Female exceeds the absolute value of 1.96\n    -   Given the number of covariances in the model, this is likely acceptable\n\n## Modification Indices for Model 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel02.mi = modificationindices(model02.fit, sort. = T)\nhead(model02.mi, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      lhs op    rhs    mi    epc sepc.lv sepc.all sepc.nox\n39     cc ~~    hsl 6.697 14.964  14.964    1.949    1.949\n68 female  ~    mse 6.697 -0.030  -0.030   -0.761   -0.761\n70 female  ~     cc 6.697 -0.012  -0.012   -0.148   -0.148\n60     cc  ~ female 6.697 -1.788  -1.788   -0.144   -0.304\n58     cc  ~    mse 6.697 -0.429  -0.429   -0.868   -0.868\n65    hsl  ~     cc 6.697  0.441   0.441    1.965    1.965\n63    hsl  ~    mse 6.696  1.124   1.124   10.128   10.128\n66 female  ~   perf 5.126 -0.032  -0.032   -0.199   -0.199\n61    hsl  ~   perf 4.410  0.774   0.774    1.739    1.739\n69 female  ~    msc 4.405 -0.004  -0.004   -0.162   -0.162\n```\n\n\n:::\n:::\n\n\n-   No modification indices are substantially large, although some exceed 3.84\n\n    -   These are not pursued given that our model demonstrates adequate fit, and adding these parameters may not be theoretically meaningful\n\n## More on Modification Indices\n\n-   Recall from our original model that we received the following modification index values for the residual covariance between MSC and USE:\n    -   MI = 41.529\n    -   EPC = 70.912\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel01.mi[2, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n31 use ~~ msc 41.517 70.912  70.912    0.386    0.386\n```\n\n\n:::\n:::\n\n\n-   The estimated residual covariance between MSC and USE in the modified model is: 70.249\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameterestimates(model02.fit) |> filter(lhs == \"use\"& op == \"~~\"& rhs == \"msc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lhs op rhs    est     se     z pvalue ci.lower ci.upper\n1 use ~~ msc 70.249 10.358 6.782      0   49.947   90.551\n```\n\n\n:::\n:::\n\n\n-   The difference in log-likelihoods is:\n    -   -2\\*(change) = 58.279\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(model01.fit, model02.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan->lavTestLRT():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not the \n   robust test that should be reported per model. A robust difference test is \n   a function of two standard (not robust) statistics.\n\n            Df   AIC   BIC  Chisq Chisq diff Df diff Pr(>Chisq)    \nmodel02.fit  8 11785 11881 14.827                                  \nmodel01.fit  9 11827 11920 58.896     58.279       1  2.275e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n-   The values given by the MI and EPC are approximations\n\n## Model Parameter Investigation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  perf ~                                                                \n    hsl               0.153    0.107    1.432    0.152    0.153    0.068\n    msc               0.037    0.009    4.147    0.000    0.037    0.215\n    mse               0.139    0.013   10.700    0.000    0.139    0.557\n  use ~                                                                 \n    mse               0.277    0.073    3.803    0.000    0.277    0.209\n  mse ~                                                                 \n    hsl               4.138    0.406   10.203    0.000    4.138    0.459\n    cc                0.393    0.105    3.723    0.000    0.393    0.194\n    female            4.168    1.160    3.593    0.000    4.168    0.166\n  msc ~                                                                 \n    mse               0.736    0.066   11.119    0.000    0.736    0.512\n    cc                0.519    0.117    4.434    0.000    0.519    0.179\n    hsl               2.824    0.593    4.764    0.000    2.824    0.218\n  cc ~                                                                  \n    hsl               0.662    0.247    2.686    0.007    0.662    0.149\n  hsl ~                                                                 \n    female            0.208    0.154    1.348    0.178    0.208    0.075\n```\n:::\n\n\n-   Two direct effects are not statistically significant:\n\n    -   $\\beta_{F, HSL} = .208$, p = .178\n    -   $\\beta_{HSL, PERF} = .153$, p = .152\n\n-   Although we retain these effects in Model 2, the overall path model suggests they may not be necessary\n\n    -   Therefore, we will remove them and re-estimate the model\n\n# Model 3: Remove non-significant effects\n\n## Model 3: `lavaan` syntax\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 03: removing HSL predicting PERF and Gender predicting HSL  ----------------\nmodel03.syntax = \n\" \n#endogenous variable equations\nperf ~ msc + mse\nuse  ~ mse\nmse  ~ hsl + cc + female \nmsc  ~ mse + cc + hsl\ncc   ~ hsl\n\n#endogenous variable intercepts\nperf ~ 1\nuse  ~ 1\nmse  ~ 1\nmsc  ~ 1\ncc   ~ 1\n\n#endogenous variable residual variances\nperf ~~ perf\nuse  ~~ use\nmse  ~~ mse\nmsc  ~~ msc\ncc   ~~ cc  \n\n#endogenous variable residual covariances\n#none specfied in the original model so these have zeros:\nperf ~~ 0*use + 0*mse + 0*msc + 0*cc \nuse  ~~ 0*mse + msc + 0*cc  \nmse  ~~ 0*msc + 0*cc \nmsc  ~~ 0*cc \n\"\n\n#estimate model\nmodel03.fit = sem(model03.syntax, data=dataMath, mimic = \"MPLUS\", estimator = \"MLR\")\nsummary(model03.fit, fit.measures = TRUE)\n```\n:::\n\n\n## Model 3: Model fit results\n\n-   We have: an identified model, a converged algorithm, and stable standard errors, so model fit should be inspected\n    -   Next – inspect model fit\n    -   Model fit seems to not be as good as we would think\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#see if model converged\ninspect(model03.fit, what=\"converged\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nfitmeasures(model03.fit)[needed_fitIndex]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rmsea        srmr         cfi         tli       chisq          df \n 0.05739989  0.03674047  0.98265706  0.96146014 18.31095521  9.00000000 \n     pvalue \n 0.03173258 \n```\n\n\n:::\n:::\n\n\n-   The largest normalized residual covariance remains that between Female and CC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals(model03.fit, type = \"normalized\")$cov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         perf    use    mse    msc     cc    hsl female\nperf   -0.063                                          \nuse    -1.380  0.025                                   \nmse    -0.140  0.129 -0.113                            \nmsc     0.024  0.303 -0.084  0.065                     \ncc     -0.394  0.417 -0.282  0.006  0.017              \nhsl     0.949  0.626 -0.055  0.138  0.010  0.000       \nfemale -1.531  0.332 -0.329 -1.886 -2.241  0.000  0.000\n```\n\n\n:::\n:::\n\n\n-   The modification index for the direct effect of Female on CC is 5.090, indicating that adding this parameter may improve model fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificationindices(model03.fit, sort. = T)[1:3,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    lhs op    rhs    mi    epc sepc.lv sepc.all sepc.nox\n36 perf  ~    use 5.625 -0.021  -0.021   -0.111   -0.111\n21 perf ~~    use 5.238 -4.264  -4.264   -0.139   -0.139\n55   cc  ~ female 5.090 -1.653  -1.653   -0.133   -0.278\n```\n\n\n:::\n:::\n\n\n-   Accordingly, we will add a direct effect from Female to CC\n\n# Model 4: Adding Female on CC\n\n## Model 4: `lavaan` syntax\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 04: Add gender predicting cc -------------------------------------------------------------\nmodel04.syntax = \" \n#endogenous variable equations\nperf ~ msc + mse\nuse  ~ mse\nmse  ~ hsl + cc + female \nmsc  ~ mse + cc + hsl\ncc   ~ hsl + female\n\n#endogenous variable intercepts\nperf ~ 1\nuse  ~ 1\nmse  ~ 1\nmsc  ~ 1\ncc   ~ 1\n\n#endogenous variable residual variances\nperf ~~ perf\nuse  ~~ use\nmse  ~~ mse\nmsc  ~~ msc\ncc   ~~ cc  \n\n#endogenous variable residual covariances\n#none specfied in the original model so these have zeros:\nperf ~~ 0*use + 0*mse + 0*msc + 0*cc \nuse  ~~ 0*mse + msc + 0*cc  \nmse  ~~ 0*msc + 0*cc \nmsc  ~~ 0*cc \n\"\nmodel04.fit = sem(model04.syntax, data=dataMath, mimic = \"MPLUS\", estimator = \"MLR\")\n```\n:::\n\n\n## Model 4: Model Fit Index\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninspect(model04.fit, what=\"converged\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nfitmeasures(model04.fit)[needed_fitIndex]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rmsea        srmr         cfi         tli       chisq          df \n 0.04531236  0.02405819  0.99039314  0.97598285 13.15766273  8.00000000 \n     pvalue \n 0.10653884 \n```\n\n\n:::\n\n```{.r .cell-code}\nresiduals(model04.fit, type = \"standardized\")$cov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         perf    use    mse    msc     cc    hsl female\nperf   -0.065                                          \nuse    -2.020  0.143                                   \nmse    -1.590  1.856 -0.871                            \nmsc     1.080  1.058  0.835  1.035                     \ncc     -0.325  0.498  0.645  0.404 -0.441              \nhsl     1.578  0.694 -1.321  0.581  0.286  0.000       \nfemale -1.560  0.433  1.420 -1.500  0.853  0.000  0.000\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel04.mi <- modificationindices(model04.fit, sort. = T)\nhead(model04.mi, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    lhs op    rhs    mi    epc sepc.lv sepc.all sepc.nox\n37 perf  ~    use 5.624 -0.021  -0.021   -0.111   -0.111\n22 perf ~~    use 5.237 -4.262  -4.262   -0.139   -0.139\n41  use  ~   perf 4.860 -1.074  -1.074   -0.201   -0.201\n51  msc  ~ female 3.243 -2.626  -2.626   -0.075   -0.156\n56  hsl  ~   perf 2.449  0.078   0.078    0.178    0.178\n```\n\n\n:::\n:::\n\n\n-   We have: an identified model, a converged algorithm, and stable standard errors, so model fit should be inspected\n\n-   No normalized residual covariances exceed the absolute value of 1.96, indicating adequate model fit\n\n-   We will leave this model as-is and interpret the results\n\n## Model 4: Parameter Interpretation\n\n-   Interpret each of these parameters as you would in regression:\n    -   A one-unit increase in HSL brings about a .695 unit increase in CC, holding Female constant\n\n    -   `std.all`: We can also interpret the standardized parameter estimates for all variables except gender\n\n    -   A 1-SD increase in HSL means CC increases by 0.153 SD\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameterestimates(model04.fit, standardized = T, ci = F) |> filter(op == \"~\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    lhs op    rhs    est    se      z pvalue std.lv std.all std.nox\n1  perf  ~    msc  0.042 0.009  4.598  0.000  0.042   0.236   0.236\n2  perf  ~    mse  0.151 0.013 11.353  0.000  0.151   0.586   0.586\n3   use  ~    mse  0.279 0.079  3.513  0.000  0.279   0.203   0.203\n4   mse  ~    hsl  4.050 0.410  9.869  0.000  4.050   0.456   0.347\n5   mse  ~     cc  0.383 0.106  3.613  0.000  0.383   0.196   0.196\n6   mse  ~ female  3.819 1.201  3.180  0.001  3.819   0.156   0.327\n7   msc  ~    mse  0.692 0.070  9.872  0.000  0.692   0.481   0.481\n8   msc  ~     cc  0.545 0.120  4.534  0.000  0.545   0.193   0.193\n9   msc  ~    hsl  2.894 0.608  4.760  0.000  2.894   0.227   0.172\n10   cc  ~    hsl  0.695 0.254  2.737  0.006  0.695   0.153   0.117\n11   cc  ~ female -1.662 0.719 -2.312  0.021 -1.662  -0.133  -0.279\n```\n\n\n:::\n:::\n\n\n## Model Interpretation: Explained Variability\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(inspect(model04.fit, what = 'r2'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        cc        use        mse        msc       perf \n0.03875216 0.04124687 0.29765620 0.48535191 0.57624994 \n```\n\n\n:::\n:::\n\n\n-   The R2 for each endogenous variable:\n    -   CC – 0.039\n    -   USE – 0.041\n    -   MSE – 0.298\n    -   MSC – 0.485\n    -   PERF – 0.576\n-   Note that College Experience and Perceived Usefulness both have relatively low proportions of variance explained by the model\n    -   The R² for Perceived Usefulness could have been increased by specifying a direct path from Math Self-Concept to Perceived Usefulness instead of their residual covariance\n\n## Overall Model Interpretation\n\n-   High School Experience and Female are significant predictors of College Experience ($\\beta_{HSL, CC} = .695, p = .006$ and $\\beta_{F, CC} = -1.662, p = .021$)\n\n    -   Female students report lower College Experience than male students\n    -   Greater High School Experience is associated with increased College Experience\n\n-   High School Experience, College Experience, and Gender are significant predictors of Math Self-Efficacy ($\\beta_{HSL, MSE}= 4.05, p < .001$; $\\beta_{CC, MSE}= .383, p < .001$; $\\beta_{F, MSE}= 3.819, p = .001$)\n\n    -   Greater High School and College Experience are associated with higher Math Self-Efficacy\n    -   Female students demonstrate higher Math Self-Efficacy than male students\n\n-   High School Experience, College Experience, and Math Self-Efficacy are significant predictors of Math Self-Concept ($\\beta_{HSL, MSC}= 2.894, p < .001$; $\\beta_{CC, MSC}= .545, p < .001$; $\\beta_{MSE, MSC}= .692, p < .001$)\n\n    -   Greater High School Experience, College Experience, and Math Self-Efficacy are associated with higher Math Self-Concept\n\n-   Higher Math Self-Efficacy is significantly associated with increased Perceived Usefulness\n\n-   Higher Math Self-Efficacy and Math Self-Concept are associated with improved Math Performance scores\n\n-   Math Self-Concept and Perceived Usefulness exhibit a significant residual covariance\n\n## Wrapping Up\n\n-   In this lecture we discussed the basics of path analysis\n    -   Model specification/identification\n    -   Model estimation\n    -   Model fit (necessary, but not sufficient)\n    -   Model modification and re-estimation\n    -   Final model parameter interpretation\n-   There is a lot to the analysis – but what is important to remember is the over-arching principal of multivariate analyses: covariance between variables is important\n    -   Path models imply very specific covariance structures\n    -   The validity of the results hinge upon accurately finding an approximation to the covariance matrix\n\n### Next Class\n\n1.  Indirect effect\n2.  Causality\n3.  Hypothesis tests\n4.  Robust ML\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}