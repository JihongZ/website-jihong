{
  "hash": "171fc5c8c033a220f1bff3c4aee0347e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04: Distribution and Estimation\"\nauthor: \"Jihong Zhang*, Ph.D\"\ninstitute: | \n  Educational Statistics and Research Methods (ESRM) Program*\n  \n  University of Arkansas\ndate: \"2024-09-16\"\nsidebar: false\nexecute: \n  echo: true\n  warning: false\noutput-location: column\nformat: \n  html: \n    page-layout: full\n    toc: true\n    toc-depth: 2\n    toc-expand: true\n    lightbox: true\n    code-fold: false\n    code-summary: \"R code\"\n  uark-revealjs:\n    scrollable: true\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: false\n    footer: \"ESRM 64503: Lecture 04: Distribution and Estimation\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    output-file: slides-index.html\nfilters:\n  - webr\n---\n\n\n\n\n\n\n## Today's Class\n\n-   Review Homework 1\n\n-   [The building blocks]{style=\"color: tomato\"}: **The basics of mathematical statistics:**\n\n    -   Random variables: Definition & Types\n\n    -   Univariate distribution\n\n        -   General terminology (e.g., sufficient statistics)\n\n        -   Univariate normal (aka Gaussian)\n\n        -   Other widely used univariate distributions\n\n    -   Types of distributions: Marginal \\| Conditional \\| Joint\n\n    -   Expected values: means, variances, and the algebra of expectations\n\n    -   Linear combinations of random variables\n\n-   [The finished product]{style=\"color: violet\"}: **How the GLM fits within statistics**\n\n    -   The GLM with the normal distribution\n\n    -   The statistical assumptions of the GLM\n\n    -   How to assess these assumptions\n\n# ESRM 64503: Homework 1\n\n## Question 1\n\n<details>\n\n-   Copy and paste your R syntax and R output that calculates the group **Senior-Old's standard error of group mean** (use the data and model we've used in class).\n\n    -   **Aim**: Test whether the group mean of senior-old significantly higher than the baseline (here, 0 score)\n\n$$\n\\mathbf{Test = \\beta_0 +\\beta_1Senior + \\beta_2New +\\beta_3Senior*New}\n$$\n\n-   The group mean of Senior-Old is $\\beta_0 + \\beta_1$.\n\n-   Based on algebra for variance, we know that\n\n$$\nVar(\\beta_0 + \\beta_1) = Var(\\beta_0)+Var(\\beta_1) +2*Cov(\\beta_0, \\beta_1)\n$$\n\n-   `vcov` function provides the variances and covariances for $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n:::\n\n\n\n\n\n\n</details>\n\n## Question 3\n\n<details>\n\n-   Copy and paste your R syntax and R output that calculates the standard error of conditional main effect of New (New vs. Old) when Senior = 1.\n\n    -   **Aim**: Test whether the conditional main effect of New when Senior significantly different from 0\n        -   In other words, when individuals are senior, whether new or old instruction method has significantly differences in their test scores\n\n$$\n\\beta_{2|Senior =1} = \\beta_2 + \\beta_3 * 1\n$$\n\n-   Based on algebra for variance, we know that\n\n$$\nVar(\\beta_2 + \\beta_3) = Var(\\beta_2)+Var(\\beta_3) +2*Cov(\\beta_2, \\beta_3)\n$$\n\n-   `vcov` function provides the variances and covariances for $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n:::\n\n\n\n\n\n\n</details>\n\n# Unit 1: Random Variables & Statistical Distribution\n\n## Definition of Random Variables\n\n-   [**Random:**]{.underline} situations in which the certainty of the outcome is unknown and is at least in part due to chance\n\n-   [**Variable:**]{.underline} a value that may change given the scope of a given problem or set of operations\n\n-   [**Random Variable**]{.underline}: a variable whose outcome depends on chance (possible values represent the potential outcomes of a future experiment)\n\nToday, we will denote a random variable with lowercase letters: ***x***\n\n**Question**: which one in the following options is random variable:\n\na.  any company's revenue in 2025\n\nb.  one specific company's monthly revenue in 2025\n\nc.  companies whose revenue is over \\$30 billion in 2025\n\n**My answer**: [only (a)]{.mohu}\n\n## Types of Random Variables\n\n1.  **Continuous**\n    -   Examples of continuous random variables:\n        -   ***x*** represents the height of a person, drawn at random\n        -   *Y* (the outcome/DV in a GLM)\n        -   Some variables like **exam scores or motivation scores** are not truly continuous variables, but it is convenient to treat them as continuous\n2.  **Discrete** (also called categorical, generally)\n    -   Examples of discrete random variables:\n        -   ***x*** represents the gender of a person, drawn at random\n        -   *Y* (outcomes like yes/no; pass/not pass; master / not master a skill; survive / die)\n3.  **Mixture of Continuous and Discrete**:\n    -   Example of a mixture: \\begin{equation}\n          x =\n        \\begin{cases}\n          RT & \\text{between 0 and 45 seconds} \\\\\n          0 & \\text{otherwise}\n        \\end{cases}       \n        \\end{equation}\n\n## Key Features of Random Variable\n\n1.  Random variables are each described by a **probability density / mass function (PDF)** – $f(x)$\n\n    -   PDF indicates relative frequency of occurrence\n\n    -   A PDF is a mathematical function that provides a rough picture of the distribution from which a random variable is drawn\n\n2.  The type of random variable dictates the name and nature of these functions:\n\n    -   [Continuous random variables]{style=\"color: tomato\"}:\n\n        -   $f(x)$ is called a probability density function\n\n        -   Area under curve must equal to 1 (found by calculus integration)\n\n        -   Height of curve (the function value $f(x)$):\n\n            -   Can be any positive number\n\n            -   Reflects relative likelihood of an observation occurring\n\n    -   [Discrete random variables]{style=\"color: violet\"}:\n\n        -   $f(x)$ is called a probability mass function\n\n        -   Sum across all values must equal 1\n\n------------------------------------------------------------------------\n\n::: {.callout-note style=\"font-size: 1.4em;\"}\nBoth maximum and minimum temperature values can be considered continuous random variables.\n:::\n\n-   Question 1: For the density plot below, what are the total probabilities when integrating over all values of maximum and minimum temperatures ({1, 1} or {0.5, 0.5})?\n\n-   **Answer**: [It is {1, 1} for maximum and minimum temperatures because they are two separate random variables.]{.mohu}\n\n\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\ntemp <- read.csv(\"data/temp_fayetteville.csv\")\ntemp$value_F <- (temp$value / 10 * 1.8) + 32\ntemp |> \n  ggplot() +\n  geom_density(aes(x = value_F, fill = datatype), col = \"white\", alpha = .8) +\n  labs(x = \"Max/Min Temperature (F) at Fayetteville, AR (Sep-2023)\", \n       caption = \"Source: National Oceanic and Atmospheric Administration (https://www.noaa.gov/)\") +\n  scale_x_continuous(breaks = seq(min(temp$value_F), max(temp$value_F), by = 5)) +\n  scale_fill_manual(values = c(\"tomato\", \"turquoise\")) +\n  theme_classic(base_size = 13) \n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n## Other key Terms\n\n-   The sample space is the set of all values that a random variable x can take:\n    -   **Example 1:** The sample space for a random variable x from a normal distribution $x \\sim N(\\mu_x, \\sigma^2_x)$ is $(-\\infty, +\\infty)$.\n    -   **Example 2:** The sample space for a random variable x representing the outcome of a coin flip is {H, T}\n    -   **Example 3:** The sample space for a random variable x representing the outcome of a roll of a die is {1, 2, 3, 4, 5, 6}\n-   When using generalized models, the key is to select a distribution with a sample space that matches the range of values obtainable from the data\n    -   Logistic regression - Match Bernoulli distribution [(Example 2)]{.mohu}\n    -   Poisson regression - Match Poisson distribution [(Example 3)]{.mohu}\n\n## Uses of Distributions in Data Analysis\n\n-   Statistical models make distributional assumptions on various parameters and / or parts of data\n\n-   These assumptions govern:\n\n    -   How models are estimated\n\n    -   How inferences are made\n\n    -   How missing data may be imputed\n\n-   If data do not follow an assumed distribution, inferences may be inaccurate\n\n    -   Sometimes a very big problem, other times not so much\n\n-   Therefore, it can be helpful to check distributional assumptions prior to running statistical analysis\n\n## Continuous Univariate distributions\n\n-   To demonstrate how continuous distributions work and look, we will discuss three:\n\n    -   Uniform distribution\n\n    -   Normal distribution\n\n    -   Chi-square distribution (relevant to F-statistics)\n\n-   Each distribution is described by a set of parameters, which we will later see provide the basis for our inferences when we analyze data\n\n-   We then place constraints on those parameters based on hypothesized effects in the data\n\n## Uniform distribution\n\n-   The uniform distribution helps us understand how continuous distributions work\n\n    -   Typically used in simulation studies where parameters are randomly generated\n\n-   For a continuous random variable x that ranges from (a, b), the uniform probability density function is:\n\n    $f(x) = \\frac{1}{b-a}$\n\n-   The uniform distribution has two parameters\n\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    x = seq(0, 3, .1)\n    y = dunif(x, min = 0, max = 3)\n    ggplot() +\n      geom_point(aes(x = x, y = y)) +\n      geom_path(aes(x = x, y = y)) +\n      theme_bw()\n    ```\n    \n    ::: {.cell-output-display}\n    ![](Lecture04_files/figure-html/unnamed-chunk-4-1.png){width=672}\n    :::\n    :::\n\n\n\n\n\n\n## More on the Uniform Distribution\n\n-   To demonstrate how PDFs work, we will try a few values:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditions <- dplyr::tribble(\n   ~x, ~a, ~b,\n   .5,  0,  1,\n  .75,  0,  1,\n   15,  0, 20,\n   15, 10, 20\n) |> \n  mutate(y = dunif(x, min = a, max = b))\nconditions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n      x     a     b     y\n  <dbl> <dbl> <dbl> <dbl>\n1  0.5      0     1  1   \n2  0.75     0     1  1   \n3 15        0    20  0.05\n4 15       10    20  0.1 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   The uniform PDF has the feature that all values of ***x*** are equally likely across the sample space of the distribution\n    -   Therefore, you do not see ***x*** in the PDF $f(x)$\n-   The mean of the uniform distribution is $\\frac{1}{2}(a+b)$\n-   The variance of the uniform distribution is $\\frac{1}{12}(b-a)^2$\n\n## Univariate Normal Distribution\n\n-   For a continuous random variable ***x*** (ranging from $-\\infty$ to $\\infty$), the univariate normal distribution function is:\n\n$$\nf(x) = \\frac1{\\sqrt{2\\pi\\sigma^2_x}}\\exp(-\\frac{(x-\\mu_x)^2}{2\\sigma^2_x})\n$$\n\n-   The shape of the distribution is governed by two parameters:\n\n    -   The mean $\\mu_x$\n\n    -   The variance $\\sigma^2_x$\n\n    -   These parameters are called **sufficient statistics** (they contain all the information about the distribution)\n\n-   The skewness (lean) and kurtosis (peakedness) are fixed\n\n-   Standard notation for normal distributions is $x\\sim N(\\mu_x, \\sigma^2_x)$\n\n    -   Read as: \"*x* follows a normal distribution with a mean $\\mu_x$ and a variance $\\sigma^2_x$\"\n\n-   Linear combinations of random variables following normal distributions result in a random variable that is normally distributed\n\n## Univariate Normal Distribution in R: `pnorm`\n\nDensity (`dnorm`), distribution function (`pnorm`), quantile function (`qnorm`) and random generation (`rnorm`) for the normal distribution with mean equal to `mean` and standard deviation equal to `sd`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"The probability of x = 0 a standard normal distribution with 0 and 1\\n\")\npnorm(0, mean = 0, sd = 1)\n\ncat(\"The probability of x = +1 a standard normal distribution with 0 and 1\\n\")\npnorm(1, mean = 0, sd = 1)\n\ncat(\"The probability of x = -1 a standard normal distribution with 0 and 1\\n\")\npnorm(-1, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe probability of x = 0 a standard normal distribution with 0 and 1\n[1] 0.5\nThe probability of x = +1 a standard normal distribution with 0 and 1\n[1] 0.8413447\nThe probability of x = -1 a standard normal distribution with 0 and 1\n[1] 0.1586553\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\"}\nZ = seq(-5, 5, .1) # Z-score\nggplot() +\n  aes(x = Z, y = pnorm(q = Z, lower.tail = TRUE)) +\n  geom_point() +\n  geom_path() +\n  labs(x = \"Z-score\", y = \"Cumulative probability\",\n       title = \"`pnorm()` gives the cumulative probability function P(X < T)\")\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n---------------------------\n\n\n::: panel-tabset\n### Exercise - pnorm\n\n95% CI: Try to confirm that the probability of x locating between $\\pm$ 2SD is around 95%.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R code\"}\nx = seq(-5, 5, .01)\ny = dnorm(x)\nx_pm2sd = c(-2, seq(-2, 2, .01), 2)\ny_pm2sd = c(0, dnorm(seq(-2, 2, .01)), 0)\ndata.frame(x, y) |> \n  ggplot() +\n  aes(x = x, y = y) +\n  geom_polygon(fill = \"grey\", alpha = .5) +\n  geom_polygon(aes(x = x_pm2sd, y = y_pm2sd), data = data.frame(x_pm2sd, y_pm2sd), fill = \"tomato\", alpha = .8) +\n  scale_x_continuous(breaks = seq(-5, 5, .5)) +\n  geom_label(x = 0, y = 0.15, label = \"~ 95%\") +\n  labs(y = \"Probability of X = x\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n```{webr-r}\nlb <- pnorm(____________) # The cumulative probability of x = -2SD\nub <- pnorm(____________) # The cumulative probability of x = 2SD\n____________\n```\n\n### Answer\n\n```{webr-r}\nlb <- pnorm(-2, mean = 0, sd = 1, lower.tail = TRUE)\nub <- pnorm(2, mean = 0, sd = 1, lower.tail = TRUE)\nub - lb\n```\n:::\n\n## Univariate Normal Distribution in R: `dnorm`\n\nFor example, `dnrom(x = 0, mean = 0, sd = 1)` will calculate the density probability of x = 1 in a standard normal distribution.\n\nSo, $\\mu_x = 0$, $x = 0$, and $\\sigma_x = 1$:\n\n$$\nf(x = 0) = \\frac1{\\sqrt{2\\pi\\sigma^2_x}}\\exp(-\\frac{(x-\\mu_x)^2}{2\\sigma^2_x}) = \\frac{1}{\\sqrt{2\\pi}} \\approx 0.398\n$$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnorm(x = 0, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3989423\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- seq(-5, 5, .1)\n\nparams <- list(\n  y_set1 = c(mu =  0, sigma2 = 0.2),\n  y_set2 = c(mu =  0, sigma2 = 1.0),\n  y_set3 = c(mu =  0, sigma2 = 5.0),\n  y_set4 = c(mu = -2, sigma2 = 0.5)\n)\n\ny <- sapply(params, function(param) dnorm(x, mean = param['mu'], sd = param['sigma2']))\n\ndt <- cbind(x, y) |> \n  as.data.frame() |> \n  pivot_longer(starts_with(\"y_\"))\n\nggplot(dt) +\n  geom_path(aes(x = x, y = value, color = name, group = name), linewidth = 1.3) +\n  scale_color_manual(values = 1:4,\n                     name = \"\",\n                     labels = c('y_set1' = expression(mu*\"=0, \"*sigma^2*\"=.02\"),\n                                'y_set2' = expression(mu*\"=0, \"*sigma^2*\"=1.0\"),\n                                'y_set3' = expression(mu*\"=0, \"*sigma^2*\"=5.0\"),\n                                'y_set4' = expression(mu*\"=-2, \"*sigma^2*\"=0.5\"))\n                     ) +\n  theme_bw() +\n  theme(legend.position = \"top\", text = element_text(size = 13))\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Chi-Square Distribution\n\n-   Another frequently used univariate distribution is the Chi-square distribution\n\n    -   Sampling distribution of the variance follows a chi-square distribution\n\n    -   Likelihood ratios follow a chi-square distribution\n\n    -   F-distributed random variable can be expressed as the ratio of two independent gamma-distributed random variables\n\n-   For a continuous random variable *x* (ranging from 0 to $\\infty$), the chi-square distribution is given by:\n\n    $$\n    \\chi^2(x) =\\frac1{2^{\\frac{\\upsilon}{2}} \\Gamma(\\frac{\\upsilon}{2})} x^{\\frac{\\upsilon}{2}-1} \\exp(-\\frac{x}2)\n    $$\n\n-   $\\Gamma(\\cdot)$ is called the gamma function\n\n-   The chi-square distribution is governed by one parameter: $\\upsilon$ (the degrees of freedom)\n\n    -   The mean is equal to $\\upsilon$; the variance is equal to 2$\\upsilon$\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- seq(0.01, 15, .01)\ndf <- c(1, 2, 3, 5, 10)\ndt2 <- as.data.frame(sapply(df, \\(df) dchisq(x, df = df)))\ndt2_with_x <- cbind(x = x, dt2)\ndt2_with_x |> \n  pivot_longer(starts_with(\"V\")) |> \n  ggplot() +\n  geom_path(aes(x = x, y = value, color = name), linewidth = 1.2) +\n  scale_y_continuous(limits = c(0, 1)) +\n  scale_color_discrete(name = \"\", labels = paste(\"df =\", df)) +\n  labs(x = \"x\", y = \"f(x)\",\n       title = \"Chi-square Distributions with Different Parameters\",\n       color = \"Parameters\") +\n  theme_bw() +\n  theme(legend.position = \"top\", text = element_text(size = 13))\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Gamma Distribution Practice\n\n-   The chi-square distribution is a special case of the gamma distribution.\n\n    -   Specifically, a chi-square distribution with (k) degrees of freedom is equivalent to a gamma distribution with a shape parameter of (k/2) and a rate parameter of (1/2) (or a scale parameter of 2).\n\n-   The gamma distribution is another important continuous distribution, particularly useful for modeling waiting times and positive continuous data. It has two parameters: shape ($\\alpha$) and rate ($\\beta$).\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Gamma distribution with different shape and rate parameters\nx <- seq(0.01, 10, .01)\nshape_params <- c(1, 2, 3, 5)\nrate_params <- c(1, 1, 2, 1)\n\n# Create data for different gamma distributions\ngamma_data <- data.frame()\nfor(i in 1:length(shape_params)) {\n  temp_data <- data.frame(\n    x = x,\n    density = dgamma(x, shape = shape_params[i], rate = rate_params[i]),\n    distribution = paste0(\"Gamma(\", shape_params[i], \", \", rate_params[i], \")\")\n  )\n  gamma_data <- rbind(gamma_data, temp_data)\n}\n\n# Plot the distributions\nggplot(gamma_data) +\n  geom_path(aes(x = x, y = density, color = distribution), linewidth = 1.2) +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\")) +\n  labs(x = \"x\", y = \"f(x)\",\n       title = \"Gamma Distributions with Different Parameters\",\n       color = \"Parameters\") +\n  theme_bw() +\n  theme(legend.position = \"top\", text = element_text(size = 13))\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n# Marginal, Joint, And Conditional Distribution\n\n## Moving from One to Multiple Random Variables\n\n-   When more than one random variable is present, there are several different types of statistical distributions:\n\n-   We will first consider two discrete random variables:\n\n    -   *x* is the outcome of the flip of a penny {$H_p$, $T_p$}\n\n        -   $f(x=H_p) = .5$; $f(x =T_p) = .5$\n\n    -   *z* is the outcome of the flip of a dime {$H_d$, $T_d$}\n\n        -   $f(z=H_d) = .5$; $f(z =T_d) = .5$\n\n-   We will consider the following distributions:\n\n    -   Marginal distribution\n\n        -   The distribution of one variable only (either $f(x)$ or $f(z)$)\n\n    -   Joint distribution\n\n        -   $f(x, z)$: the distribution of both variables (both *x* and *z*)\n\n    -   **Conditional distribution**\n\n        -   The distribution of one variable, conditional on values of the other:\n\n            -   $f(x|z)$: the distribution of *x* given *z*\n\n            -   $f(z|x)$: the distribution of *z* given x\n\n## Marginal Distribution\n\n-   Marginal distributions are what we have worked with exclusively up to this point: they represent the distribution by itself\n\n    -   Continuous univariate distributions\n\n        -   Adult heights (Normal)\n\n        -   Time between arrivals (Exponential)\n\n        -   Device lifetimes (Weibull)\n\n        -   Proportions on [0,1] (Beta)\n\n    -   Categorical distributions\n\n        -   The flip of a penny\n\n        -   The flip of a dime\n\n        -   The roll of a fair die (1–6)\n\n        -   Day of the week (Mon–Sun)\n\n-----------------------------\n\n### Visualization of marginal and joint distribution\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Marginal categorical distribution: fair die\ndie <- data.frame(face = factor(1:6), p = rep(1/6, 6))\np1 <- ggplot(die, aes(x = face, y = p)) +\n  geom_col(fill = \"#5DA5DA\") +\n  labs(title = \"Marginal: Fair Die\", x = \"Face\", y = \"Probability\") +\n  scale_y_continuous(limits = c(0, 0.25)) +\n  theme_minimal(base_size = 12)\n\n# Joint categorical distribution: penny × dime (independent, fair)\njoint <- expand.grid(penny = c(\"H\", \"T\"), dime = c(\"H\", \"T\"))\njoint$p <- 0.25\np2 <- ggplot(joint, aes(x = penny, y = dime, fill = p)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = scales::percent(p)), color = \"black\") +\n  scale_fill_gradient(low = \"#CFE8F3\", high = \"#2166AC\", labels = scales::percent) +\n  labs(title = \"Joint: Penny × Dime\", x = \"Penny\", y = \"Dime\", fill = \"Probability\") +\n  theme_minimal(base_size = 12)\n\np1\np2\n```\n\n::: {.cell-output-display}\n![marginal (die) distribution](Lecture04_files/figure-html/fig-categorical-examples-1.png){#fig-categorical-examples-1 width=576}\n:::\n\n::: {.cell-output-display}\n![joint (penny × dime) distribution](Lecture04_files/figure-html/fig-categorical-examples-2.png){#fig-categorical-examples-2 width=576}\n:::\n:::\n\n\n\n\n\n\n## Joint Distribution\n\n-   Joint distributions describe the simultaneous distribution of more than one variable\n\n    -   These represent multiple variables collected together\n\n    -   Commonly, the joint distribution function is denoted with all random variables separated by commas\n\n    -   In our example, $f(x,z)$ is the joint distribution of the outcome of flipping both a penny and a dime\n\n        -   As both are discrete, the joint distribution has four possible values:\n\n            \\(1\\) $f(x = H_p,z=H_d)$ (2) $f(x = H_p,z=T_d)$ (3) $f(x = T_p,z=H_d)$ (4) $f(x = T_p,z=T_d)$\n\n-   Joint distributions are **multivariate distributions**\n\n-   We will use joint distributions to introduce two topics\n\n    -   Joint distributions of independent variables\n\n    -   Joint distributions – used in maximum likelihood estimation\n\n--------------------------\n\n### Joint Distributions of Independent Random Variables\n\n-   Random variables are said to be independent if the occurrence of one event makes it neither more nor less probable of another event\n\n    -   For joint distributions, this means: $f(x,z)=f(x)f(z)$\n\n-   In our example, flipping a penny and flipping a dime are independent – so we can complete the following table of their joint distribution:\n\n    |   | z = $H_d$ | z = $T_d$ | Marginal (Penny) |\n    |----|----|----|----|\n    | $x = H_p$ | $\\color{tomato}{f(x=H_p, z=H_d)}$ | $\\color{tomato}{f(x=H_p, z=T_d)}$ | $\\color{turquoise}{f(z=H_p)}$ |\n    | $x = T_p$ | $\\color{tomato}{f(x= T_p, z=H_d)}$ | $\\color{tomato}{f(x=T_p, z=T_d)}$ | $\\color{turquoise}{f(z=T_d)}$ |\n    | Marginal (Dime) | $\\color{turquoise}{f(z=H_d)}$ | $\\color{turquoise}{f(z=T_d)}$ |  |\n\n------------------------------\n\n### Joint Distributions of Independent Random Variables\n\n-   Because the coin flips are independent, this becomes:\n\n|   | z = $H_d$ | z = $T_d$ | Marginal (Penny) |\n|----|----|----|----|\n| $x = H_p$ | $\\color{tomato}{f(x=H_p)f( z=H_d)}$ | $\\color{tomato}{f(x=H_p)f( z=T_d)}$ | $\\color{turquoise}{f(z=H_p)}$ |\n| $x = T_p$ | $\\color{tomato}{f(x= T_p)f( z=H_d)}$ | $\\color{tomato}{f(x=T_p)f( z=T_d)}$ | $\\color{turquoise}{f(z=T_d)}$ |\n| Marginal (Dime) | $\\color{turquoise}{f(z=H_d)}$ | $\\color{turquoise}{f(z=T_d)}$ |  |\n\n-   Then, with numbers:\n\n|   | z = $H_d$ | z = $T_d$ | Marginal (Penny) |\n|----|----|----|----|\n| $x = H_p$ | $\\color{tomato}{.25}$ | $\\color{tomato}{.25}$ | $\\color{turquoise}{.5}$ |\n| $x = T_p$ | $\\color{tomato}{.25}$ | $\\color{tomato}{.25}$ | $\\color{turquoise}{.5}$ |\n| Marginal (Dime) | $\\color{turquoise}{.5}$ | $\\color{turquoise}{.5}$ |  |\n\n---------------\n\n### Marginalizing Across a Joint Distribution\n\n-   If you had a joint distribution, $\\color{orchid}{f(x, z)}$, but wanted the marginal distribution of either variable ($f(x)$ or $f(z)$) you would have to **marginalize** across one dimension of the joint distribution.\n\n-   For [**categorical random variables**]{.underline}, marginalizing means summing across every value of z\n\n$$\nf(x) = \\sum_zf(x, z)\n$$\n\n-   For example, $f(x = H_p) = f(x = H_p, z=H_d) +f(x = H_p, z=T_d)=.5$\n\n-   For [**continuous random variables**]{.underline}, marginalizing means integrating across z\n\n    -   The integral:\n\n        $$\n        f(x) = \\int_zf(x,z)dz\n        $$\n\n## Conditional Distribution\n\n-   For two random variables x and z, a conditional distribution is written as: $f(z|x)$\n\n    -   The distribution of z given x\n\n-   The conditional distribution equals the joint distribution divided by the marginal distribution of the conditioning variable\n\n    $$\n    f(z|x) = \\frac{f(z,x)}{f(x)}\n    $$\n\n-   Conditional distributions are found everywhere in statistics\n\n    -   The general linear model uses the conditional distribution of the variable\n\n        $$\n        Y \\sim N(\\beta_0+\\beta_1X, \\sigma^2_e)\n        $$\n\n------------------\n\n### Example: Conditional Distribution\n\n-   For two discrete random variables with {0, 1} values, the conditional distribution can be shown in a contingency table:\n\n|   | z = $H_d$ | z = $T_d$ | Marginal (Penny) |\n|----|----|----|----|\n| $x = H_p$ | $\\color{tomato}{.25}$ | $\\color{tomato}{.25}$ | $\\color{turquoise}{.5}$ |\n| $x = T_p$ | $\\color{tomato}{.25}$ | $\\color{tomato}{.25}$ | $\\color{turquoise}{.5}$ |\n| Marginal (Dime) | $\\color{turquoise}{.5}$ | $\\color{turquoise}{.5}$ |  |\n\nConditional: $f(z | x= H_p)$:\n\n$f(z=H_d|x =H_p) = \\frac{f(z=H_d, x=H_p)}{f(x = H_p)} = \\frac{.25}{.5}=.5$\n\n$f(z = T_d | x = H_p)= \\frac{f(z=T_d, x=H_p)}{f(x=H_p)} = \\frac{.25}{.5} = .5$\n\n---------------------\n\n### Exercise: calculate conditional probabilities\n\nUsing the same penny (x) and dime (z) setup, suppose the joint distribution is slightly modified as follows:\n\n|   | z = $H_d$ | z = $T_d$ |\n|----|----|----|\n| $x = H_p$ | 0.30 | 0.20 |\n| $x = T_p$ | 0.20 | 0.30 |\n\n-   Compute the marginal probabilities: $P(x=H_p)$, $P(x=T_p)$, $P(z=H_d)$, $P(z=T_d)$.\n-   Compute the conditional probabilities: $P(z=H_d\\mid x=H_p)$ and $P(x=T_p\\mid z=T_d)$.\n-   Briefly show your work (fractions from the table entries).\n\n\n<details>\n<summary>Answer (click to expand)</summary>\n\nMarginals:\n\n- $P(x=H_p) = 0.30 + 0.20 = 0.50$\n- $P(x=T_p) = 0.20 + 0.30 = 0.50$\n- $P(z=H_d) = 0.30 + 0.20 = 0.50$\n- $P(z=T_d) = 0.20 + 0.30 = 0.50$\n\nConditionals:\n\n- $P(z=H_d\\mid x=H_p) = \\dfrac{0.30}{0.50} = 0.60$\n- $P(x=T_p\\mid z=T_d) = \\dfrac{0.30}{0.50} = 0.60$\n\n</details>\n\n# Expected Values and The Algebra of Expectation\n\n## Expected Values\n\n-   Expected values are statistics calculated over the sample space of a random variable: they are essentially weighted averages:\n\n\n\n\n\n\n    ::: {.cell output-location='default'}\n    \n    ```{.r .cell-code}\n    set.seed(1234)\n    x = rnorm(100, mean = 0, sd = 1)\n    w = dnorm(x, mean = 0, sd = 1)\n    mean(w * x)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [1] -0.05567835\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n\n-   Where $w$ denotes the density function of x: $f(x)$\n\n-   The weights used in computing this average correspond to the probabilities (for discrete random variables) or the densities (for continuous random variables)\n\n::: {.callout-note font-size=\"1.2em\"}\nThe expected value is represented by $E(x)$\n\nThe actual statistic being weighted by the PDF is placed in the parentheses where x appears\n:::\n\n-   Expected values allow us to understand what a statistical model implies about data, for instance:\n\n    -   How a GLM specifies the (conditional) mean and variance of a DV\n\n## Expected Value Calculation\n\n-   For discrete random variables, the expected value is found by:\n\n    $$\n    E(x) = \\sum{x \\times P(X=x)}\n    $$\n\n-   For example, the expected value of a roll of a die is:\n\n    $$\n    E(x) = (1)\\frac16+ (2)\\frac16+(3)\\frac16+(4)\\frac16+(5)\\frac16+6\\frac16\n    $$\n\n-   For continuous random variables, the expected value is found by\n\n    $$\n    E(x) = \\int_x xf(x)dx\n    $$\n\n-   We won't be calculating theoretical expected values using calculus... we use them only to understand what our models imply about the data\n\n## Variance and Covariance... As Expected Values\n\n-   A distribution's theoretical variance can also be written as an expected value:\n\n    $$\n    V(x) = E(x-E(x))^2 = E(x -\\mu_x)^2\n    $$\n\n    -   This formula helps us understand predictions made by GLMs and how that corresponds to statistical parameters we interpret\n\n-   For a roll of a die, the theoretical variance is:\n\n    $$\n    V(x) = E(x - 3.5)^2 = \\frac16(1-3.5)^2 + \\frac16(2-3.5)^2 + \\frac16(3-3.5)^2 + \\frac16(4-3.5)^2 + \\frac16(5-3.5)^2 + \\frac16(6-3.5)^2 = 2.92\n    $$\n\n-   Likewise, for a pair of random variable *x* and *z*, the covariance can be found from their joint distributions:\n\n    $$\n    \\text{Cov}(x,z)=E(xz)-E(x)E(z) = E(xz)-\\mu_x\\mu_z\n    $$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nN = 100\nx = rnorm(N, 2, 1)\nz = rnorm(N, 3, 1)\nxz = x*z\n(Cov_xz = mean(xz)-mean(x)*mean(z)) / ((N-1)/N) # unbiased covariance\ncov(x, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.02631528\n[1] -0.02631528\n```\n\n\n:::\n:::\n\n\n\n\n\n\n# Linear Combination of Random Variables\n\n## Linear Combinations of Random Variables\n\n-   A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and then adding the results $$\n    x = c + a_1z_1+a_2z_2+a_3z_3 +\\cdots+a_nz_n\n    $$\n\n-   More generally, linear combinations of random variables have specific implications for the mean, variance, and possibly covariance of the new random variable\n\n-   As such, there are predictable ways in which the means, variances, and covariances change\n\n## Algebra of Expectations\n\n-   Sums of Constants\n\n$$\nE(x+c) = E(x)+c \\\\\n\\text{Var}(x+c) = \\text{Var}(x) \\\\\n\\text{Cov}(x+c, z) = \\text{Cov}(x, z)\n$$\n\n### Example\n\nImagine for weight variable, each individual increases 3 lbs:\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nset.seed(1234)\nlibrary(ESRM64503)\nx = dataSexHeightWeight$weightLB\nc_ = 3\nz = dataSexHeightWeight$heightIN\nmean(x + c_); mean(x)+c_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186.4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186.4\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(x + c_); var(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3179.095\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3179.095\n```\n\n\n:::\n\n```{.r .cell-code}\ncov(x, z); cov(x+c_, z) # decimal place issue, near() accepts two values' diff less than .00001\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 334.8316\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 334.8316\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n-   Products of Constants:\n\n$$\nE(cx) = cE(x) \\\\\n\\text{Var}(cx) = c^2\\text{Var}(x) \\\\\n\\text{Cov}(cx, dz) = c*d*\\text{Cov}(x, z)\n$$ Imagine you wanted to convert weight from pounds to kilograms (where 1 pound = 0.453 kg) and convert height from inches to cm (where 1 inch = 2.54 cm)\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nc_ = .453\nmean(x*c_); mean(x)*c_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 83.0802\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 83.0802\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(x*c_); var(x)*c_^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 652.3789\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 652.3789\n```\n\n\n:::\n\n```{.r .cell-code}\nd_ = 2.54\ncov(c_*x, d_*z);c_*d_*cov(x, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 385.2639\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 385.2639\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n-   Sums of Multiple Random Variables:\n\n$$\nE(cx+dz) = cE(x) + dE(z) \\\\\n\\text{Var}(cx+dz) = c^2\\text{Var}(x) + d^2\\text{Var}(z) + 2c*d*\\text{Cov}(x,z)\\\\\n$$\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmean(x*c_+z*d_); mean(x)*c_+mean(z)*d_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 255.5462\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 255.5462\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(x*c_+z*d_); c_^2*var(x) + d_^2*var(z) + 2*c_*d_*cov(x, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1780.054\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1780.054\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Where We Use This Algebra\n\n-   Recall how we calculated the standard error of the conditional main effect using the simple main effect and interaction effect\n\n$$\n\\mathbf{Test = 82.20 + 2.16*Senior + 7.76*New - 3.04*Senior*New}\n$$\n\n-   The conditional main effect of Senior when New equals 1 is (2.16 - 3.04) = -0.88\n\n    -   We know that $\\text{Var}(\\beta_{\\text{Senior}})$ is 0.575, $\\text{Var}(\\beta_{\\text{Senior*New}})$ is 1.151, and $\\text{Cov}(\\beta_{\\text{Senior}}, \\beta_{\\text{Senior*New}})$ is -0.575.\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\nmodel1 <- lm(Test ~ Senior + New + Senior * New, data = dataTestExperiment)\nvcov(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept)     Senior        New Senior:New\n(Intercept)   0.2877333 -0.2877333 -0.2877333  0.2877333\nSenior       -0.2877333  0.5754667  0.2877333 -0.5754667\nNew          -0.2877333  0.2877333  0.5754667 -0.5754667\nSenior:New    0.2877333 -0.5754667 -0.5754667  1.1509333\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThen,\n\n$$\n\\mathbf{Var(\\beta_{Senior}+\\beta_{Senior*New}) = Var(\\beta_{Senior}) + Var(\\beta_{Senior*New}) + 2Cov(\\beta_{Senior},\\beta_{Senior*New})\n\\\\ = 0.575 + 1.151 - 2* 0.575 = 0.576} \n$$ Thus, $SE(\\beta_{Senior}+\\beta_{Senior*New})=0.759$\n\n## Combining the GLM with Expectations\n\n-   Using the algebra of expectations for predicting Y from X and Z:\n\n$$\n\\hat{Y}_p=E(Y_p)=E(\\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p+e_p) \\\\\n= \\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p+E(e_p) \\\\\n= \\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p\n$$\n\n-   The variance of $f(Y_p|X_p, Z_p)$:\n\n$$\nV(\\hat{Y_P}|X_p, Z_p)=V(\\beta_0+\\beta_1X_p+\\beta_2Z_p+\\beta_3X_pZ_p+e_p)\\\\ = V(\\hat{Y}_p + e_p)=\\sigma_e^2\n$$\n\n## Understanding These Concepts in the Context of Data\n\n-   Recall from the regression analysis of the height/weight data that the final model we decided to interpret was Model 5\n\n$$\nW_p = \\beta_0+\\beta_1 (H_p-\\bar{H}) +\\beta_2F_p+\\beta_3 (H_p-\\bar{H}) F_p + e_p\n$$\n\nwhere $e_p \\sim N(0, \\sigma_e^2)$\n\n\n\n\n\n\n::: {.cell output-location='default'}\n\n```{.r .cell-code}\ndat <- dataSexHeightWeight\ndat$heightIN_MC <- dat$heightIN - mean(dat$heightIN)\ndat$female <- dat$sex == 'F'\nmod5 <- lm(weightLB ~ heightIN_MC + female + female*heightIN_MC, data = dat)\nsummary(mod5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weightLB ~ heightIN_MC + female + female * heightIN_MC, \n    data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8312 -1.7797  0.4958  1.3575  3.3585 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            222.1842     0.8381  265.11  < 2e-16 ***\nheightIN_MC              3.1897     0.1114   28.65 3.55e-15 ***\nfemaleTRUE             -82.2719     1.2111  -67.93  < 2e-16 ***\nheightIN_MC:femaleTRUE  -1.0939     0.1678   -6.52 7.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.175 on 16 degrees of freedom\nMultiple R-squared:  0.9987,\tAdjusted R-squared:  0.9985 \nF-statistic:  4250 on 3 and 16 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Picturing the GLM with Distributions\n\n-   The distributional assumptions of the GLM explain why we do not need to worry about whether our dependent variable is normally distributed\n\n-   Our dependent variable should be conditionally normal\n\n-   We can check this assumption by examining the residuals: $e_p \\sim N(0, \\sigma^2_e)$\n\n\n\n\n\n\n::: {.cell output-location='default'}\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Assessing Distributional Assumptions Graphically\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod5)\n```\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-21-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lecture04_files/figure-html/unnamed-chunk-21-4.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Hypothesis Tests for Normality\n\nIf a given test is **significant**, it indicates that your data do not come from a normal distribution\n\nIn practice, tests will often provide conflicting information. The best way to evaluate normality is to consider both plots and tests together (approximate normality is usually sufficient)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(mod5$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  mod5$residuals\nW = 0.95055, p-value = 0.3756\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Wrapping Up\n\n-   Today was an introduction to mathematical statistics as a way to understand the implications statistical models make about data\n\n-   Although many of these topics do not seem directly relevant, they help provide insights that untrained analysts may not easily attain\n",
    "supporting": [
      "Lecture04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}