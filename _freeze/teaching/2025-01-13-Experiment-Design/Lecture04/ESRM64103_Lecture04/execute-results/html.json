{
  "hash": "112f002d7dac58822e2ff57626d7de04",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04: ANOVA Assumptions Checking\"\nsubtitle: \"Experimental Design in Education\"\ndate: \"2025-02-10\"\nexecute: \n  eval: true\n  echo: true\nformat: \n  html:\n    code-tools: true\n    code-line-numbers: false\n    code-fold: false\n    number-offset: 1\n    fig.width: 10\n    fig-align: center\n    message: false\n    grid:\n      sidebar-width: 350px\n  uark-revealjs:\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: true\n    number-depth: 1\n    footer: \"ESRM 64503\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    scrollable: true\n    output-file: slides-index.html\n    mermaid:\n      theme: forest  \n---\n\n\n\n\n\n\n## Class Outline\n\n-   Go through three assumptions of ANOVA and their checking statistics\n-   Post-hoc test for more group comparisons.\n-   Example: Intervention and Verbal Acquisition\n-   After-class Exercise: Effect of Sleep Duration on Cognitive Performance\n\n## ANOVA Procedure\n\n1.  **Set hypotheses**:\n    -   **Null hypothesis** ($H_0$): All group means are equal.\n    -   **Alternative hypothesis** ($H_A$): At least one group mean differs.\n2.  **Determine statistical parameters**:\n    -   Significance level $\\alpha$\n    -   Degrees of freedom for between-group ($df_b$) and within-group ($df_w$)\n    -   Find the critical F-value.\n3.  **Compute test statistic**:\n    -   Calculate **F-ratio** based on between-group and within-group variance.\n4.  **Compare results**:\n    -   Either compare $F_{\\text{obs}}$ with $F_{\\text{crit}}$ or $p$-value with $\\alpha$.\n    -   If $p < \\alpha$, reject $H_0$.\n\n## ANOVA Type I Error\n\n1.  The way to compare the means of multiple groups (i.e., more than two groups)\n\n-   Compared to multiple t-tests, the ANOVA does not inflate Type I error rates.\n    -   For F-test in ANOVA, Type I error rate is not inflated\n    -   Type I error rate is inflated when we do multiple comparison\n-   Family-wise error rate: probability of at least one Type I error occur = $1 −  (1 − \\alpha)^c$ where c =number of tests. Frequently used FWER methods include Bonferroni or FDR.\n\n::: callout-note\n## Family-Wise Error Rate\n\n1.  The **Family-Wise Error Rate** (FWER) is the probability of making at least one **Type I error** (false positive) across a set of multiple comparisons.\n2.  In **Analysis of Variance (ANOVA)**, multiple comparisons are often necessary when analyzing the differences between group means. FWER control is crucial to ensure the validity of statistical conclusions.\n:::\n\n# ANOVA Assumptions\n\n## Overview\n\n-   Like all statistical tests, ANOVA requires certain assumptions to be met for valid conclusions:\n    -   **Independence**: Observations are independent of each other.\n    -   **Normality**: The residuals (errors) follow a normal distribution.\n    -   **Homogeneity of variance (HOV)**: The variance within each group is approximately equal.\n\n## Importance of Assumptions\n\n-   **If assumptions are violated**, the results of ANOVA may not be reliable.\n-   **We call it Robustness** as to what degree ANOVAs are not influenced by the violation of assumption.\n-   Typically:\n    -   ANOVA is **robust** to minor violations of normality, especially for large sample sizes (Central Limit Theorem).\n    -   **Not robust** to violations of independence—if independence is violated, ANOVA is inappropriate.\n    -   **Moderately robust** to HOV violations if sample sizes are equal.\n\n## Assumption 1: Independence\n\n-   **Definition**: Each observation should be independent of others.\n-   **Violations**:\n    -   Clustering of data (e.g., repeated measures).\n    -   Participants influencing each other (e.g., classroom discussions).\n-   **Check (Optional)**: Use the [Durbin-Watson test]{.redcolor}.\n-   **Solution**: Repeated Measured ANOVA, Mixed ANOVA, Multilevel Model\n-   **Consequences**: If independence is violated, [ANOVA results are not valid]{.red}.\n\n------------------------------------------------------------------------\n\n[Durbin-Watson (DW) Test]{.bluecolor .bigger}\n\n::: callout-note\n-   The Durbin-Watson test is primarily used for detecting **autocorrelation** in time-series data.\n\n-   In the context of ANOVA with independent groups, residuals are generally assumed to be independent.\n\n    -   However, it's still good practice to check this assumption, especially if there's a reason to suspect potential autocorrelation.\n:::\n\n::: callout-important\n-   Properties of DW Statistic:\n    -   $H_0$: [Independence of residual is satisfied.]{.redcolor}\n    -   Ranges from 0 to 4.\n        -   A value around 2 suggests no autocorrelation.\n        -   Values approaching 0 indicate positive autocorrelation.\n        -   Values toward 4 suggest negative autocorrelation.\n    -   P-value\n        -   P-Value: A small p-value (typically \\< 0.05) indicates violation of independency\n:::\n\n------------------------------------------------------------------------\n\n[`lmtest::dwtest()`]{.bluecolor .bigger}\n\n-   Performs the Durbin-Watson test for autocorrelation of disturbances.\n\n-   The Durbin-Watson test has the null hypothesis that the autocorrelation of the disturbances is 0.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"lmtest\")\nlibrary(lmtest)\nerr1 <- rnorm(100)\n\n## generate regressor and dependent variable\nx <- rep(c(-1,1), 50)\ny1 <- 1 + x + err1 \n## perform Durbin-Watson test\ndwtest(y1 ~ x)\n```\n:::\n\n\n\n\n\n\nThis results suggest there are no autocorrelation at the alpha level of .05.\n\n## Assumption 2: Normality\n\n-   The **dependent variable (DV)** should be normally distributed within each group.\n-   **Assessments**:\n    -   Graphical methods: Histograms, [Q-Q plots]{.redcolor}.\n    -   Statistical tests:\n        -   [Shapiro-Wilk test (common)]{.redcolor}\n        -   Kolmogorov-Smirnov (KS) test (for large samples)\n        -   Anderson-Darling test (detects kurtosis issues).\n-   **Robustness**:\n    -   ANOVA is robust to normality violations for large samples.\n    -   If normality is violated, consider transformations or non-parametric tests.\n\n------------------------------------------------------------------------\n\n[Normality within Each Group]{.bluecolor .bigger}\n\n-   Assume that the DV (Y) is distributed normally within each group for ANOVA\n\n-   ANOVA is **robust** to minor violations of normality\n\n    -   So generally start with homogeneity, then assess normality\n\n![](images/clipboard-3483482689.png)\n\n------------------------------------------------------------------------\n\n[`shapiro.test()`]{.bigger}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # For reproducibility\nsample_normal_data <- rnorm(200, mean = 50, sd = 10)  # Generate normal data\nsample_nonnormal_data <- runif(200, min = 1, max = 10)  # Generate non-normal data\n\nshapiro.test(sample_normal_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sample_normal_data\nW = 0.99076, p-value = 0.2298\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(sample_nonnormal_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sample_nonnormal_data\nW = 0.95114, p-value = 2.435e-06\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform Kolmogorov-Smirnov test against a normal distribution\nks.test(scale(sample_normal_data), \"pnorm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  scale(sample_normal_data)\nD = 0.054249, p-value = 0.5983\nalternative hypothesis: two-sided\n```\n\n\n:::\n\n```{.r .cell-code}\nks.test(scale(sample_nonnormal_data), \"pnorm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  scale(sample_nonnormal_data)\nD = 0.077198, p-value = 0.1843\nalternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Assumption 3: Homogeneity of Variance (HOV)\n\n-   Variance across groups should be equal.\n-   **Assessments**:\n    -   **Levene’s test**: Tests equality of variances.\n    -   **Brown-Forsythe test**: More robust to non-normality.\n    -   **Bartlett's test**: For data with normality.\n    -   **Boxplots**: Visual inspection.\n-   **What if violated?**\n    -   **Welch’s ANOVA** (adjusted for variance differences).\n    -   **Transforming** the dependent variable.\n    -   **Using non-parametric tests** (e.g., Kruskal-Wallis).\n\n------------------------------------------------------------------------\n\n[Practical Considerations]{.bluecolor .bigger}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Computes Levene's test for homogeneity of variance across groups.\ncar::leveneTest(outcome ~ group, data = data)\n\n## Boxplots to visualize the variance by groups\nboxplot(outcome ~ group, data = data)\n\n## Brown-Forsythe test \nonewaytests::bf.test(outcome ~ group, data = data)\n\n## Bartlett's Test\nbartlett.test(outcome ~ group, data = data)\n```\n:::\n\n\n\n\n\n\n-   **Levene's test** and **Brown-Forsythe test** are often preferred when data does not meet the assumption of normality, especially for small sample sizes.\n-   **Bartlett’s test** is most powerful when the data is normally distributed and when sample sizes are equal across groups. However, it can be less reliable if these assumptions are violated.\n\n------------------------------------------------------------------------\n\n[Decision Tree]{.redcolor .bigger}\n\n\n\n\n\n\n```{dot}\n//| echo: false\ndigraph flowchart {\n    node [shape=box, style=filled, fillcolor=lightgray];\n    \n    A [label=\"Is the data normal by group?\"];\n    B [label=\"Use Bartlett's Test\"];\n    C [label=\"Are there outliers?\"];\n    D [label=\"Use Brown-Forsythe Test\"];\n    E [label=\"Use Levene's Test\"];\n    \n    A -> B [label=\"Yes\"];\n    A -> C [label=\"No\"];\n    C -> D [label=\"Yes\"];\n    C -> E [label=\"No\"];\n}\n\n```\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nchat = ellmer::chat_ollama(model = \"llama3.2\")\nchat$chat(\"How large can be considered as large sample size in one-way ANOVA\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIn one-way ANOVA (Analysis of Variance), the sample size is a critical factor \nthat affects the analysis's power and accuracy. While there isn't a universally\nagreed-upon \"large\" sample size, here are some general guidelines:\n\n1. **Minimum sample size**: The minimum sample size for one-way ANOVA can vary \ndepending on the effect size and level of significance desired. A common rule \nof thumb is to have at least 10-15 observations per group (i.e., more than 5 \ngroups).\n2. **Moderate sample size**: For a moderate sample size, many researchers use \nvalues around:\n    * 30-50 observations per group for small effect sizes (<0.1) and moderate \nsignificance levels (e.g., α=0.05).\n    * 60-80 observations per group for medium effect sizes (≈ 0.1-0.3) and \nmodest significance levels.\n    * 100 observation s per group for larger effect sizes (> 0.3) or more \nstringent significance levels.\n3. **Large sample size**: When there are:\n    * Very small cells (≈ 10 observation s per group): often, the recommended \napproach would be to conduct pairwise comparisons using post-hoc tests like \nBonferroni, Newman-Keuls, or Tukey's honestly significant difference (HSD) \ntest.\n    For larger samples (>=50 observations s per group), you should consider \ndoing an ANOVA. To see if there is a \"real\" difference in mean values between 2\nsamples, use the F-statistic to check for significance:\n        \n4. **Effect size**: Keep in mind that no sample size is \"large enough\" for a \ngiven effect size. The required sample size depends on the magnitude of the \nexpected effect.\n\nKeep in mind that these are general guidelines and may not always hold true \ndepending on your specific research design, data distribution, and assumptions \n(e.g., homogeneity of variance).\n\nAlways consult with a knowledgeable statistical consultant and consider various\nsources if you're unsure about your study's sample size or need additional \nguidance.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n## ANOVA Robustness\n\n-   **Robust to**:\n\n    -   Minor normality violations (for large samples).\n    -   Small HOV violations if group sizes are **equal**.\n\n-   **Not robust to**:\n\n    -   **Independence violations**—ANOVA is invalid if data points are dependent.\n    -   **Severe HOV violations**—Type I error rates become unreliable.\n\n-   The robustness of assumptions is something you should be carefull before/when you perform data collection. They are not something you can do after data collection has been finished.\n\n------------------------------------------------------------------------\n\n[Robustness to Violations of Normality Assumption]{.redcolor}\n\n-   ANOVA assumes that the residuals (errors) are normally distributed within each group.\n\n-   However, ANOVA is generally robust to violations of normality, particularly when **the sample size is large**.\n\n-   **Theoretical Justification**: This robustness is primarily due to the Central Limit Theorem (CLT), which states that, for sufficiently large sample sizes (typically $n≥30$ per group), the sampling distribution of the mean approaches normality, even if the underlying population distribution is non-normal.\n\n-   This means that, unless the data are heavily skewed or have extreme outliers, ANOVA results remain valid and Type I error rates are not severely inflated.\n\n------------------------------------------------------------------------\n\n### Robustness to Violations of Homogeneity of Variance\n\n-   The homogeneity of variance (homoscedasticity) assumption states that all groups should have equal variances. ANOVA can tolerate moderate violations of this assumption, particularly when:\n\n    -   **Sample sizes are equal (or nearly equal) across groups** – When groups have equal sample sizes, the F-test remains robust to variance heterogeneity because the pooled variance estimate remains balanced.\n\n    -   **The degree of variance heterogeneity is not extreme** – If the largest group variance is no more than about four times the smallest variance, ANOVA results tend to remain accurate.\n\n------------------------------------------------------------------------\n\n### ANOVA: Lack of Robustness to Violations of Independence of Errors\n\n-   The assumption of independence of errors means that observations within and between groups must be uncorrelated. Violations of this assumption severely compromise ANOVA’s validity because:\n\n    -   **Inflated Type I error rates** – If errors are correlated (e.g., due to clustering or repeated measures), standard errors are underestimated, leading to an increased likelihood of falsely rejecting the null hypothesis.\n    -   **Biased parameter estimates** – When observations are not independent, the variance estimates do not accurately reflect the true variability in the data, distorting F-statistics and p-values.\n    -   **Common sources of dependency** – Examples include nested data (e.g., students within schools), repeated measurements on the same subjects, or time-series data. In such cases, alternatives like mixed-effects models or generalized estimating equations (GEE) should be considered.\n\n# Omnibus ANOVA Test\n\n## Overview\n\n-   **What does it test?**\n    -   Whether there is **at least one** significant difference among means.\n-   **Limitation**:\n    -   Does **not** tell **which** groups are different.\n-   **Solution**:\n    -   Conduct **post-hoc tests**.\n\n## Individual Comparisons of Means\n\n-   If ANOVA is **significant**, follow-up tests identify **where** differences occur.\n-   **Types**:\n    -   **Planned comparisons**: Defined **before** data collection.\n    -   **Unplanned (post-hoc) comparisons**: Conducted **after** ANOVA.\n\n## Planned vs. Unplanned Comparisons\n\n-   **Planned**:\n    -   Based on **theory**.\n    -   Can be done **even if ANOVA is not significant**.\n-   **Unplanned (post-hoc)**:\n    -   **Data-driven**.\n    -   Only performed **if ANOVA is significant**.\n\n## Types of Unplanned Comparisons\n\n-   **Common post-hoc tests**:\n    1.  **Fisher’s LSD**\n    2.  **Bonferroni correction** or Adjusted p-values\n    3.  **Sidák correction**\n    4.  **Tukey’s HSD**\n\n------------------------------------------------------------------------\n\n[Fisher’s LSD]{.bluecolor}\n\n-   **Least Significant Difference test**.\n-   **Problem**: Does not control for **multiple comparisons** (inflated Type I error).\n\n[Bonferroni Correction]{.bluecolor}\n\n-   Adjusts **alpha** to reduce **Type I error**.\n-   New alpha: $\\alpha / c$ (where $c$ is the number of comparisons).\n-   **Conservative**: Less power, avoids false positives.\n\n[Family-wise Error Rate (adjusted p-values)]{.bluecolor}\n\n-   Adjusts p-values to reduce Type I error\n-   Report adjusted p-values (typically larger that original p-values)\n\n[Tukey’s HSD]{.bluecolor}\n\n-   Controls for **Type I error** across multiple comparisons.\n-   Uses a **q-statistic** from a Tukey table.\n-   Preferred when **all pairs** need comparison.\n\n## ANOVA Example: Intervention and Verbal Acquisition\n\n### Background\n\n-   Research Question: Does an intensive intervention improve students’ verbal acquisition scores?\n-   Study Design:\n    -   4 groups: Control, G1, G2, G3 (treatment levels).\n    -   Outcome variable: Verbal acquisition score (average of three assessments).\n-   Hypotheses:\n    -   $H_0$: No difference in verbal acquisition scores across groups.\n    -   $H_A$: At least one group has a significantly different mean.\n\n## Step 1: Generate Simulated Data in R\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic data for 4 groups\ndata <- tibble(\n  group = rep(c(\"Control\", \"G1\", \"G2\", \"G3\"), each = 30),\n  verbal_score = c(\n    rnorm(30, mean = 70, sd = 10),  # Control group\n    rnorm(30, mean = 75, sd = 12),  # G1\n    rnorm(30, mean = 80, sd = 10),  # G2\n    rnorm(30, mean = 85, sd = 8)    # G3\n  )\n)\n\n# View first few rows\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  group   verbal_score\n  <chr>          <dbl>\n1 Control         64.4\n2 Control         67.7\n3 Control         85.6\n4 Control         70.7\n5 Control         71.3\n6 Control         87.2\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Step 2: Summary Statistics\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics by group\ndata %>%\n  group_by(group) %>%\n  summarise(\n    mean_score = mean(verbal_score),\n    sd_score = sd(verbal_score),\n    n = n()\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  group   mean_score sd_score     n\n  <chr>        <dbl>    <dbl> <int>\n1 Control       69.5     9.81    30\n2 G1            77.1    10.0     30\n3 G2            80.2     8.70    30\n4 G3            84.2     7.25    30\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplot visualization\nggplot(data, aes(x = group, y = verbal_score, fill = group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Verbal Acquisition Scores Across Groups\", y = \"Score\", x = \"Group\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Step 3: Check ANOVA Assumptions\n\n### Assumption Check 1: Independence of residuals Check\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the ANOVA model\nanova_model <- lm(verbal_score ~ group, data = data)\n\n# Install lmtest package if not already installed\n# install.packages(\"lmtest\")\n\n# Load the lmtest package\nlibrary(lmtest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zoo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'zoo'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n\n```{.r .cell-code}\n# Perform the Durbin-Watson test\ndw_test_result <- dwtest(anova_model)\n\n# View the test results\nprint(dw_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tDurbin-Watson test\n\ndata:  anova_model\nDW = 2.0519, p-value = 0.5042\nalternative hypothesis: true autocorrelation is greater than 0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   Interpretation:\n    -   In this example, the `DW` value is close to 2, and the p-value is greater than 0.05, indicating no significant autocorrelation in the residuals.\n\n------------------------------------------------------------------------\n\n### Assumption Check 2: Normality Check\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk normality test for each group\ndata %>%\n  group_by(group) %>%\n  summarise(\n    shapiro_p = shapiro.test(verbal_score)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  group   shapiro_p\n  <chr>       <dbl>\n1 Control     0.797\n2 G1          0.961\n3 G2          0.848\n4 G3          0.568\n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   Interpretation:\n    -   If $p>0.05$, normality assumption is not violated.\n    -   If $p<0.05$, data deviates from normal distribution.\n-   Since the data meets the normality requirement and no outliers, we can use [Bartlett's or Levene's tests]{.redcolor} for HOV checking.\n\n------------------------------------------------------------------------\n\n-   Alternative Check: Q-Q Plot\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(sample = verbal_score)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~group) +\n  theme_minimal() +\n  labs(title = \"Q-Q Plot for Normality Check\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### Assumption Check 3: Homogeneity of Variance (HOV) Check\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Levene's Test for homogeneity of variance\ncar::leveneTest(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   3  1.1718 0.3236\n      116               \n```\n\n\n:::\n\n```{.r .cell-code}\nstats::bartlett.test(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  verbal_score by group\nBartlett's K-squared = 3.5385, df = 3, p-value = 0.3158\n```\n\n\n:::\n\n```{.r .cell-code}\nonewaytests::bf.test(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Brown-Forsythe Test (alpha = 0.05) \n------------------------------------------------------------- \n  data : verbal_score and group \n\n  statistic  : 14.32875 \n  num df     : 3 \n  denom df   : 109.9888 \n  p.value    : 6.030497e-08 \n\n  Result     : Difference is statistically significant. \n------------------------------------------------------------- \n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   Interpretation:\n    -   If $p>0.05$, variance is homogeneous (ANOVA assumption met).\n    -   If $p<0.05$, variance differs across groups (consider Welch’s ANOVA).\n-   It turns out our data does not violate the homogeneity of variance assumption.\n\n## Step 4: Perform One-Way ANOVA\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_model <- aov(verbal_score ~ group, data = data)\nsummary(anova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ngroup         3   3492  1164.1   14.33 5.28e-08 ***\nResiduals   116   9424    81.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   Interpretation:\n    -   If $p<0.05$, at least one group mean is significantly different.\n    -   If $p>0.05$, fail to reject $H0$ (no significant differences).\n\n## Step 5: Post-Hoc Tests (Tukey’s HSD)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tukey HSD post-hoc test\ntukey_results <- TukeyHSD(anova_model)\nround(tukey_results$group, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             diff    lwr    upr p adj\nG1-Control  7.611  1.545 13.677 0.008\nG2-Control 10.715  4.649 16.782 0.000\nG3-Control 14.720  8.654 20.786 0.000\nG2-G1       3.104 -2.962  9.170 0.543\nG3-G1       7.109  1.042 13.175 0.015\nG3-G2       4.005 -2.062 10.071 0.318\n```\n\n\n:::\n:::\n\n\n\n\n\n\n-   Interpretation:\n    -   Identifies which groups differ.\n    -   If $p<0.05$, the groups significantly differ.\n        -   G1-Control\n        -   G2-Control\n        -   G3-Control\n        -   G3-G1\n\n------------------------------------------------------------------------\n\n### multcomp: Family-wise Error Rate Control\n\nOther multicomparison method allow you to choose which method for adjust p-values.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(multcomp)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mvtnorm\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: survival\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: TH.data\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: MASS\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'TH.data'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:MASS':\n\n    geyser\n```\n\n\n:::\n\n```{.r .cell-code}\n# install.packages(\"multcomp\")\n### set up multiple comparisons object for all-pair comparisons\n# head(model.matrix(anova_model))\ncomprs <- rbind(\n  \"G1 - Ctrl\" = c(0, 1, 0,  0),\n  \"G2 - Ctrl\" = c(0, 0, 1,  0),\n  \"G3 - Ctrl\" = c(0, 0, 0,  1),\n  \"G2 - G1\" = c(0, -1, 1,  0),\n  \"G3 - G1\" = c(0, -1, 0,  1),\n  \"G3 - G2\" = c(0, 0, -1,  1)\n)\ncht <- glht(anova_model, linfct = comprs)\nsummary(cht, test = adjusted(\"fdr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = verbal_score ~ group, data = data)\n\nLinear Hypotheses:\n               Estimate Std. Error t value Pr(>|t|)    \nG1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** \nG2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***\nG3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***\nG2 - G1 == 0      3.104      2.327   1.334  0.18487    \nG3 - G1 == 0      7.109      2.327   3.055  0.00419 ** \nG3 - G2 == 0      4.005      2.327   1.721  0.10555    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- fdr method)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n### Side-by-Side comparison of two methods\n\n::::: columns\n::: {.column width=\"50%\"}\n#### TukeyHSD method\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"TukeyHSD method\" code-line-numbers=\"false\"}\n             diff    lwr    upr p adj\nG1-Control  7.611  1.545 13.677 0.008\nG2-Control 10.715  4.649 16.782 0.000\nG3-Control 14.720  8.654 20.786 0.000\nG2-G1       3.104 -2.962  9.170 0.543\nG3-G1       7.109  1.042 13.175 0.015\nG3-G2       4.005 -2.062 10.071 0.318\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n#### FDR method\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"multcomp package\" code-line-numbers=\"false\"}\n               Estimate Std. Error t value Pr(>|t|)    \nG1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** \nG2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***\nG3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***\nG2 - G1 == 0      3.104      2.327   1.334  0.18487    \nG3 - G1 == 0      7.109      2.327   3.055  0.00419 ** \nG3 - G2 == 0      4.005      2.327   1.721  0.10555 \n```\n:::\n\n\n\n\n\n:::\n\n-   The differences in p-value adjustment between the `TukeyHSD` method and the `multcomp` package stem from how each approach calculates and applies the multiple comparisons correction. Below is a detailed explanation of these differences\n:::::\n\n### Comparison of Differences\n\n| Feature | `TukeyHSD()` (Base R) | `multcomp::glht()` |\n|----|----|----|\n| **Distribution Used** | Studentized Range (q-distribution) | t-distribution |\n| **Error Rate Control** | Strong FWER control | Flexible error control |\n| **Simultaneous Confidence Intervals** | Yes | Typically not (depends on method used) |\n| **Adjustment Method** | Tukey-Kramer adjustment | Single-step, Westfall, Holm, Bonferroni, etc. |\n| **P-value Differences** | More conservative (larger p-values) | Slightly different due to t-distribution |\n\n## Step 6: Reporting ANOVA Results\n\n[Result]{.bigger}\n\n1.  We first examined three assumptions of ANOVA for our data as the preliminary analysis. According to the Durbin-Watson test, the Shapiro-Wilk normality test, and the Bartletts' test, the sample data meets all assumptions of the one-way ANOVA modeling.\n\n2.  A one-way ANOVA was then conducted to examine the effect of three intensive intervention methods (Control, G1, G2, G3) on verbal acquisition scores. There was a statistically significant difference between groups, $F(3,116)=14.33$, $p<.001$.\n\n3.  To further examine which intervention method is most effective, we performed Tukey's post-hoc comparisons. The results revealed that all three intervention methods have significantly higher scores than the control group (G1-Ctrl: p = .003; G2-Ctrl: p \\< .001; G3-Ctrl: p \\< .001). Among three intervention methods, G3 seems to be the most effective. Specifically, G3 showed significantly higher scores than G1 (p = .004). However, no significant difference was found between G2 and G3 (p = .105).\n\n[Discussion]{.bigger}\n\nThese findings suggest that higher intervention intensity improves verbal acquisition performance, which is consistent with prior literatures \\[xxxx/references\\]\n\n# Aftre-Class Exercise: Effect of Sleep Duration on Cognitive Performance\n\n## Background\n\n-   Research Question:\n\n    -   Does the amount of sleep affect cognitive performance on a standardized test?\n\n-   Study Design\n\n    -   Independent variable: Sleep duration (3 groups: Short (≤5 hrs), Moderate (6-7 hrs), Long (≥8 hrs)).\n    -   Dependent variable: Cognitive performance scores (measured as test scores out of 100).\n\n## Data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate synthetic data for sleep study\nsleep_data <- tibble(\n  sleep_group = rep(c(\"Short\", \"Moderate\", \"Long\"), each = 30),\n  cognitive_score = c(\n    rnorm(30, mean = 65, sd = 10),  # Short sleep group (≤5 hrs)\n    rnorm(30, mean = 75, sd = 12),  # Moderate sleep group (6-7 hrs)\n    rnorm(30, mean = 80, sd = 8)    # Long sleep group (≥8 hrs)\n  )\n)\n\n# View first few rows\nhead(sleep_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  sleep_group cognitive_score\n  <chr>                 <dbl>\n1 Short                  78.7\n2 Short                  59.4\n3 Short                  68.6\n4 Short                  71.3\n5 Short                  69.0\n6 Short                  63.9\n```\n\n\n:::\n:::\n\n\n\n\n\n\nGo through all six steps.\n\n## Answer:\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 5: Perform One-Way ANOVA\nanova_sleep <- aov(cognitive_score ~ sleep_group, data = sleep_data)\nsummary(anova_sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nsleep_group  2   3764  1881.9   15.88 1.32e-06 ***\nResiduals   87  10311   118.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n::: heimu\n-   A one-way ANOVA was conducted to examine the effect of sleep duration on cognitive performance.\n\n-   There was a statistically significant difference in cognitive test scores across sleep groups, $F(2,87)=15.88$,$p<.001$.\n\n-   Tukey's post-hoc test revealed that participants in the Long sleep group (M=81.52,SD=6.27) performed significantly better than those in the Short sleep group (M=65.68,SD=12.55), p\\<.01.\n\n-   These results suggest that inadequate sleep is associated with lower cognitive performance.\n:::\n",
    "supporting": [
      "ESRM64103_Lecture04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}