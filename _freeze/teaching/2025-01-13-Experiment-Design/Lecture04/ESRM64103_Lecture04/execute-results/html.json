{
  "hash": "bd73c911f295bf935f1a185f5df1f3a0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04: ANOVA Assumptions Checking\"\nsubtitle: \"Experimental Design in Education\"\ndate: \"2025-02-10\"\nexecute: \n  eval: true\n  echo: true\nformat: \n  html:\n    code-tools: true\n    code-line-numbers: false\n    code-fold: false\n    number-offset: 1\n    fig.width: 10\n    fig-align: center\n    message: false\n    grid:\n      sidebar-width: 350px\n  uark-revealjs:\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: true\n    number-depth: 1\n    footer: \"ESRM 64503\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    scrollable: true\n    output-file: slides-index.html\n    mermaid:\n      theme: forest\nengine: knitr      \nfilters:\n  - webr      \n---\n\n## Class Outline\n\n-   Go through three assumptions of ANOVA and their checking statistics\n-   Post-hoc tests for more group comparisons\n-   Example: Intervention and Verbal Acquisition\n-   After-class Exercise: Effect of Sleep Duration on Cognitive Performance\n\n## Review ANOVA Procedure\n\n1.  **Set hypotheses**:\n    -   **Null hypothesis** ($H_0$): All group means are equal.\n    -   **Alternative hypothesis** ($H_A$): At least one group mean differs.\n2.  **Determine statistical parameters**:\n    -   Significance level $\\alpha$\n    -   Degrees of freedom for between-group ($df_b$) and within-group ($df_w$)\n    -   Find the critical F-value.\n3.  **Compute test statistic**:\n    -   Calculate **F-ratio** based on between-group and within-group variance.\n4.  **Compare results**:\n    -   Either compare $F_{\\text{obs}}$ with $F_{\\text{crit}}$ or $p$-value with $\\alpha$.\n    -   If $p < \\alpha$, reject $H_0$.\n\n## ANOVA Type I Error\n\n1.  The way to compare the means of multiple groups (i.e., more than two groups)\n\n-   Compared to multiple t-tests, the ANOVA does not inflate Type I error rates.\n    -   For F-test in ANOVA, Type I error rate is not inflated\n    -   Type I error rate is inflated when we do multiple comparison\n-   **Family-wise error rate (FWER)**: probability of at least one Type I error occur = $1 −  (1 − \\alpha)^c$ where c =number of tests. Frequently used FWER methods include Bonferroni or FDR.\n\n------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\nalpha <-  .05\nc <- seq(10)\nprob_TypeOneError <- 1 - (1 - alpha)^c\n\nplot(x = c, y = prob_TypeOneError, type = \"b\", \n     xlab = \"Number of comparisons\", ylab = \"Pr(#error > 1)\",\n     main = \"Type I error inflation with number of tests increases\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n::: callout-note\n## Family-Wise Error Rate\n\n1.  The **Family-Wise Error Rate** (FWER) is the probability of making at least one **Type I error** (false positive) across a set of multiple comparisons.\n2.  In **Analysis of Variance (ANOVA)**, multiple comparisons are often necessary when analyzing the differences between group means. FWER control is crucial to ensure the validity of statistical conclusions.\n:::\n\n# ANOVA Assumptions\n\n## Overview\n\n-   Like all statistical tests, ANOVA requires certain assumptions to be met for valid conclusions:\n    -   **Independence**: Observations are independent of each other.\n    -   **Normality**: The residuals (errors) follow a normal distribution.\n    -   **Homogeneity of variance (HOV)**: The variance within each group is approximately equal.\n\n## Importance of Assumptions\n\n-   **If assumptions are violated**, the results of ANOVA may not be reliable.\n-   **We call models robust** when ANOVAs are not easily influenced by the violation of assumptions.\n-   Typically:\n    -   ANOVA is **robust** to minor violations of normality, especially for large sample sizes (Central Limit Theorem).\n    -   **Not robust** to violations of independence—if independence is violated, ANOVA is inappropriate.\n    -   **Moderately robust** to HOV violations if sample sizes are equal.\n\n## Assumption 1: Independence\n\n-   **Definition**: Each observation should be independent of others.\n-   **Violations**:\n    -   Clustering of data (e.g., repeated measures).\n    -   Participants influencing each other (e.g., classroom discussions).\n-   **Check (Optional)**: Use the [Durbin-Watson test]{.redcolor}.\n-   **Solution**: Repeated Measures ANOVA, Mixed ANOVA, Multilevel Model\n-   **Consequences**: If independence is violated, [ANOVA results are not valid]{.red}.\n\n----------------------------------\n\n**Example: Demonstrating Independence Violation**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate data with dependency (autocorrelation)\nset.seed(1234)\nn_per_group <- 20\ngroups <- 3\n\n# Create dependent data using autoregressive structure\ngenerate_dependent_data <- function(n, mean_val, sd_val, rho = 0.7) {\n  y <- numeric(n)\n  y[1] <- rnorm(1, mean_val, sd_val)\n  for(i in 2:n) {\n    y[i] <- rho * y[i-1] + rnorm(1, mean_val * (1-rho), sd_val * sqrt(1-rho^2))\n  }\n  return(y)\n}\n\n# Generate data for 3 groups with different means but with dependency\ndependent_data <- data.frame(\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group)),\n  score = c(\n    generate_dependent_data(n_per_group, 50, 10),  # Group A\n    generate_dependent_data(n_per_group, 55, 10),  # Group B\n    generate_dependent_data(n_per_group, 60, 10)   # Group C\n  )\n)\n\n# Compare with independent data\nindependent_data <- data.frame(\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group)),\n  score = c(\n    rnorm(n_per_group, 50, 10),  # Group A\n    rnorm(n_per_group, 55, 10),  # Group B\n    rnorm(n_per_group, 60, 10)   # Group C\n  )\n)\n\n# Run ANOVA on both datasets\nanova_dependent <- aov(score ~ group, data = dependent_data)\nanova_independent <- aov(score ~ group, data = independent_data)\n\n# Compare results\ncat(\"ANOVA with DEPENDENT data:\\n\")\nprint(summary(anova_dependent))\ncat(\"\\nANOVA with INDEPENDENT data:\\n\")\nprint(summary(anova_independent))\n\n# Check Durbin-Watson test\ncat(\"\\nDurbin-Watson test for dependent data:\")\nprint(lmtest::dwtest(anova_dependent))\ncat(\"\\nDurbin-Watson test for independent data:\")\nprint(lmtest::dwtest(anova_independent))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nANOVA with DEPENDENT data:\n            Df Sum Sq Mean Sq F value Pr(>F)  \ngroup        2    695   347.3    3.72 0.0303 *\nResiduals   57   5321    93.4                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nANOVA with INDEPENDENT data:\n            Df Sum Sq Mean Sq F value Pr(>F)\ngroup        2    377  188.55   2.389  0.101\nResiduals   57   4500   78.94               \n\nDurbin-Watson test for dependent data:\n\tDurbin-Watson test\n\ndata:  anova_dependent\nDW = 0.74297, p-value = 3.078e-09\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nDurbin-Watson test for independent data:\n\tDurbin-Watson test\n\ndata:  anova_independent\nDW = 1.8171, p-value = 0.1641\nalternative hypothesis: true autocorrelation is greater than 0\n```\n\n\n:::\n:::\n\n\n**Key observations:**\n\n- Dependent data often shows artificially **smaller p-values** (inflated significance)\n- Standard errors are **underestimated** when independence is violated\n- Durbin-Watson test helps detect this violation (DW ≠ 2, p < 0.05)\n\n------------------------------------------------------------------------\n\n### Durbin-Watson (DW) Test\n\n::: callout-note\n-   The Durbin-Watson test is primarily used for detecting **autocorrelation** in time-series data.\n\n-   In the context of ANOVA with independent groups, residuals are generally assumed to be independent.\n\n    -   It's good practice to check this assumption, especially if there's a reason to suspect potential autocorrelation.\n:::\n\n::: callout-important\n-   Properties of DW Statistic:\n    -   $H_0$: [Independence of residual is satisfied.]{.redcolor}\n    -   Ranges from 0 to 4.\n        -   A value around 2 suggests no autocorrelation.\n        -   Values approaching 0 indicate positive autocorrelation.\n        -   Values toward 4 suggest negative autocorrelation.\n    -   P-value\n        -   P-Value: A small p-value (typically \\< 0.05) indicates violation of independence\n:::\n\n---------------------------------------------\n\n**Real-world examples of independence violations:**\n\n- **Dependent data scenarios**:\n  - **Repeated measures**: Testing the same students multiple times (pre-test, mid-test, post-test)\n  - **Classroom clusters**: Students in the same classroom influence each other's performance\n  - **Time series**: Daily test scores where yesterday's performance affects today's\n  - **Spatial correlation**: Schools in the same district sharing similar resources and outcomes\n\n- **Independent data scenarios**:\n  - **Random sampling**: Selecting students from different schools across different regions\n  - **Single measurement**: Each participant tested only once with no interaction\n  - **Controlled assignment**: Participants randomly assigned to groups with no cross-contamination\n\n**Why this matters**: When observations are dependent, they don't provide as much unique information as independent observations. This leads to overconfident results (smaller standard errors and p-values).\n\n---------------------------------------------\n\n### Visualization of indepdence test\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Use the same data from previous example\nn_per_group <- 20\n\n# Generate dependent and independent data\ndependent_data <- data.frame(\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group)),\n  score = c(\n    generate_dependent_data(n_per_group, 50, 10),\n    generate_dependent_data(n_per_group, 55, 10),\n    generate_dependent_data(n_per_group, 60, 10)\n  ),\n  observation = 1:(3*n_per_group),\n  type = \"Dependent (AR)\"\n)\n\nindependent_data <- data.frame(\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group)),\n  score = c(\n    rnorm(n_per_group, 50, 10),\n    rnorm(n_per_group, 55, 10),\n    rnorm(n_per_group, 60, 10)\n  ),\n  observation = 1:(3*n_per_group),\n  type = \"Independent\"\n)\n\n# Combine data\ncombined_data <- rbind(dependent_data, independent_data)\n\n# Plot 1: Time series plot showing autocorrelation\np1 <- ggplot(combined_data, aes(x = observation, y = score, color = group)) +\n  geom_line(size = 0.8) + geom_point(size = 1.5) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(title = \"Time Series: Independence vs Dependence\",\n       x = \"Observation Order\", y = \"Score\") +\n  theme_minimal() + theme(legend.position = \"bottom\")\n\n# Plot 2: Lag plot to show autocorrelation\ncreate_lag_data <- function(data) {\n  data$score_lag1 <- c(NA, data$score[-length(data$score)])\n  return(data[!is.na(data$score_lag1), ])\n}\n\nlag_dependent <- create_lag_data(dependent_data)\nlag_independent <- create_lag_data(independent_data)\nlag_combined <- rbind(lag_dependent, lag_independent)\n\np2 <- ggplot(lag_combined, aes(x = score_lag1, y = score)) +\n  geom_point(aes(color = group), alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\", size = 0.5) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(title = \"Lag Plot: Current vs Previous Observation\",\n       x = \"Score (t-1)\", y = \"Score (t)\") +\n  theme_minimal() + theme(legend.position = \"bottom\")\n\n# Display plots\ngrid.arrange(p1, p2, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-html/unnamed-chunk-3-1.png){width=1152}\n:::\n:::\n\n\n**Key Visual Differences:**\n- **Dependent data**: Shows smooth, connected patterns with visible trends\n- **Independent data**: Shows random scatter with no systematic patterns\n- **Lag plot**: Dependent data shows positive correlation between consecutive observations\n\n------------------------------------------------------------------------\n\n[`lmtest::dwtest()`]{.bluecolor .bigger}\n\n-   Performs the Durbin-Watson test for autocorrelation of disturbances.\n\n-   The Durbin-Watson test has the null hypothesis that the autocorrelation of the disturbances is 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"lmtest\")\nlibrary(lmtest)\nerr1 <- rnorm(100)\n\n## generate regressor and dependent variable\nx <- rep(c(-1,1), 50)\ny <- 1 + x + err1 \ndat <- data.frame(x = x, y = y)\n## perform Durbin-Watson test\ndwtest(y ~ x, data = dat)\n```\n:::\n\n\nThese results suggest there is no autocorrelation at the alpha level of .05.\n\n--------------------------------\n\n::: panel-tabset\n### In-class quiz\n\n-   Below is the R code that has loaded two datasets for you. `dat1` and `dat2`, try to examine which dataset violates independence.\n\n```{webr-r}\n#| context: setup\n\nset.seed(1999)\n\n# Create dependent data using autoregressive structure\ngenerate_dependent_data <- function(n, mean_val, sd_val, rho = 0.7) {\n  y <- numeric(n)\n  y[1] <- rnorm(1, mean_val, sd_val)\n  for(i in 2:n) {\n    y[i] <- rho * y[i-1] + rnorm(1, mean_val * (1-rho), sd_val * sqrt(1-rho^2))\n  }\n  return(y)\n}\n\n# Use the same data from previous example\nn_per_group <- 30\n\n# Generate dependent and independent data\ndat1 <- data.frame(\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group)),\n  score = c(\n    generate_dependent_data(n_per_group, 50, 10),\n    generate_dependent_data(n_per_group, 55, 10),\n    generate_dependent_data(n_per_group, 60, 10)\n  )\n)\n\ndat2 <- data.frame(\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group)),\n  score = c(\n    rnorm(n_per_group, 50, 10),\n    rnorm(n_per_group, 55, 10),\n    rnorm(n_per_group, 60, 10)\n  )\n)\n```\n\n```{webr-r}\n#| editor-code-line-numbers: 4-7\nprint(dat1)\nprint(dat2)\n\n## Test which data violate independence assumption\nlibrary(________)\nprint(dwtest(________))\nprint(dwtest(________))\n\n```\n\n### Answer\n```{webr-r}\n# DW = 0.8248, p-value = 6.68e-11, dat1 violates independence\n# DW = 2.2514, p-value = 0.8386, dat2 does not violates independence\nlibrary(lmtest)\nprint(dwtest(score ~ group, data = dat1))\nprint(dwtest(score ~ group, data = dat2))\n```\n\n:::\n\n## Assumption 2: Normality\n\n-   The **dependent variable (DV)** should be normally distributed within each group.\n-   **Assessments**:\n    -   Graphical methods: Histograms, [Q-Q plots]{.redcolor}.\n    -   Statistical tests:\n        -   [Shapiro-Wilk test (common)]{.redcolor}\n        -   Kolmogorov-Smirnov (KS) test (for large samples)\n        -   Anderson-Darling test (detects kurtosis issues).\n-   **Robustness**:\n    -   ANOVA is robust to normality violations for large samples.\n    -   If normality is violated, consider transformations or non-parametric tests.\n\n------------------------------------------------------------------------\n\n### Normality check for the whole samples\n\n-   Assume that the DV (Y) is distributed normally within each group for ANOVA\n\n-   ANOVA is **robust** to minor violations of normality\n\n    -   So generally start with homogeneity, then assess normality\n\n![](images/clipboard-3483482689.png)\n\n------------------------------------------------------------------------\n\n### R function `shapiro.test()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # For reproducibility\nsample_normal_data <- rnorm(200, mean = 50, sd = 10)  # Generate normal data\nsample_nonnormal_data <- sample(1:10, 200, replace = TRUE)  # Generate non-normal data\n\ncat(\"Normality assumption holds:\")\nshapiro.test(sample_normal_data)\ncat(\"Normality assumption does not hold:\")\nshapiro.test(sample_nonnormal_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNormality assumption holds:\n\tShapiro-Wilk normality test\n\ndata:  sample_normal_data\nW = 0.99076, p-value = 0.2298\n\nNormality assumption does not hold:\n\tShapiro-Wilk normality test\n\ndata:  sample_nonnormal_data\nW = 0.93581, p-value = 1.005e-07\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### R function `ks.test()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform Kolmogorov-Smirnov test against a normal distribution\ncat(\"Normality assumption holds:\")\nks.test(scale(sample_normal_data), \"pnorm\")\ncat(\"Normality assumption does not hold:\")\nks.test(scale(sample_nonnormal_data), \"pnorm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in ks.test.default(scale(sample_nonnormal_data), \"pnorm\"): ties should\nnot be present for the one-sample Kolmogorov-Smirnov test\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNormality assumption holds:\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  scale(sample_normal_data)\nD = 0.054249, p-value = 0.5983\nalternative hypothesis: two-sided\n\nNormality assumption does not hold:\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  scale(sample_nonnormal_data)\nD = 0.12037, p-value = 0.006084\nalternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\n\n::: panel-tabset\n### In-class quiz II\n\nPerform Shapiro-Wilk and Kolmogorov-Smirnov for both datasets. Tell me which test works or does not work and why.\n\n```{webr-r}\n#| editor-code-line-numbers: 6-11\nset.seed(2345)\nnormal_data <- rnorm(50, mean = 50, sd = 10)  # normal data\nnonnormal_data <- sample(1:10, 50, replace = TRUE)  # non-normal\n\n### Perform Shapiro-Wilk and Kolmogorov-Smirnov for both data\n### Shapiro-Wilk\n_____(normal_data)\n_____(nonnormal_data)\n### Kolmogorov-Smirnov\n_____(____(normal_data, _____))\n_____(____(nonnormal_data, _____))\n```\n\n### Answer\n\n```{webr-r}\nshapiro.test(normal_data)\nshapiro.test(nonnormal_data)\nks.test(scale(normal_data), \"pnorm\")\nks.test(scale(nonnormal_data), \"pnorm\")\n```\n:::\n\n-----------------------\n\n### Normaliy check for each group\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Generate three-group data with normal distributions\nlibrary(dplyr) # we need this package to group the data\nset.seed(123)\ngroup_data <- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  score = c(rnorm(30, mean = 50, sd = 10),  # Group A\n            rnorm(30, mean = 55, sd = 12),  # Group B\n            rnorm(30, mean = 48, sd = 9))   # Group C\n)\n\n# Run Shapiro-Wilk test for each group\ngroup_data |>\n  group_by(group) |>\n  summarize(\n    shapiro_p_value = shapiro.test(score)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  group shapiro_p_value\n  <chr>           <dbl>\n1 A               0.797\n2 B               0.961\n3 C               0.848\n```\n\n\n:::\n:::\n\n\n**Interpretation of Normality Checking Results:**\n\n- If all p-values > 0.05: All groups satisfy normality assumption - proceed with ANOVA\n- If one group has p-value < 0.05 but others are normal:\n  - ANOVA is generally robust to minor normality violations, especially with equal sample sizes\n  - Consider the sample size: larger samples (n > 30) make ANOVA more robust\n  - Options: (1) Continue with ANOVA if violation is mild, (2) Apply data transformation (log, square root), or (3) Use non-parametric alternative (Kruskal-Wallis test)\n- If multiple groups violate normality: Strong evidence against using ANOVA - consider transformations or non-parametric tests\n\n--------\n\n::: panel-tabset\n### In-class quiz III\n\nPerform Shapiro-Wilk for each group datasets. Tell me which group is not normal.\n\n```{webr-r}\n#| context: setup\nset.seed(2345)\ngroup_data2 <- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  score = c(round(rnorm(30, mean = 50, sd = 10), 2),  # Group A\n            sample(seq(50, 100, .01), 30, replace = TRUE),  # Group B\n            round(rnorm(30, mean = 48, sd = 9), 2))   # Group C\n)\n\n```\n\n```{webr-r}\n#| editor-code-line-numbers: 5-9\nlibrary(dplyr) # we need this package to group the data\nhead(group_data2) # check first 6 rows\n\n### Perform Shapiro-Wilk for each group\ngroup_data2 |> \n  ______(______) |> \n  ______(\n    shapiro_p_value = __________(________)$__________\n  )\n```\n\n### Answer\n\n```{webr-r}\ngroup_data2 |>\n  group_by(group) |>\n  summarize(\n    shapiro_p_value = shapiro.test(score)$p.value\n  )\n```\n\n\n:::\n\n--------------------------------------------------------------------\n\n## Assumption 3: Homogeneity of Variance (HOV)\n\n-   Variance across groups should be equal.\n-   **Assessments**:\n    -   **Levene’s test**: Tests equality of variances.\n    -   **Brown-Forsythe test**: More robust to non-normality.\n    -   **Bartlett's test**: For data with normality.\n    -   **Boxplots**: Visual inspection.\n-   **What if violated?**\n    -   **Welch’s ANOVA** (adjusted for variance differences).\n    -   **Transforming** the dependent variable.\n    -   **Using non-parametric tests** (e.g., Kruskal-Wallis).\n\n------------------------------------------------------------------------\n\n[Practical Considerations]{.bluecolor .bigger}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Computes Levene's test for homogeneity of variance across groups.\ncar::leveneTest(outcome ~ group, data = data)\n\n## Boxplots to visualize the variance by groups\nboxplot(outcome ~ group, data = data)\n\n## Brown-Forsythe test \nonewaytests::bf.test(outcome ~ group, data = data)\n\n## Bartlett's Test\nbartlett.test(outcome ~ group, data = data)\n```\n:::\n\n\n-   **Levene's test** and **Brown-Forsythe test** are often preferred when data does not meet the assumption of normality, especially for small sample sizes.\n-   **Bartlett’s test** is most powerful when the data is normally distributed and when sample sizes are equal across groups. However, it can be less reliable if these assumptions are violated.\n\n------------------------------------------------------------------------\n\n[Decision Tree]{.redcolor .bigger}\n\n```{dot}\n//| echo: false\ndigraph flowchart {\n    node [shape=box, style=filled, fillcolor=lightgray];\n    \n    A [label=\"Is the data normal by group?\"];\n    B [label=\"Use Bartlett's Test\"];\n    C [label=\"Are there outliers?\"];\n    D [label=\"Use Brown-Forsythe Test\"];\n    E [label=\"Use Levene's Test\"];\n    \n    A -> B [label=\"Yes\"];\n    A -> C [label=\"No\"];\n    C -> D [label=\"Yes\"];\n    C -> E [label=\"No\"];\n}\n\n```\n\n\n------------------------------------------------------------------------\n\n## ANOVA Robustness\n\n-   **Robust to**:\n\n    -   Minor normality violations (for large samples).\n    -   Small HOV violations if group sizes are **equal**.\n\n-   **Not robust to**:\n\n    -   **Independence violations**—ANOVA is invalid if data points are dependent.\n    -   **Severe HOV violations**—Type I error rates become unreliable.\n\n-   The robustness of assumptions is something you should be careful about before/when you perform data collection. They are not something you can address after data collection has been finished.\n\n------------------------------------------------------------------------\n\n[Robustness to Violations of Normality Assumption]{.redcolor}\n\n-   ANOVA assumes that the residuals (errors) are normally distributed within each group.\n\n-   However, ANOVA is generally robust to violations of normality, particularly when **the sample size is large**.\n\n-   **Theoretical Justification**: This robustness is primarily due to the Central Limit Theorem (CLT), which states that, for sufficiently large sample sizes (typically $n≥30$ per group), the sampling distribution of the mean approaches normality, even if the underlying population distribution is non-normal.\n\n-   This means that unless the data are heavily skewed or have extreme outliers, ANOVA results remain valid and Type I error rates are not severely inflated.\n\n------------------------------------------------------------------------\n\n### Robustness to Violations of Homogeneity of Variance\n\n-   The homogeneity of variance (homoscedasticity) assumption states that all groups should have equal variances. ANOVA can tolerate moderate violations of this assumption, particularly when:\n\n    -   **Sample sizes are equal (or nearly equal) across groups** – When groups have equal sample sizes, the F-test remains robust to variance heterogeneity because the pooled variance estimate remains balanced.\n\n    -   **The degree of variance heterogeneity is not extreme** – If the largest group variance is no more than about four times the smallest variance, ANOVA results tend to remain accurate.\n\n------------------------------------------------------------------------\n\n### ANOVA: Lack of Robustness to Violations of Independence of Errors\n\n-   The assumption of independence of errors means that observations within and between groups must be uncorrelated. Violations of this assumption severely compromise ANOVA’s validity because:\n\n    -   **Inflated Type I error rates** – If errors are correlated (e.g., due to clustering or repeated measures), standard errors are underestimated, leading to an increased likelihood of falsely rejecting the null hypothesis.\n    -   **Biased parameter estimates** – When observations are not independent, the variance estimates do not accurately reflect the true variability in the data, distorting F-statistics and p-values.\n    -   **Common sources of dependency** – Examples include nested data (e.g., students within schools), repeated measurements on the same subjects, or time-series data. In such cases, alternatives like mixed-effects models or generalized estimating equations (GEE) should be considered.\n\n# Omnibus ANOVA Test\n\n## Overview\n\n-   **What does it test?**\n    -   Whether there is **at least one** significant difference among means.\n-   **Limitation**:\n    -   Does **not** tell **which** groups are different.\n-   **Solution**:\n    -   Conduct **post-hoc tests**.\n\n## Individual Comparisons of Means\n\n-   If ANOVA is **significant**, follow-up tests identify **where** differences occur.\n-   **Types**:\n    -   **Planned comparisons**: Defined **before** data collection.\n    -   **Unplanned (post-hoc) comparisons**: Conducted **after** ANOVA.\n\n## Planned vs. Unplanned Comparisons\n\n-   **Planned**:\n    -   Based on **theory**.\n    -   Can be done **even if ANOVA is not significant**.\n-   **Unplanned (post-hoc)**:\n    -   **Data-driven**.\n    -   Only performed **if ANOVA is significant**.\n\n## Types of Unplanned Comparisons\n\n-   **Common post-hoc tests**:\n    1.  **Fisher’s LSD**\n    2.  **Bonferroni correction** or Adjusted p-values\n    3.  **Sidák correction**\n    4.  **Tukey’s HSD**\n\n------------------------------------------------------------------------\n\n### Post-hoc test: \n\n[Tukey’s HSD]{.bluecolor}\n\n-   Controls for **Type I error** across multiple comparisons.\n-   Uses a **q-statistic** from a Tukey table.\n-   Preferred when **all pairs** need comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov_result <- aov(score ~ group, group_data) # run the ANOVA model first\nTukeyHSD(aov_result) # call TukeyHSD followed by the model to perform Tukey multiple comparisons\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ group, data = group_data)\n\n$group\n         diff        lwr       upr     p adj\nB-A  7.611098   1.902128 13.320067 0.0057521\nC-A -1.309179  -7.018149  4.399791 0.8483773\nC-B -8.920277 -14.629246 -3.211307 0.0009985\n```\n\n\n:::\n:::\n\n\n\n[Family-wise Error Rate (adjusted p-values)]{.bluecolor}\n\n-   Adjusts p-values to reduce Type I error\n-   Report adjusted p-values (typically larger that original p-values)\n\n## ANOVA Example: Intervention and Verbal Acquisition\n\n### Background\n\n-   Research Question: Does an intensive intervention improve students’ verbal acquisition scores?\n-   Study Design:\n    -   4 groups: Control, G1, G2, G3 (treatment levels).\n    -   Outcome variable: Verbal acquisition score (average of three assessments).\n-   Hypotheses:\n    -   $H_0$: No difference in verbal acquisition scores across groups.\n    -   $H_A$: At least one group has a significantly different mean.\n\n### Step 1: Generate Simulated Data in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic data for 4 groups\ndata <- tibble(\n  group = rep(c(\"Control\", \"G1\", \"G2\", \"G3\"), each = 30),\n  verbal_score = c(\n    rnorm(30, mean = 70, sd = 10),  # Control group\n    rnorm(30, mean = 75, sd = 12),  # G1\n    rnorm(30, mean = 80, sd = 10),  # G2\n    rnorm(30, mean = 85, sd = 8)    # G3\n  )\n)\n\n# View first few rows\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  group   verbal_score\n  <chr>          <dbl>\n1 Control         64.4\n2 Control         67.7\n3 Control         85.6\n4 Control         70.7\n5 Control         71.3\n6 Control         87.2\n```\n\n\n:::\n:::\n\n\n**Function explanations:**\n\n- `library(tidyverse)`: Loads the tidyverse package collection, including dplyr, ggplot2, and tibble\n- `set.seed(123)`: Sets the random number generator seed to ensure reproducible results across runs\n- `tibble()`: Creates a modern data frame (tibble) with improved printing and subsetting behavior\n- `rep(c(\"Control\", \"G1\", \"G2\", \"G3\"), each = 30)`: Repeats each group label 30 times, creating 120 total observations (30 per group)\n- `rnorm(30, mean = X, sd = Y)`: Generates 30 random numbers from a normal distribution with specified mean and standard deviation\n- `c()`: Combines the four vectors of random numbers into a single vector of 120 values\n- `head(data)`: Displays the first 6 rows of the dataset for preview\n\n### Step 2: Summary Statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics by group\ndata %>%\n  group_by(group) %>%\n  summarise(\n    mean_score = mean(verbal_score),\n    sd_score = sd(verbal_score),\n    n = n()\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  group   mean_score sd_score     n\n  <chr>        <dbl>    <dbl> <int>\n1 Control       69.5     9.81    30\n2 G1            77.1    10.0     30\n3 G2            80.2     8.70    30\n4 G3            84.2     7.25    30\n```\n\n\n:::\n:::\n\n\n**Function explanations:**\n\n- `group_by(group)`: Groups the data by the `group` variable, allowing subsequent operations to be performed separately for each group\n- `summarise()`: Creates summary statistics for each group, collapsing the grouped data into summary values\n- `mean(verbal_score)`: Calculates the arithmetic mean of verbal scores within each group\n- `sd(verbal_score)`: Computes the standard deviation of verbal scores within each group\n- `n()`: Counts the number of observations in each group\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplot visualization\nlibrary(ggplot2)  # Load ggplot2 package for creating visualizations\nggplot(data, aes(x = group, y = verbal_score, fill = group)) +  # Create base plot with group on x-axis, scores on y-axis, fill by group\n  geom_boxplot() +  # Add boxplot layer to show distribution of scores within each group\n  theme_minimal() +  # Apply minimal theme for clean appearance\n  labs(title = \"Verbal Acquisition Scores Across Groups\", y = \"Score\", x = \"Group\")  # Add descriptive labels\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Step 3: Check ANOVA Assumptions\n\n### Assumption Check 1: Independence of residuals Check\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the ANOVA model\nanova_model <- lm(verbal_score ~ group, data = data)\n\n# Load the lmtest package\nlibrary(lmtest)\n\n# Perform the Durbin-Watson test\ndw_test_result <- dwtest(anova_model)\n\n# View the test results\nprint(dw_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tDurbin-Watson test\n\ndata:  anova_model\nDW = 2.0519, p-value = 0.5042\nalternative hypothesis: true autocorrelation is greater than 0\n```\n\n\n:::\n:::\n\n\n-   Interpretation:\n    -   In this example, the `DW` value is close to 2, and the p-value is greater than 0.05, indicating no significant autocorrelation in the residuals.\n\n------------------------------------------------------------------------\n\n### Assumption Check 2: Normality Check\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk normality test for each group\ndata |> \n  group_by(group) |> \n  summarise(\n    shapiro_p = shapiro.test(verbal_score)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  group   shapiro_p\n  <chr>       <dbl>\n1 Control     0.797\n2 G1          0.961\n3 G2          0.848\n4 G3          0.568\n```\n\n\n:::\n:::\n\n\n-   Interpretation:\n    -   If $p>0.05$, normality assumption is not violated.\n    -   If $p<0.05$, data deviates from normal distribution.\n-   Since the data meets the normality requirement and no outliers, we can use [Bartlett's or Levene's tests]{.redcolor} for HOV checking.\n\n------------------------------------------------------------------------\n\n-   Alternative Check: Q-Q Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(sample = verbal_score)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~group) +\n  theme_minimal() +\n  labs(title = \"Q-Q Plot for Normality Check\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture04_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Assumption Check 3: Homogeneity of Variance (HOV) Check\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::leveneTest(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n```\n\n\n:::\n\n```{.r .cell-code}\nstats::bartlett.test(verbal_score ~ group, data = data)\nonewaytests::bf.test(verbal_score ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   3  1.1718 0.3236\n      116               \n\n\tBartlett test of homogeneity of variances\n\ndata:  verbal_score by group\nBartlett's K-squared = 3.5385, df = 3, p-value = 0.3158\n\n\n\tBrown-Forsythe Test\n\ndata:  verbal_score and group\nF = 14.329, num df = 3.00, denom df = 109.99, p-value = 6.03e-08\n```\n\n\n:::\n:::\n\n\n-   Interpretation:\n    -   If $p>0.05$, variance is homogeneous (ANOVA assumption met).\n    -   If $p<0.05$, variance differs across groups (consider Welch’s ANOVA).\n-   It turns out our data does not violate the homogeneity of variance assumption.\n\n### Step 4: Perform One-Way ANOVA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_model <- aov(verbal_score ~ group, data = data)\nsummary(anova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ngroup         3   3492  1164.1   14.33 5.28e-08 ***\nResiduals   116   9424    81.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n-   Interpretation:\n    -   If $p<0.05$, at least one group mean is significantly different.\n    -   If $p>0.05$, fail to reject $H0$ (no significant differences).\n\n### Step 5: Post-Hoc Tests (Tukey's HSD)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tukey HSD post-hoc test\ntukey_results <- TukeyHSD(anova_model)\nround(tukey_results$group, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             diff    lwr    upr p adj\nG1-Control  7.611  1.545 13.677 0.008\nG2-Control 10.715  4.649 16.782 0.000\nG3-Control 14.720  8.654 20.786 0.000\nG2-G1       3.104 -2.962  9.170 0.543\nG3-G1       7.109  1.042 13.175 0.015\nG3-G2       4.005 -2.062 10.071 0.318\n```\n\n\n:::\n:::\n\n\n-   Interpretation:\n    -   Identifies which groups differ.\n    -   If $p<0.05$, the groups significantly differ.\n        -   G1-Control\n        -   G2-Control\n        -   G3-Control\n        -   G3-G1\n\n------------------------------------------------------------------------\n\n### Step 6: Family-wise Error Rate Control\n\n`glht` function in multcomp package allow us to perform family-wise error rate control (using `adjusted p-values`).\nAlternatively, we can use `TukeyHSD` to adjust p values to control inflated Type I errpr.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(multcomp)\n# install.packages(\"multcomp\")\n### set up multiple comparisons object for all-pair comparisons\n# head(model.matrix(anova_model))\ncomprs <- rbind(\n  \"G1 - Ctrl\" = c(0, 1, 0,  0),\n  \"G2 - Ctrl\" = c(0, 0, 1,  0),\n  \"G3 - Ctrl\" = c(0, 0, 0,  1),\n  \"G2 - G1\" = c(0, -1, 1,  0),\n  \"G3 - G1\" = c(0, -1, 0,  1),\n  \"G3 - G2\" = c(0, 0, -1,  1)\n)\ncht <- glht(anova_model, linfct = comprs)\nsummary(cht, test = adjusted(\"fdr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = verbal_score ~ group, data = data)\n\nLinear Hypotheses:\n               Estimate Std. Error t value Pr(>|t|)    \nG1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** \nG2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***\nG3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***\nG2 - G1 == 0      3.104      2.327   1.334  0.18487    \nG3 - G1 == 0      7.109      2.327   3.055  0.00419 ** \nG3 - G2 == 0      4.005      2.327   1.721  0.10555    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- fdr method)\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Side-by-Side comparison of two methods\n\n::::: columns\n::: {.column width=\"50%\"}\n#### TukeyHSD method\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"TukeyHSD method\" code-line-numbers=\"false\"}\n             diff    lwr    upr p adj\nG1-Control  7.611  1.545 13.677 0.008\nG2-Control 10.715  4.649 16.782 0.000\nG3-Control 14.720  8.654 20.786 0.000\nG2-G1       3.104 -2.962  9.170 0.543\nG3-G1       7.109  1.042 13.175 0.015\nG3-G2       4.005 -2.062 10.071 0.318\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n#### FDR method\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"multcomp package\" code-line-numbers=\"false\"}\n               Estimate Std. Error t value Pr(>|t|)    \nG1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** \nG2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***\nG3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***\nG2 - G1 == 0      3.104      2.327   1.334  0.18487    \nG3 - G1 == 0      7.109      2.327   3.055  0.00419 ** \nG3 - G2 == 0      4.005      2.327   1.721  0.10555 \n```\n:::\n\n:::\n\n-   The differences in p-value adjustment between the `TukeyHSD` method and the `multcomp` package stem from how each approach calculates and applies the multiple comparisons correction. Below is a detailed explanation of these differences.\n:::::\n\n### Comparison of Differences\n\n| Feature | `TukeyHSD()` (Base R) | `multcomp::glht()` |\n|----|----|----|\n| **Distribution Used** | Studentized Range (q-distribution) | t-distribution |\n| **Error Rate Control** | Strong FWER control | Flexible error control |\n| **Simultaneous Confidence Intervals** | Yes | Typically not (depends on method used) |\n| **Adjustment Method** | Tukey-Kramer adjustment | Single-step, Westfall, Holm, Bonferroni, etc. |\n| **P-value Differences** | More conservative (larger p-values) | Slightly different due to t-distribution |\n\n### Step 6: Reporting ANOVA Results\n\n[Result]{.bigger}\n\n1.  We first examined three assumptions of ANOVA for our data as the preliminary analysis. According to the Durbin-Watson test, the Shapiro-Wilk normality test, and the Bartletts' test, the sample data meets all assumptions of the one-way ANOVA modeling.\n\n2.  A one-way ANOVA was then conducted to examine the effect of three intensive intervention methods (Control, G1, G2, G3) on verbal acquisition scores. There was a statistically significant difference between groups, $F(3,116)=14.33$, $p<.001$.\n\n3.  To further examine which intervention method is most effective, we performed Tukey's post-hoc comparisons. The results revealed that all three intervention methods have significantly higher scores than the control group (G1-Ctrl: p = .003; G2-Ctrl: p \\< .001; G3-Ctrl: p \\< .001). Among the three intervention methods, G3 seems to be the most effective. Specifically, G3 showed significantly higher scores than G1 (p = .004). However, no significant difference was found between G2 and G3 (p = .105).\n\n[Discussion]{.bigger}\n\nThese findings suggest that higher intervention intensity improves verbal acquisition performance, which is consistent with prior literature \\[xxxx/references\\].\n\n# After-Class Exercise: Effect of Sleep Duration on Cognitive Performance\n\n## Background\n\n-   Research Question:\n\n    -   Does the amount of sleep affect cognitive performance on a standardized test?\n\n-   Study Design\n\n    -   Independent variable: Sleep duration (3 groups: Short (≤5 hrs), Moderate (6-7 hrs), Long (≥8 hrs)).\n    -   Dependent variable: Cognitive performance scores (measured as test scores out of 100).\n\n---------------------------------------------\n\n::: panel-tabset\n### Exercise\n\n```{webr-r}\n#| context: setup\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate synthetic data for sleep study\nsleep_data <- data.frame(\n  sleep_group = rep(c(\"Short\", \"Moderate\", \"Long\"), each = 30),\n  cognitive_score = c(\n    rnorm(30, mean = 65, sd = 10),  # Short sleep group (≤5 hrs)\n    rnorm(30, mean = 75, sd = 12),  # Moderate sleep group (6-7 hrs)\n    rnorm(30, mean = 80, sd = 8)    # Long sleep group (≥8 hrs)\n  )\n)\n```\n\n```{webr-r}\n\n# load packages you need\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lmtest)\nlibrary(car)\n\n# View first few rows\nhead(sleep_data)\n\n# Summary statistics by sleep group\nsleep_data  |> \n  group_by(sleep_group) |> \n  summarise(\n    _______\n  )\n\n# Step 3: Check ANOVA Assumptions\n# Indepdendence check: Perform the Durbin-Watson test\ndw_test_result <- ____(____(_________, data = sleep_data))\n\n# View the test results\nprint(dw_test_result)\n\n# Normality check: Shapiro-Wilk normality test for each group\nsleep_data |> \n  group_by(sleep_group) |> \n  summarise(\n    shapiro_p = ______(_______)$p.value\n  )\n\n# HOV check: Levene's Test for homogeneity of variance\n______(_______, data = sleep_data)\n\n# Step 5: Perform One-Way ANOVA\nanova_sleep <- ________(__________, data = sleep_data)\nsummary(anova_sleep)\n\n# Step 6: Type I error control -- Tukey HSD post-hoc test\ntukey_sleep <- TukeyHSD(_______)\ntukey_sleep\n```\n\n### Answer\n\n```{webr-r}\n#| eval: false\n#| echo: false\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lmtest)\nlibrary(car)\n\n# View first few rows\nhead(sleep_data)\n\n# Summary statistics by sleep group\nsleep_data  |> \n  group_by(sleep_group) |> \n  summarise(\n    mean_score = mean(cognitive_score),\n    sd_score = sd(cognitive_score),\n    n = n()\n  )\n\n# Boxplot visualization\nggplot(sleep_data, aes(x = sleep_group, y = cognitive_score, fill = sleep_group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Cognitive Performance Across Sleep Groups\", y = \"Test Score\", x = \"Sleep Duration\")\n\n# Step 3: Check ANOVA Assumptions\n# Indepdendence check: Perform the Durbin-Watson test\ndw_test_result <- dwtest(aov(cognitive_score ~ sleep_group, data = sleep_data))\n\n# View the test results\nprint(dw_test_result)\n\n# Normality check: Shapiro-Wilk normality test for each group\nsleep_data |> \n  group_by(sleep_group) |> \n  summarise(\n    shapiro_p = shapiro.test(cognitive_score)$p.value\n  )\nggplot(sleep_data, aes(sample = cognitive_score)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~sleep_group) +\n  theme_minimal() +\n  labs(title = \"Q-Q Plot for Normality Check\")\n\n# HOV check: Levene's Test for homogeneity of variance\nleveneTest(cognitive_score ~ sleep_group, data = sleep_data)\n\n# Step 5: Perform One-Way ANOVA\nanova_sleep <- aov(cognitive_score ~ sleep_group, data = sleep_data)\nsummary(anova_sleep)\n\n# Step 6: Type I error control -- Tukey HSD post-hoc test\ntukey_sleep <- TukeyHSD(anova_sleep)\ntukey_sleep\n```\n\n\n::: heimu\n-   A one-way ANOVA was conducted to examine the effect of sleep duration on cognitive performance.\n\n-   There was a statistically significant difference in cognitive test scores across sleep groups, $F(2,87)=15.88$,$p<.001$.\n\n-   Tukey's post-hoc test revealed that participants in the Long sleep group (M=81.52, SD=6.27) performed significantly better than those in the Short sleep group (M=65.68, SD=12.55), p\\<.01.\n\n-   These results suggest that inadequate sleep is associated with lower cognitive performance.\n:::\n",
    "supporting": [
      "ESRM64103_Lecture04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}