{
  "hash": "23986031d4c14b2ed6999eb38f6bc074",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 08: Power Analysis\"\nsubtitle: \"Experimental Design in Education\"\ndate: \"2025-03-07\"\ndate-modified: \"2026-02-02\"\nexecute:\n  eval: true\n  echo: true\n  warning: false\n  message: false\nformat:\n  html:\n    code-tools: true\n    code-line-numbers: false\n    code-fold: true\n    code-summary: \"Click to see R code\"\n    number-offset: 1\n    fig.width: 10\n    fig-align: center\n    message: false\n    grid:\n      sidebar-width: 350px\n  uark-revealjs:\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: true\n    number-depth: 1\n    footer: \"ESRM 64503\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    scrollable: true\n    output-file: slides-index.html\n    mermaid:\n      theme: forest\nfilters:\n  - webr\n---\n\n\n\n# Introduction {background-color=\"#0056A7\"}\n\n## Learning Objectives\n\nBy the end of this lecture, you will be able to:\n\n1. Define statistical power and explain its importance in experimental design\n2. Understand the relationship between Type I and Type II errors\n3. Identify the key components of power analysis\n4. Calculate required sample sizes for different experimental designs\n5. Apply power analysis to common research scenarios\n\n## Why This Matters\n\n::: {.callout-note}\n### Real-World Impact in Education Research\n- **Underpowered studies** waste limited education funding and may miss important interventions\n- **Overpowered studies** consume resources that could benefit more students\n- Power analysis is **required** for grant proposals (IES, NSF, NIH)\n- Helps determine if research questions are feasible with available schools/classrooms\n- **Ethical obligation**: Don't ask teachers/students to participate in futile studies\n:::\n\n# What is Power Analysis? {background-color=\"#0056A7\"}\n\n## Definition: Statistical Power\n\n::: {.callout-tip}\n### In Plain English\nImagine you're testing whether a new teaching method works better than the old one. **Power** is the chance that your study will be able to detect the improvement, *if the improvement is really there*.\n\nThink of it like a metal detector:\n\n- A **high-power** detector can find small coins buried deep in the sand\n- A **low-power** detector might miss those same coins, even though they're there\n:::\n\n**Formal Definition:** Statistical power is the probability that a study will correctly reject a false null hypothesis.\n\n$$\\text{Power} = 1 - \\beta$$\n\nwhere $\\beta$ is the probability of a Type II error (false negative)\n\n::: {.callout-tip}  \n### In Plain English\n**In other words:** Power tells us how good our study is at discovering real effects. Higher power means we're less likely to miss something important.\n:::\n\n## The Four Possible Outcomes\n\n```{mermaid}\n%%| fig-width: 8\n%%| fig-height: 4\nflowchart TD\n    A[Reality: Null Hypothesis is TRUE] --> B{Decision}\n    B -->|Reject H0| C[Type I Error<br/>False Positive<br/>α]\n    B -->|Fail to Reject H0| D[Correct Decision<br/>1 - α]\n\n    E[Reality: Alternative Hypothesis is TRUE] --> F{Decision}\n    F -->|Reject H0| G[Correct Decision<br/>Power = 1 - β]\n    F -->|Fail to Reject H0| H[Type II Error<br/>False Negative<br/>β]\n\n    style C fill:#ff6b6b\n    style H fill:#ff6b6b\n    style D fill:#51cf66\n    style G fill:#51cf66\n```\n\n## Understanding Errors\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Type I Error (α)\n- **False Positive**\n- Rejecting true null hypothesis\n- Usually set at α = 0.05\n- \"Finding an effect that isn't there\"\n:::\n\n::: {.column width=\"50%\"}\n### Type II Error (β)\n- **False Negative**\n- Failing to reject false null hypothesis\n- Usually set at β = 0.20\n- \"Missing an effect that is there\"\n:::\n:::\n\n## Power in Context\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visualization of statistical power showing the relationship between null and alternative distributions](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n# Why Do Power Analysis? {background-color=\"#0056A7\"}\n\n## Critical Reasons\n\n1. **Resource Efficiency**\n   - Avoid wasting time, money, and effort on underpowered studies\n   - Don't collect more data than necessary\n\n2. **Ethical Considerations**\n   - Minimize participant burden and exposure to risks\n   - Ensure research has realistic chance of success\n\n3. **Scientific Integrity**\n   - Detect meaningful effects when they exist\n   - Avoid publishing false negatives\n\n4. **Funding Requirements**\n   - Grant agencies require power analyses\n   - Demonstrates research feasibility\n\n## Consequences of Low Power\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Too Small Sample\n- Miss effective interventions (Type II error)\n- Waste teacher and student time\n- Publish false negatives (\"it doesn't work\")\n- Discourage future research on promising approaches\n- Contribute to replication crisis\n:::\n\n::: {.column width=\"50%\"}\n### Too Large Sample\n- Unnecessary recruitment burden on schools\n- Waste limited education funding\n- Find statistically significant but educationally trivial results\n- Fewer resources for other important studies\n:::\n:::\n\n## Power Analysis is NOT About Gaming the System\n\n::: {.callout-warning}\n### Common Misconceptions\n\nPower analysis should **help determine if a question can be reasonably answered**, not:\n\n- Justify a predetermined sample size\n- Defend what you want to study anyway\n- Manipulate effect sizes to get funding\n- Written defensively after the study is planned\n:::\n\n# The Logic of Power Analysis {background-color=\"#0056A7\"}\n\n## Five Key Components\n\nEvery power analysis involves specifying values for:\n\n1. **Significance Level (α)**: Usually 0.05\n2. **Power (1 - β)**: Usually 0.80 (minimum 80%)\n3. **Effect Size (δ)**: Meaningful difference to detect\n4. **Variability (σ)**: Standard deviation of measurements\n5. **Sample Size (n)**: What we usually solve for\n\n::: {.callout-note}\nIf you know any four, you can calculate the fifth!\n:::\n\n## Significance Level (α)\n\n- **Type I error rate** - probability of false positive\n- Conventionally set at **α = 0.05** (5%)\n- Controls how \"strict\" we are about declaring effects significant\n- Relates to **critical value** for hypothesis testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Critical value for one-tailed test, alpha = 0.05\nqnorm(0.95)  # z = 1.645\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.644854\n```\n\n\n:::\n\n```{.r .cell-code}\n# Critical value for two-tailed test, alpha = 0.05\nqnorm(0.975)  # z = 1.96\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n:::\n\n\n## Power (1 - β)\n\n- **Probability of detecting a true effect**\n- Conventionally set at **0.80** (80%) or higher\n- Higher power = more certainty in detecting effects\n- Common choices: 70%, 80%, 90%\n\n::: {.callout-important}\n### Minimum Standards\n\n- **80% power** is considered minimum acceptable\n- **90% power** for clinical trials or high-stakes research\n- Choice depends on how certain you want to be\n:::\n\n## Effect Size (δ)\n\nThe **meaningful difference** you want to detect\n\nMust be specified based on:\n\n- Prior research literature\n- **Educational/practical significance** (not just statistical)\n- Subject matter expertise\n- Pilot data\n\n::: {.callout-tip}\n### Cohen's d (Standardized Effect Size)\nOften expressed as: $d = \\frac{|\\delta|}{\\sigma}$ (effect size relative to SD)\n\n- **Small effect:** d ≈ 0.2 (subtle but meaningful in education)\n- **Medium effect:** d ≈ 0.5 (typical for many interventions)\n- **Large effect:** d ≈ 0.8 (rare in education research)\n\n**Reality check:** Most education interventions have d = 0.2-0.4\n:::\n\n## Variability (σ)\n\n**Standard deviation** of measurements\n\nEstimate from:\n\n- Prior published research\n- Pilot studies\n- Similar studies in literature\n- Expert knowledge\n\n::: {.callout-warning}\n### Impact on Sample Size\nSample size increases proportionally to variance:\n$$n \\propto \\sigma^2$$\n**Doubling the SD quadruples the required sample size!**\n:::\n\n## Relationships Among Components\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Understanding (σ/δ)²: The Signal-to-Noise Ratio\n\n::: {.callout-note icon=false}\n### Why Does Sample Size Depend on (σ/δ)²?\n\nThe ratio **σ/δ** represents **noise-to-signal**:\n\n- **δ** = effect size (the \"signal\" you want to detect)\n- **σ** = variability (the \"noise\" that obscures the signal)\n\n**Intuitive examples:**\n\n| Scenario | δ | σ | σ/δ | Interpretation | n needed |\n|----------|---|---|-----|----------------|----------|\n| Strong signal, low noise | 10 | 5 | 0.5 | Easy to detect | Small |\n| Weak signal, low noise | 5 | 5 | 1.0 | Moderate | Medium |\n| Weak signal, high noise | 5 | 20 | 4.0 | Hard to detect | Large |\n\n**Why squared?**\n\n- Doubling noise (σ) means 4× more participants needed\n- Halving effect size (δ) means 4× more participants needed\n- This quadratic relationship comes from variance, not an arbitrary choice!\n:::\n\n## Key Facts to Remember\n\n::: {.incremental}\n1. **Sample size increases with power**: More power → larger sample needed\n2. **Sample size increases with smaller detectable differences**: Smaller effect → larger sample (quadratically!)\n3. **Sample size increases with variance**: More variability → larger sample (quadratically!)\n4. **One-sided tests require smaller samples** than two-sided tests\n5. **The (z_α + z_β)² term** represents the required distance between distributions\n6. **The (σ/δ)² term** represents the signal-to-noise ratio\n:::\n\n# Power Analysis: One-Sample Case {background-color=\"#0056A7\"}\n\n## One-Sample Situation\n\nTesting if a mean differs from a known value:\n\n- $H_0: \\mu = \\mu_0$ (null hypothesis)\n- $H_1: \\mu \\neq \\mu_0$ or $\\mu < \\mu_0$ or $\\mu > \\mu_0$ (alternative)\n\n**Required sample size formula:**\n\n$$n = (z_\\alpha + z_\\beta)^2 \\left(\\frac{\\sigma}{\\delta}\\right)^2$$\n\nwhere:\n\n- $z_\\alpha$ = critical value for significance level\n- $z_\\beta$ = critical value for power\n- $\\delta = \\mu_1 - \\mu_0$ = effect size\n\n## Where Does This Formula Come From?\n\n::: {.callout-note icon=false}\n### The Logic Behind the Formula\n\n**Key insight:** The distributions under H₀ and H₁ must be separated enough that:\n\n1. The critical value from H₀ cuts off α (Type I error)\n2. The critical value from H₁ cuts off β (Type II error)\n\n**The distance between distributions** (in standard error units) must span both critical values:\n\n$$\\frac{\\delta}{SE} = z_\\alpha + z_\\beta$$\n\nSince $SE = \\frac{\\sigma}{\\sqrt{n}}$, we have:\n\n$$\\frac{\\delta}{\\sigma/\\sqrt{n}} = z_\\alpha + z_\\beta$$\n\nSolving for n:\n$$\\sqrt{n} = \\frac{(z_\\alpha + z_\\beta) \\cdot \\sigma}{\\delta}$$\n\nSquare both sides:\n$$n = (z_\\alpha + z_\\beta)^2 \\left(\\frac{\\sigma}{\\delta}\\right)^2$$\n:::\n\n\n## Visual Intuition\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n::: {.callout-tip}\n### The Key Insight\n\nThe formula squared because:\n\n1. We need: $\\delta = (z_\\alpha + z_\\beta) \\times SE$\n2. Since $SE = \\sigma/\\sqrt{n}$, we have: $\\delta = (z_\\alpha + z_\\beta) \\times \\frac{\\sigma}{\\sqrt{n}}$\n3. Rearranging for $\\sqrt{n}$: $\\sqrt{n} = \\frac{(z_\\alpha + z_\\beta) \\times \\sigma}{\\delta}$\n4. **Squaring both sides** to isolate n: $n = \\frac{(z_\\alpha + z_\\beta)^2 \\times \\sigma^2}{\\delta^2}$\n\nThe squaring comes from solving for n, not from squaring the sum first!\n:::\n\n## Example: Reading Achievement Study\n\n::: {.callout-note icon=false}\n### Research Question\nDo students in a new literacy program have higher reading scores compared to the national average?\n\n**Known Information:**\n\n- National average reading score: μ₀ = 100\n- Standard deviation: σ = 20\n- Meaningful difference: δ = 5 points (improvement)\n- Desired power: 80%\n- Significance level: α = 0.05 (one-tailed)\n:::\n\n## Calculating Sample Size: Step by Step\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Given values\nmu0 <- 100      # national average score\nsigma <- 20     # standard deviation\ndelta <- 5      # meaningful improvement\nalpha <- 0.05   # significance level\npower <- 0.80   # desired power\n\n# Step 1: Find critical values\nz_alpha <- qnorm(1 - alpha)  # = 1.645 for one-tailed\nz_beta <- qnorm(power)       # = 0.842 for 80% power\n\ncat(\"Step 1: Critical values\\n\")\ncat(\"  z_alpha =\", round(z_alpha, 3), \"\\n\")\ncat(\"  z_beta  =\", round(z_beta, 3), \"\\n\\n\")\n\n# Step 2: Calculate required distance (in SE units)\nrequired_distance <- z_alpha + z_beta\ncat(\"Step 2: Required distance between distributions\\n\")\ncat(\"  z_alpha + z_beta =\", round(required_distance, 3), \"standard errors\\n\\n\")\n\n# Step 3: Set up the equation\n# We need: delta = required_distance × SE\n# We need: delta = required_distance × (sigma / sqrt(n))\n# Solve for sqrt(n)\nsqrt_n <- required_distance * sigma / delta\ncat(\"Step 3: Solve for sqrt(n)\\n\")\ncat(\"  sqrt(n) = (\", round(required_distance, 3), \" × \", sigma, \") / \", delta, \"\\n\")\ncat(\"  sqrt(n) =\", round(sqrt_n, 3), \"\\n\\n\")\n\n# Step 4: Square to get n\nn <- sqrt_n^2\ncat(\"Step 4: Square to get n\\n\")\ncat(\"  n = (\", round(sqrt_n, 3), \")²\\n\")\ncat(\"  n =\", round(n, 2), \"\\n\\n\")\n\ncat(\"Required sample size:\", ceiling(n), \"students\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1: Critical values\n  z_alpha = 1.645 \n  z_beta  = 0.842 \n\nStep 2: Required distance between distributions\n  z_alpha + z_beta = 2.486 standard errors\n\nStep 3: Solve for sqrt(n)\n  sqrt(n) = ( 2.486  ×  20 ) /  5 \n  sqrt(n) = 9.946 \n\nStep 4: Square to get n\n  n = ( 9.946 )²\n  n = 98.92 \n\nRequired sample size: 99 students\n```\n\n\n:::\n:::\n\n\n## Formula Summary: What Each Part Means\n\n$$n = \\underbrace{(z_\\alpha + z_\\beta)^2}_{\\text{Separation needed}} \\times \\underbrace{\\left(\\frac{\\sigma}{\\delta}\\right)^2}_{\\text{Signal-to-noise}}$$\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### $(z_\\alpha + z_\\beta)^2$\n\n**What it controls:**\n\n- Type I error (α)\n- Type II error (β)\n- Power (1 - β)\n\n**Typical values:**\n\n- α = 0.05, Power = 80%\n- z_α = 1.645, z_β = 0.842\n- Sum = 2.487\n- $(z_\\alpha + z_\\beta)^2 \\approx 6.2$\n:::\n\n::: {.column width=\"50%\"}\n### $(\\sigma/\\delta)^2$\n\n**What it controls:**\n\n- Effect size you want to detect (δ)\n- Population variability (σ)\n\n**Typical values:**\n\n- Small effect: σ/δ = 5 → 25\n- Medium effect: σ/δ = 2 → 4\n- Large effect: σ/δ = 1.25 → 1.6\n\n**Total:** n = 6.2 × (σ/δ)²\n:::\n:::\n\n## Using the `pwr` Package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effect size (standardized)\neffect_size <- abs(delta) / sigma\n\n# Power analysis using pwr package\nresult <- pwr.t.test(\n  d = effect_size,\n  sig.level = alpha,\n  power = power,\n  type = \"one.sample\",\n  alternative = \"greater\"  # or \"two.sided\", \"less\", \"greater\"\n)\n\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 100.2877\n              d = 0.25\n      sig.level = 0.05\n          power = 0.8\n    alternative = greater\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify our manual calculation matches\ncat(\"\\nManual calculation:\",\n    ceiling(n),\n    \"\\npwr package:\",\n    abs(ceiling(result$n)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nManual calculation: 99 \npwr package: 101\n```\n\n\n:::\n:::\n\n\n## Paired Comparisons\n\nThe one-sample formula applies to **paired (blocked) designs**:\n\nResponse: $D = Y_2 - Y_1$ (difference between paired measurements)\n\nExamples:\n\n- Pre-test vs. Post-test\n- Treatment vs. Control on same subject\n- Left eye vs. Right eye\n\n**Key:** Need SD of the **differences**, not individual measurements\n\n## Example: Pre-Post Test Anxiety Study\n\n::: {.callout-note icon=false}\n### Research Question\nDoes a mindfulness intervention reduce student test anxiety?\n\n**Study Design:**\n\n- Paired comparison: post-intervention vs. baseline on same students\n- SD of differences: σ = 5 points\n- Meaningful reduction: δ = -2.5 points\n- Power: 80%, α = 0.05\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate sample size for paired comparison\nsigma_diff <- 5.0\ndelta_anxiety <- -2.5\neffect <- abs(delta_anxiety) / sigma_diff\n\npwr.t.test(d = effect, sig.level = 0.05, power = 0.80,\n           type = \"paired\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\n## Exploring Power Curves\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create power curve for different sample sizes\nsample_sizes <- seq(5, 50, by = 1)\neffect_size <- 0.5\n\npower_values <- sapply(sample_sizes, function(n) {\n  pwr.t.test(n = n, d = effect_size, sig.level = 0.05,\n             type = \"one.sample\", alternative = \"two.sided\")$power\n})\n\nggplot(data.frame(n = sample_sizes, power = power_values),\n       aes(x = n, y = power)) +\n  geom_line(linewidth = 1.2, color = \"#1971c2\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = 40, y = 0.83, label = \"80% Power\", color = \"red\") +\n  labs(x = \"Sample Size\", y = \"Statistical Power\",\n       title = \"Power Curve for One-Sample t-test\",\n       subtitle = \"Effect size = 0.5, α = 0.05\") +\n  theme_minimal(base_size = 14) +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n# Power Analysis: Two-Sample Case {background-color=\"#0056A7\"}\n\n## Two-Sample Comparison\n\nComparing means of two independent groups:\n\n- $H_0: \\mu_2 - \\mu_1 = 0$\n- $H_1: \\mu_2 - \\mu_1 \\neq 0$ (or $<$ or $>$)\n\n**Both means are unknown** - different from one-sample case!\n\n## Two-Sample Formulas\n\n### Case 1: Unequal Variances\n**Total sample size:**\n$$N = (z_\\alpha + z_\\beta)^2 \\left[\\frac{\\sigma_1 + \\sigma_2}{\\delta}\\right]^2$$\n\nAllocate samples proportional to SDs:\n$$n_1 = N \\cdot \\frac{\\sigma_1}{\\sigma_1 + \\sigma_2}, \\quad n_2 = N \\cdot \\frac{\\sigma_2}{\\sigma_1 + \\sigma_2}$$\n\n### Case 2: Equal Variances\n**Sample size per group:**\n$$n = 2(z_\\alpha + z_\\beta)^2 \\left(\\frac{\\sigma}{\\delta}\\right)^2$$\n\n## Example: Math Achievement by SES\n\n::: {.callout-note icon=false}\n### Research Question\nCompare math achievement scores between students from high and low socioeconomic status (SES) backgrounds\n\n**Known Information:**\n- High SES group: σ₁ = 12 points\n- Low SES group: σ₂ = 15 points (larger variability)\n- Meaningful difference: δ = 10 points\n- Power: 80%, α = 0.05\n:::\n\n## Calculation: Unequal Variances\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Given values\nsigma1 <- 12    # High SES SD\nsigma2 <- 15    # Low SES SD\ndelta <- 10     # Effect size\nalpha <- 0.05\npower <- 0.80\n\n# Critical values\nz_alpha <- qnorm(1 - alpha/2, lower.tail = TRUE)  # two-tailed\nz_beta <- qnorm(power, lower.tail = TRUE)\n\n# Total sample size\nN <- (z_alpha + z_beta)^2 * ((sigma1 + sigma2) / delta)^2\n\n# Allocate proportional to SDs\nn1 <- ceiling(N * sigma1 / (sigma1 + sigma2))\nn2 <- ceiling(N * sigma2 / (sigma1 + sigma2))\n\ncat(\"High SES students:\", n1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHigh SES students: 26 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Low SES students:\", n2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLow SES students: 32 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total sample size:\", n1 + n2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal sample size: 58\n```\n\n\n:::\n:::\n\n\n## Using `pwr` for Two-Sample Tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For equal variances (using larger SD as planning value)\nsigma <- 15\neffect_size <- abs(delta) / sigma\n\nresult <- pwr.t.test(\n  d = effect_size,\n  sig.level = 0.05,\n  power = 0.80,\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 36.30569\n              d = 0.6666667\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nTotal sample size:\", ceiling(result$n * 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTotal sample size: 73\n```\n\n\n:::\n:::\n\n\n## Example: Study Skills Workshop\n\n::: {.callout-note icon=false}\n### Research Question\nDoes a study skills workshop improve GPA compared to no intervention?\n\n**Study Design:**\n\n- Two independent groups (workshop vs. control)\n- Equal SDs: σ = 0.5 GPA points\n- Meaningful change: δ = 0.3 GPA points\n- Power: 80%, α = 0.05\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma <- 0.5\ndelta <- 0.3\neffect_size <- delta / sigma\n\npwr.t.test(d = effect_size, sig.level = 0.05, power = 0.80,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 44.58577\n              d = 0.6\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n## Visualizing Two-Sample Power\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n# Power Analysis: Other Designs {background-color=\"#0056A7\"}\n\n## Log-Normal Distributions\n\nWhen data are **log-normally distributed**:\n\n- Easier to specify effects as **percentage changes**\n- Variability expressed as **coefficient of variation**\n\n$$c = \\frac{\\sqrt{\\text{Var}(Y)}}{E(Y)}$$\n\n**Sample size per group:**\n$$n = \\frac{2(z_\\alpha + z_\\beta)^2 c^2}{[\\log(1+f)]^2}$$\n\nwhere $f$ is the proportionate change (e.g., 0.20 for 20% increase)\n\n## Example: Reaction Time Study\n\n::: {.callout-note icon=false}\n### Research Question\nCompare reaction times between students with ADHD receiving medication vs. placebo\n\n**Known Information:**\n\n- Coefficient of variation: c = 0.30\n- Expected difference: 20% faster reaction time with medication (f = -0.20)\n- Power: 80%, α = 0.05\n\n*Note: Reaction times typically follow log-normal distributions*\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Given values\nc <- 0.30       # Coefficient of variation\nf <- 0.20       # 20% proportionate change\nalpha <- 0.05\npower <- 0.80\n\n# Critical values\nz_alpha <- qnorm(alpha, lower.tail = TRUE)\nz_beta <- qnorm(1 - power, lower.tail = TRUE)\n\n# Sample size calculation\nn <- 2 * (z_alpha + z_beta)^2 * c^2 / (log(1 + f))^2\ncat(\"Sample size per group:\", ceiling(n), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size per group: 34 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total sample size:\", ceiling(n) * 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal sample size: 68\n```\n\n\n:::\n:::\n\n\n## Cluster Randomized Designs\n\n**Clusters** = groups of experimental units\n\nCommon examples:\n\n- In education: Students within classrooms, classrooms within schools\n- Also: Patients within clinics, siblings within families\n\n**Intracluster Correlation (ρ):**\n\n- Measures similarity within clusters\n- Range: 0 to 1\n- ρ = 0: units independent (no clustering effect)\n- ρ = 1: units identical within cluster\n- **Typical values in education:** 0.10-0.25 for students in classrooms\n\n**Sample size adjustment:**\n$$n = km = 2(z_\\alpha + z_\\beta)^2 \\left(\\frac{\\sigma}{\\delta}\\right)^2 [1 + (m-1)\\rho]$$\n\nwhere $k$ = number of clusters, $m$ = units per cluster\n\n## Impact of Intracluster Correlation\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## Cluster Design: Key Insights\n\n::: {.incremental}\n1. **High ICC → Need more clusters, not more units per cluster**\n   - With ρ = 0.20, adding more students per classroom doesn't help proportionally\n   - Better to recruit more classrooms with fewer students each\n   - In education research, typical ICC for classrooms: 0.10-0.25\n\n2. **Ignoring ICC leads to underpowered studies**\n   - Don't calculate as if students are independent\n   - Must account for clustering in design\n   - Failure to account for ICC is a common error in educational research\n\n3. **Design Efficiency**\n   - Maximize number of clusters (k = classrooms, schools)\n   - Consider cluster size (m) and practical constraints\n   - Balance statistical needs with recruitment feasibility\n:::\n\n## Example: Students in Classrooms\n\n::: {.callout-note icon=false}\n### Scenario\n- Need 100 students total with ρ = 0.20\n- Cluster size m = 20 (students per classroom)\n\n**Incorrect approach (ignoring ICC):**\n$$n = 100 \\text{ students} \\rightarrow 5 \\text{ classrooms}$$\n\n**Correct approach (accounting for ICC):**\n$$n = 100 \\times [1 + (20-1)(0.20)] = 100 \\times 4.8 = 480 \\text{ students}$$\n$$\\rightarrow 24 \\text{ classrooms needed (12 per condition)}$$\n:::\n\n::: {.callout-warning}\nAlways account for clustering structure in your design!\n:::\n\n# Power Analysis: ANOVA Designs {background-color=\"#0056A7\"}\n\n## One-Way ANOVA\n\nComparing means across **multiple groups** (k ≥ 3)\n\n**Effect size (f):**\n$$f = \\frac{\\sigma_{\\text{between}}}{\\sigma_{\\text{within}}}$$\n\nConventional values:\n\n- Small: f = 0.10\n- Medium: f = 0.25\n- Large: f = 0.40\n\n## ANOVA Power Analysis in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Comparing 4 groups\nk <- 4              # number of groups\neffect_size <- 0.25 # medium effect\nalpha <- 0.05\npower <- 0.80\n\n# Power analysis\npwr.anova.test(\n  k = k,\n  f = effect_size,\n  sig.level = alpha,\n  power = power\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n```\n\n\n:::\n:::\n\n\n## Factorial ANOVA\n\nFor **2×2 factorial design**:\n\n- Two factors, each with 2 levels\n- Tests main effects and interaction\n\n::: {.callout-note}\n**Note:** `pwr.f2.test()` returns `v` (denominator degrees of freedom). To get total sample size: $n = v + u + 1$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effect size for factorial design\neffect_size <- 0.25\nalpha <- 0.05\npower <- 0.80\n\n# Numerator df = (levels - 1)\n# For 2x2: df = 1 for each main effect and interaction\n\nresult <- pwr.f2.test(\n  u = 1,              # numerator df for one effect\n  f2 = effect_size^2, # f-squared effect size\n  sig.level = alpha,\n  power = power\n)\n\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 125.5312\n             f2 = 0.0625\n      sig.level = 0.05\n          power = 0.8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate total sample size from v (denominator df)\n# Formula: n = v + u + 1\nn_total <- ceiling(result$v + result$u + 1)\ncat(\"\\nTotal sample size needed:\", n_total, \"participants\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTotal sample size needed: 128 participants\n```\n\n\n:::\n:::\n\n\n## Understanding pwr.f2.test Output\n\n::: {.callout-tip}\n### Interpreting the Results\n\nWhen using `pwr.f2.test()`, the output shows:\n\n- **u** = numerator degrees of freedom (number of predictors/effects tested)\n- **v** = denominator degrees of freedom (error df)\n- **f2** = Cohen's f² effect size\n- **sig.level** = α level\n- **power** = statistical power\n\n**Important:** To get the total sample size needed:\n\n$$n = v + u + 1$$\n\n**Example:** If u = 1 and v = 125.53, you need n = 128 participants total.\n:::\n\n## Visualizing ANOVA Power\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n# Power Analysis: Proportions and Correlations {background-color=\"#0056A7\"}\n\n## Comparing Two Proportions\n\nTesting difference between two proportions (e.g., success rates)\n\n**Effect size (h):**\n$$h = 2[\\arcsin(\\sqrt{p_1}) - \\arcsin(\\sqrt{p_2})]$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Compare graduation rates\np1 <- 0.70  # Control group\np2 <- 0.80  # Treatment group\n\n# Calculate effect size\nES.h(p1, p2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.2319843\n```\n\n\n:::\n\n```{.r .cell-code}\n# Power analysis\npwr.2p.test(\n  h = ES.h(p1, p2),\n  sig.level = 0.05,\n  power = 0.80,\n  alternative = \"two.sided\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.2319843\n              n = 291.6887\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n```\n\n\n:::\n:::\n\n\n## Testing Correlations\n\nTesting if correlation differs from zero\n\n**Effect size = r** (the correlation itself)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test if correlation r = 0.30 is significant\nr <- 0.30\n\npwr.r.test(\n  r = r,\n  sig.level = 0.05,\n  power = 0.80,\n  alternative = \"two.sided\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n## Chi-Square Tests\n\nFor contingency tables:\n\n**Effect size (w):**\n\n- Small: w = 0.10\n- Medium: w = 0.30\n- Large: w = 0.50\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: 3x3 contingency table\ndf <- (3 - 1) * (3 - 1)  # degrees of freedom\n\npwr.chisq.test(\n  w = 0.30,         # medium effect\n  df = df,\n  sig.level = 0.05,\n  power = 0.80\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 132.6143\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n```\n\n\n:::\n:::\n\n\n# Software Tools for Power Analysis {background-color=\"#0056A7\"}\n\n## Available Software\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### R Packages\n- **`pwr`**: Most common for psychology/education\n- **`WebPower`**: Web-based interface\n- **`simr`**: Simulation-based for mixed models\n- **`powerAnalysis`**: Educational resource\n- **`clusterPower`**: Cluster randomized trials\n:::\n\n::: {.column width=\"50%\"}\n### Standalone Software\n- **G*Power**: Free, widely used in psychology\n- **Optimal Design**: Multilevel designs in education\n- **PowerUpR**: R package for education research\n- **PASS**: Commercial, extensive capabilities\n- **Russell Lenth's applets**: Free online tools\n:::\n:::\n\n## Using G*Power\n\nPopular free software for power analysis:\n\n1. Select test family (t-test, ANOVA, regression, etc.)\n2. Choose specific test type\n3. Specify:\n\n   - Effect size\n   - α level\n   - Power or sample size\n\n4. Get results with visualizations\n\nDownload: [https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower)\n\n# Practical Guidelines {background-color=\"#0056A7\"}\n\n## Before Running Your Study\n\n::: {.incremental}\n1. **Review literature** for:\n\n   - Expected effect sizes\n   - Variability estimates\n   - Similar study designs\n\n2. **Consider pilot studies** to:\n\n   - Estimate parameters\n   - Test procedures\n   - Refine hypotheses\n\n3. **Specify meaningful effects**:\n\n   - Clinical/practical significance\n   - Minimum detectable difference\n   - Cost-benefit considerations\n:::\n\n## Conducting Power Analysis\n\n::: {.incremental}\n1. **Be conservative**:\n\n   - Use realistic (not optimistic) effect sizes\n   - Account for attrition/dropout\n   - Consider multiple comparisons\n\n2. **Perform sensitivity analysis**:\n\n   - Vary effect sizes\n   - Check impact of assumptions\n   - Explore \"what if\" scenarios\n\n3. **Document everything**:\n\n   - Assumptions and their sources\n   - Calculations and software used\n   - Rationale for parameter choices\n:::\n\n## Common Pitfalls to Avoid\n\n::: {.callout-warning}\n### Don't:\n1. Manipulate effect sizes to justify small samples\n2. Ignore clustering or dependence\n3. Use published effect sizes uncritically (publication bias!)\n4. Forget about attrition/missing data\n5. Conduct power analysis after data collection (\"post-hoc power\")\n6. Ignore practical constraints (budget, time, recruitment)\n:::\n\n## Post-Hoc Power Analysis\n\n::: {.callout-important}\n### Why Not to Do It\n**Post-hoc power analysis** (after collecting data) is controversial:\n\n- Circular reasoning: uses observed effect to estimate power\n- Non-significant results always have low post-hoc power\n- Doesn't change interpretation of results\n- Confidence intervals more informative\n\n**Exception:** Informing future studies with pilot data\n:::\n\n# Applied Examples {background-color=\"#0056A7\"}\n\n## Example 1: Reading Intervention\n\n::: {.callout-note icon=false}\n### Research Question\nDoes a new reading intervention improve test scores compared to standard curriculum?\n\n**Information:**\n\n- Two independent groups (intervention vs. control)\n- σ = 15 points (from prior studies)\n- Meaningful improvement: 5 points\n- Power: 80%, α = 0.05 (two-tailed)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffect_size <- 5 / 15  # 0.333\npwr.t.test(d = effect_size, sig.level = 0.05, power = 0.80,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n**Need ~143 students per group, 286 total**\n\n## Example 2: Class Size Study\n\n::: {.callout-note icon=false}\n### Research Question\nDo smaller class sizes improve student achievement? Compare 3 class size conditions.\n\n**Information:**\n\n- Three groups: Small (15), Medium (25), Large (35)\n- σ = 10 points\n- Expected η² = 0.06 (medium effect)\n- Convert to f: $f = \\sqrt{\\frac{\\eta^2}{1-\\eta^2}} = 0.25$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\neta_squared <- 0.06\nf <- sqrt(eta_squared / (1 - eta_squared))\n\npwr.anova.test(k = 3, f = f, sig.level = 0.05, power = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 51.32635\n              f = 0.2526456\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n```\n\n\n:::\n:::\n\n\n**Need ~52 students per class, 156 total**\n\n## Example 3: Correlation Study\n\n::: {.callout-note icon=false}\n### Research Question\nIs there a significant correlation between homework time and GPA?\n\n**Expected correlation:** r = 0.30 (medium)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.r.test(r = 0.30, sig.level = 0.05, power = 0.80,\n           alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n**Need ~85 students**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ESRM64103_Lecture08_PowerAnalysis_files/figure-html/unnamed-chunk-25-1.png){width=768}\n:::\n:::\n\n\n## Example 4: Multilevel Design\n\n::: {.callout-note icon=false}\n### Research Question\nTesting intervention effects with students nested in classrooms\n\n**Information:**\n\n- 20 students per classroom\n- ICC = 0.15 (typical for education)\n- Effect size d = 0.40\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Base sample size (ignoring clustering)\nbase <- pwr.t.test(d = 0.40, power = 0.80, sig.level = 0.05,\n                   type = \"two.sample\")\nbase_n <- ceiling(base$n)\n\n# Adjust for clustering\nm <- 20           # cluster size\nrho <- 0.15       # ICC\ndesign_effect <- 1 + (m - 1) * rho\n\nadjusted_n <- ceiling(base_n * design_effect)\n\ncat(\"Base sample size per group:\", base_n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBase sample size per group: 100 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Design effect:\", round(design_effect, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesign effect: 3.85 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adjusted sample size per group:\", adjusted_n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted sample size per group: 385 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of classrooms needed per group:\",\n    ceiling(adjusted_n / m))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of classrooms needed per group: 20\n```\n\n\n:::\n:::\n\n\n# Summary and Recommendations {background-color=\"#0056A7\"}\n\n## Key Takeaways\n\n::: {.incremental}\n1. **Power analysis is essential** for ethical, efficient research\n2. **Minimum 80% power** to detect meaningful effects\n3. **Four key inputs**: α, power, effect size, variability\n4. **Sample size relationships**:\n\n   - Increases with desired power\n   - Increases quadratically with smaller effect sizes\n   - Increases with greater variability\n5. **Account for study design**: clustering, pairing, multiple groups\n:::\n\n## Recommendations for Practice\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Planning Stage\n\n- Conduct early in research design\n- Use realistic effect sizes\n- Consider attrition\n- Perform sensitivity analyses\n- Document all assumptions\n:::\n\n::: {.column width=\"50%\"}\n### Resources\n\n- Use established software (pwr, G*Power)\n- Consult statistician\n- Review similar studies\n- Conduct pilot studies\n- Get peer review of power analysis\n:::\n:::\n\n## Decision Framework\n\n```{mermaid}\n%%| fig-width: 10\nflowchart TD\n    A[Research Question] --> B{Feasible<br/>Sample Size?}\n    B -->|Yes| C[Conduct Study]\n    B -->|No| D{Can Increase<br/>Sample?}\n    D -->|Yes| E[Revise Budget/<br/>Timeline]\n    D -->|No| F{Can Accept<br/>Larger δ?}\n    F -->|Yes| G[Revise Research<br/>Question]\n    F -->|No| H[Abandon/<br/>Restructure]\n    E --> C\n    G --> C\n\n    style C fill:#51cf66\n    style H fill:#ff6b6b\n```\n\n## Further Reading\n\n1. **Cohen, J. (1988).** *Statistical power analysis for the behavioral sciences* (2nd ed.). Routledge.\n   - The classic reference for power analysis in psychology and education\n\n2. **Faul, F., Erdfelder, E., Lang, A. G., & Buchner, A. (2007).** G*Power 3: A flexible statistical power analysis program. *Behavior Research Methods, 39*, 175-191.\n   - Free software widely used in psychology and education\n\n3. **Lakens, D. (2022).** Sample size justification. *Collabra: Psychology, 8*(1), 33267.\n   - Modern perspective on justifying sample sizes\n\n4. **Spybrook, J., et al. (2011).** Optimal Design Plus Empirical Evidence: Documentation for the Optimal Design software.\n   - Specialized for cluster randomized trials in education\n\n5. **Ledolter, J., & Kardon, R. (2020).** Focus on data: Statistical design of experiments and sample size selection using power analysis. *Investigative Ophthalmology & Visual Science, 61*(8), 11.\n   - General principles applicable across disciplines\n\n# Questions? {background-color=\"#0056A7\"}\n\n## Resources and Practice\n\n::: {.callout-tip}\n### Practice Exercises\n\n1. Calculate sample size for your own research question\n2. Explore power curves with different parameters\n3. Compare results across different software\n4. Conduct sensitivity analysis\n:::\n\n### Contact Information\n\n- Office Hours: [Schedule]\n- Email: [Your email]\n- Course Website: [Link]\n",
    "supporting": [
      "ESRM64103_Lecture08_PowerAnalysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}