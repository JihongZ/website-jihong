{
  "hash": "90e8a83c11820a2ff315212620f43c1d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 02: Hypothesis Testing\"\nsubtitle: \"Experimental Design in Education\"\ndate: \"2025-08-18\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\nformat: \n  html:\n    code-tools: true\n    code-line-numbers: false\n    code-fold: false\n    number-offset: 1\n    fig.width: 10\n    fig-align: center\n    message: false\n    grid:\n      sidebar-width: 350px\n  uark-revealjs:\n    chalkboard: true\n    embed-resources: false\n    code-fold: false\n    number-sections: true\n    number-depth: 1\n    footer: \"ESRM 64503\"\n    slide-number: c/t\n    tbl-colwidths: auto\n    scrollable: true\n    output-file: slides-index.html\n    mermaid:\n      theme: forest  \n---\n\n## Presentation Outline\n\n-   **Types of Statistics**\n    -   Descriptive: Summarize data (central tendency, variability, shape)\n    -   Inferential: Make population inferences from samples\n    -   Predictive: Make predictions for new data\n-   **Hypothesis Testing Steps**\n    1.  State $H_0$ and $H_A$\n    2.  Set $\\alpha$ level\n    3.  Compute test statistics\n    4.  Conduct test and make decision\n-   **ANOVA Fundamentals**\n    -   One dependent variable (DV), one independent variable (IV) with multiple levels\n    -   Between-subjects and within-subjects designs\n    -   Interaction effects\n-   **Test Components**\n    -   Error types (Type I and Type II)\n    -   Variance decomposition ($SS_{Total}$, $SS_{Between}$, $SS_{Within}$)\n    -   F-statistics and critical values\n-   **Examples**\n    -   Political attitudes study\n    -   Sleep and academic performance study\n-   **Decision Making**\n    -   Compare $F_{observed}$ vs $F_{critical}$\n    -   Compare p-value vs $\\alpha$\n    -   Interpret at $(1-\\alpha)$ confidence level\n\n## Types of Statistics\n\nStatistics can be classified by purpose:\n\n1. Descriptive Statistics\n2. Inferential Statistics\n3. Predictive Statistics\n\n------------------------------------------------------------------------\n\n\n### 1. Descriptive Statistics\n\n-   **Definition**: Describes and summarizes the collected data using numbers/values\n    -   Central tendency: mean, median, mode\n    -   Variability: range, interquartile range (IQR), variance, standard deviation\n    -   Shape of distribution: skewness, kurtosis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"moments\")\nmoments::skewness(c(1:10, 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.793716\n```\n\n\n:::\n\n```{.r .cell-code}\nmoments::skewness(rnorm(100, 0, 1)) # should be close to 0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1654912\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n-   **Examples** of skewness with two graphs:\n\n![Examples of positive and negative skewness in distributions](images/clipboard-2603774303.png)\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n# Simulate a skewed normal distribution using beta distribution\nneg_skewed_data <- rbeta(10000,5,2)\nhist(neg_skewed_data, main = \"Negative Skewed Distribution\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture02_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\npos_skewed_data <- rbeta(10000,2,5)\nhist(pos_skewed_data, main = \"Positive Skewed Distribution\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture02_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyr)\ndata.frame(\n  neg = neg_skewed_data,\n  pos = pos_skewed_data\n  ) |>\n  pivot_longer(c(neg, pos), names_to = \"Type\") |> \n  ggplot(aes(y = value, fill = Type)) +\n  geom_boxplot() +\n  scale_fill_discrete(labels = c(\"Negative Skewed\", \"Positive Skewed\"), name = \"\")\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture02_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 2. Inferential Statistics\n\n-   **Definition**: Uses probability theory to infer/estimate population characteristics from a sample using hypothesis testing\n-   Visual representation shows:\n    -   Population → Sampling → Sample\n    -   Sample → Inference → Population\n        -   Sample is analyzed using descriptive statistics\n        -   Inferential statistics used to make conclusions about population\n\n![The relationship between population, sample, and inference](images/clipboard-3795101287.png)\n\n------------------------------------------------------------------------\n\n### 3. Predictive Statistics\n\n-   **Definition**: Use observed data to produce the most accurate prediction possible for new data. Here, the primary goal is that the predicted values have the highest possible fidelity to the true value of the new data.\n\n-   **Example**: A simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month.\n\n![Example of predictive statistics workflow](images/clipboard-3105485841.png)\n\n## Which type of statistics to use\n\n1.  How many houses burned in California wildfire in the first week?\n\n    -   [Descriptive]{.heimu}\n\n2.  Which factor is most important causing the fires?\n\n    -   [Inference]{.heimu}\n\n3.  How likely the California wildfire will not happen again in next 5 years?\n\n    -   [Predictive]{.heimu}\n\n4.  How likely human will live on Mars?\n\n    -   [Not statistics. Sci-Fi]{.heimu}\n\n5.  Which type of statistics is used by ChatGPT?\n\n    ![ChatGPT uses predictive statistics for text generation](images/clipboard-793465966.png){width=\"500\"}\n\n## Statistical Hypothesis Testing Steps\n\n**Steps for Inferential Statistical Testing:**\n\n1.  State null hypothesis ($H_0$) and alternative hypothesis ($H_A$)\n    -   Null hypothesis must be some statement that is **statistically testable**.\n2.  Set alpha α (type I error rate) to determine significance levels\n    -   rejection region vs. p-value\n3.  Compute test statistics (i.e., F-statistics)\n4.  Conduct hypothesis testing:\n    -   Compare test statistics: critical value vs. observed value\n    -   Compare alpha and p-value\n\n## ANOVA Introduction\n\n-   ANOVA is one of the most frequently used statistical tool for inferential statistics in experimental design.\n-   Settings for Analysis of Variance (ANOVA):\n    -   One dependent variable (DV), \"Outcome\"\n    -   One independent variable (IV) with multiple levels, \"Group\"\n-   **Example question**: \"Are there mean differences in SAT math scores (**outcome**) for different high school program types (**group**)?\"\n-   Course covers advanced ANOVA topics:\n    -   Group comparisons (Group A vs. B vs. C)\n    -   Model comparisons\n    -   Between/within-subject design\n    -   Interaction effects\n\n## Types of ANOVA: Key Differences\n\n-   [**One-Way ANOVA**]{.underline}\n\n    -   **Purpose**: Tests **one** factor with three or more levels on a **continuous** outcome.\n\n    -   **Use Case**: Comparing means across multiple groups (e.g., diet types on weight loss).\n\n-   [**Two-Way ANOVA**]{.underline}\n\n    -   **Purpose**: Examines **two factors and their interaction** on a **continuous** outcome.\n\n    -   **Use Case**: Studying effects of diet and exercise on weight loss.\n\n-   [**Repeated Measures ANOVA**]{.underline}\n\n    -   **Purpose**: Tests the same subjects under **different conditions or time points**.\n\n    -   **Use Case**: Longitudinal studies measuring the same outcome over time (e.g., cognitive tests after varying sleep durations).\n\n-   [**Mixed-Design ANOVA**]{.underline}\n\n    -   **Purpose**: Combines **between-subjects and within-subjects** factors in one analysis.\n\n    -   **Use Case**: Evaluating treatment effects over time with control and experimental groups.\n\n-   [**Multivariate Analysis of Variance (MANOVA)**]{.underline}\n\n    -   **Purpose**: Assesses **multiple continuous outcomes** (dependent variables) influenced by independent variables.\n\n    -   **Use Case**: Impact of psychological interventions on anxiety, stress, and self-esteem.\n\n# Example 1: Political Study on Tax Reform Attitudes\n\n## Background\n\n-   A political scientist studies tax reform attitudes across political groups:\n    -   **Groups**: Democrats (n=4), Republicans (n=5), Independents (n=8)\n    -   **Outcome measure**: Attitude scores (higher score = greater concern for tax reform)\n    -   **Analysis**: Conducted at $\\alpha = .05$\n    -   **Variables**:\n        -   `party`: Political affiliation\n        -   `scores`: Attitude scores for survey respondents\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Install the package ESRM64103 from GitHub\nremotes::install_github(\"JihongZ/ESRM64103\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ESRM64103)\nlibrary(dplyr)\nexp_political_attitude\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         party scores\n1     Democrat      4\n2     Democrat      3\n3     Democrat      5\n4     Democrat      4\n5     Democrat      4\n6   Republican      6\n7   Republican      5\n8   Republican      3\n9   Republican      7\n10  Republican      4\n11  Republican      5\n12 Independent      8\n13 Independent      9\n14 Independent      8\n15 Independent      7\n16 Independent      8\n```\n\n\n:::\n:::\n\n\n## Descriptive Statistics: Summary Statistics\n\n-   Calculate mean, standard deviation, and variance for each political group\n\n-   Grand mean across all groups: 5.625\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Grand mean\nmean(exp_political_attitude$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.625\n```\n\n\n:::\n\n```{.r .cell-code}\nexp_political_attitude$party <- factor(exp_political_attitude$party, levels = c(\"Democrat\", \"Republican\", \"Independent\"))\nmean_byGroup <- exp_political_attitude |> \n  group_by(party) |> \n  summarise(Mean = mean(scores),\n            SD = round(sd(scores), 2),\n            Vars = round(var(scores), 2),\n            N = n())\nmean_byGroup\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  party        Mean    SD  Vars     N\n  <fct>       <dbl> <dbl> <dbl> <int>\n1 Democrat        4  0.71   0.5     5\n2 Republican      5  1.41   2       6\n3 Independent     8  0.71   0.5     5\n```\n\n\n:::\n:::\n\n\n## Descriptive Statistics: Bar Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(data = mean_byGroup) +\n  geom_bar(mapping = aes(x = party, y = Mean, fill = party), stat = \"identity\", width = .5) +\n  geom_label(aes(x = party, y = Mean, label = Mean), nudge_y = .3) +\n  labs(title = \"Attitudes Toward the Tax Return\") +\n  theme(text = element_text(size = 15))\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture02_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Descriptive Statistics: Multiple Variables\n\nIn research, we often need to compute descriptive statistics for multiple continuous variables simultaneously. Here are several approaches:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show data simulation code\"}\n# Generate simulated student performance data\nset.seed(2025)\nn_students <- 100\n\nstudent_data <- data.frame(\n  student_id = 1:n_students,\n  math_score = rnorm(n_students, mean = 75, sd = 12),\n  reading_score = rnorm(n_students, mean = 78, sd = 10),\n  science_score = rnorm(n_students, mean = 72, sd = 11),\n  study_hours = rgamma(n_students, shape = 2, scale = 3),\n  attendance_rate = rbeta(n_students, shape1 = 8, shape2 = 2) * 100\n)\n```\n:::\n\n\n### Method 1: Base R with `sapply()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select only continuous variables\ncontinuous_vars <- student_data[, c(\"math_score\", \"reading_score\", \"science_score\", \"study_hours\", \"attendance_rate\")]\n\n# Compute mean, sd, and range for each variable\ndesc_stats <- data.frame(\n  Mean = sapply(continuous_vars, mean),\n  SD = sapply(continuous_vars, sd),\n  Min = sapply(continuous_vars, min),\n  Max = sapply(continuous_vars, max),\n  Range = sapply(continuous_vars, function(x) max(x) - min(x))\n)\n\nround(desc_stats, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Mean    SD   Min    Max Range\nmath_score      74.94 12.24 45.59 109.23 63.64\nreading_score   78.09  9.65 58.84 100.50 41.66\nscience_score   72.10 10.83 40.66  99.39 58.73\nstudy_hours      6.03  4.81  0.40  26.14 25.74\nattendance_rate 80.53 10.53 49.08  99.10 50.02\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Method 2: Using `dplyr` package with `across()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nstudent_data |>\n  summarise(\n    across(\n      c(math_score, reading_score, science_score, study_hours, attendance_rate),\n      list(\n        Mean = ~mean(.x),\n        SD = ~sd(.x),\n        Min = ~min(.x),\n        Max = ~max(.x),\n        Range = ~max(.x) - min(.x)\n      ),\n      .names = \"{.col}_{.fn}\"\n    )\n  ) |>\n  tidyr::pivot_longer(\n    everything(),\n    names_to = c(\"Variable\", \"Statistic\"),\n    names_sep = \"_(?=[^_]+$)\",\n    values_to = \"Value\"\n  ) |>\n  tidyr::pivot_wider(\n    names_from = Statistic,\n    values_from = Value\n  ) |>\n  mutate(across(where(is.numeric), ~round(.x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 6\n  Variable         Mean    SD   Min   Max Range\n  <chr>           <dbl> <dbl> <dbl> <dbl> <dbl>\n1 math_score      74.9  12.2   45.6 109.   63.6\n2 reading_score   78.1   9.65  58.8 100.   41.7\n3 science_score   72.1  10.8   40.7  99.4  58.7\n4 study_hours      6.03  4.81   0.4  26.1  25.7\n5 attendance_rate 80.5  10.5   49.1  99.1  50.0\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Method 3: Using `psych::describe()`\n\nThe `psych` package provides a comprehensive `describe()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\n\ncontinuous_vars |>\n  describe() |>\n  select(n, mean, sd, min, max, range) |>\n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  n  mean    sd   min    max range\nmath_score      100 74.94 12.24 45.59 109.23 63.64\nreading_score   100 78.09  9.65 58.84 100.50 41.66\nscience_score   100 72.10 10.83 40.66  99.39 58.73\nstudy_hours     100  6.03  4.81  0.40  26.14 25.74\nattendance_rate 100 80.53 10.53 49.08  99.10 50.02\n```\n\n\n:::\n:::\n\n\n### Grouped Descriptive Statistics\n\nYou can also compute descriptives by groups:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show grouping variable code\"}\n# Add a grouping variable\nstudent_data$program <- sample(c(\"STEM\", \"Arts\", \"Social Sciences\"),\n                               n_students, replace = TRUE,\n                               prob = c(0.4, 0.3, 0.3))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute descriptives by program\nstudent_data |>\n  group_by(program) |>\n  summarise(\n    N = n(),\n    Math_Mean = mean(math_score),\n    Math_SD = sd(math_score),\n    Reading_Mean = mean(reading_score),\n    Reading_SD = sd(reading_score),\n    Science_Mean = mean(science_score),\n    Science_SD = sd(science_score)\n  ) |>\n  mutate(across(where(is.numeric) & !N, ~round(.x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  program             N Math_Mean Math_SD Reading_Mean Reading_SD Science_Mean\n  <chr>           <int>     <dbl>   <dbl>        <dbl>      <dbl>        <dbl>\n1 Arts               30      74.2    12.7         80.5       10.1         72.8\n2 STEM               44      74.3    12.4         77.6        9.1         73.4\n3 Social Sciences    26      76.9    11.7         76.1        9.8         69.2\n# ℹ 1 more variable: Science_SD <dbl>\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Steps of ANOVA\n\n1.  State the null hypothesis and alternative hypothesis:\n\n    -   $H_0$: $\\bar{X}_{dem}$ = $\\bar{X}_{rep}$ = $\\bar{X}_{ind}$\n    -   $H_A$: At least two groups are significantly different\n    -   Question: Why not testing $\\bar{SD}_{dem}$ = $\\bar{SD}_{rep}$ = $\\bar{SD}_{ind}$?\n    -   Answer: [You definitely can in statistics. Variances homogeneity.]{.heimu}\n\n2.  Set the significant alpha = 0.05\n\n3.  Quick Review of F-statistics:\n\n    $$\n    F_{obs} = \\frac{SS_b/df_b}{SS_w/df_w}\n    $$\n\n    -   Degrees of freedom: $df_b$ = 3 (groups) - 1 = 2, $df_w$ = 16 (samples) - 3 (groups) = 13\n\n    -   Between-group sum of squares:\n        $$SS_b = \\sum_{j=1}^{g} n_j(\\bar{Y}_j - \\bar{Y})^2 = 43.75$$\n        where $n_j$ is group sample size, $\\bar{Y}_j$ is group mean, and $\\bar{Y}$ is the grand mean.\n\n    -   Within-group sum of squares:\n        $$SS_w = \\sum_{j=1}^{3} \\sum_{i=1}^{n_j}(Y_{ij}-\\bar{Y}_j)^2 = 14.00$$\n        where $Y_{ij}$ is individual $i$'s score in group $j$.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    GrandMean <- mean(exp_political_attitude$scores)\n    ## Between-group Sum of Squares\n    sum(mean_byGroup$N * (mean_byGroup$Mean - GrandMean)^2)\n    ## Within-group Sum of Squares\n    SSw_dt <- exp_political_attitude |> \n      group_by(party) |> \n      mutate(GroupMean = mean(scores),\n             Diff_sq = (scores - GroupMean)^2) \n    sum(SSw_dt$Diff_sq)\n    ```\n    :::\n\n\n    -   $F_{critical}$ (df_num = 2, df_deno = 13) = 3.81\n    -   $F_{observed}$ = 20.31\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    anova_model <- lm(scores ~ party, data = exp_political_attitude)\n    anova(anova_model)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    Analysis of Variance Table\n    \n    Response: scores\n              Df Sum Sq Mean Sq F value    Pr(>F)    \n    party      2  43.75 21.8750  20.312 9.994e-05 ***\n    Residuals 13  14.00  1.0769                      \n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    ```\n    \n    \n    :::\n    :::\n\n\nResults show rejection of $H_0$ ($F_{obs}$ > $F_{critical}$)\n\n## Step 1: State the null hypothesis and alternative hypothesis\n\n1.  Formulate the **null hypothesis** ($H_0$) and the **alternative hypothesis** ($H_A$)\n    -   Prior to any statistical tests, start with a working hypothesis based on an initial guess about the phenomenon.\n    -   Example: Investigating whether political groups affect their attitudes.\n        -   Research question: \"Is there a variance in attitude score among different groups?\"\n        -   Hypothesis: **\"Different political groups will show varied attitudes.\"**\n    -   Operational Definitions:\n        -   **Null hypothesis** ($H_0$): No observed difference or effect (\"Something is something\").\n            -   Group A's mean - Group B's mean = 0\n        -   **Alternative hypothesis** ($H_A$): Noticeable difference or effect, contrary to $H_0$ (\"Something is not something\")\n    -   The adequacy of the data will dictate if $H_0$ can be confidently rejected.\n\n## Step 2: Rejection region (alpha)\n\nF-statistic has two degree of freedoms (df = 2). This is the density distribution of F-statistics for degree of freedoms as 2 and 13.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Set degrees of freedom for the numerator and denominator\nnum_df <- 2  # Change this as per your specification\nden_df <- 13  # Change this as per your specification\n\n# Generate a sequence of F values\nf_values <- seq(0, 8, length.out = 1000)\n\n# Calculate the density of the F-distribution\nf_density <- df(f_values, df1 = num_df, df2 = den_df)\n\n# Create a data frame for plotting\ndata_to_plot <- data.frame(F_Values = f_values, Density = f_density)\ndata_to_plot$Reject05 <- data_to_plot$F_Values > 3.81\ndata_to_plot$Reject01 <- data_to_plot$F_Values > 6.70\n# Plot the density using ggplot2\nggplot(data_to_plot) +\n  geom_area(aes(x = F_Values, y = Density), fill = \"grey\", \n            data = filter(data_to_plot, !Reject05)) + # Draw the line\n  geom_area(aes(x = F_Values, y = Density), fill = \"yellow\", \n            data = filter(data_to_plot, Reject05)) + # Draw the line\n  geom_area(aes(x = F_Values, y = Density), fill = \"tomato\", \n            data = filter(data_to_plot, Reject01)) + # Draw the line\n  geom_vline(xintercept = 3.81, linetype = \"dashed\", color = \"red\") +\n  geom_label(label = \"F_crit = 3.81 (alpha = .05)\", x = 3.81, y = .5, color = \"red\") +\n  geom_vline(xintercept = 6.70, linetype = \"dashed\", color = \"royalblue\") +\n  geom_label(label = \"F_crit = 6.70 (alpha = .01)\", x = 6.70, y = .5, color = \"royalblue\") +\n  ggtitle(\"Density of F-Distribution\") +\n  xlab(\"F values\") +\n  ylab(\"Density\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](ESRM64103_Lecture02_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n1.  Set the alpha $\\alpha$ (i.e., type I error rate)—rejection rate vs. p-value\n\n    -   Alpha determines several values for statistical hypothesis testing: the critical value of the test statistics, the rejection region, etc.\n\n    -   Large sample sizes typically use lower alpha levels: .01 or .001 (more restrictive rejection rate)\n\n2.  When we conduct hypothesis testing, four possible outcomes can occur:\n\n+----------------------+-------------------------+--------------------------+\n|                      | **Reality**             |                          |\n+----------------------+-------------------------+--------------------------+\n| **Decision**         | $H_0$ is true           | $H_0$ is false           |\n+----------------------+-------------------------+--------------------------+\n| Fail to reject $H_0$ | Correct Decision        | Error made.              |\n|                      |                         |                          |\n|                      |                         | Type II error ($\\beta$). |\n+----------------------+-------------------------+--------------------------+\n| Reject $H_0$         | Error made.             | Correct Decision (Power) |\n|                      |                         |                          |\n|                      | Type I error ($\\alpha$) |                          |\n+----------------------+-------------------------+--------------------------+\n\n: Type I & II Error\n\n## Step 3: Compute the test statistics\n\n-   Investigate where the variability in the outcome comes from.\n\n    -   In this study: Do people's attitude scores differ because of their political party affiliation?\n\n    -   When we have factors influencing the outcome, the total variability can be decomposed as follows:\n\n![Sources of variability: Total variance can be decomposed into between-group and within-group variance](images/clipboard-2219252399.png)\n\n------------------------------------------------------------------------\n\n### F-statistics\n\n-   **Core idea**: Comparing the variances between groups and within groups to ascertain if the means of different groups are significantly different from each other.\n\n-   **Logic**: If the **between-group variance** (due to systematic differences caused by the independent variable) is significantly greater than the **within-group variance** (attributable to random error), the observed differences between group means are likely not due to chance.\n\n-   **F-statistics formula for one-way ANOVA:**\n\n    $$\n    F_{obs} = \\frac{SS_{between}/df_{between}}{SS_{within}/df_{within}}\n    $$\n\n    -   Degrees of freedom: $df_{between}$ = 3 (groups) - 1 = 2, $df_{within}$ = 16 (samples) - 3 (groups) = 13\n    -   $SS_{between}$ = $\\sum n_j(\\bar{Y}_j - \\bar{Y})^2$ = 43.75\n        -   Variability in the differences between groups (weighted by group sample size)\n    -   $SS_{within}$ = $\\sum_{j=1}^{3} \\sum_{i=1}^{n_j}(Y_{ij}-\\bar{Y}_j)^2$ = 14.00; where $Y_{ij}$ is individual $i$'s score in group $j$\n        -   Random error within groups—individuals differ in attitudes for unknown reasons\n\n## Step 4: Conduct a hypothesis testing\n\n-   In addition to the comparison of the critical value and the observed value of the test statistics, we can also compare the alpha and the p-value:\n\n![F-distribution showing rejection regions for different alpha levels](images/clipboard-3877350302.png)\n\n-   We determine $F_{crit}$ by setting $\\alpha$ value.\n    -   $\\alpha$ = (acceptable) type I error rate = probability that we wrongly reject $H_0$ when $H_0$ is true\n-   From the data, we obtain $F_{obs}$ with p-value.\n    -   p-value = probability of datasets having F-statistics larger than $F_{obs}$\n-   If the F statistic from the data ($F_{obs}$) is larger than $F_{critical}$, then you are in the rejection region and can reject $H_0$ and accept $H_A$ with $(1-\\alpha)$ level of confidence.\n-   If the p-value obtained from the ANOVA is less than $\\alpha$, then reject $H_0$ and accept $H_A$ with $(1-\\alpha)$ level of confidence.\n\n## Step 5: Results Report\n\nA one-way ANOVA was conducted to compare the level of concern for tax reform among three political groups: Democrats, Republicans, and Independents. There was a significant effect of political affiliation on tax reform concern at the $p < .001$ level for the three conditions [$F(2, 13) = 20.31$, $p < .001$]. This result indicates significant differences in attitudes toward tax reform among the groups.\n\n## Note: Relationship Between P-values and Type I Error\n\n1.  **P-values**: The probability of observing data as extreme as, or more extreme than, the data observed **under the assumption that the null hypothesis is true**.\n\n    -   The lower the p-value, the less likely we would see the observed data given the null hypothesis is true\n\n    -   **Question**: Given that we already have the observed data, does a lower p-value mean the null hypothesis is unlikely to be true?\n\n    -   **Answer**: No. $P(\\text{observed data} | H_0 = \\text{true}) \\neq P(H_0 = \\text{true} | \\text{observed data})$. P-values are often misconstrued as the probability that the null hypothesis is true given the observed data. However, this interpretation is incorrect.\n\n2.  **Type I error**, also known as a \"false positive,\" occurs when the null hypothesis is incorrectly rejected when it is, in fact, true.\n\n3.  The alpha level $\\alpha$ set before conducting a test (commonly $\\alpha = 0.05$) defines the cutoff point for the p-value below which the null hypothesis will be rejected.\n\n    -   A p-value less than the alpha level suggests a low probability that the observed data would occur if the null hypothesis were true. Consequently, rejecting the null hypothesis in this context implies there is a statistically significant difference likely not due to random chance.\n\n## Note: Limitations of p-values\n\nRelying solely on p-values to reject the null hypothesis can be problematic for several reasons:\n\n-   **Binary Decision Making**: The use of a threshold (e.g., $\\alpha = 0.05$) to determine whether to reject the null hypothesis reduces the complexity of the data to a binary decision. This can oversimplify the interpretation and overlook nuances in the data.\n\n    -   **Alternatives**: Confidence intervals, Bayesian statistics (reporting posterior distributions)\n\n-   **Neglect of Effect Size**: P-values do not convey the size or practical importance of an effect. A very small effect can produce a small p-value if the sample size is large enough, leading to rejection of the null hypothesis even when the effect may not be practically significant.\n\n    -   **Solution**: Report effect sizes that are independent of sample size\n\n-   **Probability of Extremes Under the Null**: Since p-values quantify the extremeness of the observed data under the null hypothesis, they do not address whether similarly extreme data could also occur under alternative hypotheses. This can lead to an overemphasis on the null hypothesis and potentially disregard other plausible explanations for the data.\n\n    -   **Solution**: Explore theory, find alternative explanations, try varied models\n\n# Example 2: the Effect of Sleep on Academic Performance (Simulation)\n\n## Background\n\n-   A study investigates the effect of different sleep durations on the academic performance of university students. Three groups are defined based on nightly sleep duration: Less than 6 hours, 6 to 8 hours, and more than 8 hours.\n\n-   We can simulate the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate data for three sleep groups\nless_than_6_hours <- rnorm(30, mean = 65, sd = 10)\nsix_to_eight_hours <- rnorm(50, mean = 75, sd = 8)\nmore_than_8_hours <- rnorm(20, mean = 78, sd = 7)\n\n# Combine data into a single data frame\nsleep_data <- data.frame(\n  Sleep_Group = factor(c(rep(\"<6 hours\", 30), rep(\"6-8 hours\", 50), rep(\">8 hours\", 20))),\n  Exam_Score = c(less_than_6_hours, six_to_eight_hours, more_than_8_hours)\n)\n\n# View the first few rows of the dataset\nhead(sleep_data)\n```\n:::\n\n\n## Descriptive Statistics\n\n-   **Groups**:\n\n    -   **Less than 6 hours**: 30 students\n\n    -   **6 to 8 hours**: 50 students\n\n    -   **More than 8 hours**: 20 students\n\n-   **Performance Metric**: Average exam scores out of 100.\n\n    -   **Less than 6 hours**: Mean = 65, SD = 10\n\n    -   **6 to 8 hours**: Mean = 75, SD = 8\n\n    -   **More than 8 hours**: Mean = 78, SD = 7\n\n## Your Turn:\n\n#### F-test\n\n-   **Analysis**: One-way ANOVA was conducted to compare the average exam scores among the three groups.\n\n-   **Results**: $F_{observed}$ = _[Calculate from your analysis]_, $p$ = _[Report p-value]_\n\n#### Interpretation\n\n-   **Alpha Level**: $\\alpha = 0.05$\n\n-   **P-value Interpretation**: [Compare your p-value to alpha and interpret the result]{.heimu}\n\n-   **Conclusion**: [Based on the results, what can you conclude about the effect of sleep duration on academic performance?]{.heimu}\n\n## Homework 1\n\nDue on next Tuesday 5PM.\n",
    "supporting": [
      "ESRM64103_Lecture02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}