{
  "hash": "66860aa0e254d79a696eef35aa7762f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 05\"\nsubtitle: \"Bayesian Model fit and comparison\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\n  data-background-opacity: \"0.9\"\nexecute: \n  echo: true\n  eval: false\nformat:\n  html:\n    code-tools: true\n    code-line-numbers: false\n    code-fold: true\n    code-summary: 'Click here to see R code'\n    number-offset: 1\n    fig.width: 10\n    fig-align: center\n    message: false\n  revealjs:\n    output-file: slides-index.html\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: true  # choose \"false \"if want to show all together\n    theme: [serif, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    transition: slide\n    background-transition: fade\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-link: true\n    code-annotations: hover\n    code-copy: true\n    highlight-style: arrow\n    code-block-border-left: true\n    code-block-background: \"#b22222\"\nbibliography: references.bib\n---\n\n\n\n\n## Today's Lecture Objectives\n\n1.  Bayesian methods for determining how well a model fits the data (absolute fit)\n\n2.  Bayesian methods for determining which model fits better (relative model fit)\n\n3.  In previous class...\n\n    1.  We estimated the empty model and the full model (you can try other constrained model between the empty and full model)\n\n        -   Empty model: a model without any covariate (predictors)\n\n        -   Full model: a model with all possible covariates and up-to highest interaction effects\n\n    2.  We also make the Stan code more efficient by vectorizing the parameters and the data\n\n    3.  The next question is **how to determine the model is \"good\" enough**\n\n        -   \"good\" has multiple meanings\n\n        -   a typical criteria is to what degree the model can generate a \"simulated\" data that is similar to the original data\n\n        -   or, the likelihood of original data given the parameters of model\n\n## Types of Model fit\n\n-   Absolute model fit\n\n    -   PPMC\n\n    -   In SEM, RMSEA, chi-square, SRMR, and GFI\n\n-   Relative model fit\n\n    -   information criterion\n\n-   Incremental model fit (not frequently used other than SEM)\n\n    -   A special type of absolute model fit - how a model fits to a saturated model\n\n    -   In SEM, comparative fit index (CFI), Tucker-Lewis index (TLI)\n\n## Absolute Model Fit: PPMC\n\n**Posterior predictive model checking** [@gelman1996] is a Bayesian method evaluation technique for determining if a model fits the data.\n\n-   Absolute model fit: \"Does my model fit my data well?\"\n\n    -   Recall that \"model is a simplified version of the true data-generation process\"\n\n    -   Thus, the model should be able to reproduce the \"data\" that is similar to observed data\n\n    -   In machine learning, this is also called \"validation\", typically used as a separate \"validation data sets\"\n\n-   Overall idea: if a model fits the data well, then simulated data based on the model will resemble the observed data\n\n------------------------------------------------------------------------\n\n### Ingredients in PPMC:\n\n::: nonincremental\n-   Original data\n\n    -   Typically, characterized by some set of statistics (i.e., sample mean, standard deviation, covariance) applied to data\n\n-   Simulated data\n\n    ::: nonincremental\n    -   Generated from partial/all posterior draws in the Markov Chain\n\n    -   Summarized by the same set of statistics\n    :::\n:::\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture5_PPMC_DataSets.png){fig-align=\"center\" width=\"600\"}\n\n------------------------------------------------------------------------\n\n## PPMC Example: Linear Models\n\nThe linear model from our example was:\n\n$$\n\\text{WeightLB}_p = \\beta_0 + \\beta_1 \\text{HeightIN}_p + \\beta_2 \\text{Group2}_p + \\beta_3\\text{Group3}_p \\\\ \n+\\beta_4 \\text{HeightIN}_p\\text{Group2}_p \\\\\n+\\beta_5 \\text{HeightIN}_p\\text{Group3}_p \\\\\n+ e_p\n$$\n\nwith:\n\n::: nonincremental\n-   $\\text{Group2}_p$ the binary indicator of person $p$ being in group 2\n-   $\\text{Group}3_p$ the binary indicator of person $p$ being in group 3\n-   $e_p \\sim N(0, \\sigma_e)$\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_full_new$summary(variables = c('beta', 'sigma'))\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### PPMC Process\n\nThe PPMC Process can be summarized as follows:\n\n1.  Select parameters from a single (sampling) iteration of the Markov Chain\n2.  Using the selected parameters and the model to simulate a data set with the sample size (with same number of observations/variables)\n3.  From the simulated data set, calculate selected summary statistics (e.g., mean, sd)\n4.  Repeat steps 1-3 for a fixed number of iterations (perhaps across the whole chain)\n5.  When done, compare position of observed summary statistics to that of the distribution of summary statistics from simulated data sets (predictive distribution)\n\n------------------------------------------------------------------------\n\n### Step 1: Assemble the posterior draws\n\nFor our linear model, let's denote our observed dependent variable as $Y$\n\n-   Note that independent variables are not modeled (not explained by statistical formula), so we cannot examine them.\n\nFirst, let's assemble the posterior draws ($1000 \\times 4 \\times 7$):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposteriorSample = fit_full_new$draws(variables = c('beta','sigma'), format = 'draws_matrix')\nposteriorSample\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Step 2: One draw from posterior\n\nNext, let's draw one set of posterior values of parameters at random with replacement:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nsampleIteration = sample(x = 1:nrow(posteriorSample), size = 1, replace = TRUE)\nsampleIteration\nposteriorSample[sampleIteration,]\n```\n:::\n\n\n\n\nThose draws of $\\boldsymbol{\\beta}$ and $\\sigma$ can be used to generate a predicted $\\hat{y}$:\n\n$$\n\\hat{y}_i \\sim N(\\mathbf{X}\\boldsymbol{\\beta}_i, \\sigma_i)\n$$\n\n-   $i$ represents the $i$ th draw\n\n------------------------------------------------------------------------\n\n### Step 3: One simulated data\n\nWe then generate data based on this sampled iteration and our model distributional assumption:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample Size\nN = nrow(dat)\n# beta\nbetaVector = matrix(posteriorSample[sampleIteration, 1:6], ncol = 1)\nbetaVector\n# sigma\nsigma = posteriorSample[sampleIteration, 7]\n# X\nFullModelFormula = as.formula(\"WeightLB ~ HeightIN60 + DietGroup + HeightIN60*DietGroup\")\nX = model.matrix(FullModelFormula, data = dat)\n\nsimY = rnorm(n = N, mean = X %*% betaVector, sd = sigma)\nhead(simY)\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Step 4: Summary Statistics of Simulated Data\n\nNote that we do not want to directly compare simulated data to the observed data.\n\nInstead, we extract some characteristics of simulated/observed data for comparison using summary statistics.\n\nThere are some advantages:\n\n1.  We may have research interests in only some characteristics of data (whether our model predict the mean of dependent variables)\n2.  We can QC detailed aspects of the fitting process of model\n\nIn this case, for example, we may be interested in whether the model captures the \"location\" or \"scale\" of WeightLB\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(dat$WeightLB)\n(simMean = mean(simY))\nsd(dat$WeightLB)\n(simSD = sd(simY))\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Step 5: Looping across all posterior samples\n\nWe can repeat step 2-4 for a set number of samples\n\nOptionally, we can choose to use up all iterations we have for Markov Chains ($I = 4000$) in practice\n\n\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nI = nrow(posteriorSample)\n## create empty simSD and simMean as placeholders\nsimSD = simMean = rep(NA, I)\nfor (i in 1:I) {\n  # beta\n  betaVector = matrix(posteriorSample[i, 1:6], ncol = 1)\n  # sigma\n  sigma = posteriorSample[i, 7]\n  # X\n  simY = rnorm(n = N, mean = X %*% betaVector, sd = sigma)\n  simMean[i] = mean(simY)\n  simSD[i] = sd(simY)\n}\npar(mfrow = 1:2)\nhist(simMean)\nhist(simSD)\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Compare to the observed mean\n\n::: columns\n::: {.column width=\"50%\"}\nWe can now compare our observed mean and standard deviation with that of the sample values.\n\n-   Blue line: the average value of predicted WeightLB\n\n-   Red line: the observed mean value of WeightLB\n\n-   The PDF of predictive values of summary statistics of WeightLB is called `posterior predictive distribution`\n:::\n\n::: {.column width=\"50%\"}\nPPMC can be checked using visual inspection:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nppp <- data.frame(\n  simMean = simMean, \n  simSD = simSD\n)\nggplot(ppp) +\n  geom_histogram(aes(x = simMean), fill = \"grey\", col = \"black\") +\n  geom_vline(xintercept = mean(dat$WeightLB), col = \"red\", size = 1.4) + # red line: location of mean of predicted WeightLB by model\n  geom_vline(xintercept = mean(simMean), col = \"blue\", size = 1.4, alpha = 0.5) # blue line: location of mean of WeightLB\n```\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Compare to the observed SD\n\n::: columns\n::: {.column width=\"50%\"}\nSimilarly, let's compare SD to the posterior predictive distribution of SD of WeightLB\n\n-   the observed SD is located as the center of posterior predictive distribution (PPD)\n\n-   the average mean of PPD is slightly higher than the observed SD\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ppp) +\n  geom_histogram(aes(x = simSD), fill = \"grey\", col = \"black\") +\n  geom_vline(xintercept = sd(dat$WeightLB), col = \"red\", size = 1.4) + # red line: location of mean of predicted WeightLB by model\n  geom_vline(xintercept = mean(simSD), col = \"blue\", size = 1.4, alpha = 0.5) # blue line: location of mean of WeightLB\n```\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n## PPMC Characteristics\n\nPPMC methods are very useful\n\n::: nonincremental\n-   They provide a visual way to determine if the model fits the observed data\n-   They are the main method of assessing absolute fit in Bayesian models\n-   Absolute fit assesses if a model fits the data instead of comparing to another model\n:::\n\nBut, there are some drawbacks to PPMC methods\n\n::: nonincremental\n-   Almost any statistic can be used\n    -   Some are better than others (mean and SD of outcomes are nice choices for linear regression)\n-   No standard determining how much misfit is too much\n-   May be overwhelming to compute depending on your model\n:::\n\n**Question**: Can PPMC be used for models with maximum likelihood estimation or ordinary least squares?\n\n------------------------------------------------------------------------\n\n## Posterior Predictive P-values\n\n::: columns\n::: {.column width=\"50%\"}\nWe can summarize the PPMC using a type of \"p-value\"\n\n> Personally, I don't like the name \"p-value\", sounds like we are trying to justify our results using significance testing\n\nDifferent from the frequentist \"p-value\" (if the null hypothesis is true, the probability of the observed data existing)\n\n-   The PPP-value: the proportion of times the statistic from the simulated data exceeds that of the observed data\n\n-   Useful to determine how far off a statistic is from its posterior predictive distribution\n:::\n\n::: {.column width=\"50%\"}\nIf these p-values were:\n\n1.  near 0 or 1, indicating your model is far off your data\n2.  near .5, indicating your model fits your data in terms of the statistics you examined\n\nThe PPP-value for mean:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(simMean > mean(dat$WeightLB))\n```\n:::\n\n\n\n\nThe PPP-value for SD:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(simSD > sd(dat$WeightLB))\n```\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Compute PPP-values within Stan\n\nWe can use the `generated quantities` block of Stan to compute PPP-values for us:\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ngenerated quantities{\n  // simulated data\n  array[N] real weightLB_rep = normal_rng(X*beta, sigma);\n  // posterior predictive distribution for mean and SD\n  real mean_weightLB = mean(weightLB);\n  real sd_weightLB = sd(weightLB);\n  real mean_weightLB_rep = mean(to_vector(weightLB_rep));\n  real<lower=0> sd_weightLB_rep = sd(to_vector(weightLB_rep));\n  // ppp-values for mean and sd\n  int<lower=0, upper=1> ppp_mean = (mean_weightLB_rep > mean_weightLB);\n  int<lower=0, upper=1> ppp_sd = (sd_weightLB_rep > sd_weightLB);\n}\n```\n:::\n\n\n\n\nIt will give us:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_full_ppp$summary(variables = c('mean_weightLB_rep', 'sd_weightLB_rep', 'ppp_mean', 'ppp_sd'))\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Advantages or disadvantages of Computing PPP within Stan\n\n::: columns\n::: {.column width=\"60%\"}\nPros:\n\n1.  Built-in functions of Stan to generate simulated data for example `normal_rng()`, making PPP-values estimated much faster\n2.  Nice visual inspection tools existed - `bayesplot`\n\nCons:\n\n1.  Not allowed to debug each step in PPMC if something wrong\n2.  Cannot adjust the statistics and need to re-run the whole MCMC sampling, which is time-consuming\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens_chains(fit_full_ppp$draws('mean_weightLB_rep'))\n```\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Relative Model Fit\n\nRelative model fit: used to compare 2 or more competing models in terms of their mode fit. Sometime, it is also called model selection.\n\n-   In non-Bayesian models, Information Criteria are often used to make comparisons\n\n    -   AIC, BIC, DIC etc.\n\n    -   Typically IC is a function of log-likelihood and penalty\n\n    -   The model with the lowest IC is the model that fits best\n\n-   Bayesian model fit is similar\n\n    -   Uses an index value\n\n    -   The model with the lowest index is the model that fits best\n\n-   Recent advances in Bayesian model fit use indices that are tied to make cross-validation predictions (inspired by machine learning):\n\n    -   Fit model leaving one observation out (LOO)\n\n    -   Calculate statistics related to prediction (for instance, log-likelihood of that observation conditional on model parameters)\n\n    -   Do for all observations\n\n-   New Bayesian indices try to mirror these leave-one-out predictions (but approximate these due to time constraints)\n\n------------------------------------------------------------------------\n\n## Deviance Information Indices\n\nWhen late 1990s and early 2000s, the **Deviance Information Criterion** was popular for relative Bayesian model fit comparisons. It is proved not as good as LOO or WAIC. But let's have a look at:\n\n$$\n\\text{DIC} = p_D + \\overline{D(\\theta)}\n$$\n\nwhere $p_D$ is the estimated number of parameters as follows:\n\n$$p_D = \\overline{D(\\theta)} - D(\\bar\\theta)$$and where\n\n$$\nD(\\theta) = -2 \\log(p(y \\mid \\theta)) + C\n$$\n\nC is a constant that cancels out when model comparisons are made\n\nHere.\n\n-   $\\overline{D(\\theta)}$ is the average log likelihood of the data (y) given the parameters ($\\theta$) computed across all samples\n\n-   $D(\\bar\\theta)$ is the log likelihood of the data (y) computed at the average of the parameters ($\\theta$) computed across all samples\n\n------------------------------------------------------------------------\n\n### DIC in Stan\n\nSome program like JAGS will have provided DIC directly, but Stan does not provides direct ways of calculating DIC:\n\nWe can manually calculate approximated DIC by replacing $\\bar\\theta$ as median of posteriors $\\dot\\theta$ if the posterior distributions are almost normal.\n\n::: columns\n::: {.column width=\"50%\"}\n**DIC for Full Model**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract log-likelihood and parameters\nposterior_draws <- fit_full_ppp$draws(c('log_lik', 'beta'), format = 'draws_matrix')\nlog_like_draws <- rowSums(posterior_draws[,colnames(posterior_draws) %in% paste0(\"log_lik[\", 1:30, \"]\")]) \n# find draws of loglikelihood that have parameters equaling to median values\ntheta_draws <- posterior_draws[, colnames(posterior_draws) %in% paste0(\"beta[\", 1:6, \"]\")]\ntheta_ave_iteration = apply(round(theta_draws, 3), 2, \\(x) which(x == median(x))) |> unlist() |> as.numeric()\nlog_like_draws_meantheta = (-2)*mean(log_like_draws[theta_ave_iteration])\nmean_log_like_draws = (-2)*mean(log_like_draws)\n# compute DIC\nDIC_full = mean_log_like_draws - log_like_draws_meantheta + mean_log_like_draws\nDIC_full\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n**DIC for Empty Model**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract log-likelihood and parameters\nposterior_draws <- fit_empty_ppp$draws(c('log_lik', 'beta0'), format = 'draws_matrix')\nlog_like_draws <- rowSums(posterior_draws[,colnames(posterior_draws) %in% paste0(\"log_lik[\", 1:30, \"]\")]) \n# find draws of loglikelihood that have parameters equaling to median values\ntheta_draws <- posterior_draws[, colnames(posterior_draws) == \"beta0\"]\ntheta_ave_iteration = which(round(theta_draws,2) == median(round(theta_draws,2)))\nlog_like_draws_meantheta = (-2)*mean(log_like_draws[theta_ave_iteration])\nmean_log_like_draws = (-2)*mean(log_like_draws)\n# compute DIC\nDIC_empty = mean_log_like_draws - log_like_draws_meantheta + mean_log_like_draws\nDIC_empty\n```\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Issues to DIC\n\nThe DIC has fallen out of favor recently (not used in Stan)\n\n::: nonincremental\n-   Has issues when parameters are discrete\n\n-   Not fully Bayesian (point estimate of average of parameter values)\n\n-   Can give negative values for estimated numbers of parameters in a model\n:::\n\n**WAIC** (Widely applicable or Watanabe-Akaike information criterion, Watanabe, 2010) corrects some of the problems with DIC:\n\n::: nonincremental\n-   Fully Bayesian (uses entire posterior distribution)\n\n-   Asymptotically equal to Bayesian cross-validation\n\n-   Invariant to parameterization\n:::\n\n------------------------------------------------------------------------\n\n## Watanabe-Akaike information criterion (WAIC)\n\n::: nonincremental\n-   A more frequently used model comparison indices for Bayesian analysis\n\n-   Using the `loo` R package, we can calculate WAIC with the `waic()` function:\n:::\n\n::: columns\n::: {.column width=\"50%\"}\n**WAIC for the full model**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo::waic(fit_full_ppp$draws('log_lik'))\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n**WAIC for the empty model**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo::waic(fit_empty_ppp$draws('log_lik'))\n```\n:::\n\n\n\n:::\n:::\n\n::: nonincremental\n-   Here:\n    -   `elpd_waic` is the expected log pointwise predictive density for WAIC\n\n    -   `p_waic` is the WAIC-version estimated number of parameter, similar to $p(D)$ in DIC, which is a penalty to the likelihood for more parameters\n\n    -   `waic` is the WAIC index used for model comparisons (lowest value is best fitting; -2\\*`elpd_waic`)\n-   Note that WAIC needs a `log_lik` variable in the model analysis to be calculated correctly. `cmdstan` will automate calculate this variable.\n:::\n\n------------------------------------------------------------------------\n\n## LOO: Approximation to Leave-one-out\n\nBig picture:\n\n::: nonincremental\n1.  Besides WAIC, other comparative fit indices include LOO via Pareto Smoothed Important Sampling (PSIS) via Stan's `LOO` package\n\n2.  WAIC/LOO can be used for model comparison with lowest value suggesting better model fit\n\n3.  Different from DIC, LOO via PSIS attempts to \"approximate\" the process of leave-one-out cross-validation (LOO-CV) using a sampling based-approach\n\n    ::: nonincremental\n    -   Gives a finite-sample approximation\n    -   Implemented in Stan\n    -   Can quickly compare models\n    -   Gives warnings when it may be less reliable to use\n    :::\n\n4.  The details of computation of LOO are very technical, but are nicely compiled in Vehtari, Gelman, and Gabry (2017).\n:::\n\n------------------------------------------------------------------------\n\n### LOO: Leave-one-out in Stan\n\nUsing `loo` package, we can calculate efficient approximate leave-one-out cross-validation (LOO-CV)\n\n::: columns\n::: {.column width=\"50%\"}\nFull model's LOO:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_loo_res <- fit_full_ppp$loo('log_lik', save_psis = TRUE)\nfull_loo_res$estimates\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\nEmpty model's LOO:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nempty_loo_res <- fit_empty_ppp$loo('log_lik', save_psis = TRUE)\nempty_loo_res$estimates\n```\n:::\n\n\n\n:::\n:::\n\nHere:\n\n-   `elpd_loo` is the expected log pointwise predictive density for LOO (recall that posterior predictive distribution has some uncertainty around the mean value...)\n\n-   `p_loo` is the LOO calculation of number of model parameters (a penalty to the likelihood for more parameters)\n\n-   `looic` is the LOO index used for model comparisons â€” lowest value suggests best fitting -2`elpd_loo`\n\n------------------------------------------------------------------------\n\n### LOO: Comparing Model with LOO\n\nAlternative, you can use the built-in function `loo_compare` in `loo` package to compare alternative models:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo::loo_compare(list(empty = fit_empty_ppp$loo(), full = fit_full_ppp$loo()))\n```\n:::\n\n\n\n\nThis function calculats the standard error of the difference in `elpd` (expected log pointwise predictive density) between models\n\n::: nonincremental\n-   The SE gives an indication of the standard error in the estimate (relative to the size)\n-   Can use this to downweight the choice of models when the standard error is high (Open Question: how high is high?)\n    -   $$\n        0 \\in \\text{elpd_diff}\\ \\pm\\ 1.96*\\text{se_diff}\n        $$\n-   Note: `elpd_diff` is `looic` divided by -2 (on the log-likelihood scale, not the deviance scale)\n    -   Here, we interpret the results as the model with full predictors is preferred to the model with empty predictors.\n    -   The \"confidence interval\" of the `elpd_diff` does not include 0, indicating we can be fairly certain of this result\n:::\n\n------------------------------------------------------------------------\n\n### LOO: Pareto smoothed importance sampling (PSIS)\n\n::: nonincremental\n-   Estimated a vector of Pareto shape parameters $k$ for individuals which represents the reliability of sampling:\n\n    -   $k < .5$ (good) suggests the estimate converges quickly\n\n    -   $.5 < k < .7$ (ok) suggests the estimate converges slowly\n\n    -   $.7 < k < 1$ (bad) suggests bad performance\n\n    -   $k > 1$ (very bad)\n\n-   PSIS screens all cases that have bad diagnostic values. The percentage may tells some information regarding reliability of Bayesian estimation.\n\n-   Criteria: Estimated tail shape parameter\n\n-   This is new area, please see more references [@vehtari2016]\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npareto_k_table(full_loo_res)\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Compile the relative model fit indices\n\nBelow shows the all three model comparison fit indices for the empty model and the full model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  Model = c(\"Full Model\", \"Empty Model\"),\n  DIC = c(DIC_full, DIC_empty),\n  WAIC = c(WAIC_full, WAIC_empty),\n  LOOIC = c(LOO_full, LOO_empty)\n)\n```\n:::\n\n\n\n\n::: nonincremental\n-   All fit indices suggest the full model is better than the empty model\n\n-   For simple models, the results of DIC/WAIC/LOOIC should be consistent\n\n-   In some situations that they are inconsistent, please use WAIC/LOO as standards.\n:::\n\n------------------------------------------------------------------------\n\n## General points about Bayesian Model Comparison\n\n::: nonincremental\n-   Note, WAIC and LOO will converge as sample size increases (WAIC is asymptotic value of LOO)\n-   Latent variable models present challenges (could be your dissertation project)\n    -   Need log likelihood with latent variable being integrated out\n-   Missing data present challenges\n    -   Need log likelihood with missing data integrated out\n-   Generally, using LOO is recommended (but providing both is appropriate)\n:::\n\n------------------------------------------------------------------------\n\n## Cutting-Edge Research Field: Approximation Algorithm\n\n::: nonincremental\n-   MCMC sampling could be very computational time-consuming.\n\n-   For number of parameters up to thousands or even millions, it could be a challenge to get the estimation.\n\n-   Thus, approximation algorithms has been developed, such as Laplace Approximation, Variational Inference [@dhaka2020; @yao2018], [Pathfinder methods](https://statmodeling.stat.columbia.edu/2021/08/10/pathfinder-a-parallel-quasi-newton-algorithm-for-reaching-regions-of-high-probability-mass/) [@zhang2021].\n:::\n\nQuoted from [Gelman's blog](https://statmodeling.stat.columbia.edu/2023/02/08/implementing-laplace-approximation-in-stan-whats-happening-under-the-hood/):\n\n> ::: columns\n> ::: {.column width=\"50%\"}\n> **Advantages of Laplace:**\n>\n> ::: nonincremental\n> -   Relatively cheap to compute, given that we already have a mode-finder in Stan.\n> -   Easy to understand, use, and communicate.\n> -   Works reasonably well in many examples (wherever the posterior can be well approximated by a normal distribution).\n> -   Easy to take draws from the normal approximation, also easy to compute importance ratios and use [Pareto-smoothed importance sampling](http://www.stat.columbia.edu/~gelman/research/unpublished/psis4.pdf).\n> :::\n> :::\n>\n> ::: {.column width=\"50%\"}\n> **Limitations of Laplace:**\n>\n> ::: nonincremental\n> -   Sometimes the normal approx is pretty bad (funnels, multimodal distributions, long tails).\n>\n> -   Sometimes the joint mode is useless or does not even exist (funnels, etc.), in which case the model itself would need to be altered in some way to get a stable mode.\n> :::\n> :::\n> :::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Stan's LBFGS algorithm\nfit_full_optim <- mod_full_ppp$optimize(data = data_full_new, seed = 1234, jacobian = TRUE)\nfit_full_laplace <- mod_full_ppp$laplace(data = data_full_new, mode = fit_full_optim, draws = 4000)\n\n# Run 'variational' method to use ADVI to approximate posterior\nfit_full_vb <- mod_full_ppp$variational(data = data_full_new, seed = 1234, draws = 4000)\n\n# Run 'pathfinder' method, a new alternative to the variational method\nfit_pf <- mod_full_ppp$pathfinder(data = data_full_new, seed = 1234, draws = 4000)\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n### Approximation Algorithm vs. Full MCMC\n\nThough they are very quicker to converge, the accuracy of these approximation algorithms largely depend on the complexity of modeling (number of parameters, prior distributions, latent variables etc.)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsumm_list <- lapply(c(fit_full_ppp, fit_full_laplace, fit_full_vb, fit_pf), \\(x) \n                    x$summary(c(\"beta\", \"sigma\"))[c('variable', 'mean',\"q5\", \"q95\")])\nsumm_list[[1]]$Algorithm = \"MCMC\"\nsumm_list[[2]]$Algorithm = \"Laplace Approx.\"\nsumm_list[[3]]$Algorithm = \"Variational Inference\"\nsumm_list[[4]]$Algorithm = \"Pathfinder\"\nsumm_forplot <- Reduce(rbind, summ_list)  \nsumm_forplot$Algorithm = factor(summ_forplot$Algorithm, levels = c(\"MCMC\", \"Laplace Approx.\", \"Pathfinder\", \"Variational Inference\"))\n\nggplot(summ_forplot) +\n  geom_col(aes(x = variable, y = mean, fill = Algorithm), position = position_dodge()) +\n  geom_errorbar(aes(x = variable, ymax = q95, ymin = q5, fill = Algorithm),position = position_dodge2(padding = 0.6)) +\n  labs(x = '', y = '') +\n  theme_classic()\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nrbind(\n  cbind(data.frame(Algorithm = \"Laplace Approx.\"), fit_full_laplace$draws(paste0(\"beta[\", 1:6, \"]\"), format = \"draws_df\")[, 1:6]),\n  cbind(data.frame(Algorithm = \"Variational Inference\"), fit_full_vb$draws(paste0(\"beta[\", 1:6, \"]\"), format = \"draws_df\")[, 1:6]),\n  cbind(data.frame(Algorithm = \"Pathfinder\"), fit_pf$draws(paste0(\"beta[\", 1:6, \"]\"))[, 1:6]),\n  cbind(data.frame(Algorithm = \"MCMC\"), fit_full_ppp$draws(paste0(\"beta[\", 1:6, \"]\"), format = \"draws_df\")[, 1:6])\n) |>\n  mutate(Algorithm = factor(Algorithm, levels = c(\"MCMC\", \"Laplace Approx.\", \"Pathfinder\", \"Variational Inference\"))) |> \n  pivot_longer(starts_with(\"beta\"), names_to = \"Parameter\", values_to = \"Draws\") |>\n  ggplot() +\n  geom_density(aes(x = Draws, fill = Algorithm), alpha = 0.4) +\n  facet_wrap(~Parameter, scales = \"free\")\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Wrapping up\n\nThe three lectures using linear models was built to show nearly all parts needed in a Bayesian analysis\n\n-   MCMC specifications\n\n-   Prior specifications\n\n-   Assessing MCMC convergence\n\n-   Reporting MCMC results\n\n-   Determining if a model fits the data (absolute fit)\n\n-   Determining which model fits the data better (relative fit)\n\nAll of these topics will be with us when we start model complicated models in our future lecture.\n\n------------------------------------------------------------------------\n\n## Next Class\n\n1.  Generalized measurement models\n\n## Reference\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}