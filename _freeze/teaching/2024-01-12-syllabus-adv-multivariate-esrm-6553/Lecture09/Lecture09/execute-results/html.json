{
  "hash": "7f1c6f6ebff9b7299997fe9249f9adb2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 09\"\nsubtitle: \"Generalized Measurement Models: Modeling Observed Dichotomous Data\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\nexecute: \n  echo: false\n  eval: false\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: false  # choose \"false \"if want to show all together\n    transition: slide\n    background-transition: fade\n    theme: [simple, ../pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    scrollable: true\n    slide-number: true\n    chalkboard: true\n    number-sections: false\n    code-line-numbers: true\n    code-annotations: below\n    code-copy: true\n    code-summary: ''\n    highlight-style: arrow\n    view: 'scroll' # Activate the scroll view\n    scrollProgress: true # Force the scrollbar to remain visible\n    mermaid:\n      theme: neutral\n#bibliography: references.bib\n---\n\n\n\n\n\n## Previous Class\n\n1.  Dive deep into factor scoring\n2.  Show how different initial values affect Bayesian model estimation\n3.  Show how parameterization differs for standardized latent variables vs. marker item scale identification\n\n## Today's Lecture Objectives\n\n1.  Show how to estimate unidimensional latent variable models with dichotomous data\n    1.  Also know as [Item response theory]{.underline} (IRT) or [Item factor analysis]{.underline} (IFA)\n2.  Show how to estimate different parameterizations of IRT/IFA models\n3.  Describe how to obtain IRT/IFA auxiliary statistics from Markov Chains\n4.  Show variations of various dichotomous-data models.\n\n## Example Data: Conspiracy Theories\n\n-   Today's example is from a bootstrap resample of 177 undergraduate students at a large state university in the Midwest.\n-   The survey was a measure of 10 questions about their beliefs in various conspiracy theories that were being passed around the internet in the early 2010s\n-   All item responses were on a 5-point Likert scale with:\n    1.  Strong Disagree $\\rightarrow$ 0\n    2.  Disagree $\\rightarrow$ 0\n    3.  Neither Agree nor Disagree $\\rightarrow$ 0\n    4.  Agree $\\rightarrow$ 1\n    5.  Strongly Agree $\\rightarrow$ 1\n-   The purpose of this survey was to study individual beliefs regarding conspiracies.\n-   Our purpose in using this instrument is to provide a context that we all may find relevant as many of these conspiracies are still prevalent.\n\n## Make Our Data Dichotomous ([not a good idea in practice]{.underline})\n\nTo show dichotomous-data models with our data, we will arbitrarilly dichotomize our item responses:\n\n-   {0}: Response is *Strongly disagree or disagree, or Neither* (1-3)\n\n-   {1}: Response is *Agree, or Strongly agree* (4-5)\n\nNow, we could argue that a **1** represents someone who agrees with a statement and **0** represents someone who disagrees or is neutral.\n\nNote that this is only for illustrative purpose, such dichotomization shouldn't be done because\n\n-   There are distributions for multinomial categories\n\n-   The results will reflect more of our choice for 0/1\n\nBut we first learn dichotomous data models before we get to models for polytomous models.\n\n## Examining Dichotomous Data\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nhere() starts at /Users/jihong/Documents/Projects/website-jihong\n\nLoading required package: Rcpp\n\nThis is blavaan 0.5-8\n\nOn multicore systems, we suggest use of future::plan(\"multicore\") or\n  future::plan(\"multisession\") for faster post-MCMC computations.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-1-1.png){width=1728}\n:::\n:::\n\n\n\n\n\n::: callout-note\n## Note\n\nThese items have a relatively low proportion of people agreeing with each conspiracy statement\n\n-   Highest mean: .69\n\n-   Lowest mean: .034\n:::\n\n## Dichotomous Data Distribution: Bernoulli\n\nThe Bernoulli distribution is a one-trial version of the Binomial distribution\n\n-   Sample space (support) $Y \\in {0,1}$\n\nThe probability mass function:\n\n$$\nP(Y=y)=\\pi^y(1-\\pi)^{1-y}\n$$\n\nThe Bernoulli distribution has only one parameter: $\\pi$ (typically, known as the probability of success: Y=1)\n\n-   Mean of the distribution: $E(Y)=\\pi$\n\n-   Variance of the distribution: $Var(Y)=\\pi(1-\\pi)$\n\n## Definition: Dichotomous vs. Binary\n\nNote the definitions of some of the words for data with two values:\n\n-   Dichotomous: Taking two values (without numbers attached)\n\n-   Binary: either zero or one\n\nTherefore:\n\n1.  Not all dichotomous variable are binary, i.e., {2,7} is a dichotomous but not binary variable\n2.  All binary variables are dichotomous\n\nFinally:\n\n1.  Bernoulli distributions are for binary variables\n2.  Most dichotomous variables can be recorded as binary variables without loss of model effects\n\n## Models with Bernoulli Distributions\n\nGeneralized linear models using Bernoulli distributions put a linear model onto a transformation of the mean\n\n-   [Link function]{.underline} maps the mean $E(Y)$ from its original range of \\[0,1\\] to (-$\\infty$, $\\infty$);\n\n-   For an unconditional (empty) model, this is shown here:\n\n$$\nf(E(Y)) =f(\\pi)\n$$\n\n## Link Functions for Bernoulli Distributions\n\nCommon choices for the link function in [latent variable modeling]{.underline}:\n\n1.  Logit (or log odds):\n\n$$\nf(\\pi)=\\log(\\frac\\pi{1-\\pi})\n$$\n\n2.  Probit:\n\n$$\nf(\\pi)=\\Phi^{-1}(\\pi)\n$$\n\nWhere $\\Phi$ is the inverse cumulative distribution of a standard normal distribution\n\n$$\n\\boldsymbol{\\Phi}(Z)=\\int_{-\\infty}^Z\\frac1{\\sqrt{2\\pi}}\\exp(\\frac{-x^2}{2})dx\n$$\n\n------------------------------------------------------------------------\n\n### Visualization of Logit and Probit\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n------------------------------------------------------------------------\n\n## Less Common Link Functions\n\nIn the generalized linear models literature, there are a number of different link functions:\n\n-   Log-log: $f(\\pi)=-\\log(-\\log(\\pi))$\n\n-   Complementary Log-log: $f(\\pi)=\\log(-\\log(1-\\pi))$\n\nMost of these seldom appear in latent variable models\n\n-   Each has a slightly different curve shape\n\n## Inverse Link Functions\n\nOur latent variable models will be defined on the scale of the link function\n\n-   Sometimes we wish to convert back to the scale of the data\n\n-   Example: **Test characteristic curves** mapping $\\theta_p$ onto an expected test score\n\nFor this, we need the inverse link function\n\n-   Logit (or log odds) link function:\n\n$$\n\\text{logit}(\\pi)=\\log(\\frac\\pi{1-\\pi})\n$$\n\n-   Logit (or log odds) inverse link function:\n\n$$\n\\pi=\\frac{\\exp(logit(\\pi))}{1+\\exp(logit(\\pi))} \\\\\n= \\frac1{1+\\exp(-logit(\\pi))} \\\\\n= (1 + \\exp(-logit(\\pi)))^{-1}\n$$\n\n# Latent Variable Models with Bernoulli Distributions\n\n## Define Latent Variable Models with Bernoulli Distributions\n\nTo define a LVM for binary responses using a Bernoulli Distribution\n\n-   To start, we will use the logit link function\n\n-   We will begin with the linear predictor we had from the normal distribution models ([*Confirmatory factor analysis*]{.underline}: $\\mu_i + \\lambda_i\\theta_p)$\n\nFor an item $i$ and a person $p$, the model becomes:\n\n$$\nP(Y_{pi}=1|\\theta_p) = \\text{logit}^{-1}(\\mu_i + \\lambda_i\\theta_p)\n$$\n\n-   Note: the mean $\\pi_i$ is replaced by $P(Y_{pi}=1|\\theta_p)$\n\n    -   This is the mean of the observed variable, conditional on $\\theta_p$;\n\n-   The item intercept (easiness, location) is $\\mu_i$: the expected logit when $\\theta_p=0$\n\n-   The item discrimination is $\\lambda_i$: the change in the logit for a one-unit increase in $\\theta_p$\n\n## Extension: A more general form\n\nA 3-PL Item Response Theory Model with same statistical form but different notations:\n\n$$\nP(Y_{pi}=1|\\theta_p,c_i,a_i,b_j)=c_i+(1-c_i)\\text{logit}^{-1}(\\alpha_i\\theta_p+d_i)\n$$\n\n$$\nP(Y_{pi}=1|\\theta_p,c_i,a_i,b_j)=c_i+(1-c_i)\\text{logit}^{-1}(\\alpha_i(\\theta_p-b_i))\n$$\n\nwhere\n\n-   $\\theta_p$ is the latent variable for examinee $p$, representing the examinee's proficiency such that higher values indicate more proficency\n\n-   $a_i$, $d_i$, $c_i$ are item parameters:\n\n    -   $a_i$: the capability of item to discriminate between examinees with lower and higher values along the latent variables;\n\n    -   $d_i$: item \"easiness\"\n\n    -   $b_i$: item \"difficulty\", $b_i=d_i/(-a_i)$\n\n    -   $c_i$: \"pseudo-guessing\" parameter – examinees with low proficiency may have a nonzero probability of a correct response due to guessing\n\n## Model Family Names\n\nDepending on your field, the model from the previous slide can be called:\n\n-   The two-parameter logistic (2PL) model with slope/intercept parameterization\n\n-   An item factor model\n\nThese names reflect the terms given to the model in diverging literature:\n\n-   2PL: Education measurement\n\n> Birnbaum, A. (1968). Some Latent Trait Models and Their Use in Inferring an Examinee’s Ability. In F. M. Lord & M. R. Novick (Eds.), Statistical Theories of Mental Test Scores (pp. 397-424). Reading, MA: Addison-Wesley.\n\n-   Item factor analysis: Psychology\n\n> Christofferson, A.(1975). Factor analysis of dichotomous variables. Psychometrika , 40, 5-22.\n\nEstimation methods are the largest difference between the two families.\n\n## Differences from Normal Distributions\n\nRecall our [normal distribution models]{.underline}:\n\n$$\nY_{pi}=\\mu_i+\\lambda_i\\theta_p+e_{p,i};\\\\\ne_{p,i}\\sim N(0, \\psi_i^2)\n$$\n\nCompared to our [Bernoulli distribution models]{.underline}:\n\n$$\nlogit(P(Y_{pi}=1))=\\mu_i+\\lambda_i\\theta_p\n$$\n\nDifferences:\n\n1.  No residual (unique) variance components $\\psi_i^2$ in Bernoulli distribution;\n2.  Only one parameter in the distribution; variance is a function of the mean;\n3.  Identity link function in normal distribution: $f(E(Y_{pi}|\\theta_p))=E(Y_{pi}|\\theta_p)$\n    -   Model scale and data scale are the same\n4.  Logit link function in Bernoulli distribution\n    -   Model scale is different from data scale\n\n## From Model Scale to Data Scale\n\nCommonly, the IRT or IFA model is shown on the data scale (using the inverse link function):\n\n$$\nP(Y_{pi}=1)=\\frac{\\exp(\\mu_i+\\lambda_i\\theta_p)}{1+\\exp(\\mu_i+\\lambda_i\\theta_p)}\n$$\n\nThe core of the model (the terms in the exponent on the right-hand side) is the same\n\nModels are equivalent:\n\n1.  $P(Y_{pi}=1)$ is on the data scale;\n2.  $logit(P(Y_{pi}=1))$ is on the model (link) scale;\n\n## Modeling All Data\n\nAs with the normal distribution (CFA) models, we use the Bernoulli distribution for all observed variables:\n\n$$\nlogit(P(Y_{p1}=1))=\\mu_1+\\lambda_1\\theta_p \\\\\nlogit(P(Y_{p2}=1))=\\mu_2+\\lambda_2\\theta_p \\\\\nlogit(P(Y_{p3}=1))=\\mu_3+\\lambda_3\\theta_p \\\\\nlogit(P(Y_{p4}=1))=\\mu_4+\\lambda_4\\theta_p \\\\\nlogit(P(Y_{p5}=1))=\\mu_5+\\lambda_5\\theta_p \\\\\n\\dots \\\\\nlogit(P(Y_{p10}=1))=\\mu_{10}+\\lambda_{10}\\theta_p \\\\\n$$\n\n## Measurement Model Analysis Procedure\n\n1.  Specify model\n2.  Specify scale identification method for latent analysis\n3.  Estimate model\n4.  Examine model-data fit\n5.  Iterate between steps 1-4 until adequate fit is achieved\n\n**Measurement Model Auxiliary Steps:**\n\n1.  Score estimation (and secondary analysis with scores)\n2.  Item evaluation\n3.  Scale construction\n4.  Equating\n5.  Measurement invariance / differential item functioning\n\n## Model Specification\n\nThe set of equations on the previous slide formed [**Step #1**]{.underline} of the Measurement Model Analysis\n\n1.  Specify Model\n\nThe next step is:\n\n2.  Specify scale identification method for latent variables\n\nWe will initially assume $\\theta_p \\sim N(0,1)$, which allows us to estimate all item parameters of the model, that we call standardization\n\n## Likelihood Functions\n\nThe likelihood of [**item 1**]{.underline} is the function of production of all individuals' responses:\n\n$$\nf(Y_{pi}|\\lambda_1)=\\prod_{p=1}^{P}(\\pi_{p1})^{Y_{p1}}(1-\\pi_{p1})^{1-Y_{p1}}\n$$ {#eq-likelihood}\n\nTo simplify @eq-likelihood, we take the log:\n\n$$\n\\log f(Y_{pi}|\\lambda_1)=\\Sigma_{p-1}^{P}\\log[(\\pi_{p1})^{Y_{pi}}(1-\\pi_{p1})^{1-Y_{pi}}]\n$$ {#eq-loglikelihood}\n\nSince we know from logit function that:\n\n$$\n\\pi_{pi}=\\frac{\\exp(\\mu_1+\\lambda_1\\theta_p)}{1+\\exp(\\mu_1+\\lambda_1\\theta_p)}\n$$\n\nWhich then becomes:\n\n$$\n\\log f(Y_{pi}|\\lambda_1)=\\Sigma_{p-1}^{P}\\log[(\\frac{\\exp(\\mu_1+\\lambda_1\\theta_p)}{1+\\exp(\\mu_1+\\lambda_1\\theta_p)})^{Y_{pi}}(1-\\frac{\\exp(\\mu_1+\\lambda_1\\theta_p)}{1+\\exp(\\mu_1+\\lambda_1\\theta_p)})^{1-Y_{pi}}]\n$$\n\n## Model (Data) Log Likelihood Functions\n\nAs an example for $\\lambda_1$:\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Model (Data) Log Likelihood Function for $\\theta$\n\nFor each person, the same model likelihood function is used\n\n-   Only now it varies across each item response\n\n-   Example: Person 1\n\n$$\nf(Y_{1i}|\\theta_1)=\\prod_{i=1}^{I}(\\pi_{1i})^{Y_{1i}}(1-\\pi_{1i})^{1-Y_{1i}}\n$$\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n# Implementing Bernoulli Outcomes in Stan\n\n## Stan's `model` Block\n\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nmodel {\n  \n  lambda ~ multi_normal(meanLambda, covLambda); // Prior for item discrimination/factor loadings\n  mu ~ multi_normal(meanMu, covMu);             // Prior for item intercepts\n  \n  theta ~ normal(0, 1);                         // Prior for latent variable (with mean/sd specified)\n  \n  for (item in 1:nItems){\n    Y[item] ~ bernoulli_logit(mu[item] + lambda[item]*theta);\n  }\n  \n}\n```\n:::\n\n\n\n\n\nFor logit models without lower / upper asymptote parameters, `Stan` has a convenient `bernoulli_logit` function\n\n-   Automatically has the link function embedded\n\n-   The catch: The data has to be defined as an integer\n\nAlso, note that there are few differences from the model with normal outcomes (CFA)\n\n-   No $\\psi$ parameters\n\n## Stan's `parameters` Block\n\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\nparameters {\n  vector[nObs] theta;                // the latent variables (one for each person)\n  vector[nItems] mu;                 // the item intercepts (one for each item)\n  vector[nItems] lambda;             // the factor loadings/item discriminations (one for each item)\n}\n```\n:::\n\n\n\n\n\nOnly change from normal outcomes (CFA) model:\n\n-   No $\\psi$ (psi) parameters\n\n## Stan's `data{}` Block\n\n\n\n\n\n::: {.cell output.var='display'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> nObs;                            // number of observations\n  int<lower=0> nItems;                          // number of items\n  array[nItems, nObs] int<lower=0, upper=1>  Y; // item responses in an array\n\n  vector[nItems] meanMu;             // prior mean vector for intercept parameters\n  matrix[nItems, nItems] covMu;      // prior covariance matrix for intercept parameters\n  \n  vector[nItems] meanLambda;         // prior mean vector for discrimination parameters\n  matrix[nItems, nItems] covLambda;  // prior covariance matrix for discrimination parameters\n}\n```\n:::\n\n\n\n\n\nOne difference from normal outcome model:\n\n`array[nItems, nObs] int<lower=0, upper=1>  Y;`\n\n-   Arrays are types of matrices (with more than two dimensions possible)\n\n    -   Allows for different types of data (here Y are integers)\n\n        -   Integer-valued variables needed for `bernoulli_logit()` function\n\n-   Arrays are row-major (meaning order of items and persons is switched)\n\n    -   Can define differently\n\n## Change to Data List for Stan Import\n\nThe switch of items and observations in the `array` statement means the data imported have to be transposed:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelIRT_2PL_SI_data = list(\n  nObs = nObs,\n  nItems = nItems,\n  Y = t(conspiracyItemsDichtomous), \n  meanMu = muMeanVecHP,\n  covMu = muCovarianceMatrixHP,\n  meanLambda = lambdaMeanVecHP,\n  covLambda = lambdaCovarianceMatrixHP\n)\n```\n:::\n\n\n\n\n\n## Running the Model in `Stan`\n\nThe `Stan` program takes longer to run than in linear models:\n\n-   Number of parameters: 197\n\n    -   10 observed variables: $\\mu_i$ and $\\lambda_i$ for $i = 1\\dots10$\n\n    -   177 latent variables: $\\theta_p$ for $p=1\\dots177$\n\n-   `cmdstanr` samples call:\n\n\n\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| eval: false\nmodelIRT_2PL_SI_samples = modelIRT_2PL_SI_stan$sample(\n  data = modelIRT_2PL_SI_data,\n  seed = 02112022,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 5000,\n  iter_sampling = 5000,\n  init = function() list(lambda=rnorm(nItems, mean=5, sd=1))\n)\n```\n````\n:::\n\n\n\n\n\n-   Note: typically, longer chains are needed for larger models like this\n\n-   Note: Starting values added (mean of 5 is due to logit function limits)\n\n    -   Helps keep definition of parameters (stay away from opposite mode)\n\n    -   Too large of value can lead to `NaN` values (exceeding numerical precision)\n\n## Model Results\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'cmdstanr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is cmdstanr version 0.8.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan path: /Users/jihong/.cmdstan/cmdstan-2.36.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan version: 2.36.0\n```\n\n\n:::\n:::\n\n\n\n\n\nCheck convergence with $\\hat R$ (PSRF):\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      rhat       \n Min.   :0.9999  \n 1st Qu.:1.0001  \n Median :1.0002  \n Mean   :1.0002  \n 3rd Qu.:1.0004  \n Max.   :1.0010  \n```\n\n\n:::\n:::\n\n\n\n\n\n-   Item Parameter Results:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 10\n   variable    mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail\n   <chr>      <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>\n 1 mu[1]      -2.27  -2.23 0.389 0.373  -2.96 -1.69  1.00   10459.   10267.\n 2 mu[2]      -3.98  -3.87 0.825 0.767  -5.47 -2.82  1.00   10367.    9220.\n 3 mu[3]      -4.36  -4.24 0.909 0.853  -6.03 -3.11  1.00    9487.    8934.\n 4 mu[4]      -5.62  -5.44 1.34  1.26   -8.07 -3.77  1.00   11126.    9699.\n 5 mu[5]      -6.91  -6.62 1.90  1.77  -10.4  -4.36  1.00    9307.    9026.\n 6 mu[6]      -5.70  -5.48 1.41  1.29   -8.30 -3.83  1.00    9499.    8609.\n 7 mu[7]      -6.10  -5.86 1.56  1.44   -8.99 -4.01  1.00    9993.    8759.\n 8 mu[8]      -9.71  -9.42 2.62  2.60  -14.5  -5.92  1.00   14173.   11574.\n 9 mu[9]      -5.87  -5.66 1.49  1.38   -8.65 -3.82  1.00   10408.    9160.\n10 mu[10]     -4.99  -4.83 1.12  1.04   -7.01 -3.46  1.00   10083.    8465.\n11 lambda[1]   1.71   1.67 0.435 0.423   1.06  2.47  1.00    7367.    9356.\n12 lambda[2]   2.67   2.59 0.740 0.692   1.64  4.02  1.00    7839.    8497.\n13 lambda[3]   2.42   2.33 0.759 0.705   1.36  3.80  1.00    7624.    7123.\n14 lambda[4]   3.71   3.57 1.08  1.03    2.20  5.68  1.00    8935.    9175.\n15 lambda[5]   5.43   5.19 1.62  1.51    3.23  8.41  1.00    8350.    9162.\n16 lambda[6]   3.40   3.24 1.08  0.993   1.94  5.40  1.00    7379.    7790.\n17 lambda[7]   3.71   3.55 1.16  1.09    2.11  5.84  1.00    8614.    8116.\n18 lambda[8]   5.41   5.24 1.70  1.67    2.93  8.51  1.00   11540.   11101.\n19 lambda[9]   4.64   4.47 1.29  1.21    2.86  7.00  1.00    8560.    9090.\n20 lambda[10]  2.83   2.72 0.874 0.825   1.59  4.41  1.00    8457.    8456.\n```\n\n\n:::\n:::\n\n\n\n\n\n## Modeling Strategy vs. Didactic Strategy\n\nAt this point, one should investigate model fit of the model we just ran (PPP, WAIC, LOO)\n\n-   If the model does not fit, then all model parameters could be biased\n\n    -   Both item parameters and person parameters ($\\mu_i$, $\\lambda_i$, $\\theta_p$)\n\n-   Moreover, the uncertainty accompanying each parameter (the posterior standard deviation) may also be biased\n\n    -   Especially bad for psychometric models as we quantify reliability with these numbers\n\n## Investigating Item Parameters\n\nOne plot that can help provide information about the item parameters is the item characteristic curve (ICC)\n\n-   The ICC is the plot of the expected value of the response conditional on the value of the latent traits, for a range of latent trait values\n\n$$\nE(Y_{pi}|\\theta_p)=\\frac{\\exp(\\mu_i+\\lambda_i\\theta_p)}{1+\\exp(\\mu_i+\\lambda_i\\theta_p)}\n$$\n\n-   Because we have sampled values for each parameter, we can plot one ICC for each posterior draws\n\n## Posterior ICC Plots\n\n![ICC for 10 items](Code/expected_scores_ICC.png){fig-align=\"center\"}\n\n## Item 5 ICC\n\n![](Code/ICC_Item5.png){fig-align=\"center\"}\n\n# Investigating the Item Parameters\n\n## Trace plots for $\\mu_i$\n\n![](Code/traceplot_mu.png){fig-align=\"center\"}\n\n## Density plots for $\\mu_i$\n\n![](Code/densityplot_mu.png){fig-align=\"center\"}\n\n## Trace plots for $\\lambda_i$\n\n## ![](Code/traceplot_lambda.png)\n\n## Density plots for $\\lambda_i$\n\n![](Code/densityplot_lambda.png){fig-align=\"center\"}\n\n## Bivariate plots for $\\mu_i$ and $\\lambda_i$\n\n![](Code/bivariate_lambda1_mu1.png){fig-align=\"center\"}\n\n## Latent Variables\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 177 × 10\n    variable     mean median    sd   mad      q5   q95  rhat ess_bulk ess_tail\n    <chr>       <dbl>  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>    <dbl>    <dbl>\n  1 theta[1]   -0.457 -0.393 0.771 0.788 -1.84   0.686  1.00   30500.   13873.\n  2 theta[2]    1.47   1.46  0.238 0.233  1.09   1.86   1.00    8777.   11807.\n  3 theta[3]    1.51   1.50  0.244 0.241  1.12   1.92   1.00    8370.   11997.\n  4 theta[4]   -0.454 -0.382 0.768 0.771 -1.84   0.671  1.00   30720.   14251.\n  5 theta[5]   -0.451 -0.384 0.768 0.774 -1.83   0.680  1.00   30700.   12840.\n  6 theta[6]   -0.449 -0.381 0.766 0.772 -1.81   0.681  1.00   30823.   13118.\n  7 theta[7]    0.261  0.343 0.556 0.527 -0.772  1.02   1.00   20344.   12386.\n  8 theta[8]    0.480  0.554 0.485 0.434 -0.430  1.13   1.00   19416.   11766.\n  9 theta[9]   -0.461 -0.396 0.761 0.785 -1.81   0.662  1.00   32046.   14796.\n 10 theta[10]  -0.455 -0.391 0.774 0.802 -1.84   0.677  1.00   31176.   14674.\n 11 theta[11]  -0.450 -0.385 0.762 0.787 -1.80   0.673  1.00   30931.   13838.\n 12 theta[12]  -0.448 -0.379 0.767 0.786 -1.83   0.676  1.00   32110.   13432.\n 13 theta[13]  -0.449 -0.369 0.779 0.794 -1.85   0.692  1.00   33002.   12115.\n 14 theta[14]  -0.463 -0.395 0.775 0.787 -1.83   0.685  1.00   32905.   13840.\n 15 theta[15]  -0.458 -0.396 0.770 0.784 -1.83   0.680  1.00   30779.   14384.\n 16 theta[16]  -0.454 -0.387 0.763 0.776 -1.82   0.677  1.00   32739.   14097.\n 17 theta[17]  -0.459 -0.383 0.771 0.786 -1.84   0.680  1.00   31754.   14551.\n 18 theta[18]  -0.455 -0.390 0.758 0.774 -1.80   0.669  1.00   30244.   12924.\n 19 theta[19]   1.26   1.26  0.256 0.248  0.831  1.67   1.00    9820.   12100.\n 20 theta[20]  -0.459 -0.395 0.763 0.771 -1.82   0.676  1.00   29995.   13659.\n 21 theta[21]  -0.459 -0.385 0.775 0.788 -1.86   0.684  1.00   30952.   13566.\n 22 theta[22]  -0.457 -0.388 0.771 0.781 -1.84   0.679  1.00   29283.   14053.\n 23 theta[23]  -0.455 -0.385 0.765 0.779 -1.82   0.674  1.00   29673.   13955.\n 24 theta[24]  -0.457 -0.394 0.770 0.778 -1.84   0.683  1.00   28874.   13416.\n 25 theta[25]   0.250  0.328 0.562 0.533 -0.797  1.02   1.00   21958.   11757.\n 26 theta[26]   1.10   1.11  0.268 0.250  0.643  1.51   1.00   13632.   12826.\n 27 theta[27]   0.746  0.790 0.374 0.340  0.0633 1.27   1.00   18359.   11203.\n 28 theta[28]  -0.453 -0.376 0.777 0.773 -1.87   0.688  1.00   31732.   14297.\n 29 theta[29]   0.850  0.888 0.340 0.309  0.244  1.34   1.00   16689.   11256.\n 30 theta[30]   1.42   1.42  0.237 0.231  1.04   1.81   1.00    8920.   11391.\n 31 theta[31]  -0.457 -0.387 0.761 0.770 -1.82   0.674  1.00   32384.   15059.\n 32 theta[32]  -0.458 -0.391 0.778 0.796 -1.84   0.692  1.00   33305.   12106.\n 33 theta[33]  -0.456 -0.385 0.765 0.772 -1.82   0.681  1.00   30544.   14436.\n 34 theta[34]  -0.454 -0.384 0.771 0.790 -1.82   0.689  1.00   30237.   14405.\n 35 theta[35]  -0.457 -0.388 0.775 0.784 -1.85   0.679  1.00   31052.   15148.\n 36 theta[36]  -0.457 -0.395 0.769 0.790 -1.84   0.681  1.00   32515.   13230.\n 37 theta[37]  -0.451 -0.378 0.773 0.791 -1.83   0.687  1.00   32388.   14023.\n 38 theta[38]  -0.461 -0.390 0.771 0.782 -1.83   0.681  1.00   30644.   13735.\n 39 theta[39]  -0.455 -0.386 0.770 0.785 -1.82   0.676  1.00   28524.   13479.\n 40 theta[40]  -0.459 -0.390 0.774 0.783 -1.84   0.674  1.00   30985.   14185.\n 41 theta[41]  -0.455 -0.387 0.773 0.771 -1.86   0.681  1.00   32230.   13360.\n 42 theta[42]  -0.461 -0.389 0.766 0.784 -1.84   0.666  1.00   29580.   13829.\n 43 theta[43]  -0.450 -0.383 0.759 0.773 -1.82   0.667  1.00   31264.   14672.\n 44 theta[44]  -0.450 -0.383 0.761 0.770 -1.81   0.665  1.00   31665.   13545.\n 45 theta[45]   0.980  1.00  0.299 0.278  0.455  1.42   1.00   13989.   11341.\n 46 theta[46]  -0.453 -0.390 0.769 0.780 -1.83   0.685  1.00   28889.   12706.\n 47 theta[47]  -0.448 -0.384 0.764 0.787 -1.80   0.683  1.00   32802.   13739.\n 48 theta[48]  -0.441 -0.377 0.763 0.794 -1.79   0.692  1.00   29030.   13880.\n 49 theta[49]  -0.454 -0.381 0.771 0.772 -1.85   0.670  1.00   29003.   14052.\n 50 theta[50]   0.851  0.886 0.339 0.307  0.241  1.34   1.00   15680.   10429.\n 51 theta[51]  -0.451 -0.384 0.770 0.785 -1.83   0.684  1.00   32348.   14439.\n 52 theta[52]  -0.453 -0.385 0.767 0.780 -1.82   0.675  1.00   32585.   13420.\n 53 theta[53]  -0.452 -0.386 0.767 0.783 -1.81   0.680  1.00   30511.   13102.\n 54 theta[54]  -0.452 -0.394 0.772 0.797 -1.82   0.701  1.00   32147.   14392.\n 55 theta[55]  -0.442 -0.371 0.759 0.759 -1.79   0.676  1.00   31541.   13599.\n 56 theta[56]  -0.455 -0.389 0.774 0.790 -1.83   0.689  1.00   31674.   13981.\n 57 theta[57]  -0.446 -0.378 0.760 0.771 -1.80   0.666  1.00   30740.   14430.\n 58 theta[58]  -0.463 -0.399 0.775 0.782 -1.84   0.681  1.00   32070.   14093.\n 59 theta[59]   0.249  0.330 0.567 0.539 -0.793  1.03   1.00   21373.   11756.\n 60 theta[60]  -0.451 -0.384 0.760 0.766 -1.82   0.666  1.00   29591.   12609.\n 61 theta[61]   1.18   1.19  0.256 0.240  0.744  1.58   1.00   11702.   11992.\n 62 theta[62]  -0.452 -0.384 0.770 0.774 -1.83   0.682  1.00   30832.   13753.\n 63 theta[63]  -0.460 -0.389 0.775 0.789 -1.84   0.679  1.00   28320.   13207.\n 64 theta[64]   1.10   1.11  0.263 0.249  0.650  1.51   1.00   13077.   12994.\n 65 theta[65]  -0.460 -0.393 0.770 0.791 -1.83   0.684  1.00   33409.   14098.\n 66 theta[66]  -0.457 -0.382 0.764 0.776 -1.84   0.665  1.00   30183.   14007.\n 67 theta[67]  -0.456 -0.384 0.759 0.770 -1.82   0.670  1.00   30756.   14791.\n 68 theta[68]  -0.450 -0.375 0.771 0.780 -1.84   0.684  1.00   31975.   14152.\n 69 theta[69]  -0.455 -0.385 0.768 0.770 -1.83   0.678  1.00   32341.   14187.\n 70 theta[70]  -0.454 -0.385 0.769 0.781 -1.83   0.674  1.00   29753.   14084.\n 71 theta[71]  -0.456 -0.386 0.766 0.779 -1.82   0.673  1.00   30752.   14284.\n 72 theta[72]   1.27   1.27  0.239 0.230  0.873  1.66   1.00   10442.   11963.\n 73 theta[73]  -0.446 -0.380 0.764 0.770 -1.82   0.674  1.00   29411.   13401.\n 74 theta[74]  -0.452 -0.388 0.766 0.777 -1.82   0.689  1.00   31288.   13629.\n 75 theta[75]  -0.449 -0.381 0.760 0.768 -1.81   0.672  1.00   29867.   13113.\n 76 theta[76]   2.33   2.29  0.382 0.364  1.77   3.01   1.00   10971.   12287.\n 77 theta[77]  -0.469 -0.402 0.781 0.796 -1.86   0.689  1.00   26618.   13635.\n 78 theta[78]   0.743  0.787 0.375 0.339  0.0574 1.27   1.00   18948.   11638.\n 79 theta[79]   1.26   1.26  0.253 0.246  0.837  1.67   1.00   10294.   12686.\n 80 theta[80]  -0.452 -0.384 0.767 0.779 -1.83   0.673  1.00   32540.   14006.\n 81 theta[81]  -0.459 -0.393 0.767 0.784 -1.83   0.672  1.00   31333.   13892.\n 82 theta[82]  -0.454 -0.389 0.762 0.778 -1.81   0.675  1.00   34913.   14416.\n 83 theta[83]  -0.455 -0.383 0.771 0.788 -1.83   0.685  1.00   30564.   14183.\n 84 theta[84]   1.58   1.57  0.241 0.238  1.20   1.99   1.00    8749.   10292.\n 85 theta[85]   0.482  0.551 0.475 0.440 -0.401  1.13   1.00   19151.   12502.\n 86 theta[86]  -0.455 -0.390 0.765 0.784 -1.81   0.686  1.00   32271.   13338.\n 87 theta[87]   0.419  0.493 0.505 0.474 -0.520  1.10   1.00   18565.   12167.\n 88 theta[88]  -0.447 -0.370 0.761 0.761 -1.80   0.684  1.00   31629.   14820.\n 89 theta[89]  -0.457 -0.390 0.772 0.779 -1.83   0.676  1.00   30883.   12640.\n 90 theta[90]   0.250  0.331 0.563 0.537 -0.787  1.03   1.00   23189.   12188.\n 91 theta[91]  -0.450 -0.382 0.768 0.786 -1.82   0.682  1.00   32606.   14400.\n 92 theta[92]   1.15   1.16  0.260 0.250  0.708  1.55   1.00   11996.   11953.\n 93 theta[93]  -0.457 -0.384 0.772 0.779 -1.84   0.689  1.00   32992.   14280.\n 94 theta[94]   1.95   1.93  0.296 0.287  1.51   2.47   1.00    8267.   11311.\n 95 theta[95]   1.48   1.48  0.235 0.227  1.11   1.88   1.00    8689.   11447.\n 96 theta[96]  -0.456 -0.386 0.774 0.778 -1.83   0.682  1.00   28902.   14284.\n 97 theta[97]  -0.463 -0.390 0.779 0.788 -1.85   0.686  1.00   32083.   14443.\n 98 theta[98]  -0.457 -0.380 0.775 0.784 -1.85   0.675  1.00   31768.   15188.\n 99 theta[99]  -0.458 -0.386 0.772 0.771 -1.85   0.672  1.00   31462.   13287.\n100 theta[100] -0.458 -0.392 0.775 0.791 -1.84   0.689  1.00   33029.   14353.\n101 theta[101] -0.456 -0.388 0.769 0.794 -1.82   0.689  1.00   31082.   14923.\n102 theta[102]  1.85   1.84  0.279 0.274  1.42   2.33   1.00    7859.   11726.\n103 theta[103] -0.451 -0.384 0.764 0.786 -1.81   0.675  1.00   32884.   15056.\n104 theta[104]  1.49   1.48  0.236 0.230  1.11   1.88   1.00    9078.   11160.\n105 theta[105] -0.456 -0.392 0.771 0.791 -1.84   0.673  1.00   32176.   15433.\n106 theta[106]  0.830  0.868 0.350 0.321  0.201  1.33   1.00   15239.   11297.\n107 theta[107]  1.58   1.57  0.238 0.234  1.20   1.99   1.00    8139.   10489.\n108 theta[108]  0.255  0.335 0.561 0.536 -0.779  1.03   1.00   23309.   11922.\n109 theta[109] -0.455 -0.380 0.770 0.779 -1.85   0.680  1.00   29033.   13002.\n110 theta[110] -0.459 -0.389 0.776 0.780 -1.86   0.684  1.00   31004.   12993.\n111 theta[111] -0.456 -0.388 0.762 0.774 -1.81   0.676  1.00   30938.   15131.\n112 theta[112]  0.255  0.329 0.553 0.536 -0.763  1.02   1.00   23485.   13098.\n113 theta[113] -0.452 -0.383 0.767 0.783 -1.82   0.691  1.00   32840.   14285.\n114 theta[114] -0.451 -0.383 0.764 0.773 -1.80   0.678  1.00   29074.   13827.\n115 theta[115]  1.49   1.48  0.237 0.231  1.11   1.88   1.00    8501.   11957.\n116 theta[116] -0.451 -0.379 0.776 0.794 -1.85   0.681  1.00   32045.   14303.\n117 theta[117] -0.457 -0.388 0.767 0.777 -1.82   0.680  1.00   29228.   14169.\n118 theta[118] -0.454 -0.388 0.769 0.786 -1.82   0.690  1.00   32596.   14241.\n119 theta[119] -0.458 -0.394 0.772 0.794 -1.83   0.680  1.00   31877.   14171.\n120 theta[120] -0.452 -0.392 0.761 0.785 -1.81   0.675  1.00   29645.   13521.\n121 theta[121]  0.600  0.665 0.446 0.407 -0.230  1.21   1.00   17302.   11767.\n122 theta[122]  0.255  0.333 0.565 0.532 -0.792  1.03   1.00   21975.   11096.\n123 theta[123] -0.448 -0.384 0.757 0.771 -1.79   0.666  1.00   32792.   13790.\n124 theta[124] -0.453 -0.380 0.771 0.789 -1.83   0.676  1.00   30435.   14634.\n125 theta[125] -0.455 -0.384 0.766 0.785 -1.82   0.670  1.00   29946.   12441.\n126 theta[126]  0.256  0.336 0.561 0.528 -0.778  1.03   1.00   23100.   11474.\n127 theta[127]  0.829  0.867 0.353 0.324  0.199  1.33   1.00   15269.   11395.\n128 theta[128] -0.461 -0.392 0.768 0.779 -1.84   0.684  1.00   29751.   14796.\n129 theta[129] -0.452 -0.386 0.762 0.766 -1.81   0.679  1.00   28945.   13926.\n130 theta[130]  1.73   1.72  0.258 0.250  1.33   2.18   1.00    7808.   11121.\n131 theta[131]  0.257  0.340 0.557 0.525 -0.780  1.02   1.00   21890.   11814.\n132 theta[132]  1.48   1.47  0.234 0.232  1.11   1.88   1.00    8456.   11129.\n133 theta[133] -0.455 -0.383 0.762 0.767 -1.82   0.663  1.00   29630.   13547.\n134 theta[134]  1.96   1.93  0.302 0.292  1.51   2.50   1.00    8374.   10212.\n135 theta[135] -0.449 -0.379 0.763 0.771 -1.81   0.671  1.00   30292.   13797.\n136 theta[136] -0.458 -0.386 0.764 0.775 -1.83   0.666  1.00   31394.   13888.\n137 theta[137] -0.451 -0.386 0.773 0.781 -1.83   0.694  1.00   30874.   13888.\n138 theta[138] -0.463 -0.393 0.777 0.797 -1.86   0.680  1.00   30554.   13935.\n139 theta[139] -0.455 -0.389 0.764 0.774 -1.81   0.668  1.00   32699.   13416.\n140 theta[140]  0.480  0.548 0.479 0.437 -0.412  1.13   1.00   19021.   11683.\n141 theta[141] -0.462 -0.387 0.783 0.791 -1.87   0.688  1.00   31746.   13858.\n142 theta[142] -0.454 -0.379 0.766 0.776 -1.82   0.667  1.00   30357.   14008.\n143 theta[143] -0.443 -0.376 0.756 0.763 -1.81   0.667  1.00   31841.   14146.\n144 theta[144] -0.462 -0.390 0.781 0.785 -1.87   0.692  1.00   31672.   13110.\n145 theta[145] -0.454 -0.382 0.767 0.782 -1.83   0.678  1.00   32076.   14630.\n146 theta[146] -0.447 -0.377 0.756 0.772 -1.81   0.672  1.00   30655.   13786.\n147 theta[147] -0.448 -0.377 0.762 0.771 -1.82   0.672  1.00   31119.   13993.\n148 theta[148]  1.27   1.27  0.240 0.232  0.878  1.66   1.00   11330.   12311.\n149 theta[149] -0.454 -0.384 0.770 0.781 -1.85   0.674  1.00   30786.   13253.\n150 theta[150] -0.453 -0.390 0.758 0.768 -1.80   0.671  1.00   31690.   14703.\n151 theta[151]  0.255  0.335 0.552 0.527 -0.781  1.01   1.00   23165.   12652.\n152 theta[152] -0.449 -0.390 0.757 0.766 -1.79   0.672  1.00   30935.   14562.\n153 theta[153]  1.38   1.38  0.235 0.229  0.999  1.77   1.00    9483.    9798.\n154 theta[154] -0.458 -0.385 0.775 0.788 -1.85   0.686  1.00   29890.   13049.\n155 theta[155] -0.455 -0.390 0.776 0.784 -1.84   0.698  1.00   30947.   14572.\n156 theta[156] -0.456 -0.389 0.775 0.786 -1.84   0.686  1.00   31529.   13385.\n157 theta[157] -0.456 -0.384 0.764 0.772 -1.84   0.670  1.00   30730.   13870.\n158 theta[158] -0.457 -0.391 0.769 0.788 -1.83   0.676  1.00   30642.   14063.\n159 theta[159]  0.839  0.880 0.352 0.321  0.198  1.34   1.00   13546.   11720.\n160 theta[160] -0.459 -0.385 0.778 0.789 -1.85   0.675  1.00   31204.   14006.\n161 theta[161]  0.823  0.860 0.348 0.323  0.189  1.32   1.00   16493.   10876.\n162 theta[162]  0.416  0.495 0.515 0.478 -0.548  1.11   1.00   18067.   11124.\n163 theta[163] -0.451 -0.374 0.768 0.782 -1.82   0.674  1.00   31514.   14971.\n164 theta[164] -0.460 -0.387 0.775 0.788 -1.84   0.683  1.00   32284.   14222.\n165 theta[165]  0.850  0.884 0.337 0.315  0.237  1.33   1.00   17238.   12852.\n166 theta[166] -0.460 -0.390 0.777 0.799 -1.84   0.684  1.00   29410.   13047.\n167 theta[167] -0.457 -0.386 0.779 0.790 -1.86   0.697  1.00   30794.   13996.\n168 theta[168]  1.10   1.11  0.266 0.254  0.639  1.51   1.00   13294.   11903.\n169 theta[169] -0.453 -0.386 0.760 0.780 -1.80   0.669  1.00   30134.   13875.\n170 theta[170] -0.456 -0.385 0.773 0.791 -1.82   0.688  1.00   28767.   14319.\n171 theta[171]  0.761  0.811 0.382 0.347  0.0602 1.29   1.00   16166.   10466.\n172 theta[172] -0.449 -0.385 0.766 0.777 -1.81   0.684  1.00   32463.   13765.\n173 theta[173] -0.452 -0.386 0.770 0.778 -1.82   0.689  1.00   32465.   14506.\n174 theta[174] -0.452 -0.385 0.762 0.772 -1.82   0.669  1.00   30771.   14502.\n175 theta[175] -0.452 -0.389 0.768 0.774 -1.83   0.677  1.00   32278.   14695.\n176 theta[176] -0.458 -0.389 0.775 0.786 -1.84   0.688  1.00   32891.   14234.\n177 theta[177] -0.452 -0.389 0.761 0.778 -1.81   0.671  1.00   32562.   14337.\n```\n\n\n:::\n:::\n\n\n\n\n\n## EAP Estimates of Latent Variables\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Comparing Two Posterior Distributions\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Comparing EAP Estimates with Posterior SDs\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Comparing EAP Estimates with Sum Scores\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture09_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Extension: Factor Score\n\n1.  Thurston's Regression Method\n2.  Bartlett's Method (maximum-likelihood)\n3.  Bayesian approach\n\nSee more on my [website](https://jihongzhang.org/posts/2024-03-12-Factor-Score-Sum-Score-Reliability/)\n\n## Next Class\n\n-   Discrimination/Difficulty Parameterization\n\n## Resources\n\n-   [Dr. Templin's slide](https://jonathantemplin.github.io/Bayesian-Psychometric-Modeling-Course-Fall2022/lectures/lecture04b/04b_Modeling_Observed_Data#/title-slide)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}