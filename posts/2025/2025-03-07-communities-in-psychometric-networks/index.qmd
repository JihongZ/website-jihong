---
title: "Psychometric Networks: Community = Clustering?"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2025-03-07"
date-modified: "`{r} Sys.Date()`"
draft: false
bibliography: references.bib
image: index_files/figure-html/plot-1.png
tbl-cap-location: top
categories:
  - R
  - Network
citation:
  type: webpage
  issued: 2025-03-07
execute: 
  warning: false
  message: false  
format: 
  html:
    code-tools: false
    code-line-numbers: false
    code-fold: true
    code-summary: "Click this to see R code"
---

::: objectives
## Overview {.unnumbered}

A friend of mine recently asked me one question about network community: in her recent research, a network without community defined has different estimation with the one with community define. Why this happens? In this blog, I try to dive a little deeper into community issues in psychometric network analysis. You can find my previous post on the estimation methods of network analysis - [How To Choose Network Analysis Estimation For Application Research](../../../notes/2024/2024-04-04-Network-Estimation-Methods/index.qmd).

This [blog](https://psych-networks.com/r-tutorial-identify-communities-items-networks/) instead aims to talk about varied aspects of *community detection* in psychometric network analysis, including the following questions:

1.  What is "node community"?
2.  Why we need communities of nodes in network analysis?
3.  How communities are generated in the network?
4.  How to identify those communities given a network?
:::

::: macwindow
```{r plot}
#| fig-height: 7
#| fig-width: 10
library(psychonetrics)
# Load bfi data from psych package:
library(psychTools)
# Also load dplyr for the pipe operator:
library(dplyr)


data(bfi)
# Let's take the agreeableness items, and gender:
ConsData <- bfi |> 
  select(A1:A5, gender) |> 
  na.omit() # Let's remove missingness (otherwise use Estimator = "FIML)

# Define variables:
vars <- names(ConsData)[1:5]

# Saturated estimation:
mod_saturated <- ggm(ConsData, vars = vars)

# Run the model:
mod_saturated <- mod_saturated |> runmodel()

# Labels:
labels <- c(
  "indifferent to the feelings of others",
  "inquire about others' well-being",
  "comfort others",
  "love children",
  "make people feel at ease")
# We can also fit an empty network:
mod0 <- ggm(ConsData, vars = vars, omega = "zero")

# Run the model:
mod0 <- mod0 |> runmodel()

# To automatically add along modification indices, we can use stepup:
mod1 <- mod0 |> stepup()

# Let's also prune all non-significant edges to finish:
mod1 <- mod1 |> prune()

qgraph::qgraph(getmatrix(mod1, "omega"),
               layout="spring",
               groups = labels,
               title = "Big 5 agreeableness",
               theme = "Borkulo",
               bg = rgb(red = 145, # same to .macwindow class in lecture.scss
                        green = 203, 
                        blue = 215, 
                        alpha = .2196078431, 
                        maxColorValue = 255),
               transparency = TRUE)
```
:::

## Big problem in the network analysis

[A]{.bigger}s said in Eiko's post [@friedTutorialHowIdentify2016] and I cited here – authors sometimes *over-interpret* the network visualization of their data that some meaningful modules exist. This over-interpretation may have Type I error (mistakenly cluster nodes with actually weak connections in a community) or Type II error (fail to cluster nodes with strong connection into a community). In other words, researchers found that relying on visual inspection of network structure can not obtain reliable conclusion about which nodes should be clustered together and which ones should not be clustered, especially in a complicated network structure (say \> 20 nodes).

Identification of node clusters is not a new topic, which is typically called "*Community Detection*" in graph theory [@fortunatoCommunityDetectionGraphs2010]. This problem arises due to a phenomenon called "Community Structure"[@girvanCommunityStructureSocial2002] or "Clustering" was found in varied types of networks. This community structure has two characteristics:

::: rmdnote
1.  **Heterogeneity of node degree**: nodes with high degree (more neighbors/ high betweenness) coexist with nodes with low degree.
2.  **Heterogeneity of edge strength**: high concentrations of edges with certain "group" of nodes even though those "groups" are visual inspected.
:::

Given this community structure, *Community* (or called clusters or modules) is defined as **groups of nodes which probably share common properties and / or play similar roles within the graph** [@fortunatoCommunityDetectionGraphs2010]. This is actually a very broad definition which does not answer what are "common properties" and "similar roles" in a network graph. The reason of this vagueness is because of variety of network in different areas.

For example, within a social network of one person, one individual is considered as node and his/her social relationship with other people as edges, then "communities" may be conceptualized as *family*, *company*, or any other social groups. The latent common causes of community in social network may be the shared emotional pattern constructed via the family education, close levels of cognitive abilities within same university/college/classroom, in or similar behavioral pattern under company policy. In protein-protein interaction networks, communities are likely to group proteins having the same specific function within the cell. In the graph of the World Wide Web, community may correspond to groups of web pages dealing with the same or related topics. The regression-based relationship between community and external factors has not been well developed to my knowledge, i.e., the relationship between depression community with age, education, and other disorders. Currently, community-level centrality-based network scores [@zhangEvaluatingGeneralNetwork2024] can be used as measurement of the community, which then can potentially be used as independent variables or outcome in a regression model. However, network scores are suffering from interpretability in psychometrics and relevant research on this topic is still sparse. Similar idea can be found in other psychometric modeling, such as latent regression model in latent variable modeling literature [@anderssonEstimationLatentRegression2021; @yamaguchiFullyGibbsSampling2023] or the structural model in structural equation modeling.

[B]{.bigger}ack to the community conceptualization in psychological area, especially psychiatry, the community is usually associated with *syndromes* or *comorbidity*. According to [Wikipedia](https://en.wikipedia.org/wiki/Syndrome), a syndrome is a set of medical signs and symptoms which are correlated with each other and often associated with a particular disease or disorder. Some psychological syndromes may present comorbidity of symptoms, which refers to simultaneous presence of two or more psychological symptoms in a same individual within a time frame (co-occurring, concurrent). In a earlier paper of @borsboomStructureDSM2002, Borsboom took the DSM-IV ([Diagnostic and Statistical Manual of Mental Disorders](https://en.wikipedia.org/wiki/Diagnostic_and_Statistical_Manual_of_Mental_Disorders)) as example and argued that "... Commodity appears to be, at least partially and in particular for mood, anxiety, and substance abuse disorders, encoded in the structure of the diagnostic criteria themselves" [@borsboomStructureDSM2002], but he did not link the "commodity" of a cluster of highly-connected symptoms to the statistical concept "community" in network graph. It is until the paper of @cramerComorbidityNetworkPerspective2010, the symptom community as a local connectivity was considered as *syndromes*, while symptoms clusters are interconnected by individual symptoms ("bridge symptoms") that form the boundaries between the various basic syndromes of psychiatric disorders [@goekoopNetworkViewPsychiatric2014]. @goekoopNetworkViewPsychiatric2014 also emphasized that the presence of bridge symptoms that connects symptom clusters (communities / syndromes) plays a key role that can be identified as potential targets for treatment.

## Identify Community

There are two ways of defining communities in psychometric network: the exploratory method and the confirmatory method. The exploratory method has a long root in the graphical theory (e.g., exploratory graphical analysis). Exploratory graphical analysis (EGA) makes use of community detection algorithm to detect potential communities in a complex network structure.

In psychometric literature, however, confirmatory network analysis has become more and more popular [@du]. Although the value of exploratory research remains indisputable, the field of psychology is facing an increasing demand for theory- and hypothesis-testing research, which serves to confirm, refute, or refine existing theories [@du; @fried2020]. For example, many "communities" should correspond to theoretical constructs in psychology or education settings. That is, psychopathology syndrome mentioned above must have one precise definition in DSM-V for the clinician references, so relevant indicators (nodes) then should be grouped into one community prior to the network analysis. The network modeling with pre-defined communities is called confirmatory network analysis. As said by @hevey2018, I cited here, "... much of the research in psychological networks has been based on exploratory data analyses to generate networks; there is a need to progress towards confirmatory network modelling wherein hypotheses about network structure are formally tested."

@fried2020 mentioned some challenges in psychology research and argued that the core issues—latent theories, weak theories, and conflating theoretical and statistical models—are common and harmful, facilitate invalid inferences, and stand in the way of theory failure and reform.

::: rmdwarning
1.  The statistical and theoretical models are conflated without concrete evidence of the existence of psychological constructs.
2.  Unclear theoretical grounds and unclear what the theory actually explains/predicts.
:::

Theories are those tools can explain, predict, control phenomena. They are also in one point between strong to weak given to what degree they can explain, predict, and control phenomena. Typically, theorectical model is related to the date generation process—how data is generated, which is unknown in prior. We can link theory to data by using statistical modeling that impose assumptions on the data. However, the statistical models themselves can not be used to measure how strong or weak the theory is.

::: macwindow
Relationships among theory, statistical model, and data ![](theory_model.png){fig-align="center"}
:::

### Community Detection Algorithm

Numerous prior studies have compared different community detection methods in varied network structure [@yangComparativeAnalysisCommunity2016; @gatesMonteCarloEvaluation2016]. In the framework of latent factor analysis or psychometric network, @christensenComparingCommunityDetection2024 recently conducted a Monte-Carlo simulation study compared various node community detection algorithm. To interpret the results, @christensenComparingCommunityDetection2024 even created a [Shiny App](https://alex-christensen.shinyapps.io/community_detection_results/) to interactive present the results.

| Algorithm | Feature | Citation |
|------------------------|------------------------|------------------------|
| Walktrap | random walks | @ponsComputingCommunitiesLarge2006 |
| Infomap | random walks | @rosvallMapsRandomWalks2008 |
| Fast-greedy | hierarchical clustering; modularity-based | @clausetFindingCommunityStructure2004 |
| Louvian | hierarchical clustering; modularity-based | @blondelFastUnfoldingCommunities2008 |
| Leading Eigenvalue | eigenvalue | @newmanModularityCommunityStructure2006 |
| Label Propagation | non-parametric | @raghavanLinearTimeAlgorithm2007 |
| Spinglass | non-parametric | @reichardtStatisticalMechanicsCommunity2006 |
| Edge Betweenness | non-parametric | @girvanCommunityStructureSocial2002 |
| Triangulated Maximally Filtered Graph (TMFG) | score based | @massaraNetworkFilteringBig2017 |
| Parallel Analysis | EFA |  |

: Different types of community detection algorithms {#tbl-algorithm tbl-colwidths="\[35,40,25\]"}

@tbl-algorithm shows the list of community detection algorithms and their features. In the framework of psychometric assessment (\< 50 nodes, balanced item number per factor), the best-performing algorithms are GLASSO with unidimentional adjustment, Louvian, Fast-greedy, Walktrap, and parallel analysis. The evaluation criteria include:

1.  The percentage of correct number of factors (PC)
2.  Mean absolute error (MAE; the average absolute deviation away from the correct number of factors)
3.  Mean bias error (MBE; the average deviation away from the correct number of factors)

Assume $N$ is the total number of simulated sample date sets (Replication), $K$ is the population number of factors/communities, and $\hat{K}$ is the estimated number of factors/communities. $\mathbb{I}(\cdot)$ is the indicator function so that $\mathbb{I}(\hat{K}, K) = 1$ if $\hat{K} = K$ and $\mathbb{I}(\hat{K}, K) = 0$ otherwise. The three evaluation criteria are defined as:

$$
\mathrm{PC} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\hat{K}_i, K_i)
$$

$$
\mathrm{MAE} = \frac{1}{N} \sum_{i=1}^{N} |\hat{K}_i - K_i|
$$

$$
\mathrm{MBE} = \frac{1}{N} \sum_{i=1}^{N} (\hat{K}_i - K_i)
$$

```{r}
## K is a vector of population number of factors with the length of number replications
## K_hat is a vector of estimate number of factors with the length of number replications
cal_pc <- function(K, K_hat) {
  N <- length(K)
  pc <- sum(K_hat == K) / N
  return(pc)
}
cal_mae <- function(K, K_hat) {
  N <- length(K)
  mae <- sum(abs(K_hat - K)) / N
  return(mae)
}
cal_mbe <- function(K, K_hat) {
  N <- length(K)
  mbe <- sum(K_hat - K) / N
  return(mbe)
}
```

## Example: Accuracy and Stability for Fast Greedy and Walktrap

For illustration, we can examine the differences in accuracy and stability between the fast-greedy method and the walktrap method due to the sampling error. The sample data is the Big-five questionnaire dataset (N=2800). The bootstrapping resampling method with 100 replications and 10% total sample size is used for the analysis.

For evaluation criteria, the standardized deviation (SD) for two methods was used to examine the stability; PC, MAE, MBE for two methods were used to examine the accuracy.

::: macwindow
```{r exp1, cache=TRUE}
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Community Detection for Bootstrapping Resampled Samples"
library(igraph) # For community detection
library(foreach) # For parallel computing
library(doParallel) # For core registration

# Load bfi data from psych package:
Data <- bfi |> 
  select(A1:A5, C1:C5, E1:E5, N1:N5, O1:O5) |> 
  na.omit()

ten_perc_N = floor(nrow(Data) / 10) # Number of cases for 10% sample size
N_replications <- 100 # Number of bootstrapping resampling

# Function to get the number of communities by fast-greedy and walktrap
get_resampling_community_n <- \(dat, i, size) {
  set.seed(1234 + i)
  sampled_cases <- dat[sample(x = 1:nrow(dat), size = size), ]
  # Define variables:
  vars <- names(dat)
  
  # Saturated estimation:
  mod_sparsed <- ggm(sampled_cases, vars = vars) |> runmodel()  |>  stepup() |>  prune()
  
  ## Get edge weights
  omega_mat <- getmatrix(mod_sparsed, "omega")
  
  ## Convert to igraph object from adjcent matrix
  ggm_igraph <- graph_from_adjacency_matrix(adjmatrix = abs(omega_mat), 
                                            weighted = TRUE, 
                                            mode = "undirected")
  
  ## Apply Walktrap / Fast Greedy Community Detection Algoritm 
  ggm_wt <- cluster_walktrap(ggm_igraph)
  ggm_fg <- cluster_fast_greedy(ggm_igraph)
  
  ## Get number of detected communities 
  return(
    c(K_wt = length(ggm_wt), 
      K_fg = length(ggm_fg),
      size = size, 
      seed = (1234 + i))
  )
}

cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
res <- foreach(i = 1:N_replications, 
               .packages = c("psychonetrics", "igraph"),
               .combine = "rbind") %dopar% 
  get_resampling_community_n(dat = Data, i = i, size = ten_perc_N)
stopCluster(cl)
```

```{r}
K_wt <- res[,1]
K_fg <- res[,2]
report <- data.frame(
  Algorithm = c("Walktrap", "Fast-Greedy"),
  PC = sapply(list(K_wt, K_fg), \(x) cal_pc(x, rep(5, length(x))) ),
  MAE = sapply(list(K_wt, K_fg), \(x) cal_mae(x, rep(5, length(x))) ),
  MBE = sapply(list(K_wt, K_fg), \(x) cal_mbe(x, rep(5, length(x))) )
)
kableExtra::kable(report)
```

```{r}
summary_tbl <- data.frame(
  Algorithm = c("Walktrap", "Fast-Greedy"),
  Min = sapply(list(K_wt, K_fg), min),
  Max = sapply(list(K_wt, K_fg), max),
  Mean = sapply(list(K_wt, K_fg), mean),
  SD = sapply(list(K_wt, K_fg), sd)
)
kableExtra::kable(summary_tbl)
```
:::

::: callout-note
### Interpretation

Overall, the fast-greedy method performs better than the walktrap method given its higher value of PC and lower absolute values of MAE and MBE across 100 bootstrapping re-sampling iterations with the $\frac{1}{10}$ sample size. The fast-greedy is also more stable than the walktrap given smaller variability ($\mathrm{SD_{FG}}=.65$; $\mathrm{SD_{WT}}=1.13$).
:::

### Sample Size Effect on Community Detection

We can further examine the effect of sample size on the community detection. The sample sizes are 100, 200, 500, 1000, and 2000. The bootstrapping resampling method with 100 replications is used for variability in each condition.

::: macwindow
```{r exp1_n, cache=TRUE}
#| warning: false
#| code-summary: "Community Detection by Sample Size: MCMC"
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
res_byN <- foreach(N = c(100, 200, 500, 1000, 2000), 
        .packages = c("psychonetrics", "igraph"),
        .combine = "rbind") %:%
  foreach(i = 1:N_replications, .combine = "rbind") %dopar%
  get_resampling_community_n(dat = Data, i = i, size = N)
stopCluster(cl)
```

```{r}
#| label: fig-comm-detect-n
#| fig-cap: "Community Detection by Sample Size"
#| fig-cap-location: top
#| code-summary: "Community Detection by Sample Size: Visualization"

library(tidyverse)
res_byN_tbl <- as.data.frame(res_byN) 

res_byN_smy <- res_byN_tbl |> 
  group_by(size) |> 
  summarise(
    wt_mean = mean(K_wt),
    fg_mean = mean(K_fg),
    wt_sd = sd(K_wt),
    fg_sd = sd(K_fg)
  )

library(ggplot2)
library(tidyr)
res_byN_smy |> 
  tidyr::pivot_longer(cols = -size, names_to = "Algorithm", values_to = "K") |>
  tidyr::separate(Algorithm, into = c("Algorithm", "Stat"), sep = "_") |> 
  tidyr::pivot_wider(names_from = Stat, values_from = K) |> 
  ggplot(aes(x = size, color = Algorithm, group = Algorithm)) +
  geom_path(aes(y = mean), linewidth = 1.5) +
  geom_point(aes(y = mean), size = 3) +
  geom_errorbar(aes(ymin = mean - 1.96*sd, ymax = mean + 1.96*sd), 
                width = 70, linewidth = 1.1, alpha = .7) +
  labs(title = "Community Detection by Sample Size",
       x = "Sample Size",
       y = "Number of Communities") +
  scale_color_manual(values = c("royalblue", "tomato"), labels = c("Fast-Greedy", "Walktrap")) +
  scale_x_continuous(breaks = c(100, 200, 500, 1000, 2000)) +
  theme_minimal()
```
:::

As shown in @fig-comm-detect-n, for both methods, as sample size gets larger, the number of communities detected by both methods tends to align with the theoretical number of communities, $K = 5$. From N = 100 to N = 200 has strong increase in the accuracy of the number of communities detected, while from N = 200 to N = 500 has moderate improvement in the number of communities detected. After N = 500, the number of communities detected is stable.
