---
title: "Large Language Models"
subtitle: "Examples with R and Python"
date: "2025-04-29"
date-modified: "`{r} Sys.Date()`"
draft: false
bibliography: ../references.bib
image: ../images/thumbnail_chatgpt.png
tbl-cap-location: top
citation:
  type: webpage
  issued: 2025-03-07
execute: 
  cache: true  
format: 
  html:
    code-tools: false
    code-line-numbers: false
    code-fold: false
    code-summary: "Click this to see code"
---

# Model architecture

## Transformer

The original Transformer architecture was introduced in the paper [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) by Vaswani et al. in 2017.
The Transformer model has since become the foundation for many state-of-the-art natural language processing (NLP) models, including BERT, GPT-3, and T5.

### Terminology

-   Self-attention: an attention mechanism relating different positions of a single sequence to compute a representation of the sequence.

    -   Q,K,V matrix: query, keys, values.
        The output is computed as a weighted sum of the values.
        All these three are key component of Attention function.
        The following formula is also known as Scaled dot-product attention.

        $$
        \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
        $$

    -   where queries and keys have the dimension $d_k$

> the dot products get larger variances when the dimension of q and k increase.
> So they scale the dot product by $\frac{1}{\sqrt{d_k}}$ to make sure the dot product has close variance with q and k.

```{r}
library(ggplot2)
N_rep = 1000
dot_product <- function(i, d_k, var_q = 1) {
  set.seed(i)
  q = rnorm(d_k, 0, sd = sqrt(var_q))
  k = rnorm(d_k, 0, sd = sqrt(var_q))
  crossprod(q, k)
}

var_dot_product <- function(N_rep, d_k, var_q = 1) {
  dot_product_values <- sapply(1:N_rep, \(x) dot_product(x, d_k = d_k, var_q))
  var(dot_product_values)
}

to_plot <- data.frame(
  d_k_values = c(1, 10, 100, 1000),
  var_dot_product_values = sapply(c(1, 10, 100, 1000), \(x) var_dot_product(N_rep = N_rep, 
                                                                   d_k = x))
)

ggplot(to_plot, aes(x = d_k_values, y = var_dot_product_values)) +
  geom_path() +
  geom_point() +
  labs(x = "d_k", y = "Variance of dot product of q and k")
```

```{r}
var_q_values = c(1, 10, 25, 100, 400) # variances of q and k
d_k = 2 # dimension of k

# function to generate the variaance of scaled dot products
var_scaled_dot_product <- function(N_rep, d_k, var_q = 1) {
  scaled_dot_product_values <- sapply(1:N_rep,\(x){dot_product(x, d_k = d_k, var_q = var_q)/sqrt(d_k)} )
  var(scaled_dot_product_values)
}

var_scaled_dot_product_values <- sapply(var_q_values, 
                                        \(x) var_scaled_dot_product(N_rep = N_rep, 
                                                                    d_k = d_k, 
                                                                    var_q = x) )

data.frame(
  var_q_values = var_q_values,
  var_scaled_dot_product_values = var_scaled_dot_product_values
) |> 
ggplot(aes(x = var_q_values, y = var_scaled_dot_product_values)) +
  geom_path() +
  geom_point() +
  labs(x = "variances of q and k", y = "Variance of scaled dot product of q and k")
```

-   Multi-Head Attention:

-   Encoder-decoder structure: The basic structure of most neural sequence transduction models.

    -   Encoder maps an input to a sequence of continuous representations.

    -   Decoder generates an output sequence given the continuous representation of encoder

![The Transformer - model architecture in the original paper](images/clipboard-1162313639.png){fig-align="center" width="500"}

# Evaluation metrics

## Item Similarity

Various metrics can be used to evaluate the similarity between two sentences (e.g., math items) at the lexical level.
They are typically called vector similarity measures.

### BLEU

BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of text that has been machine-translated from one natural language to another (see [geeksforgeeks](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)). It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of n items) between the candidate and reference translations. Higher values indicate better quality translations.

The formula of BLEU score is:

$$
\text{BLEU}(C, R) = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log p_n\right)
$$

where:

- $C$ is the candidate translation,
- $R$ is the reference translation,
- $BP$ is the brevity penalty, which penalizes translations that are shorter than the reference translation,
- $p_n$ is the precision of n-grams in the candidate translation,
- $w_n$ is the weight assigned to the n-gram precision, typically set to $\frac{1}{N}$ for $n=1,2,\ldots,N$.

```{python}
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
def calculate_bleu(candidate, reference):
    # Tokenize the sentences
    candidate_tokens = candidate.split()
    reference_tokens = reference.split()
    
    # Calculate BLEU score
    smoothing_function = SmoothingFunction().method1
    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing_function)
    
    return bleu_score

candidate = "The cat sat on the mat."
reference = "The cat is sitting on the mat."

bleu_score = calculate_bleu(candidate, reference)
print(f"BLEU score: {bleu_score:.4f}")
```


### Cosine Similarity

The cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. In NLP, it is often used to compare the similarity of two text documents or sentences by representing them as vectors in a high-dimensional space.

The formula of the cosine similarity is:

$$
\text{cosine\_similarity}(A, B) = \frac{A \cdot B}{||A|| \cdot ||B||}
$$

where $A$ and $B$ are the two vectors, $A \cdot B$ is the dot product of the vectors, and $||A||$ and $||B||$ are the magnitudes (or norms) of the vectors.

To convert words into vectors, we can use a simple method called **word2vec**. To be more specific, we can **represent a word as a vector of character frequencies**, where each character in the word is counted and represented as a dimension in the vector space.

::: panel-tabset
## Python

```{python}
def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
  
a = 'safasfeqefscwaeeafweeaeawaw'
b = 'tsafdstrdfadsdfdswdfafdwaed'
c = 'optykop;lvhopijresokpghwji7'

va = word2vec(a)
vb = word2vec(b)
vc = word2vec(c)

print(cosdis(va,vb))
print(cosdis(vb,vc))
print(cosdis(vc,va))
```

## R

```{r}
a = 'safasfeqefscwaeeafweeaeawaw'
b = 'tsafdstrdfadsdfdswdfafdwaed'
c = 'optykop;lvhopijresokpghwji7'

cosine_similarity <- function(a, b) {
  a <- strsplit(a, "")[[1]]
  b <- strsplit(b, "")[[1]]
  
  a_freq <- table(a)
  b_freq <- table(b)
  
  common_chars <- intersect(names(a_freq), names(b_freq))
  
  dot_product <- sum(a_freq[common_chars] * b_freq[common_chars])
  norm_a <- sqrt(sum(a_freq^2))
  norm_b <- sqrt(sum(b_freq^2))
  
  return(dot_product / (norm_a * norm_b))
}

cosine_similarity(a, b)
cosine_similarity(b, c)
cosine_similarity(a, c)
```

:::



### Edit Similarity

