---
title: "Lecture 02: General Linear Model"
subtitle: "Descriptive Statistics and Basic Statistics"
author: "Jihong Zhang*, Ph.D"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2024-08-18"
sidebar: false
execute: 
  echo: true
format: 
  html: 
    page-layout: full
    toc: true
    toc-depth: 2
    lightbox: true
    code-fold: true
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: true
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503: Lecture 02 - Descriptive Statistics"
    slide-number: c/t
    tbl-colwidths: auto
    output-file: slides-index.html
#jupyter: python3
---

## Learning Objectives

1.  Univariate Descriptive Statistics
    -   Central tendency: Mean, Median, Mode
    -   Variation/Spread: Standard deviation (SD), Variance, Range
2.  Bivariate descriptive statistics
    -   Correlation
    -   Covariance
3.  Types of variable distributions
    -   Marginal
    -   Joint
    -   Conditional
4.  Bias in estimators

## Data for Today's Lecture

-   To help demonstrate the concepts of today's lecture, we will be using a toy data set with three variables

    -   **Female (Gender)**: Male (= 0) or Female (= 1)

    -   **Height**: in inches

    -   **Weight**: in pounds

-   The goal of lecture 02 will be to build a general **linear model** that predicts a person's weight

    -   **Linear (regression) model**: a statistical model for an outcome that uses a linear combination (a weighted sum) of one or more predictor variables to produce an estimate of an observation's predicted value

    -   $$
        \mathbb{y} = \beta_0+\beta_1 \mathbf{X}
        $$

-   All models we learnt today will follow this framework.

## Visualizing the Data

::: columns
::: {.column width="40%"}
```{r}
library(ESRM64504) # INSTALL: pak::pak("JihongZ/ESRM64504")
library(kableExtra)
dataSexHeightWeight$female = dataSexHeightWeight$sex == "F"
kable(dataSexHeightWeight,
      caption = 'toy data set') |> 
  kable_styling(font_size = 20)
```
:::

::: {.column width="60%"}
```{r}
pairs(dataSexHeightWeight[, c('female', 'heightIN', 'weightLB')])
```
:::
:::

## Histograms of Height and Weight

::: columns
::: {.column width="50%"}
```{r}
#| fig-cap: "Pairwise Scatter Points"
hist(dataSexHeightWeight$weightLB, main = 'Weight', xlab = 'Pounds')
```
:::

::: {.column width="50%"}
```{r}
hist(dataSexHeightWeight$heightIN, main = 'Height', xlab = 'Inches')
```
:::
:::

## Descriptive Statistics Overview

-   First, we can inspect each variable individually (**marginal distribution**) through a set of descriptive statistics

    -   Visual way: histogram plot or density plot

    -   Statistical way: Central tendency and Variability

        -   Mean, Median, Mode

        -   SD, Range

-   Second, we can also summarize the **joint (bivariate) distribution** of two variables through a set of descriptive statistics:

    -   **Joint vs. Marginal**: joint distribution describes more than one variable simultaneously

    -   Common bivariate descriptive statistics:

        -   Correlation and covariance

## Descriptive Statistics for Toy Data: Marginal

```{r}
#| message: false
#| code-fold: show  
#| output-location: column
library(dplyr)
## wide-format marginal description
wide_marginal_desp <- dataSexHeightWeight |> 
  summarise(across(c(heightIN, weightLB, female), 
                   list(mean = mean, sd = sd, var = var)))
kable(wide_marginal_desp)

```

```{r}
#| message: false
#| code-fold: show  
#| output-location: column
library(tidyr)
wide_marginal_desp |> 
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") |> 
  separate(Variable, sep = "_", into = c("Variable", "Stats")) |> 
  pivot_wider(names_from = Stats, values_from = Value) |> 
  kable(digits = 3)
```

## Descriptive Statistics for Toy Data: Joint

```{r}
#| output-location: column
#| code-fold: show
cor_cov_mat <- matrix(nrow = 3, ncol = 3)
colnames(cor_cov_mat) <- rownames(cor_cov_mat) <- c("heightIN", "weightLB", "female")
cov_mat <- cov(dataSexHeightWeight[, c("heightIN", "weightLB", "female")])
cor_mat <- cor(dataSexHeightWeight[, c("heightIN", "weightLB", "female")])
## Assign values
cor_cov_mat[lower.tri(cor_cov_mat)] <- cor_mat[lower.tri(cor_mat)]
cor_cov_mat[upper.tri(cor_cov_mat)] <- cov_mat[upper.tri(cov_mat)]
diag(cor_cov_mat) <- diag(cov_mat)
kable(cor_cov_mat, digits = 3)
```

-   Note:

    -   Diagonal: Variance

    -   Above Diagonal (upper triangle): Covaraince

    -   Below Diagonal (lower triangle): Correlation

-   **Question:** What we can tell regarding the relationships among three variables?

## Re-examining the Concept of Variance

-   Variability is a central concept in advanced statistics

    -   In multivariate statistic, covariance is also central

-   Two formulas for the variance (about the same when $N$ is larget):

    ::: columns
    ::: {.column width="50%"}
    $$
    S^2_Y= \frac{\Sigma_{p=1}^{N}(Y_p-\bar Y)}{N-1}
    $$ {#eq-unbiased-var}
    :::

    ::: {.column width="50%"}
    $$
    S^2_Y= \frac{\Sigma_{p=1}^{N}(Y_p-\bar Y)}{N}$$ {#eq-biased-var}
    :::
    :::

-   @eq-unbiased-var: **Unbiased** or "sample"

-   @eq-biased-var: **Biased/ML** or "population"

Here: $p$ = person;

## Biased VS. Unbiased Estimator of Variance

```{r}
#| code-summary: 'Variances population/sample estimators'
#| message: false
#| code-fold: show
#| code-block-height: 100px
set.seed(1234)
sample_sizes = seq(10, 300, 10) # set up varied levels of sample sizes
variance_mat <- matrix(NA, nrow = length(sample_sizes), ncol = 4) # placeholder for variance estimators
colnames(variance_mat) <- c("sample_size", "population_variance", "sample_biasvariance", "sample_unbiasvariance")
iterator = 0

for (sample_size in sample_sizes) {
  iterator = iterator + 1
  variance_mat[iterator, 1] <- sample_size
  sample_points <- rnorm(sample_size, 0, 10)
  
  sample_var_biased <- sum((sample_points - mean(sample_points))^2) / sample_size # biased estimation of variance
  sample_var_unbiased <- sum((sample_points - mean(sample_points))^2) / (sample_size - 1) # unbiased estimation of variance
  
  variance_mat[iterator, 2] <- 100
  variance_mat[iterator, 3] <- sample_var_biased
  variance_mat[iterator, 4] <- sample_var_unbiased
}
kable(variance_mat[1:4, ])
```

## Biased VS. Unbiased Estimator of Variance (Cont.)

```{r}
#| message: false
#| output-location: column
#| code-fold: show
library(ggplot2)
variance_mat |> 
  as.data.frame() |> 
  pivot_longer(-sample_size) |> 
  ggplot() +
  geom_line(aes(x = sample_size, y = value, color = name), linewidth = 1.1) +
  labs(x = "Sample Size", y = "Estimates of Variance") +
  scale_color_manual(values = 1:3, labels = c("Population Variance", "Sample Biased Variance (ML)", "Sample Unbiased Variance"), name = "Estimator") +
  theme_classic() +
  theme(text = element_text(size = 25)) 
```

**Take home note**: When sample size is small, unbiased variance estimators can get the estimate of variance closer to the population variance than the biased one.

## Interpretation of Variance

-   The variance describes the spread of a variable in squared units (which come from $(Y_p - \bar Y)^2$ term in the equation)

-   Variance: **the average squared distance of an observation from the mean**

    -   For the toy sample, the **variance of height** is 55.358 inches squared

    -   For the toy sample, the **variance of weight** is 3179.095 pounds squared

    -   The **variance of female** — not applicable in the sample way!

        -   How is the sample equally distributed across different groups: 50/50 -\> largest variance

-   Because squared units are difficult to work with, we typically use the standard deviation – which is reported in units

-   Standard deviation: the average distance of an observation from the mean

    -   SD of Height: 7.44 inches

    -   SD of Weight: 56.383 pounds

## Variance/SD as a More General Statistical Concept

-   Variance (and the standard deviation) is a concept that is appled across statistics – not just for data
