---
title: "Lecture 11: Introduction to Bayesian Statistics and Markov Chain Monte Carlo Estimation"
subtitle: ""
author: "Jihong Zhang*, Ph.D"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2024-10-09"
date-modified: "2024-10-11"
draft: false
sidebar: false
execute: 
  echo: true
  warning: false
output-location: default
code-annotations: below
highlight-style: "atom-one"
format: 
  uark-revealjs:
    scrollable: true
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: false
    footer: "ESRM 64503 - Lecture 11: Introduction to Bayesian"
    slide-number: c/t
    tbl-colwidths: auto
    output-file: slides-index.html
  html: 
    page-layout: full
    toc: true
    toc-depth: 2
    toc-expand: true
    lightbox: true
    code-fold: false
    fig-align: center
filters:
  - quarto
  - line-highlight
---

## Today's Lecture Objectives

1.  Bayes' Theorem

2.  Likelihood function vs. Posterior distribution

3.  How to report posterior distribution of parameters

4.  Bayesian update

5.  Bayesian Linear Regression and Multivariate Regression

# Brief Introduction to Bayesian

## What is Bayesian?

-  A statistical inference based on **Bayesian Probability Theory to estimate uncertainty**

[![Bayesian Data Analysis course material by Aki Vehtari](images/clipboard-292128306.png){width="600"}](https://github.com/avehtari/BDA_course_Aalto)

-  What are key components of Bayesian models?

    1.  Likelihood function - likelihood of **data given assumed probability distribution**
    2.  Prior distribution - belief / previous evidences of **parameters**
    3.  Posterior distribution - updated information of parameters given our data and modeling
    4.  Posterior predictive distribution - future / predicted data

-  What are the differences between Bayesian with Frequentist analysis?

    1.  **Input**: prior distribution: **Bayesian** vs. hypothesis of fixed parameters: **frequentist**
    2.  **Estimation process**: Markov Chain Monte Carlo (MCMC) vs. Maximum Likelihood Estimation (MLE)
    3.  **Result**: posterior distribution vs. point estimates of parameters
    4.  **Accuracy check**: credible interval (plausibility of the parameters having those values) vs. confidence interval (the proportion of infinite samples having the fixed parameters)

## Bayes' Theorem: How Bayesian Statistics Work

Bayesian methods rely on Bayes' Theorem

$$
P(\boldsymbol{\theta} | Data) = \frac{P(Data|\boldsymbol{\theta})P(\boldsymbol{\theta})}{P(Data)} \propto P(Data|\boldsymbol{\theta}) P(\boldsymbol{\theta})
$$

Where:

1.  P: probability distribution function (PDF)
2.  $P(\boldsymbol{\theta}|Data)$ : the [posterior distribution]{.underline} of parameter $\boldsymbol{\theta}$, given the observed data
3.  $P(Data|\boldsymbol{\theta})$: the likelihood function (conditional distributin) of the observed data, given the parameters
4.  $P(\boldsymbol{\theta})$: the [prior distribution]{.underline} of parameter $\boldsymbol{\theta}$
5.  $P(Data)$: the marginal distribution of the observed data

------------------------------------------------------------------------

**A Live Bayesian Example**

-   Suppose we want to assess the probability of rolling a "1" on a six-sided die:

    $$
    \boldsymbol{\theta} \equiv p_{1} = P(D = 1)
    $$

-   Suppose we collect a sample of data (N = 5):\
    $$Data \equiv X = \{0, 1, 0, 1, 1\}$$

-   The [prior distribution]{.underline} of parameters is denoted as $P(p_1)$: how much the probability of "1" will occur we "believe" ;

-   The [likelihood function]{.underline} is denoted as $P(X | p_1)$;

-   The [posterior distribution]{.underline} is denoted as $P(p_1|X)$

-   The aim of Bayesian is to estimate [the probability of rolling a "1" give the Data]{.underline} (posterior distribution) as:

    $$
    P(p_1|X) = \frac{P(X|p_1)P(p_1)}{P(X)} \propto P(X|p_1) P(p_1)
    $$

------------------------------------------------------------------------

## Why use Bayesian

1.  Bayesian approach to statistics is more intuitive; it resembles how we think about probability in everyday life (our prior belif and our updated belief after observing the data) â€“ in the odds of hypotheses, not those of data. In comparison, the frequentist conclusion sounds complex and difficult to comprehend.
2.  Bayesian approach is much easier to construct very complicated model: using MCMC, we did not need to derive likelihood function by ourselves.
3.  Bayesian approach is very flexible to incorporate prior (old, history) data; while the frequentist inference is static, which only use current data.

## Step 1: Choose the Likelihood function (Model)

-   The Likelihood function $P(X|p_1)$ follows Binomial distribution of 3 successes out of 5 samples:\
    $$P(X|p_1) = \prod_{i =1}^{N=5} p_1^{X_i}(1-p_1)^{X_i} 
    \\= (1-p_i) \cdot p_i \cdot (1-p_i) \cdot p_i \cdot p_i$$

-   We use Bernoulli (Binomial) Distribution (feat. *Jacob Bernoulli, 1654-1705*) because Bernoulli dist. has nice statistical probability.

    -   "Nice" means making totally sense in normal life-- a common belief. For example, the $p_1$ value that maximizes the Bernoulli-based likelihood function is $Mean(X)$, and the $p_1$ values that minimizes the Bernoulli-based likelihood function is 0 or 1

------------------------------------------------------------------------

### Log-likelihood Function across different values of Parameter

$$
LL(p_1, Data =\{0, 1, 0, 1, 1\})
$$

```{r}
#| echo: true
#| code-line-numbers: "2"
p1 = seq(0, 1, length.out = 21)
Likelihood = (p1)^3 * (1-p1)^2
plot(x = p1, y = log(Likelihood))
p1[which.max(Likelihood)] # p1 = 0.6 = 3 / 5
```

------------------------------------------------------------------------

### Log-likelihood Function given different Data sets

$$
LL(p_1 \in \{0, 0.2,0.4, 0.6, 0.8,1\}, Data \in \{0/5, 1/5, 2/5, 3/5, 4/5, 5/5\})
$$

```{r}
#| echo: true
#| code-fold: true
#| code-line-numbers: "5"
library(tidyverse)
p1 = c(0, 2, 4, 6, 8, 10) / 10
nTrails = 5
nSuccess = 0:nTrails
Likelihood = sapply(p1, \(x) choose(nTrails,nSuccess)*(x)^nSuccess*(1-x)^(nTrails - nSuccess))
Likelihood_forPlot <- as.data.frame(Likelihood)
colnames(Likelihood_forPlot) <- p1
Likelihood_forPlot$Sample = factor(paste0(nSuccess, " out of ", nTrails), levels = paste0(nSuccess, " out of ", nTrails))
# plot
Likelihood_forPlot %>% 
  pivot_longer(-Sample, names_to = "p1") %>% 
  mutate(`log(Likelihood)` = log(value)) %>% 
  ggplot(aes(x = Sample, y = `log(Likelihood)`, color = p1)) +
  geom_point(size = 2) +
  geom_path(aes(group=p1), size = 1.3)
```

------------------------------------------------------------------------

## Step 2: Choose the Prior Distribution for $p_1$

We must now pick the prior distribution of $p_1$:

$$
P(p_1)
$$

-   Compared to likelihood function, we have much more choices. Many distributions to choose from

-   To choose [prior distribution]{.underline}, think about what we know about a "fair" die.

    -   the probability of rolling a "1" is about $\frac{1}{6}$

    -   the probabilities much higher/lower than $\frac{1}{6}$ are very unlikely

-   Let's consider a Beta distribution:

    $$
    p_1 \sim Beta(\alpha, \beta)
    $$

------------------------------------------------------------------------

### Prior for probability parameter: The Beta Distribution

For parameters that range between 0 and 1, the beta distribution with two hyperparameters {$\alpha$, $\beta$} makes a good choice for prior distribution:

$$
P(p_1) = \frac{(p_1)^{a-1} (1-p_1)^{\beta-1}}{B(\alpha,\beta)}
$$

Where denominator $B$ is:

$$
B(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
$$

and (fun fact: derived by *Daniel Bernoulli (1700-1782)*),

$$
\Gamma(\alpha) = \int_0^{\infty}t^{\alpha-1}e^{-t}dt
$$

------------------------------------------------------------------------

### More nice properties: Beta Distribution

-   The Beta distribution has a mean of $\frac{\alpha}{\alpha+\beta}$ and a mode of $\frac{\alpha -1}{\alpha + \beta -2}$ for $\alpha > 1$ & $\beta > 1$; (fun fact: when $\alpha\ \&\ \beta< 1$, pdf is U-shape, what that mean?)

-   $\alpha$ and $\beta$ are called [hyperparameters]{.underline} of the parameter $p_1$;

-   [**Hyperparameters**]{.underline} are parameters of prior parameters ;

-   When $\alpha = \beta = 1$, the distribution is uniform distribution;

-   To make sure $P(p_1 = \frac{1}{6})$ is the largest, we can:

    -   Many choices: $\alpha =2$ and $\beta = 6$ has the same mode as $\alpha = 100$ and $\beta = 496$; (hint: $\beta = 5\alpha - 4$)

    -   The differences between choices is how strongly we feel in our beliefs

![](images/beta_distribution.png)

------------------------------------------------------------------------

### Conjugacy of Beta Distribution

When the binomial likelihood is multiplied by the beta prior, the result is proportional to another beta distribution:

-   $\text{Posterior} \propto \theta^x (1 - \theta)^{n-x} \times \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}$
-   Simplifies to: $\theta^{x + \alpha - 1} (1 - \theta)^{n - x + \beta - 1}$

This is the kernel of a beta distribution with updated parameters $\alpha' = x + \alpha$ and $\beta' = n - x + \beta$. The fact that the posterior is still a beta distribution is what makes the beta distribution a conjugate prior for the binomial likelihood.

> Human langauge: Both beta (prior) and binomial (likelihood) are so-called "exponetial family". The muliplication of them is still a "exponential family" distribution.

------------------------------------------------------------------------

### How strongly we believe in the prior

::: columns
::: {.column width="40%"}
-   The Beta distribution has a variance of $\frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha + \beta + 1)}$
-   The smaller prior variance means the prior is more [informative]{.underline}
    -   [Informative]{.underline} priors are those that have relatively small variances
    -   [Uninformative]{.underline} priors are those that have relatively large variances
-   Suppose we have four sets of hyperparameters choices: (2,6);(20,96);(100,496);(200,996)
:::

::: {.column width="50%"}
```{r, fig.width=10,fig.height=11}
#| code-fold: true
# Function for variance of Beta distribution
varianceFun = function(alpha_beta){
  alpha = alpha_beta[1]
  beta = alpha_beta[2]
  return((alpha * beta)/((alpha + beta)^2*(alpha + beta + 1)))
}

# Multiple prior choices from uninformative to informative
alpha_beta_choices = list(
  I_dont_trust = c(2, 6), 
  not_strong =   c(20, 96), 
  litte_strong = c(100, 496), 
  very_strong =  c(200, 996))

## Transform to data.frame for plot
alpha_beta_variance_plot <- t(sapply(alpha_beta_choices, varianceFun)) %>% 
  as.data.frame() %>% 
  pivot_longer(everything(), names_to = "Degree", values_to = "Variance") %>% 
  mutate(Degree = factor(Degree, levels = unique(Degree))) %>% 
  mutate(Alpha_Beta = c(
    "(2, 6)",
    "(20, 96)",
    "(100, 496)",
    "(200, 996)"
  ))

alpha_beta_variance_plot %>% 
  ggplot(aes(x = Degree, y = Variance)) +
  geom_col(aes(fill = Degree)) +
  geom_text(aes(label = Alpha_Beta), size = 6) +
  labs(title = "Variances along difference choices of alpha and beta") +
  theme(text = element_text(size = 21)) 
```
:::
:::

------------------------------------------------------------------------

### Visualize prior distribution $P(p_1)$

```{r}
#| code-fold: true
dbeta <- function(p1, alpha, beta) {
  # probability density function
  PDF = ((p1)^(alpha-1)*(1-p1)^(beta-1)) / beta(alpha, beta)
  return(PDF)
}

condition <- data.frame(
  alpha = c(2, 20, 100, 200),
  beta = c(6, 96, 496, 996)
)
pdf_bycond <- condition %>% 
  nest_by(alpha, beta) %>% 
  mutate(data = list(
    dbeta(p1 = (1:99)/100, alpha = alpha, beta = beta)
  ))

## prepare data for plotting pdf by conditions
pdf_forplot <- Reduce(cbind, pdf_bycond$data) %>% 
  t() %>% 
  as.data.frame() ## merge conditions together
colnames(pdf_forplot) <- (1:99)/100 # add p1 values as x-axis
pdf_forplot <- pdf_forplot %>% 
  mutate(Alpha_Beta = c( # add alpha_beta conditions as colors
    "(2, 6)",
    "(20, 96)",
    "(100, 496)",
    "(200, 996)"
  )) %>% 
  pivot_longer(-Alpha_Beta, names_to = 'p1', values_to = 'pdf') %>% 
  mutate(p1 = as.numeric(p1),
         Alpha_Beta = factor(Alpha_Beta,levels = unique(Alpha_Beta)))

pdf_forplot %>% 
  ggplot() +
  geom_vline(xintercept = 0.17, col = "black") +
  geom_point(aes(x = p1, y = pdf, col = Alpha_Beta)) +
  geom_path(aes(x = p1, y = pdf, col = Alpha_Beta, group = Alpha_Beta)) + 
  scale_color_discrete(name = "Prior choices")

```

**Question: WHY NOT USE NORMAL DISTRIBUTION?**

------------------------------------------------------------------------

## Step 3: The Posterior Distribution

Choose a Beta distribution as the prior distribution of $p_1$ is very convenient:

-   When combined with Bernoulli (Binomial) data likelihood, the [posterior distribution]{.underline} ($P(p_1|Data)$) can be derived analytically

-   The posterior distribution is also a Beta distribution

    -   $\alpha' = \alpha + \sum_{i=1}^{N}X_i$ ($\alpha'$ is parameter of the posterior distribution)

    -   $\beta' = \beta + (N - \sum_{i=1}^{N}X_i)$ ($\beta'$ is parameter of the posterior distribution)

-   The Beta distribution is said to be a [conjugate prior]{.underline} in Bayesian analysis: A prior distribution that leads to posterior distribution of the same family

    -   Prior and Posterior distribution are all Beta distribution

------------------------------------------------------------------------

### Visualize the posterior distribution $P(p_1|Data)$

```{r}
#| code-fold: true
dbeta_posterior <- function(p1, alpha, beta, data) {
  alpha_new = alpha + sum(data) 
  beta_new = beta + (length(data) - sum(data) )
  # probability density function
  PDF = ((p1)^(alpha_new-1)*(1-p1)^(beta_new-1)) / beta(alpha_new, beta_new)
  return(PDF)
}
# Observed data
dat = c(0, 1, 0, 1, 1)

condition <- data.frame(
  alpha = c(2, 20, 100, 200),
  beta = c(6, 96, 496, 996)
)

pdf_posterior_bycond <- condition %>% 
  nest_by(alpha, beta) %>% 
  mutate(data = list(
    dbeta_posterior(p1 = (1:99)/100, alpha = alpha, beta = beta,
                    data = dat)
  ))

## prepare data for plotting pdf by conditions
pdf_posterior_forplot <- Reduce(cbind, pdf_posterior_bycond$data) %>% 
  t() %>% 
  as.data.frame() ## merge conditions together
colnames(pdf_posterior_forplot) <- (1:99)/100 # add p1 values as x-axis
pdf_posterior_forplot <- pdf_posterior_forplot %>% 
  mutate(Alpha_Beta = c( # add alpha_beta conditions as colors
    "(2, 6)",
    "(20, 96)",
    "(100, 496)",
    "(200, 996)"
  )) %>% 
  pivot_longer(-Alpha_Beta, names_to = 'p1', values_to = 'pdf') %>% 
  mutate(p1 = as.numeric(p1),
         Alpha_Beta = factor(Alpha_Beta,levels = unique(Alpha_Beta)))

pdf_posterior_forplot %>% 
  ggplot() +
  geom_vline(xintercept = 0.17, col = "black") +
  geom_point(aes(x = p1, y = pdf, col = Alpha_Beta)) +
  geom_path(aes(x = p1, y = pdf, col = Alpha_Beta, group = Alpha_Beta)) + 
  scale_color_discrete(name = "Prior choices") +
  labs( y = '')
```

------------------------------------------------------------------------

### Manually calculate summaries of the posterior distribution

To determine the estimate of $p_1$ using data $\{0, 1, 0, 1,1\}$, we need to summarize the posterior distribution:

-   With prior hyperparameters $\alpha = 2$ and $\beta = 6$ and the data $\sum_{i=1}^{N}X = 3$ and $N = 5$:

    -   $\hat p_1 = \frac{\alpha'}{\alpha' + \beta'}= \frac{2+3}{2+3+6+2}=\frac{5}{13} = .3846$

    -   SD = `r sqrt(varianceFun(c(2+3, 6+2)))`

-   With prior hyperparameters $\alpha = 100$ and $\beta = 496$:

    -   $\hat p_1 = \frac{100+3}{100+3+496+2}=\frac{103}{601} = .1714$

    -   SD = `r sqrt(varianceFun(c(100+3, 496+2)))`

```{r}
uninfo <- pdf_posterior_forplot %>% filter(Alpha_Beta == "(2, 6)")
paste0("Sample Posterior Mean: ", mean(uninfo$p1 * uninfo$pdf)) # .3885
```

```{r}
uninfo2 <- pdf_posterior_forplot %>% filter(Alpha_Beta == "(100, 496)")
paste0("Sample Posterior Mean: ", mean(uninfo2$p1 * uninfo2$pdf)) # 0.1731
```

# Bayesian Analysis in R: Stan Program

## Introduction to Stan

-   Stan is a widely used MCMC estimation program

    -   Most recent; has many convenient features

    -   Actually does several methods of estimation (ML, Variational Bayes)

-   You create a model using Stan's syntax

    -   Stan translates your model to a custom-built C++ syntax

    -   Stan then compiles your model into its own executable program

-   You then run the program to estimate your model

    -   If you use R, the interface can be seamless

------------------------------------------------------------------------

### Stan and Rstudio

-   Option 1: Stan has its own syntax which can be built in stand-alone text files (.stan)

    -   Rstudio will let you create a *stan* file in the new `File` menu

    -   Rstudio also has syntax highlighting in Stan files

    -   Save `.stan` to the same path of your `.r`

-   Option 2: You can also use Stan in a interactive way in **qmd** file

    -   Which is helpful when you want to test multiple stan models and report your results at the same time
    
![Create a new Stan file in Rstudio](Fig2_CreateStanFile.png)

![Stan and R code blocks in qmd file](Fig2_StanCodeinQmd.png)


------------------------------------------------------------------------

## Install CmdStan

-   [cmdstanr Installation](https://mc-stan.org/cmdstanr/articles/cmdstanr.html#installing-cmdstan)

-   [cmdstanr Users' Guide](https://mc-stan.org/docs/cmdstan-guide/cmdstan-installation.html#troubleshooting-the-installation)

------------------------------------------------------------------------

### Stan Code for the example model

```{r}
library(cmdstanr)
register_knitr_engine(override = FALSE)
```

```{cmdstan, output.var='dice_model', cache=FALSE}
data {
  int<lower=0> N; // sample size
  array[N] int<lower=0, upper=1> y; // observed data
  real<lower=1> alpha; // hyperparameter alpha
  real<lower=1> beta; // hyperparameter beta
}
parameters {
  real<lower=0,upper=1> p1; // parameters
}
model {
  p1 ~ beta(alpha, beta); // prior distribution
  for(n in 1:N){
    y[n] ~ bernoulli(p1); // model
  }
}
```

------------------------------------------------------------------------

```{r}
#| code-fold: false
# mod <- cmdstan_model('dice_model.stan')
data_list1 <- list(
  y = c(0, 1, 0, 1, 1),
  N = 5,
  alpha = 2,
  beta = 6
)
data_list2 <- list(
  y = c(0, 1, 0, 1, 1),
  N = 5,
  alpha = 100,
  beta = 496
)
fit1 <- dice_model$sample(
  data = data_list1,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 1000
)
fit2 <- dice_model$sample(
  data = data_list2,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 1000
)
```

------------------------------------------------------------------------

#### Uninformative prior distribution (2, 6) and posterior distribution

```{r}
#| code-fold: true
bayesplot::mcmc_dens(fit1$draws('p1')) +
  geom_path(aes(x = p1, y = pdf), data = pdf_posterior_forplot %>% filter(Alpha_Beta == '(2, 6)'), col = "red") +
  geom_vline(xintercept = 1/6, col = "green") +
  geom_vline(xintercept = 3/5, col = "purple") +
  labs(title = "Posterior distribution using uninformative prior vs. the conjugate prior (The Gibbs sampler) method", caption = 'Green: mode of prior distribution; Purple: expected value from observed data; Red: Gibbs sampling method')
fit1$summary()
```

------------------------------------------------------------------------

#### Informative prior distribution (100, 496) and posterior distribution

```{r}
#| code-fold: true
bayesplot::mcmc_dens(fit2$draws('p1')) +
  geom_path(aes(x = p1, y = pdf), data = pdf_posterior_forplot %>% filter(Alpha_Beta == '(100, 496)'), col = "red") +
  geom_vline(xintercept = 1/6, col = "green") +
  geom_vline(xintercept = 3/5, col = "purple") +
  labs(title = "Posterior distribution using informative prior vs. the conjugate prior (The Gibbs sampler) method", caption = 'Green: mode of prior distribution; Purple: expected value from observed data; Red: Gibbs sampling method')
fit2$summary()
```

------------------------------------------------------------------------

## Bayesian updating

We can use the posterior distribution as a prior!

1.  data {0, 1, 0, 1, 1} with the prior hyperparameter {2, 6} -\> posterior parameter {5, 9}

2.  new data {1, 1, 1, 1, 1} with the prior hyperparameter {5, 9} -\> posterior parameter {10, 9}

3.  one more new data {0, 0, 0, 0, 0} with the prior hyperparameter {10, 9} -\> posterior parameter {10, 14}

# Bayesian Linear Regression

## Our Data Today

```{r}
library(ESRM64503)
dat <- dataSexHeightWeight
dat$Female <- ifelse(dat$sex == "F", 1, 0)

# Linear Model with Least Squares
## Center independent variable - HeightIN for better interpretation
dat$heightIN <- dat$heightIN - 60

## an empty model suggested by data
EmptyModel <- lm(weightLB ~ 1, data = dat)

## Examine assumptions and leverage of fit
### Residual plot, Q-Q residuals, Scale-Location
# plot(EmptyModel)

## Look at ANOVA table
### F-values, Sum/Mean of square of residuals
# anova(EmptyModel)

## look at parameter summary
summary(EmptyModel)
```

## Stan Syntax for Empty Model

```{r}
#| echo: false
library(cmdstanr)
register_knitr_engine(override = FALSE) 
```

A Stan file saved as `EmptyModel.stan`

-   Each line ends with a semi colon `;`
-   Comments are put in with `//`
-   Each block starts with `block type` and is surrounded by curly brackets `{}`

```{r}
stanModel <- "
data {
  int<lower=0> N;
  vector[N] weightLB;
}
parameters {
  real beta0;
  real<lower=0> sigma;
}
model {
  beta0 ~ normal(0, 1000);
  sigma ~ uniform(0, 100000);
  weightLB ~ normal(beta0, sigma);
}
"
model00.fromString = cmdstan_model(stan_file = write_stan_file(stanModel)) #<1>
```

1.  the `data` block include the observed information
2.  sample size (declared as a integer value)
3.  dependent variable (declared as vector with the length as sample size)
4.  the `parameter` block includes all parameters we want to estimate
5.  intercept parameter
6.  standard deviation of residuals
7.  the `model` block includes the prior information and likelihood
8.  prior distribution for `beta_0`
9.  prior distribution for SD of residuals
10. likelihood function for observed data

## Prepare data for Empty Model

```{r}
#| eval: true
# build R list containing data for Stan: Must be named what "data" are listed in analysis
stanData = list(
  N = nrow(dat), #<1>
  weightLB = dat$weightLB #<2>
)
```

1.  Sample size, `nrow()` calculates the number of rows of data
2.  Vector of dependent variable

-   Stan needs the data you declared in you syntax to be able to run

-   Within R, we can pass this data list to Stan via a list object

-   The entries in the list should correspond to the data block of the Stan syntax

## Run Markov Chains in CmdStanr

```{r}
#| eval: true
#| result: 'hide'
# run MCMC chain (sample from posterior)
model00.samples = model00.fromString$sample(
  data = stanData, #<1>
  seed = 1, #<2>
  chains = 3, #<3>
  parallel_chains = 4, #<4>
  iter_warmup = 2000, #<5>
  iter_sampling = 1000 #<6>
)
```

1.  Data list
2.  Random number seed
3.  Number of chains (and parallel chains)
4.  Number of warmup iterations (more details shortly)
5.  Number of sampling iterations

::: nonincremental
-   Within the compiled program and the data, the next step is to run the Markov Chain Monte Carlo (MCMC) sampling (this is basically equivalent to drawing from a posterior distribution)

-   In `cmdstanr`, running the MCMC sampling comes from the `$sample()` function that is part of the compiled program object
:::

## Results of Bayesian Empty Model

```{r}
model00.samples$summary()[c('variable','mean', 'rhat')]
bayesplot::mcmc_trace(model00.samples$draws("beta0"))
```

## Bayesian full model using `blavaan`

![](images/clipboard-1983629374.png)

## Bayesian Full Model

```{r}
library(blavaan)
library(lavaan)

model01.syntax = 
" 
#endogenous variable equations
weightLB ~ Female
heightIN ~ Female

heightIN ~~ weightLB
"

#estimate model with Bayesian
model01.fit_mcmc = bsem(model01.syntax, data=dat)
#estimate model with MCMC
model01.fit_ML = sem(model01.syntax, data=dat)
```

## Trace Plots of Parameters

```{r}
plot(model01.fit_mcmc)
```

## Posterior Distribution of coefficient

### MLE results

```{r}
parameterestimates(model01.fit_ML)
```

### MCMC results

```{r}
post_coefs <- blavaan::blavInspect(model01.fit_mcmc, "mcmc")
coda::densplot(post_coefs)
```

## Summary of coefficients

```{r}
summary(model01.fit_mcmc)
```

```{r}
standardizedsolution(model01.fit_mcmc)
```

## Wrapping up

Today is a quick introduction to Bayesian Concept

-   Bayes' Theorem: Fundamental theorem of Bayesian Inference
-   Prior distribution: What we know about the parameter before seeing the data
    -   **hyperparameter**: parameter(s) of the prior distribution(s)
    -   **Uninformative** prior: Prior distribution that does not convey any information about the parameter
    -   **Informative** prior: Prior distribution that conveys information about the parameter
    -   **Conjugate** prior: Prior distribution that makes the posterior distribution the same family as the prior distribution
-   Likelihood: What the data tell us about the parameter
    -   Likelihood function: Probability of the data given the parameter
    -   Likelihood principle: All the information about the data is contained in the likelihood function
    -   Likelihood is a sort of "scientific judgment" on the generation process of the data
-   Posterior distribution: What we know about the parameter after seeing the data
