---
title: "Lecture 04: ANOVA Assumptions Checking"
subtitle: "Experimental Design in Education"
author: "Jihong Zhang*, Ph.D"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2025-08-18"
sidebar: false
execute: 
  eval: true
  echo: true
format: 
  # html: 
  #   page-layout: full
  #   toc: true
  #   toc-depth: 2
  #   lightbox: true
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503: Lecture 04"
    slide-number: c/t
    tbl-colwidths: auto
    scrollable: true
    mermaid:
      theme: forest
---

## Class Outline

-   Go through three assumptions of ANOVA and their checking statistics
-   Omnibus test for more group comparisons.
-   Example: Intervention and Verbal Acquisition
-   Exercise: Effect of Sleep Duration on Cognitive Performance

## ANOVA Procedure

1.  **Set hypotheses**:
    -   **Null hypothesis** ($H_0$): All group means are equal.
    -   **Alternative hypothesis** ($H_A$): At least one group mean differs.
2.  **Determine statistical parameters**:
    -   Significance level $\alpha$
    -   Degrees of freedom for between-group ($df_b$) and within-group ($df_w$)
    -   Find the critical F-value.
3.  **Compute test statistic**:
    -   Calculate **F-ratio** based on between-group and within-group variance.
4.  **Compare results**:
    -   Either compare $F_{\text{obs}}$ with $F_{\text{crit}}$ or $p$-value with $\alpha$.
    -   If $p < \alpha$, reject $H_0$.

## ANOVA Type I Error

1.  The way to compare the means of multiple groups (i.e., more than two groups)

-   Compared to multiple t-tests, the ANOVA does not inflate Type I error rates.
    -   For F-test in ANOVA, Type I error rate is not inflated
    -   Type I error rate is inflated when we do multiple comparison
-   Family-wise error rate: probability of at least one Type I error occur = $1 −  (1 − \alpha)^c$ where c =number of tests

::: callout-note
## Family-Wise Error Rate

1.  The **Family-Wise Error Rate** (FWER) is the probability of making at least one **Type I error** (false positive) across a set of multiple comparisons.
2.  In **Analysis of Variance (ANOVA)**, multiple comparisons are often necessary when analyzing the differences between group means. FWER control is crucial to ensure the validity of statistical conclusions.
:::

# ANOVA Assumptions

## Overview

-   Like all statistical tests, ANOVA requires certain assumptions to be met for valid conclusions:
    -   **Independence**: Observations are independent of each other.
    -   **Normality**: The residuals (errors) follow a normal distribution.
    -   **Homogeneity of variance (HOV)**: The variance within each group is approximately equal.

## Importance of Assumptions

-   **If assumptions are violated**, the results of ANOVA may not be reliable.
-   **We call it Robustness** as to what degree ANOVAs are not influenced by the violation of assumption.
-   Typically:
    -   ANOVA is **robust** to minor violations of normality, especially for large sample sizes (Central Limit Theorem).
    -   **Not robust** to violations of independence—if independence is violated, ANOVA is inappropriate.
    -   **Moderately robust** to HOV violations if sample sizes are equal.

## Assumption 1: Independence

-   **Definition**: Each observation should be independent of others.
-   **Violations**:
    -   Clustering of data (e.g., repeated measures).
    -   Participants influencing each other (e.g., classroom discussions).
-   **Check (Optional)**: Use the [Durbin-Watson test]{.redcolor}.
-   **Solution**: Repeated Measured ANOVA, Mixed ANOVA, Multilevel Model
-   **Consequences**: If independence is violated, [ANOVA results are not valid]{.red}.

------------------------------------------------------------------------

[Durbin-Watson (DW) Test]{.bluecolor .bigger}

::: callout-note
-   The Durbin-Watson test is primarily used for detecting **autocorrelation** in time-series data.

-   In the context of ANOVA with independent groups, residuals are generally assumed to be independent.

    -   However, it's still good practice to check this assumption, especially if there's a reason to suspect potential autocorrelation.
:::

::: callout-important
-   Properties of DW Statistic:
    -   $H_0$: [Independence of residual is satisfied.]{.redcolor}
    -   Ranges from 0 to 4.
        -   A value around 2 suggests no autocorrelation.
        -   Values approaching 0 indicate positive autocorrelation.
        -   Values toward 4 suggest negative autocorrelation.
    -   P-value
        -   P-Value: A small p-value (typically \< 0.05) indicates violation of independency
:::

------------------------------------------------------------------------

[`lmtest::dwtest()`]{.bluecolor .bigger}

-   Performs the Durbin-Watson test for autocorrelation of disturbances.

-   The Durbin-Watson test has the null hypothesis that the autocorrelation of the disturbances is 0.

```{r}
#| eval: false
err1 <- rnorm(100)

## generate regressor and dependent variable
x <- rep(c(-1,1), 50)
y1 <- 1 + x + err1 
## perform Durbin-Watson test
dwtest(y1 ~ x)
```

This results suggest there are no autocorrelation at the alpha level of .05.

## Assumption 2: Normality

-   The **dependent variable (DV)** should be normally distributed within each group.
-   **Assessments**:
    -   Graphical methods: Histograms, [Q-Q plots]{.redcolor}.
    -   Statistical tests:
        -   [Shapiro-Wilk test (common)]{.redcolor}
        -   Kolmogorov-Smirnov (KS) test (for large samples)
        -   Anderson-Darling test (detects kurtosis issues).
-   **Robustness**:
    -   ANOVA is robust to normality violations for large samples.
    -   If normality is violated, consider transformations or non-parametric tests.

------------------------------------------------------------------------

[Normality within Each Group]{.bluecolor .bigger}

-   Assume that the DV (Y) is distributed normally within each group for ANOVA

-   ANOVA is **robust** to minor violations of normality

    -   So generally start with homogeneity, then assess normality

![](images/clipboard-3483482689.png)

------------------------------------------------------------------------

[`shapiro.test()`]{.bigger}

```{r}
#| eval: true
set.seed(123)  # For reproducibility
sample_normal_data <- rnorm(200, mean = 50, sd = 10)  # Generate normal data
sample_nonnormal_data <- runif(200, min = 1, max = 10)  # Generate non-normal data

shapiro.test(sample_normal_data)
shapiro.test(sample_nonnormal_data)
```

------------------------------------------------------------------------

```{r}
# Perform Kolmogorov-Smirnov test against a normal distribution
ks.test(scale(sample_normal_data), "pnorm")
ks.test(scale(sample_nonnormal_data), "pnorm")
```

## Assumption 3: Homogeneity of Variance (HOV)

-   Variance across groups should be equal.
-   **Assessments**:
    -   **Levene’s test**: Tests equality of variances.
    -   **Brown-Forsythe test**: More robust to non-normality.
    -   **Bartlett's test**: For data with normality.
    -   **Boxplots**: Visual inspection.
-   **What if violated?**
    -   **Welch’s ANOVA** (adjusted for variance differences).
    -   **Transforming** the dependent variable.
    -   **Using non-parametric tests** (e.g., Kruskal-Wallis).

------------------------------------------------------------------------

[Practical Considerations]{.bluecolor .bigger}

```{r}
#| eval: false
## Computes Levene's test for homogeneity of variance across groups.
car::leveneTest(outcome ~ group, data = data)

## Boxplots to visualize the variance by groups
boxplot(outcome ~ group, data = data)

## Brown-Forsythe test 
onewaytests::bf.test(outcome ~ group, data = data)

## Bartlett's Test
bartlett.test(outcome ~ group, data = data)
```

-   **Levene's test** and **Brown-Forsythe test** are often preferred when data does not meet the assumption of normality, especially for small sample sizes.
-   **Bartlett’s test** is most powerful when the data is normally distributed and when sample sizes are equal across groups. However, it can be less reliable if these assumptions are violated.

---

[Decision Tree]{.redcolor .bigger}


```{dot}
//| echo: false
digraph flowchart {
    node [shape=box, style=filled, fillcolor=lightgray];
    
    A [label="Is the data normal by group?"];
    B [label="Use Bartlett's Test"];
    C [label="Are there outliers?"];
    D [label="Use Brown-Forsythe Test"];
    E [label="Use Levene's Test"];
    
    A -> B [label="Yes"];
    A -> C [label="No"];
    C -> D [label="Yes"];
    C -> E [label="No"];
}

```

------------------------------------------------------------------------

## ANOVA Robustness

-   **Robust to**:

    -   Minor normality violations (for large samples).
    -   Small HOV violations if group sizes are **equal**.

-   **Not robust to**:

    -   **Independence violations**—ANOVA is invalid if data points are dependent.
    -   **Severe HOV violations**—Type I error rates become unreliable.

-   The robustness of assumptions is something you should be carefull before/when you perform data collection. They are not something you can do after data collection has been finished.

------------------------------------------------------------------------

[Robustness to Violations of Normality Assumption]{.redcolor}

-   ANOVA assumes that the residuals (errors) are normally distributed within each group.

-   However, ANOVA is generally robust to violations of normality, particularly when **the sample size is large**.

-   **Theoretical Justification**: This robustness is primarily due to the Central Limit Theorem (CLT), which states that, for sufficiently large sample sizes (typically $n≥30$ per group), the sampling distribution of the mean approaches normality, even if the underlying population distribution is non-normal.

-   This means that, unless the data are heavily skewed or have extreme outliers, ANOVA results remain valid and Type I error rates are not severely inflated.

------------------------------------------------------------------------

### Robustness to Violations of Homogeneity of Variance

-   The homogeneity of variance (homoscedasticity) assumption states that all groups should have equal variances. ANOVA can tolerate moderate violations of this assumption, particularly when:

    -   **Sample sizes are equal (or nearly equal) across groups** – When groups have equal sample sizes, the F-test remains robust to variance heterogeneity because the pooled variance estimate remains balanced.

    -   **The degree of variance heterogeneity is not extreme** – If the largest group variance is no more than about four times the smallest variance, ANOVA results tend to remain accurate.

------------------------------------------------------------------------

### ANOVA: Lack of Robustness to Violations of Independence of Errors

-   The assumption of independence of errors means that observations within and between groups must be uncorrelated. Violations of this assumption severely compromise ANOVA’s validity because:

    -   **Inflated Type I error rates** – If errors are correlated (e.g., due to clustering or repeated measures), standard errors are underestimated, leading to an increased likelihood of falsely rejecting the null hypothesis.
    -   **Biased parameter estimates** – When observations are not independent, the variance estimates do not accurately reflect the true variability in the data, distorting F-statistics and p-values.
    -   **Common sources of dependency** – Examples include nested data (e.g., students within schools), repeated measurements on the same subjects, or time-series data. In such cases, alternatives like mixed-effects models or generalized estimating equations (GEE) should be considered.

# Omnibus ANOVA Test

## Overview

-   **What does it test?**
    -   Whether there is **at least one** significant difference among means.
-   **Limitation**:
    -   Does **not** tell **which** groups are different.
-   **Solution**:
    -   Conduct **post-hoc tests**.

## Individual Comparisons of Means

-   If ANOVA is **significant**, follow-up tests identify **where** differences occur.
-   **Types**:
    -   **Planned comparisons**: Defined **before** data collection.
    -   **Unplanned (post-hoc) comparisons**: Conducted **after** ANOVA.

## Planned vs. Unplanned Comparisons

-   **Planned**:
    -   Based on **theory**.
    -   Can be done **even if ANOVA is not significant**.
-   **Unplanned (post-hoc)**:
    -   **Data-driven**.
    -   Only performed **if ANOVA is significant**.

## Types of Unplanned Comparisons

-   **Common post-hoc tests**:
    1.  **Fisher’s LSD**
    2.  **Bonferroni correction** or Adjusted p-values
    3.  **Sidák correction**
    4.  **Tukey’s HSD**

---

[Fisher’s LSD]{.bluecolor}

-   **Least Significant Difference test**.
-   **Problem**: Does not control for **multiple comparisons** (inflated Type I error).

[Bonferroni Correction]{.bluecolor}

-   Adjusts **alpha** to reduce **Type I error**.
-   New alpha: $\alpha / c$ (where $c$ is the number of comparisons).
-   **Conservative**: Less power, avoids false positives.

[Family-wise Error Rate (adjusted p-values)]{.bluecolor}

- Adjusts p-values to reduce Type I error
- Report adjusted p-values (typically larger that original p-values)

[Tukey’s HSD]{.bluecolor}

-   Controls for **Type I error** across multiple comparisons.
-   Uses a **q-statistic** from a Tukey table.
-   Preferred when **all pairs** need comparison.

## ANOVA Example: Intervention and Verbal Acquisition

### Background

-   Research Question: Does an intensive intervention improve students’ verbal acquisition scores?
-   Study Design:
    -   4 groups: Control, G1, G2, G3 (treatment levels).
    -   Outcome variable: Verbal acquisition score (average of three assessments).
-   Hypotheses:
    -   $H_0$: No difference in verbal acquisition scores across groups.
    -   $H_A$: At least one group has a significantly different mean.

## Step 1: Generate Simulated Data in R

```{r}
# Load necessary libraries
library(tidyverse)

# Set seed for reproducibility
set.seed(123)

# Generate synthetic data for 4 groups
data <- tibble(
  group = rep(c("Control", "G1", "G2", "G3"), each = 30),
  verbal_score = c(
    rnorm(30, mean = 70, sd = 10),  # Control group
    rnorm(30, mean = 75, sd = 12),  # G1
    rnorm(30, mean = 80, sd = 10),  # G2
    rnorm(30, mean = 85, sd = 8)    # G3
  )
)

# View first few rows
head(data)
```

## Step 2: Summary Statistics

```{r}
# Summary statistics by group
data %>%
  group_by(group) %>%
  summarise(
    mean_score = mean(verbal_score),
    sd_score = sd(verbal_score),
    n = n()
  )
```

------------------------------------------------------------------------

```{r}
# Boxplot visualization
ggplot(data, aes(x = group, y = verbal_score, fill = group)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Verbal Acquisition Scores Across Groups", y = "Score", x = "Group")
```

## Step 3: Check ANOVA Assumptions

### Assumption Check 1: Independence of residuals Check

```{r}
# Fit the ANOVA model
anova_model <- lm(verbal_score ~ group, data = data)

# Install lmtest package if not already installed
# install.packages("lmtest")

# Load the lmtest package
library(lmtest)

# Perform the Durbin-Watson test
dw_test_result <- dwtest(anova_model)

# View the test results
print(dw_test_result)
```

-   Interpretation:
    -   In this example, the `DW` value is close to 2, and the p-value is greater than 0.05, indicating no significant autocorrelation in the residuals.

------------------------------------------------------------------------

### Assumption Check 2: Normality Check

```{r}
# Shapiro-Wilk normality test for each group
data %>%
  group_by(group) %>%
  summarise(
    shapiro_p = shapiro.test(verbal_score)$p.value
  )
```

-   Interpretation:
    -   If $p>0.05$, normality assumption is not violated.
    -   If $p<0.05$, data deviates from normal distribution.

-   Since the data meets the normality requirement and no outliers, we can use [Bartlett's or Levene's tests]{.redcolor} for HOV checking.


------------------------------------------------------------------------

-   Alternative Check: Q-Q Plot

```{r}
ggplot(data, aes(sample = verbal_score)) +
  geom_qq() + geom_qq_line() +
  facet_wrap(~group) +
  theme_minimal() +
  labs(title = "Q-Q Plot for Normality Check")
```

------------------------------------------------------------------------

### Assumption Check 3: Homogeneity of Variance (HOV) Check

```{r}
# Levene's Test for homogeneity of variance
car::leveneTest(verbal_score ~ group, data = data)
stats::bartlett.test(verbal_score ~ group, data = data)
onewaytests::bf.test(verbal_score ~ group, data = data)
```

-   Interpretation:
    -   If $p>0.05$, variance is homogeneous (ANOVA assumption met).
    -   If $p<0.05$, variance differs across groups (consider Welch’s ANOVA).
    
-   It turns out our data does not violate the homogeneity of variance assumption.    

## Step 4: Perform One-Way ANOVA

```{r}
anova_model <- aov(verbal_score ~ group, data = data)
summary(anova_model)
```

-   Interpretation:
    -   If $p<0.05$, at least one group mean is significantly different.
    -   If $p>0.05$, fail to reject $H0$ (no significant differences).

## Step 5: Post-Hoc Tests (Tukey’s HSD)

```{r}
# Tukey HSD post-hoc test
tukey_results <- TukeyHSD(anova_model)
round(tukey_results$group, 3)
```

-   Interpretation:
    -   Identifies which groups differ.
    -   If $p<0.05$, the groups significantly differ.
        -   G1-Control
        -   G2-Control
        -   G3-Control
        -   G3-G1

------------------------------------------------------------------------

### multcomp: Family-wise Error Rate Control

Other multicomparison method allow you to choose which method for adjust p-values.

```{r}
library(multcomp)
# install.packages("multcomp")
### set up multiple comparisons object for all-pair comparisons
# head(model.matrix(anova_model))
comprs <- rbind(
  "G1 - Ctrl" = c(0, 1, 0,  0),
  "G2 - Ctrl" = c(0, 0, 1,  0),
  "G3 - Ctrl" = c(0, 0, 0,  1),
  "G2 - G1" = c(0, -1, 1,  0),
  "G3 - G1" = c(0, -1, 0,  1),
  "G3 - G2" = c(0, 0, -1,  1)
)
cht <- glht(anova_model, linfct = comprs)
summary(cht, test = adjusted("fdr"))

```

------------------------------------------------------------------------

### Side-by-Side comparison of two methods

::::: columns
::: {.column width="50%"}
#### TukeyHSD method

```{r}
#| eval: false
#| code-line-numbers: false
#| code-summary: "TukeyHSD method"
             diff    lwr    upr p adj
G1-Control  7.611  1.545 13.677 0.008
G2-Control 10.715  4.649 16.782 0.000
G3-Control 14.720  8.654 20.786 0.000
G2-G1       3.104 -2.962  9.170 0.543
G3-G1       7.109  1.042 13.175 0.015
G3-G2       4.005 -2.062 10.071 0.318
```
:::

::: {.column width="50%"}
#### FDR method

```{r}
#| eval: false
#| code-line-numbers: false
#| code-summary: "multcomp package"
               Estimate Std. Error t value Pr(>|t|)    
G1 - Ctrl == 0    7.611      2.327   3.270  0.00283 ** 
G2 - Ctrl == 0   10.715      2.327   4.604 3.20e-05 ***
G3 - Ctrl == 0   14.720      2.327   6.325 2.94e-08 ***
G2 - G1 == 0      3.104      2.327   1.334  0.18487    
G3 - G1 == 0      7.109      2.327   3.055  0.00419 ** 
G3 - G2 == 0      4.005      2.327   1.721  0.10555 
```
:::

-   The differences in p-value adjustment between the `TukeyHSD` method and the `multcomp` package stem from how each approach calculates and applies the multiple comparisons correction. Below is a detailed explanation of these differences
:::::

### Comparison of Differences

| Feature | `TukeyHSD()` (Base R) | `multcomp::glht()` |
|-------------------|----------------------------|-------------------------|
| **Distribution Used** | Studentized Range (q-distribution) | t-distribution |
| **Error Rate Control** | Strong FWER control | Flexible error control |
| **Simultaneous Confidence Intervals** | Yes | Typically not (depends on method used) |
| **Adjustment Method** | Tukey-Kramer adjustment | Single-step, Westfall, Holm, Bonferroni, etc. |
| **P-value Differences** | More conservative (larger p-values) | Slightly different due to t-distribution |

## Step 6: Reporting ANOVA Results

[Result]{.bigger}

1. We first examined three assumptions of ANOVA for our data as the preliminary analysis. According to the Durbin-Watson test, the Shapiro-Wilk normality test, and the Bartletts' test, the sample data meets all assumptions of the one-way ANOVA modeling.

2. A one-way ANOVA was then conducted to examine the effect of three intensive intervention methods (Control, G1, G2, G3) on verbal acquisition scores. There was a statistically significant difference between groups, $F(3,116)=14.33$, $p<.001$. 

3. To further examine which intervention method is most effective, we performed Tukey's post-hoc comparisons. The results revealed that all three intervention methods have significantly higher scores than the control group (G1-Ctrl: p = .003; G2-Ctrl: p < .001; G3-Ctrl: p < .001). Among three intervention methods, G3 seems to be the most effective. Specifically, G3 showed significantly higher scores than G1 (p = .004). However, no significant difference was found between G2 and G3 (p = .105). 

[Discussion]{.bigger}

These findings suggest that higher intervention intensity improves verbal acquisition performance, which is consistent with prior literatures [xxxx/references]

# Aftre-Class Exercise: Effect of Sleep Duration on Cognitive Performance

## Background

-   Research Question:

    -   Does the amount of sleep affect cognitive performance on a standardized test?

-   Study Design

    -   Independent variable: Sleep duration (3 groups: Short (≤5 hrs), Moderate (6-7 hrs), Long (≥8 hrs)).
    -   Dependent variable: Cognitive performance scores (measured as test scores out of 100).

## Data

```{r}
# Set seed for reproducibility
set.seed(42)

# Generate synthetic data for sleep study
sleep_data <- tibble(
  sleep_group = rep(c("Short", "Moderate", "Long"), each = 30),
  cognitive_score = c(
    rnorm(30, mean = 65, sd = 10),  # Short sleep group (≤5 hrs)
    rnorm(30, mean = 75, sd = 12),  # Moderate sleep group (6-7 hrs)
    rnorm(30, mean = 80, sd = 8)    # Long sleep group (≥8 hrs)
  )
)

# View first few rows
head(sleep_data)
```

Go through all six steps.

## Answer:

```{r}
#| eval: false
#| echo: false
# Summary statistics by sleep group
sleep_data %>%
  group_by(sleep_group) %>%
  summarise(
    mean_score = mean(cognitive_score),
    sd_score = sd(cognitive_score),
    n = n()
  )
```

```{r}
#| eval: false
#| echo: false
# Boxplot visualization
ggplot(sleep_data, aes(x = sleep_group, y = cognitive_score, fill = sleep_group)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Cognitive Performance Across Sleep Groups", y = "Test Score", x = "Sleep Duration")
```

```{r}
#| eval: false
#| echo: false
# Step 3: Check ANOVA Assumptions
# Perform the Durbin-Watson test
dw_test_result <- dwtest(aov(cognitive_score ~ sleep_group, data = sleep_data))

# View the test results
print(dw_test_result)
```

```{r}
#| eval: false
#| echo: false
# Shapiro-Wilk normality test for each group
sleep_data %>%
  group_by(sleep_group) %>%
  summarise(
    shapiro_p = shapiro.test(cognitive_score)$p.value
  )
ggplot(sleep_data, aes(sample = cognitive_score)) +
  geom_qq() + geom_qq_line() +
  facet_wrap(~sleep_group) +
  theme_minimal() +
  labs(title = "Q-Q Plot for Normality Check")
```

```{r}
#| eval: false
#| echo: false
# Levene's Test for homogeneity of variance
library(car)
leveneTest(cognitive_score ~ sleep_group, data = sleep_data)
```

```{r}
# Step 5: Perform One-Way ANOVA
anova_sleep <- aov(cognitive_score ~ sleep_group, data = sleep_data)
summary(anova_sleep)
```

```{r}
#| eval: false
#| echo: false
# Tukey HSD post-hoc test
tukey_sleep <- TukeyHSD(anova_sleep)
tukey_sleep
```

::: heimu
-   A one-way ANOVA was conducted to examine the effect of sleep duration on cognitive performance.

-   There was a statistically significant difference in cognitive test scores across sleep groups, $F(2,87)=15.88$,$p<.001$.

-   Tukey's post-hoc test revealed that participants in the Long sleep group (M=81.52,SD=6.27) performed significantly better than those in the Short sleep group (M=65.68,SD=12.55), p\<.01.

-   These results suggest that inadequate sleep is associated with lower cognitive performance.
:::
