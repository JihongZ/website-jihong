---
title: "Lecture 02: Hypothesis Testing"
subtitle: "Experimental Design in Education"
author: "Jihong Zhang*, Ph.D"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2025-08-18"
sidebar: false
execute: 
  eval: true
  echo: true
format: 
  #html: 
  #  page-layout: full
  #  toc: true
  #  toc-depth: 2
  #  lightbox: true
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503: Lecture02"
    slide-number: c/t
    tbl-colwidths: auto
    scrollable: true
#jupyter: python3
---

## Presentation Outline

-   Types of Statistics
    -   Descriptive: Summarize data (central tendency, variability, shape)
    -   Inferential: Makes population inferences from samples
-   Hypothesis Testing
    1.  State $H_0$ and $H_A$
    2.  Set Œ± level
    3.  Compute test statistics
    4.  Conduct test
-   ANOVA Fundamentals
    -   One DV, one IV with multiple levels
    -   Between/within designs
    -   Interaction effects
-   Test Components
    -   Error types (I & II)
    -   Variance analysis (SST, SSB, SSW)
    -   F-statistics and critical values
-   Examples
    -   Weight loss study (control, diet, exercise)
    -   Political attitudes (Democrats, Republicans, Independents)
-   Decision Making
    -   Compare F-observed vs F-critical
    -   Compare p-value vs Œ±
    -   Interpret at (1-Œ±) confidence level

## Types of Statistics

### 1. Descriptive Statistics

-   **Definition**: Describes and summarizes the collected data using numbers/values
    -   Central tendency: mean, median, mode
    -   Variability: range, interquartile range (IQR), variance, standard deviation
    -   Shape of distribution: skewness, kurtosis
-   **Examples** of skewness with two graphs:

![](images/clipboard-2603774303.png)

------------------------------------------------------------------------

### 2. Inferential Statistics

-   **Definition**: Uses probability theory to infer/estimate population characteristics from a sample using hypothesis testing
-   Visual representation shows:
    -   Population ‚Üí Sampling ‚Üí Sample
    -   Sample ‚Üí Inference ‚Üí Population
        -   Sample is analyzed using descriptive statistics
        -   Inferential statistics used to make conclusions about population

![](images/clipboard-3795101287.png)

------------------------------------------------------------------------

### 3. Predictive Statistics

-   **Definition**: Use observed data to produce the most accurate prediction possible for new data. Here, the primary goal is that the predicted values have the highest possible fidelity to the true value of the new data.

-   **Example**: A simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month.

## Which type statistics to use

1.  How many structures burned in California wildfire in the first week?

    -   [Descriptive]{.heimu}

2.  Which factor is most important causing the fires?

    -   [Inference]{.heimu}

3.  How likely the California wildfire will not happen again in next 5 years?

    -   [Predictive]{.heimu}

4.  How likely human will live on Mars?

    -   [Not statistics. Sci-Fi]{.heimu}

5.  ChatGPT?

    ![](images/clipboard-793465966.png){width="500"}

## Statistical Hypothesis Testing Steps

**To perform inference statistics, we need to go through following steps:**

1.  State null hypothesis (H‚ÇÄ) and alternative hypothesis ($H_A$)
2.  Set alpha Œ± (type I error rate) to determine significance levels
    -   rejection region vs. p-value
3.  Compute test statistics (i.e., F-statistics)
4.  Conduct hypothesis testing:
    -   Compare test statistics: critical value vs. observed value
    -   Compare alpha and p-value

## ANOVA Introduction

-   ANOVA is one of the most frequently used statistical tool for inference statistics in experimental design.
-   Settings for Analysis of Variance (ANOVA):
    -   One dependent variable (DV),
    -   One independent variable (IV) with multiple levels
-   **Example question**: "Are there mean differences in SAT math scores for high school program type (college prep, vocational, general)?"
-   Course covers advanced ANOVA topics:
    -   Model comparisons
    -   Between/within-subject design
    -   Interaction effects

## Types of ANOVA: Key Differences

-   [**One-Way ANOVA**]{.underline}

    -   **Purpose**: Tests **one** factor with three or more levels on a **continuous** outcome.

    -   **Use Case**: Comparing means across multiple groups (e.g., diet types on weight loss).

-   [**Two-Way ANOVA**]{.underline}

    -   **Purpose**: Examines **two factors and their interaction** on a **continuous** outcome.

    -   **Use Case**: Studying effects of diet and exercise on weight loss.

-   [**Repeated Measures ANOVA**]{.underline}

    -   **Purpose**: Tests the same subjects under **different conditions or time points**.

    -   **Use Case**: Longitudinal studies measuring the same outcome over time (e.g., cognitive tests after varying sleep durations).

-   [**Mixed-Design ANOVA**]{.underline}

    -   **Purpose**: Combines **between-subjects and within-subjects** factors in one analysis.

    -   **Use Case**: Evaluating treatment effects over time with control and experimental groups.

-   [**Multivariate Analysis of Variance (MANOVA)**]{.underline}

    -   **Purpose**: Assesses **multiple continuous outcomes** (dependent variables) influenced by independent variables.

    -   **Use Case**: Impact of psychological interventions on anxiety, stress, and self-esteem.

# Example 1: Political Study on Tax Reform Attitudes

## Background

-   A political scientist is interested in politicians‚Äô attitudes toward tax reform and conducts a survey of Republicans, Independents, and Democrats.
-   Political scientist study on tax reform attitudes:
    -   Groups: Democrats, Republicans, Independents
    -   Data shows attitude scores (higher score = greater concern for tax reform)
    -   Analysis conducted at Œ± = .05
    -   Data includes:
        -   `party`: Democrats (4), Republicans (5), Independents (8)
        -   `scores`: attitudes scores for the survey respondents
            -   [The higher the score, the greater the concern for tax reform]{.underline}

```{r}
#| eval: false
remotes::install_github("JihongZ/ESRM64103")
library(ESRM64103)
```

```{r}
library(ESRM64103)
library(dplyr)
exp_political_attitude
```

## Descriptive Statistics: summary statistics

-   Standard deviations and variances for each group

-   Grand mean: 5.625

```{r}
# Grand mean
mean(exp_political_attitude$scores)
exp_political_attitude$party <- factor(exp_political_attitude$party, levels = c("Democrat", "Republican", "Independent"))
mean_byGroup <- exp_political_attitude |> 
  group_by(party) |> 
  summarise(Mean = mean(scores),
            SD = round(sd(scores), 2),
            Vars = round(var(scores), 2),
            N = n())
mean_byGroup
```

## Descriptive Statistics: histogram

```{r}
library(ggplot2)
ggplot(data = mean_byGroup) +
  geom_bar(mapping = aes(x = party, y = Mean, fill = party), stat = "identity", width = .5) +
  geom_label(aes(x = party, y = Mean, label = Mean), nudge_y = .3) +
  labs(title = "Attitudes Toward the Tax Return") +
  theme(text = element_text(size = 15))
```

## Analysis Steps

1.  State the null hypothesis and alternative hypothesis:

    -   $H_0$: $\bar{X}_{dem}$ = $\bar{X}_{rep}$ = $\bar{X}_{ind}$
    -   $H_A$: At least two groups are significantly different
    -   Question: Why not testing $\bar{SD}_{dem}$ = $\bar{SD}_{rep}$ = $\bar{SD}_{ind}$?
    -   Answer: [You definitely can in statistics. Variances homogeneity.]{.heimu}

2.  Set the significant alpha = 0.05

3.  Test statistics of F:

    $$
    F_{obs} = \frac{SS_b/df_b}{SS_w/df_w} 
    $$

    -   $df_b$ = 3 (groups) - 1 = 2, $df_w$ = 16 (samples) - 3 (groups) =13
    -   $SS_b$ = $\Sigma n_j(\bar{Y}_j - \bar{Y})^2$ = 43.75; where $n_j$ is group sample sizes, $\bar{Y}_j$ is group means, and $\bar{Y}$ is the grand mean.
    -   $SS_w$ =$\Sigma_{j=1}^{3} \Sigma_{1}^{n_j}(Y_{ij}-\bar{Y_j})^2$ = 14.00; where $\bar{Y}_{ij}$ is the individual i's score in group j
    -   F_critical (df_num = 2, df_deno = 13) = 3.81
    -   F_observed = 20.31

    ```{r}
    mod1 <- lm(scores ~ party, data = exp_political_attitude)
    anova(mod1)
    ```

Results show rejection of H‚ÇÄ (F_obs \> F_critical)

## Step 1: State the null hypothesis and alternative hypothesis

1.  Formulate the null hypothesis (ùêª‚ÇÄ) and the alternative hypothesis (ùêª‚Çê)
    -   Prior to any statistical tests, start with a working hypothesis based on an initial guess about the phenomenon.
    -   Example: Investigating whether diet groups affect weight loss.
        -   Research question: "Is there a variance in weight loss among different diet groups?"
        -   Hypothesis: "[**Different diet groups will show varying weight loss.**]{.underline}"
    -   Definitions:
        -   Null hypothesis (ùêª‚ÇÄ): No observed difference or effect.
        -   Alternative hypothesis (ùêª‚Çê): Noticeable difference or effect, contrary to ùêª‚ÇÄ.
    -   The adequacy of the data will dictate if ùêª‚ÇÄ can be confidently rejected.

## Step 2: Rejection region

```{r}
#| code-fold: true
# Set degrees of freedom for the numerator and denominator
num_df <- 2  # Change this as per your specification
den_df <- 13  # Change this as per your specification

# Generate a sequence of F values
f_values <- seq(0, 8, length.out = 1000)

# Calculate the density of the F-distribution
f_density <- df(f_values, df1 = num_df, df2 = den_df)

# Create a data frame for plotting
data_to_plot <- data.frame(F_Values = f_values, Density = f_density)
data_to_plot$Reject05 <- data_to_plot$F_Values > 3.81
data_to_plot$Reject01 <- data_to_plot$F_Values > 6.70
# Plot the density using ggplot2
ggplot(data_to_plot) +
  geom_area(aes(x = F_Values, y = Density), fill = "grey", 
            data = filter(data_to_plot, !Reject05)) + # Draw the line
  geom_area(aes(x = F_Values, y = Density), fill = "yellow", 
            data = filter(data_to_plot, Reject05)) + # Draw the line
  geom_area(aes(x = F_Values, y = Density), fill = "tomato", 
            data = filter(data_to_plot, Reject01)) + # Draw the line
  geom_vline(xintercept = 3.81, linetype = "dashed", color = "red") +
  geom_label(label = "F_crit = 3.81 (alpha = .05)", x = 3.81, y = .5, color = "red") +
  geom_vline(xintercept = 6.70, linetype = "dashed", color = "royalblue") +
  geom_label(label = "F_crit = 6.70 (alpha = .01)", x = 6.70, y = .5, color = "royalblue") +
  ggtitle("Density of F-Distribution") +
  xlab("F values") +
  ylab("Density") +
  theme_classic()
```

------------------------------------------------------------------------

1.  Set the alpha $\alpha$ (i.e., type I error rate)‚Äîrejection rate, vs. p-value

    -   Alpha can determine several values for the statistical hypothesis testing: the critical value of the test statistics, the rejection region, etc.

    -   Large sample size needs lower alpha level : .01/.001 (more restrict rejection rate)

## Step 3: Compute the test statistics

-   Investigate where the variability of outcome come from?

    -   In this study, do people's attitude scores differ because of political parties?

    -   Imagine we have two factors: A and B, the variability of outcome can be separated as following:

![](images/clipboard-2219252399.png)

------------------------------------------------------------------------

### F-statistics

-   **Core idea of F-stats**: comparing the variances between groups and within groups to ascertain if the means of different groups are significantly different from each other.

-   **Logic**: if the **between-group variance** (due to systematic differences caused by the independent variable) is significantly greater than the **within-group variance** (attributable to random error), the observed differences between group means are likely not due to chance.

-   F-statistics under **1-way ANOVA:**

    $$
    F_{obs} = \frac{SS_b/df_b}{SS_w/df_w}$$

    -   $df_b$ = 3 (groups) - 1 = 2, $df_w$ = 16 (samples) - 3 (groups) =13
    -   $SS_b$ = $\Sigma n_j(\bar{Y}_j - \bar{Y})^2$ = 43.75;
        -   the variability in the differences between groups (weighted by the sample size of the group)
    -   $SS_w$ =$\Sigma_{j=1}^{3} \Sigma_{1}^{n_j}(Y_{ij}-\bar{Y_j})^2$ = 14.00; where $\bar{Y}_{ij}$ is the individual i's score in group j
        -   Random error with groups - people differ in attitudes for the unkown reason

## Step 4: Conduct a hypothesis testing

-   In addition to the comparison of the critical value and the observed value of the test statistics, we also can compare the alpha and the p-value:

::::: columns
::: column
![](images/clipboard-3877350302.png)
:::

::: column
-   We determine F_crit by setting Œ± value.
    -   Œ± = (acceptable) type I error rate = probability that we wrongly reject $H_0$ when $H_0$ is true
-   From the data, we can obtain F_obs with p-value.
    -   p-value = probability of data sets have F-stats larger than F_obs
-   If the F statistic from the data (=F_obs) is larger than the F critical, then you are in the rejection region and you can reject the $H_0$ and accept the $H_A$ with (1-Œ±) level of confidence.
-   If the p-value obtained from the ANOVA is less than Œ±, then reject H0 and accept the HA with (1-Œ±) level of confidence.
:::
:::::

## Step 5: Results Report

A one-way ANOVA was conducted to compare the level of concern for tax reform among three political groups: Democrats, Republicans, and Independents. There was a significant effect of political affiliation on tax reform concern at the p \< .001 level for the three conditions \[F(2, 13) = 20.31, p \< .001\]. This result indicates significant differences in the attitudes toward tax reform among the groups.

## Note: Relationship Between P-values and Type I Error

1.  p-values: the probability of observing data as extreme as, or more extreme than, the data observed [under the assumption that the null hypothesis is true]{.underline}.
    -   Lower the p-values are, we are more likely to see the observed data given the null hypothesis is true
    -   Question: Given that we already have the observed data, does **lower p-values means the null hypothesis is unlikely to be true**, which is our goal for inference statistics?
    -   Answer: [p(observed data exists \| H0 = true) is not equal to p(H0 = true \| observed data exists), P-values are often misconstrued as the probability that the null hypothesis is true given the observed data. However, this interpretation is incorrect.]{.heimu}
2.  Type I error, also known as a "false positive," occurs when the null hypothesis is incorrectly rejected when it is, in fact, true.
3.  The alpha level set before conducting a test (commonly Œ± = 0.05) essentially defines the cut-off point for the p-value below which the null hypothesis will be rejected.
    -   **A p-value that is less than the alpha level** suggests a low probability that the observed data would occur if the null hypothesis were true. Consequently, rejecting the null hypothesis in this context implies that there is a statistically significant difference likely not due to random chance.

## Note: Limitations of p-values

Relying solely on p-values to reject the null hypothesis can be problematic for several reasons:

-   **Binary Decision Making**: The use of a threshold (e.g., Œ± = 0.05) to determine whether to reject the null hypothesis reduces the complexity of the data and the underlying phenomena to a binary decision. This can oversimplify the interpretation and overlook the nuances in the data.

    -   Confidence Intervals. Bayesian statistics - reporting posterior distribution.

-   **Neglect of Effect Size**: P-values do not convey the size or importance of an effect. A very small effect can produce a small p-value if the sample size is large enough, leading to a rejection of the null hypothesis even when the effect may not be practically significant.

    -   Independent of sample size.

-   **Probability of Extremes Under the Null**: Since p-values quantify the extremeness of the observed data under the null hypothesis, they do not address whether similarly extreme data could also occur under alternative hypotheses. This can lead to an overemphasis on the null hypothesis and potentially disregard other plausible explanations for the data.

    -   Explore theory. Find other explanations. Tried varied models.

# Example 2: English Learners' digital competency

## Background

-   **Digital literacy** is essential for effective participation in today's society, yet "digital divides"‚Äîdisparities in access to and usage of digital resources‚Äîare prevalent.

-   These divides are particularly significant for ethnolinguistic minority students, such as English learners, who face challenges within educational systems biased toward monolingual English norms.

## Research Purpose and Hypothesis

-   **Research Purpose**: The study examines whether there are [noticeable differences in Information and Communication Technology (ICT)-related achievement, self-efficacy, and perceptions]{.underline} between native English speakers and English learners in U.S. schools. It aims to identify how these differences are influenced by linguistic barriers in the context of digital literacy.

-   **Hypotheses:** The research proposes that [English learners will demonstrate lower levels of mastery experience, self-efficacy, and positive perceptions of Information and Communication Technology]{.underline} compared to native English speakers due to systemic linguistic inequalities in educational settings. These disparities arise from both tangible and intangible barriers to accessing digital resources and support.

## Statistical Testing Review - Part 1

-   Important terminology for hypothesis testing:
    -   Working Hypothesis: Direct statement of research idea/question
    -   Null Hypothesis: No difference/effect
    -   Alternative Hypothesis: Significant difference/effect
-   Example using weight loss study:
    -   Research question: "Do people in different diet groups lose different amounts of weight?"
    -   Hypothesis: "People in different diet groups will lose different amounts of weight"

## Type I & II Errors

-   Four possible outcomes in hypothesis testing:
    1.  Correct Decision (Fail to reject H‚ÇÄ when true)
    2.  Type I Error (Reject H‚ÇÄ when true)
    3.  Type II Error (Fail to reject H‚ÇÄ when false)
    4.  Correct Decision (Reject H‚ÇÄ when false)

## Alpha Levels and Critical Values

-   Alpha determines:
    -   Critical value of test statistics
    -   Rejection region
-   Example with df_b=2, df_w=13:
    -   Œ±=0.05: F_critical=3.81
    -   Œ±=0.01: F_critical=6.70

## Test Statistics Components

-   Diagram shows variance components:
    -   Total Variability (SST)
    -   Variance explained by model (SSB)
    -   Unexpected Variance (SSW)
    -   Various factor variances (SSA, SSB, SSA√óB)

## F-Statistics Calculation

-   Details for computing test statistics:
    -   F_obs = SSb/dfb √∑ SSw/dfw
    -   SSb calculation explained (between-groups variability)
    -   SSw calculation explained (within-groups variability)
    -   F_critical determined by alpha, df_b, and df_w

## Hypothesis Testing Conclusion

-   Final steps in hypothesis testing:
    -   Compare F_obs with F_critical
    -   Compare p-value with alpha
    -   Decision rules:
        -   Reject H‚ÇÄ if F_obs \> F_critical
        -   Reject H‚ÇÄ if p-value \< Œ±
    -   Confidence level: (1-Œ±)
