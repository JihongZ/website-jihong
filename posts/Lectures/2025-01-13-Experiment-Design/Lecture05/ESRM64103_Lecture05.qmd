---
title: "Lecture 05: ANOVA Plan Comparisons"
subtitle: "Experimental Design in Education"
author: "Jihong Zhang*, Ph.D"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2025-02-10"
sidebar: false
execute: 
  eval: true
  echo: true
format: 
  # html: 
  #   page-layout: full
  #   toc: true
  #   toc-depth: 2
  #   lightbox: true
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503: Lecture 05"
    slide-number: c/t
    tbl-colwidths: auto
    scrollable: true
    mermaid:
      theme: forest
filters:
  - output-line-highlight.lua
---

[Class Outline]{.redcolor .bigger}

-   Planned Contrast
-   Example: STEM vs. Non-Stem Groups
-   Effect Coding
-   Effect sizes.

# Planned Contrasts and Effect Sizes

## Planned Contrasts

-   Pre-defined:

    -   Unlike post-hoc tests, planned contrasts are determined before looking at the data, meaning the researcher has a specific hypothesis about which groups to compare.
    -   Definition: Planned contrasts are hypothesis-driven comparisons made before data collection.

-   Weights assigned:

    -   To perform a planned contrast, each group is assigned a numerical "weight" which reflects its role in the comparison, with the weights usually summing to zero.

-   Less stringent correction:

    -   Because planned contrasts are based on specific hypotheses, they typically require less stringent multiple comparison corrections compared to post-hoc tests.

## Example of Planned Contrasts

-   Imagine a study comparing the effects of three different study methods (A, B, C) on test scores. A planned contrast might be to compare the average score of method A (considered the "experimental" method) against the combined average of methods B and C (considered the "control" conditions), testing the hypothesis that method A leads to significantly higher scores than the traditional methods.

-   When to use planned contrasts:

    -   When you have a clear theoretical basis for predicting specific differences between groups in your study.
    -   When you are only interested in a few specific comparisons, not all possible pairwise comparisons.

## What Does Each Contrast Tell Us?

-   Each contrast is a mean comparison (via t-test).
-   Simple contrast (pairwise) compares two individual means.
-   Complex contrast compares a combination of group means.
-   Must be theoretically justified for meaningful interpretation.

## Simple vs. Complex Comparisons

-   **Simple Comparison:** Two groups directly compared.
    -   Example: $H_0: \mu_2 = \mu_3$
-   **Complex Comparison:** Combines means of multiple groups.
    -   Example: $H_0: (\mu_1 + \mu_2)/2 = \mu_3$
    -   Example: $H_0: (\mu_1 + \mu_2 + \mu_3)/3 = (\mu_4 + \mu_5)/2$

## Examples of Complex Comparisons

-   Helmert contrast: Compares each mean to the mean of subsequent groups.
-   Deviation contrast: Compares each mean to the grand mean.
-   Polynomial contrast: Tests for trends in ordered data.

## Planned Contrast: Orthogonal vs. Non-Orthogonal Contrasts

-   **Orthogonal Contrasts:** Independent from each other, sum of product of weights equals zero.
-   **Non-Orthogonal Contrasts:** Not independent, lead to inflated Type I error rates.
-   Orthogonal contrasts allow clear interpretation without redundancy.

## Orthogonal Planned Contrasts

-   If the same exact combination of means is not found in more than one contrast, the contrasts are independent (orthogonal)
    -   Check this by ensuring that the product of the weights across all contrasts sum to zero
-   Why does independence matter?
    -   Type I error rate is unaffected by independent (orthogonal) contrasts
    -   Interpretation of contrasts is cleaner because contrasts aren’t related (you’ve isolated effects)

| Group | Contrast 1 | Contrast 2 | Product |
|-------|------------|------------|---------|
| G1    | -2         | 0          | 0       |
| G2    | +1         | +1         | +1      |
| G3    | +1         | -1         | -1      |
| Sum   | 0          | 0          | 0       |

```{r}
contras <- matrix(
  c(-2, 1, 1,
    0, 1, -1), ncol = 2
)
contras
crossprod(contras) ## if diagnonal matix = orthogonal
```

## Computing Planned Contrasts

-   Formula for contrast value: $C = c_1\mu_1 + c_2\mu_2 + \dots + c_k\mu_k$
-   Test statistic: $t = \frac{C}{\sqrt{MSE \sum \frac{c_i^2}{n_i}}}$
    -   $MSE$: Mean Square Error from ANOVA
    -   $c_i$: Contrast coefficients
    -   $n_i$: Sample size per group

# Example - STEM vs. Non-STEM Groups

## Background

-   Hypothesis: STEM students have different growth mindset scores than non-STEM students.
-   Weights assigned:
    -   STEM (Engineering, Chemistry): $+\frac{1}{2}$
    -   Non-STEM (Education, Political Sci, Psychology): $-\frac{1}{3}$
-   Compute contrast value and test using t-statistic.

## Set Contrasts in R

```{r}
#| code-fold: true
library(tidyverse)
library(kableExtra)
# Set seed for reproducibility
set.seed(42)
dt <- read.csv("week5_example.csv")
options(digits = 5)
summary_tbl <- dt |> 
  group_by(group) |> 
  summarise(
    N = n(),
    Mean = mean(score),
    SD = sd(score),
    shapiro.test.p.values = shapiro.test(score)$p.value
  )
kable(summary_tbl)
```

- HOV Assumption: Levene's Test

```{r}
aov_fit <- aov(score ~ group, data = dt)
car::leveneTest(aov_fit) |> as.data.frame() |> kable()
```

Even though assumption checkings did not pass using original categorical levels, we may be still interested in different group contrasts.

------------------------------------------------------------------------

### Contrast Matrix

- There are multiple "canned" contrasts: Helmert, Sum (Effective Coding), Treatment

[For example, Helmert Four contrasts:]{.redcolor}

1.  g1 vs. g2: $\mu_{Engineering} = \mu_{Education}$
2.  $\frac{g1+g2}{2}$ vs. g3: $\mu_{non-Chemistry} = \mu_{Chemistry}$
3.  $\frac{g1+g2+g3}{3}$ vs. g4: $\mu_{non-Political} = \mu_{Political}$
4.  $\frac{g1+g2+g3+g4}{4}$ vs. g5: $\mu_{non-Psychology} = \mu_{Psychology}$

Summary Statistics:

```{r}
#| code-fold: true
dt$group <- factor(dt$group, levels = c("g1", "g2", "g3", "g4", "g5"))
groups <- levels(dt$group)
cH <- contr.helmert(groups) # pre-defined four contrasts
colnames(cH) <- paste0("Ctras", 1:4)
summary_ctras_tbl <- cbind(summary_tbl, cH)
kable(summary_ctras_tbl)
```

---

[Orthogonal contrast matrix]{.redcolor}

```{r}
apply(cH, 2, sum)
crossprod(cH) # diagonal -- columns are orthogonal
```

```{r}
summary(aov(score ~ group, dt))
```


## Planed Contrasts and Coding Schema

- The relationship between planned contrasts in ANOVA and coding in regression lies in how categorical variables are represented and interpreted in statistical models.

- Both approaches aim to test specific hypotheses about group differences, but their implementation varies based on the framework
  - ANOVA focuses on partitioning variance, 
  - while regression interprets categorical predictors through coding schemes.

------------------------------------------------------------------------

## Effect Coding (Deviation Coding)

- In modern statistics, Regression-style coding is statistically equivalent as ANOVA-style contrast matrix. 
  - Equivalent to ANOVA-style contrasts. ([we will use this in R to reproduce ANOVA-style contrast matrix]{.redcolor})
- Compares each level to the grand mean.

::: callout-note
Effect coding is a method of encoding categorical variables in regression models, similar to dummy coding, but with a different interpretation of the resulting coefficients. It is particularly useful when researchers want to compare each level of a categorical variable to the overall mean rather than to a specific reference category.
:::

---

### **1. Definition and Representation**
In effect coding, categorical variables are transformed into numerical variables, typically using values of -1, 0, and 1. The key difference from dummy coding is that the reference category is represented by -1 instead of 0, and the coefficients indicate deviations from the grand mean.

For a categorical variable with `k` levels, effect coding requires `k-1` coded variables. If we have a categorical variable X with three levels: $A, B, C$, the effect coding scheme could be:

| Category | $X_1$ | $X_2$ |
|----------|--------|--------|
| A        | 1      | 0      |
| B        | 0      | 1      |
| C (reference) | -1     | -1     |

The last category ($C$) is the reference group, coded as -1 for all indicator variables.

---

### **2. Interpretation of Coefficients**

:::: columns
::: {.column width="40%"}
When effect coding is used in a regression model:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
$$

- $X_1$ and $X_2$ are coded varaibles. They have no much meaning, but their coefficients are **important** 
- $\beta_0$ represents the **grand mean** of $Y$ across all categories.
- $\beta_1$ and $\beta_2$ represent the **deviation** of categories $A$ and $B$ from the grand mean.
- The reference group ($C$) does not have a separate coefficient; instead, its deviation can be inferred as $-(\beta_1 + \beta_2)$.

:::

::: {.column width="60%"}
```{r}
#| code-fold: true
#| eval: true
#| fig-height: 8
library(ggplot2)
# Create a data frame for text labels
text_data <- data.frame(
  x = rep(0.25, 3),  # Repeating the same x-coordinate
  y = c(0.3, 0.7, 0.9),  # Different y-coordinates
  label = c("C: beta[0] - beta[1] - beta[2]", 
            "A: beta[0] + 1*'×'*beta[1] + 0*'×'*beta[2]", 
            "B: beta[0] + 0*'×'*beta[1] + 1*'×'*beta[2]")  # Labels
)

# Create an empty ggplot with defined limits
ggplot() +
  geom_text(data = text_data, aes(x = x, y = y, label = label), parse = TRUE, size = 11) +
  # Add a vertical line at x = 0.5
  # geom_vline(xintercept = 0.5, color = "blue", linetype = "dashed", linewidth = 1) +
  # Add two horizontal lines at y = 0.3 and y = 0.7
  geom_hline(yintercept = c(0.35, 0.75, 0.95), color = "red", linetype = "solid", linewidth = 1) +
  geom_hline(yintercept = 0.5, color = "grey", linetype = "solid", linewidth = 1) +
  geom_text(aes(x = .25, y = .45, label = "grand mean of Y"), color = "grey", size = 11) +
  # Set axis limits
  xlim(0, 1) + ylim(0, 1) +
  labs(y = "Y", x = "") +
  # Theme adjustments
  theme_minimal() +
  theme(text = element_text(size = 20))
```
:::
::::



---

### **3. Comparison to Dummy Coding**

- **Dummy Coding**: Compares each category to a **specific reference category** (e.g., comparing A and B to C).

| Category | $X_1$ | $X_2$ |
|----------|--------|--------|
| A        | 1      | 0      |
| B        | 0      | 1      |
| C (reference) | 0     | 0     |

- **Effect Coding**: Compares each category to the **grand mean** rather than a single reference category.

---

### **4. Use Cases**

Effect coding is beneficial when:

- There is no natural baseline category, and comparisons to the **overall mean** are more meaningful.
- Researchers want to maintain **sum-to-zero constraints** for categorical variables in linear models.
- In ANOVA-style analyses, where main effects and interaction effects are tested under an equal-weight assumption.

---

### **5. Implementation in R**
Effect coding can be set in R using the `contr.sum` function:

```{r}
#| eval: false
X <- factor(c("A", "B", "C"))
contrasts(X) <- contr.sum(3) # set up effect coding in R
model <- lm(Y ~ X, data = mydata) # use linear regression to mimic ANOVA-style results
summary(model)
```


------------------------------------------------------------------------

## ANOVA: t-value formula for Defined Contrast Matrix

$t = \frac{C}{\sqrt{MSE \sum \frac{c_i^2}{n_i}}}$

```{r}
Sum_C2_n <- colSums(cH^2 / summary_tbl$N)
C <- crossprod(summary_tbl$Mean, cH)
MSE <- 5.0
t <- as.numeric(C / sqrt(MSE * Sum_C2_n))
t
tibble(
  t_value = t,
  p_value = pt(t, df = 135) ## p-values
)
```

-   g1 vs. g2: We reject the null and determine that the mean of the Education is different from the mean of Engineering in their growth mindset scores (p = 0.531).

-   $\frac{g1+g2}{2}$ vs. g3: We retain the null and determine that the mean of the Chemistry is not significant different from the mean of Education and Engineering in their growth mindset scores (p = 0.531).

------------------------------------------------------------------------

### Helmert Contrast

Remember that Planned Contrast: g1 vs. g2 from Helmert Contrast:

-   t-value: -2.495
-   p-value: 0.0069
-   df: 134

```{r}
contrasts(dt$group) <- "contr.helmert"
fit_helmert <- lm(score ~ group, dt)
contr.helmert(levels(dt$group))
summary(fit_helmert)$coefficients |> round(3)
```

------------------------------------------------------------------------

### Planned Contrast Connected to Linear Regression

-   Planned contrast can be done using `linear regression` + `contrasts`

-   Let's look at the default contrasts plan: treatment contrasts == dummy coding

```{r}
attributes(C(dt$group, treatment, 4))$contrasts
attributes(C(dt$group, sum, 4))$contrasts
attributes(C(dt$group, helmert, 4))$contrasts
```

------------------------------------------------------------------------

### Treatment Contrasts

```{r}
#| output-location: column
library(multcomp)
options(contrasts = c("contr.treatment", "contrasts"))
fit <- lm(score ~ group, dt)
unique(cbind(model.matrix(fit), group = dt$group))
```

```{r}
#| output-location: column
summary(fit)
```

------------------------------------------------------------------------

### Sum Contrasts

-   Another type of coding is **effect coding**. In R, the corresponding contrast type are the so-called **sum contrasts**.

-   A detailed post about sum contrasts can be found [here](https://learnb4ss.github.io/learnB4SS/articles/contrasts.html)

-   With sum contrasts the reference level is in fact the grand mean.

    -   $\frac{g1+g2+g3+g4+g5}{5}$ vs. g1/g2/g3/g4: the difference between mean score of g1 with grand mean across all five groups

```{r}
#| output-location: column
contrasts(dt$group) <- "contr.sum"
fit2 <- lm(score ~ group, dt)
contr.sum(levels(dt$group))
```

```{r}
#| output-location: column
summary(fit2)
```

```{r}
mean(dt$score)
summary_tbl$Mean[1:4] - mean(dt$score)
```

------------------------------------------------------------------------

## Self-defined Contrast

-   Extended Example 2 : Assume now that I think the average of the STEM groups is different than the average of the non-STEM groups

### Method 1: Calculation by Hand

```{r}
#| echo: false
(summary_tbl_ext <- cbind(summary_tbl, Contrasts = c(1/2, -1/3, 1/2, -1/3, -1/3)))
```

$$
H_0: \frac{\mu_{Engineering}+\mu_{Chemistry}}{2} = \frac{\mu_{Education}+\mu_{PoliSci}+\mu_{Psychology}}{3}
$$

weighted mean difference:

$$
C = c_1\mu_{Eng}+c_2\mu_{Edu}+c_3\mu_{Chem}+c_4\mu_{PoliSci}+c_5\mu_{Psych}\\
= \frac{1}{2}*4.25+(-\frac13)*2.75+(\frac12)*3.54+(-\frac13)*3.85+(-\frac13)*2.02\\
= 1.0173
$$

```{r}
(C <- sum(summary_tbl_ext$Contrasts*summary_tbl_ext$Mean))
```

$$
\sum\frac{c^2}{n} = \frac{(\frac12)^2}{28}+\frac{(-\frac13)^2}{28}+\frac{(\frac12)^2}{28}+\frac{(-\frac13)^2}{28}+\frac{(-\frac13)^2}{28}
$$

```{r}
(Sum_C2_n <- sum(summary_tbl_ext$Contrasts^2 / summary_tbl$N))
(MSE = sum((residuals(aov(score ~ group, dt)))^2) / (nrow(dt) - 5))
(t = as.numeric(C / sqrt(MSE * Sum_C2_n)))
```

$$
t = \frac{C}{\sqrt{MSE*\sum\frac{c^2}{n} }} = \frac{1.0173}{\sqrt{5.0011*0.029762}}=2.6368
$$

```{r}
pt(t, df = 135, lower.tail = FALSE) * 2
```

------------------------------------------------------------------------

### Method 2: Linear Regression Contrasts by R

```{r}
# set first contrast
contrasts(dt$group) <- matrix(
  c(1/2, -1/3, 1/2, -1/3, -1/3)
)
fit_extended <- lm(score ~ group, dt)
unique(model.matrix(fit_extended))[, 1:2]
```

```{r}
#| output-line-numbers: "3"
#| class-output: highlight
summary(fit_extended)$coefficient |> round(3)
```

# Effect Sizes

## Effect Sizes

-   Effect size measures the magnitude of an effect beyond statistical significance.
-   Provides context for interpreting practical significance.
-   Common measures: Eta squared ($\eta^2$), Omega squared ($\omega^2$), Cohen’s d.

## Eta Squared ($\eta^2$)

-   Proportion of total variance explained by the independent variable.
-   Formula: $\eta^2 = \frac{SS_{Model}}{SS_{Total}}$
-   Interpretation:
    -   Small: 0.01, Medium: 0.06, Large: 0.14

## Omega Squared ($\omega^2$)

-   Unbiased estimate of effect size, preferred for small samples.
-   Formula: $\omega^2 = \frac{SS_{Model} - df_{Model} \cdot MSE}{SS_{Total} + MSE}$
-   Interpretation follows $\eta^2$ scale but slightly smaller values.

## Effect Size for Planned Contrasts

-   Correlation-based effect size: $r = \sqrt{\frac{t^2}{t^2 + df}}$
-   Example: For $t = 2.49, df = 135$: $r = \sqrt{\frac{2.49^2}{2.49^2 + 135}} = 0.21$
    -   Small to moderate effect.

## Cohen’s d and Hedges’ g

-   Used for simple mean comparisons.
-   Cohen’s d formula: $d = \frac{M_1 - M_2}{SD_{pooled}}$
-   Hedges’ g corrects for small sample bias.
-   Guidelines:
    -   Small: 0.2, Medium: 0.5, Large: 0.8

## Summary

-   Planned contrasts allow hypothesis-driven mean comparisons.
-   Orthogonal contrasts maintain Type I error control.
-   Effect sizes help interpret the importance of results.
-   Combining planned contrasts with effect size measures enhances statistical analysis.
