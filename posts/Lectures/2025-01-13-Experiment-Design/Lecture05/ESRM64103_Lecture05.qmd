---
title: "Lecture 05: ANOVA Comparisons"
subtitle: "Experimental Design in Education"
author: "Jihong Zhang*, Ph.D"
institute: | 
  Educational Statistics and Research Methods (ESRM) Program*
  
  University of Arkansas
date: "2025-02-10"
sidebar: false
execute: 
  eval: true
  echo: true
format: 
  # html: 
  #   page-layout: full
  #   toc: true
  #   toc-depth: 2
  #   lightbox: true
  uark-revealjs:
    chalkboard: true
    embed-resources: false
    code-fold: false
    number-sections: true
    number-depth: 1
    footer: "ESRM 64503: Lecture 05"
    slide-number: c/t
    tbl-colwidths: auto
    scrollable: true
    mermaid:
      theme: forest
filters:
  - output-line-highlight.lua
---

[Class Outline]{.redcolor .bigger}

-   Go through three assumptions of ANOVA and their checking statistics
-   Post-hoc test for more group comparisons.
-   Example: Intervention and Verbal Acquisition
-   After-class Exercise: Effect of Sleep Duration on Cognitive Performance

# Planned Contrasts and Effect Sizes

## Planned Contrasts

-   Pre-defined:

    -   Unlike post-hoc tests, planned contrasts are determined **before** looking at the data, meaning the researcher has a specific [hypothesis about which groups to compare]{.underline}.
    -   Definition: Planned contrasts are hypothesis-driven comparisons made before data collection.

-   Weights assigned:

    -   To perform a planned contrast, each group is assigned a numerical "weight" which reflects its role in the comparison, with the weights usually summing to zero.

-   Less stringent correction:

    -   Because planned contrasts are based on specific hypotheses, they typically require less stringent multiple comparison corrections compared to post-hoc tests.

## Example of Planned Contrasts

-   Imagine a study comparing the effects of three different study methods (**A, B, C**) on test scores.

    -   One planned contrast might be to compare the average score of method A (considered the "experimental" method) against the combined average of methods B and C (considered the "control" conditions),

    -   Testing the hypothesis that method A leads to significantly higher scores than the traditional methods.

    -   $H_0: \mu_{A} = \frac{\mu_B+\mu_C}{2}$, we also call this **complex contrast**

-   **When** to use planned contrasts:

    -   When you have a clear theoretical basis for predicting specific differences between groups in your study.
    -   When you are only interested in a few specific comparisons, not all possible pairwise comparisons.

## What Does Each Contrast Tell Us?

-   Each contrast is a mean comparison (via t-test).
-   **Simple** contrast (pairwise) compares two individual means.
-   **Complex** contrast compares a combination of group means.
-   Must be theoretically justified for meaningful interpretation.

## Simple vs. Complex Comparisons

-   **Simple Comparison:** Two groups directly compared.
    -   Example: $H_0: \mu_2 = \mu_3$
-   **Complex Comparison:** Combines means of multiple groups.
    -   Example: $H_0: (\mu_1 + \mu_2)/2 = \mu_3$
    -   Example: $H_0: (\mu_1 + \mu_2 + \mu_3)/3 = (\mu_4 + \mu_5)/2$

## Today's focus: Complex Comparisons

-   We perform ominous test in last lecture, which gives us simple contrasts
-   Today we focus more on **complex contrasts**.
    -   **Helmert** contrast: Compares each mean to the mean of subsequent groups.
    -   **Sum** contrast: each group compared to grand mean
    -   **Deviation** contrast: Compares each mean to the grand mean.
    -   **Polynomial** contrast: Tests for trends in ordered data.
-   By default, R uses **treatment** contrasts:
    -   G1 vs. Treatment (reference)
    -   G2 vs. Treatment (reference)
    -   ....

## Orthogonal vs. Non-Orthogonal Contrasts

-   **Orthogonal Contrasts:** Independent from each other, sum of product of weights equals zero.

-   **Non-Orthogonal Contrasts:** Not independent, lead to inflated Type I error rates.

    ::: callout-note
    Orthogonal contrasts allow clear interpretation without **redundancy**.
    :::

-   Orthogonal contrasts follows a series of group comparisons that does not overlap variances.

## Understand orthogonal contrasts from variances

[Hermert contrast for example]{.redcolor}

-   With a logical control group, a good first contrast compares all treatment groups to the one control group.
-   To get each level of IV alone you should have one less contrast than your number of IV levels (3 levels = 2 contrasts)
-   Once an IV level appears by itself, it shouldn’t reappear in subsequent contrasts

```{dot}
//| echo: FALSE
//| fig-width: 15
digraph VarianceDiagram {
    rankdir=LR;
    
    A [label="Total Variance Explained", shape=box, style=filled, fillcolor=tomato, fontcolor=blue, fontsize=20, width=2.5, height=1.5];
    B [label="Variance for G1 and G2", shape=box, style=filled, fillcolor=lightgrey, fontcolor=red, fontsize=16, width=2.0, height=1];
    C [label="Variance for G1", shape=box, style=filled, fillcolor=lightblue, fontcolor=red];
    D [label="Variance for G2", shape=box, style=filled, fillcolor=lightblue, fontcolor=red];
    E [label="Variance for G3", shape=box, style=filled, fillcolor=lightgrey, fontcolor=red];

    A -> B;
    B -> C;
    B -> D;
    A -> E;
}
```

## Example of Orthogonal Contrasts

::: column
-   Assume our groups have three levels: g1, g2, g3

-   One example planned orthogonal contrast "tree":

    -   Contrast 1: g3 vs. (g1, g2)

    -   Contrast 2: g1 vs. g2
:::

::::: columns
::: {.column width="50%"}
![](image/g3vsg12.jpg){width="90%"}
:::

::: {.column width="50%"}
![](image/g1vsg2.jpg){width="90%"}
:::
:::::

## Orthogonal Planned Contrasts

-   If the same exact combination of means is not found in more than one contrast, the contrasts are independent (orthogonal)
    -   Check this by ensuring that the product of the weights across all contrasts sum to zero
-   For a orthogonal comparison, **contrasts are independent with each other:**
    -   We weight the means included on each side of the contrast
    -   Each contrast has a sum of weights as 0
    -   Groups not in the contrast get a weight of 0
-   Why does independence matter?
    -   Type I error rate is unaffected by independent (orthogonal) contrasts
    -   Interpretation of contrasts is cleaner because contrasts aren’t related (you’ve isolated effects)

| Group | Contrast 1 | Contrast 2 | Product |
|-------|------------|------------|---------|
| G1    | +1         | -1         | 0       |
| G2    | +1         | +1         | +2      |
| G3    | -2         | 0          | -2      |
| Sum   | 0          | 0          | 0       |

```{r}
contras <- matrix(
  c(1, 1, -2,
    -1, 1, 0), ncol = 2
)
contras
crossprod(contras) ## if diagnonal matix = orthogonal
```

## Computing Planned Contrasts

-   Formula for contrast value: $C = c_1\mu_1 + c_2\mu_2 + \dots + c_k\mu_k$
-   Test statistic: $t = \frac{C}{\sqrt{MSE \sum \frac{c_i^2}{n_i}}}$
    -   $MSE$: Mean Square Error from ANOVA
    -   $c_i$: Contrast coefficients
    -   $n_i$: Sample size per group

# Example - STEM vs. Non-STEM Groups

## Example data background

-   Hypothesis: STEM students have different **growth mindset scores**(score) than non-STEM students.
-   Weights assigned:
    -   STEM (Engineering, Chemistry): $+\frac{1}{2}$
    -   Non-STEM (Education, Political Sci, Psychology): $-\frac{1}{3}$
-   Compute contrast value and test using t-statistic.

## Set Contrasts in R

```{r}
library(tidyverse)
library(kableExtra)
library(here)
# Set seed for reproducibility
set.seed(42)
dt <- read.csv(here("posts/Lectures/2025-01-13-Experiment-Design/Lecture05","week5_example.csv"))
options(digits = 5)
summary_tbl <- dt |> 
  group_by(group) |> 
  summarise(
    N = n(),
    Mean = mean(score),
    SD = sd(score)
  )
kable(summary_tbl)
```

------------------------------------------------------------------------

### Contrast Matrix

[For example, Helmert Four contrasts:]{.redcolor}

1.  g1 vs. g2: $\mu_{Engineering} = \mu_{Education}$
2.  $\frac{g1+g2}{2}$ vs. g3: $\mu_{non-Chemistry} = \mu_{Chemistry}$
3.  $\frac{g1+g2+g3}{3}$ vs. g4: $\mu_{non-Political} = \mu_{Political}$
4.  $\frac{g1+g2+g3+g4}{4}$ vs. g5: $\mu_{non-Psychology} = \mu_{Psychology}$

Summary Statistics:

```{r}
#| code-fold: true
dt$group <- factor(dt$group, levels = c("g1", "g2", "g3", "g4", "g5"))
groups <- levels(dt$group)
cH <- contr.helmert(groups) # pre-defined four contrasts
colnames(cH) <- paste0("Ctras", 1:4)
summary_ctras_tbl <- cbind(summary_tbl, cH)
kable(summary_ctras_tbl)
```

[Features of contrast matrix]{.redcolor}

```{r}
apply(cH, 2, sum)
crossprod(cH) # diagonal -- columns are orthogonal
```

```{r}
summary(aov(score ~ group, dt))
```

------------------------------------------------------------------------

### t-value formula for defined contrast matrix

$t = \frac{C}{\sqrt{MSE \sum \frac{c_i^2}{n_i}}}$

```{r}
Sum_C2_n <- colSums(cH^2 / summary_tbl$N)
C <- crossprod(summary_tbl$Mean, cH)
MSE <- 5.0
t <- as.numeric(C / sqrt(MSE * Sum_C2_n))
t
tibble(
  t_value = t,
  p_value = pt(t, df = 135) ## p-values
)
```

-   g1 vs. g2: We reject the null and determine that the mean of the Education is different from the mean of Engineering in their growth mindset scores (p = 0.531).

-   $\frac{g1+g2}{2}$ vs. g3: We retain the null and determine that the mean of the Chemistry is not significant different from the mean of Education and Engineering in their growth mindset scores (p = 0.531).

------------------------------------------------------------------------

### Helmert Contrast

Remember that Planned Contrast: g1 vs. g2 from Helmert Contrast:

-   t-value: -2.495
-   p-value: 0.0069
-   df: 134

```{r}
contrasts(dt$group) <- "contr.helmert"
fit_helmert <- lm(score ~ group, dt)
contr.helmert(levels(dt$group))
summary(fit_helmert)$coefficients |> round(3)
```

------------------------------------------------------------------------

### Planned Contrast Connected to Linear Regression

-   Planned contrast can be done using `linear regression` + `contrasts`

-   Let's look at the default contrasts plan: treatment contrasts == dummy coding

```{r}
#| output-location: column
## treatment contrast matrix 
attributes(C(dt$group, treatment, 4))$contrasts
```


```{r}
#| output-location: column

## sum contrast matrix 
attributes(C(dt$group, sum, 4))$contrasts
```


```{r}
#| output-location: column
attributes(C(dt$group, helmert, 4))$contrasts
```

```{r}
#| output-location: column
crossprod(attributes(C(dt$group, treatment, 4))$contrasts)
```

------------------------------------------------------------------------

### Treatment Contrasts

::::: columns
::: {.column width="50%}
- For treatment contrasts, four dummy variables are created to compared:

  - G1 (ref) vs. G2
  - G1 (ref) vs. G3
  - G1 (ref) vs. G4
  - G1 (ref) vs. G5
:::

::: {.column width="50%}
- `Intercept`: G1's mean
- `group2`: G2 vs. G1
- `group3`: G3 vs. G1
- `group4`: G4 vs. G1
- `group5`: G5 vs. G1
:::  
::::: 

```{r}
#| output-location: column
library(multcomp)
contrasts(dt$group) <- "contr.treatment"
fit <- lm(score ~ group, dt)
unique(cbind(model.matrix(fit), group = dt$group))
```

```{r}
#| output-location: column
summary(fit)$coefficients
```

------------------------------------------------------------------------

### Sum Contrasts

-   Another type of coding is **effect coding**. In R, the corresponding contrast type are the so-called **sum contrasts**.

-   A detailed post about sum contrasts can be found [here](https://learnb4ss.github.io/learnB4SS/articles/contrasts.html)

-   With sum contrasts the reference level is in fact the grand mean.

    -   $\frac{g1+g2+g3+g4+g5}{5}$ vs. g1/g2/g3/g4: the difference between mean score of g1 with grand mean across all five groups

```{r}
#| output-location: column
contrasts(dt$group) <- "contr.sum"
fit2 <- lm(score ~ group, dt)
contr.sum(levels(dt$group))
```

```{r}
#| output-location: column
summary(fit2)$coefficients
```

```{r}
#| output-location: column
mean(dt$score) # (Intercept) grand mean
```


```{r}
#| output-location: column
tibble(
  Label = paste0("group", 1:4),
  Estimate = summary_tbl$Mean[1:4] - mean(dt$score) 
)
```

------------------------------------------------------------------------

## Self-defined Contrast

-   Extended Example 2 : Assume now that I think the average of the STEM groups is different than the average of the non-STEM groups

### Method 1: Calculation by Hand

```{r}
#| echo: false

(summary_tbl_ext <- cbind(summary_tbl, Contrasts = c(1/2, -1/3, 1/2, -1/3, -1/3)))
```

$$
H_0: \frac{\mu_{Engineering}+\mu_{Chemistry}}{2} = \frac{\mu_{Education}+\mu_{PoliSci}+\mu_{Psychology}}{3}
$$

weighted mean difference:

$$
C = c_1\mu_{Eng}+c_2\mu_{Edu}+c_3\mu_{Chem}+c_4\mu_{PoliSci}+c_5\mu_{Psych}\\
= \frac{1}{2}*4.25+(-\frac13)*2.75+(\frac12)*3.54+(-\frac13)*3.85+(-\frac13)*2.02\\
= 1.0173
$$

```{r}
(C <- sum(summary_tbl_ext$Contrasts*summary_tbl_ext$Mean))
```

$$
\sum\frac{c^2}{n} = \frac{(\frac12)^2}{28}+\frac{(-\frac13)^2}{28}+\frac{(\frac12)^2}{28}+\frac{(-\frac13)^2}{28}+\frac{(-\frac13)^2}{28}
$$

```{r}
(Sum_C2_n <- sum(summary_tbl_ext$Contrasts^2 / summary_tbl$N))
(MSE = sum((residuals(aov(score ~ group, dt)))^2) / (nrow(dt) - 5))
(t = as.numeric(C / sqrt(MSE * Sum_C2_n)))
```

$$
t = \frac{C}{\sqrt{MSE*\sum\frac{c^2}{n} }} = \frac{1.0173}{\sqrt{5.0011*0.029762}}=2.6368
$$

```{r}
pt(t, df = 135, lower.tail = FALSE) * 2
```

------------------------------------------------------------------------

### Method 2: Linear Regression Contrasts by R

```{r}
# set first contrast
contrasts(dt$group) <- matrix(
  c(1/2, -1/3, 1/2, -1/3, -1/3)
)
fit_extended <- lm(score ~ group, dt)
unique(model.matrix(fit_extended))[, 1:2]
```

```{r}
#| output-line-numbers: "3"
#| class-output: highlight
summary(fit_extended)$coefficient |> round(3)
```

# Effect Sizes

## What is Effect Sizes

-   Effect size measures the magnitude of an effect beyond statistical significance.
    -   Put simply: a p-value is partially dependent on sample size and does not give us any insight into the strength of the relationship
    -   Lower p-value → just increase sample size
-   Provides context for interpreting practical significance.
    -   In scientific experiments, it is often useful to know not only whether an experiment has a statistically significant effect, but also the size (magnitude) of any observed effects.
-   Common measures: Eta squared ($\eta^2$), Omega squared ($\omega^2$), Cohen’s d.

::: callout-note
Many psychology journals require the reporting of effect sizes
:::

## Eta Squared

-   $\eta^2$: Proportion of total variance explained by the independent variable.
-   Formula: $\eta^2 = \frac{SS_{Model}}{SS_{Total}}$
-   Interpretation:
    -   Small: 0.01, Medium: 0.06, Large: 0.14

```{r}
(F_table <- as.data.frame(anova(fit)))
```

```{r}
(eta_2 <- F_table$`Sum Sq`[1] / sum(F_table$`Sum Sq`))
```

[Interpretation: 11.69% of variance in the DV is due to group differences.]{.redcolor}

## Drawbacks of Eta Squared

1.  **As you add more variables to the model, the proportion explained by any one variable will automatically decrease.**
    -   This makes it hard to compare the effect of a single variable in different studies.
    -   Partial Eta Squared solves this problem. There, the denominator is not the total variation in Y, but the unexplained variation in Y plus the variation explained just by that IV.
        -   Any variation explained by other IVs is removed from the denominator.
    -   In a one-way ANOVA, Eta Squared and Partial Eta Squared will be equal, but this isn’t true in models with more than one independent variable (factorial ANOVA).
2.  **Eta Squared is a biased measure of population variance explained (although it is accurate for the sample).**
    -   It always overestimates it. This bias gets very small as sample size increases, but for small samples an unbiased effect size measure is Omega Squared.

## Omega Square

-   Omega Squared ($\omega^2$) has the same basic interpretation but uses unbiased measures of the variance components.
    -   Because it is an unbiased estimate of population variances, Omega Squared is always smaller than Eta Squared. \## Omega Squared ($\omega^2$)
-   Unbiased estimate of effect size, preferred for small samples.
-   Formula: $\omega^2 = \frac{SS_{Model} - df_{Model} \cdot MSE}{SS_{Total} + MSE}$
-   Interpretation follows $\eta^2$ scale but slightly smaller values.

```{r}
F_table
```

```{r}
#| results: hold
attach(F_table) #<1>
(Omega_2 <- (`Sum Sq`[1] - Df[1] * MSE) / (sum(`Sum Sq`) + MSE)) #<2>
detach(F_table)
```

1.  Attach data set so that you can directly call the columns without "\$"
2.  The formula of Omega square

## Effect Size for Planned Contrasts

-   Correlation-based effect size: $r = \sqrt{\frac{t^2}{t^2 + df}} = \sqrt{\frac{F}{F + df}}$
-   Example: For $t = 2.49, df = 135$: $r = \sqrt{\frac{2.49^2}{2.49^2 + 135}} = 0.21$
    -   Small to moderate effect.

```{r}
(coef_tbl <- as.data.frame(summary(fit)$coefficients))
```

```{r}
#| results: hold
attach(coef_tbl)
round(sqrt(`t value`^2 / (`t value`^2 + 135)), 3)
detach(coef_tbl)
```
- Shows a small to moderate positive relationship between g1 with g5.

## Cohen’s d and Hedges’ g

-   Used for simple mean comparisons.
-   Cohen’s d formula: $d = \frac{M_1 - M_2}{SD_{pooled}}$
-   Hedges’ g corrects for small sample bias.
-   Guidelines:
    -   Small: 0.2, Medium: 0.5, Large: 0.8

## Guideline of Effect Size

![](images/clipboard-1482285190.png)

- For our example: there is a significant effect of academic program on growth mindset scores (F(4,135)=4.47).
- Academic program explains 11.69% of variance in growth mindset scores. This is a
large medium to large effect ($\eta^1$ = 0.1169).

## Summary

-   Planned contrasts allow hypothesis-driven mean comparisons.
-   Orthogonal contrasts maintain Type I error control.
-   Effect sizes help interpret the importance of results.
-   Combining planned contrasts with effect size measures enhances statistical analysis.

## Homework 2

### Aim: Practice the whole procedure of One-Way ANOVA

### Procedure: 
- Find a dataset, 
  - Provide some background information, 
  - Set up research question and hypothesis
  - Checking assumptions
  - Perform ANOVA Analysis
  - Perform ominous test
  - Design some planned orthogonal contrasts 
  - Calculate Effect Size 
  - Report your results
  - R Code

### Submission requirement
1. 2-3 page word document including 1-page report and 1-2 page R code syntax.

### Grading: AI + Peers

One AI and two peers will review your homework based on the following criteria:

1. Coverage: how your assignment cover what we learn and the requirements.
2. Structure: how well is your report well organized
3. Coding Skill: Do you use the correct R packages or 